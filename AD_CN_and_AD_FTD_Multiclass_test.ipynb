{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOz7QaAON9sOOjPKTVAgHbK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Hwang/EEG-AD-FTD-Detection/blob/main/AD_CN_and_AD_FTD_Multiclass_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GH-MwOGyJ7db",
        "outputId": "f1ac04a0-0c00-4ccb-ee8f-84e7437023fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그중에 2025 Lab Research 폴더 안을 확인\n",
        "!ls \"/content/drive/MyDrive/2025_Lab_Research\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/2025_Lab_Research')\n",
        "\n",
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on\", DEVICE)  # → “cuda” 가 뜨면 GPU 정상"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5yO85uCkfTr2",
        "outputId": "1131a835-7837-4e0c-9927-22eea215863b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'AD vs FTD vs CN Binary Classification'   eeg_holdout-5.db\n",
            "'Binary Classification LOSO Test.ipynb'   eeg_holdout-6.db\n",
            "'Colab Files'\t\t\t\t  eeg_holdout-7.db\n",
            "'Data Preparation.gdoc'\t\t\t  eeg_holdout-8.db\n",
            " eeg_dataset.py\t\t\t\t  eeg_holdout-9.db\n",
            " EEGformer_model_training.ipynb\t\t  eeg_holdout.db\n",
            " eegformer_optuna_cv_3.db\t\t  eeg_holdout_fixed_1.db\n",
            " eegformer_optuna_cv_4.db\t\t  eeg_optuna_trial_1.db\n",
            " eegformer_optuna_cv_5.db\t\t  eeg_optuna_trial_2.db\n",
            " eeg_grid_search-10.db\t\t\t  eeg_optuna_trial_3.db\n",
            " eeg_grid_search-11.db\t\t\t 'EEG Transformer Architecture.gdoc'\n",
            " eeg_grid_search-12.db\t\t\t 'Lab Info'\n",
            " eeg_grid_search-13.db\t\t\t 'Lab Research Final Report'\n",
            " eeg_grid_search-14.db\t\t\t 'Lab Research Paper Review'\n",
            " eeg_grid_search-15.db\t\t\t 'Meeting Note.gdoc'\n",
            " eeg_grid_search-16.db\t\t\t  model-data\n",
            " eeg_grid_search-17.db\t\t\t  model-data.zip\n",
            " eeg_grid_search-18.db\t\t\t  model_optimized_2.py\n",
            " eeg_grid_search-2.db\t\t\t  model_optimized_3.py\n",
            " eeg_grid_search-3.db\t\t\t  model_optimized_4.py\n",
            " eeg_grid_search-4.db\t\t\t  model_optimized_5.py\n",
            " eeg_grid_search-5.db\t\t\t  model_optimized_6.py\n",
            " eeg_grid_search-6.db\t\t\t  model_optimized_7.py\n",
            " eeg_grid_search-7.db\t\t\t  model_optimized.py\n",
            " eeg_grid_search-8.db\t\t\t  models_depracated.py\n",
            " eeg_grid_search-9.db\t\t\t  models.py\n",
            " eeg_grid_search.db\t\t\t  models_with_dropout_1.py\n",
            " eeg_holdout-1.db\t\t\t  models_with_minimal_dropout.py\n",
            " eeg_holdout-2.db\t\t\t  Practice_Note0.ipynb\n",
            " eeg_holdout-3.db\t\t\t  __pycache__\n",
            " eeg_holdout-4.db\n",
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()  # 첫 실행 시 API 키 입력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "id": "Z0ypDbps8twz",
        "outputId": "5416a4d2-c485-4467-a409-1771c1b79a99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjh8032\u001b[0m (\u001b[33mjh8032-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install wandb\n",
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZsFa5x-ffbX",
        "outputId": "6a3ec096-e6aa-4928-8883-14a3b85ae158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m24.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Collecting mne\n",
            "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n",
            "Downloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m117.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Balancing the Dataset\n",
        "- To overcome the dataset imbalance"
      ],
      "metadata": {
        "id": "ExEqvBB-vxVj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 재현성을 위해 랜덤 시드 고정\n",
        "random.seed(0)\n",
        "\n",
        "# ─── 설정 ─────────────────────────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = 'labels.json'\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4  # 환경에 맞게 조정\n",
        "\n",
        "# ─── 1) labels.json 로드 & train 데이터만 추출 ─────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_data = [d for d in all_meta if d['type'] == 'train']\n",
        "\n",
        "# ─── 2) 클래스별 분리 ───────────────────────────────────────────────\n",
        "train_data_A = [d for d in train_data if d['label'] == 'A']  # AD\n",
        "train_data_C = [d for d in train_data if d['label'] == 'C']  # CN\n",
        "train_data_F = [d for d in train_data if d['label'] == 'F']  # FTD\n",
        "\n",
        "# ─── 3) 원본 분포 출력 ─────────────────────────────────────────────\n",
        "print(\"Before Balancing\")\n",
        "print(f\"  A (AD): {len(train_data_A)}\")\n",
        "print(f\"  C (CN): {len(train_data_C)}\")\n",
        "print(f\"  F (FTD): {len(train_data_F)}\")\n",
        "\n",
        "# ─── 4) Manual Class Balancing 계산 ───────────────────────────────\n",
        "# 제시하신 방식: 각 클래스 쌍의 평균 크기 중 최소값\n",
        "min_samples = min(\n",
        "    (len(train_data_A) + len(train_data_C)) / 2,\n",
        "    (len(train_data_A) + len(train_data_F)) / 2,\n",
        "    (len(train_data_C) + len(train_data_F)) / 2\n",
        ")\n",
        "\n",
        "a_index = int(min(min_samples, len(train_data_A)))\n",
        "c_index = int(min(min_samples, len(train_data_C)))\n",
        "f_index = int(min(min_samples, len(train_data_F)))\n",
        "\n",
        "# ─── 5) 언더샘플링 수행 ───────────────────────────────────────────\n",
        "balanced_train_data = (\n",
        "    random.sample(train_data_A, a_index) +\n",
        "    random.sample(train_data_C, c_index) +\n",
        "    random.sample(train_data_F, f_index)\n",
        ")\n",
        "random.shuffle(balanced_train_data)\n",
        "\n",
        "# ─── 6) 언더샘플링 후 분포 출력 ───────────────────────────────────\n",
        "print(\"\\nAfter Balancing\")\n",
        "print(f\"  A (AD): {a_index}\")\n",
        "print(f\"  C (CN): {c_index}\")\n",
        "print(f\"  F (FTD): {f_index}\")\n",
        "print(f\"  Total: {len(balanced_train_data)}\")\n",
        "\n",
        "# ─── 7) EEGDataset + DataLoader 예시 ─────────────────────────────\n",
        "dataset = EEGDataset(DATA_DIR, balanced_train_data)\n",
        "loader  = DataLoader(dataset,\n",
        "                     batch_size=BATCH_SIZE,\n",
        "                     shuffle=True,\n",
        "                     num_workers=NUM_WORKERS)\n",
        "\n",
        "# loader를 돌면서 데이터 확인 예시\n",
        "for X, y in loader:\n",
        "    print(\"Batch X shape:\", X.shape)  # (batch, channels, timepoints)\n",
        "    print(\"Batch y:\", y)\n",
        "    break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CCdrvQqsvwuh",
        "outputId": "105cc594-8ecf-4023-8009-972f04356673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "Before Balancing\n",
            "  A (AD): 1388\n",
            "  C (CN): 1102\n",
            "  F (FTD): 729\n",
            "\n",
            "After Balancing\n",
            "  A (AD): 915\n",
            "  C (CN): 915\n",
            "  F (FTD): 729\n",
            "  Total: 2559\n",
            "Batch X shape: torch.Size([32, 19, 1425])\n",
            "Batch y: tensor([2, 0, 1, 1, 2, 0, 1, 0, 1, 0, 1, 1, 0, 0, 2, 0, 2, 0, 2, 2, 2, 0, 0, 1,\n",
            "        2, 1, 0, 0, 1, 2, 1, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxqj1gBcSeGr",
        "outputId": "5a85944e-031d-4fcb-d938-85e37135bb84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline Results (Machine Learning) AD vs CN\n",
        "- LightGBM\n",
        "- SVM\n",
        "- KNN\n",
        "- MLP\n",
        "- Random Forests"
      ],
      "metadata": {
        "id": "VEHJW-Q5SYjM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline_models.py  – Classical baselines for FTD vs CN\n",
        "# -------------------------------------------------------\n",
        "import os, json, random, time, copy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.pipeline       import Pipeline\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn.metrics        import (\n",
        "    recall_score, f1_score, accuracy_score, confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.svm            import SVC\n",
        "from sklearn.neighbors      import KNeighborsClassifier\n",
        "from sklearn.ensemble       import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm               import LGBMClassifier   # pip install lightgbm\n",
        "\n",
        "from eeg_dataset import EEGDataset                 # ← your existing loader\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ─── Paths & constants (adjust as needed) ───────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = 'labels.json'\n",
        "NUM_WORKERS = 4               # DataLoader workers\n",
        "\n",
        "# ─── Dataset: returns flat NumPy feature vector + label ─────────\n",
        "class FlatEEGDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps EEGDataset but outputs (1‑D float32 NumPy array, int label).\n",
        "    Works whether EEGDataset returns a tensor or an ndarray.\n",
        "    \"\"\"\n",
        "    def __init__(self, raw_ds, metas, label_map):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.labels = [label_map[d['label']] if isinstance(d['label'], str)\n",
        "                       else d['label'] for d in metas]\n",
        "\n",
        "    def __len__(self): return len(self.raw_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]                       # tensor OR ndarray\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x = x.flatten().cpu().numpy()\n",
        "        else:  # already ndarray\n",
        "            x = x.flatten()\n",
        "        return x.astype(np.float32), self.labels[idx]\n",
        "\n",
        "# ─── Utility functions ──────────────────────────────────────────\n",
        "def count_labels(meta, cls0='F', cls1='C'):\n",
        "    n0 = sum(1 for d in meta if d['label'] in (cls0, 0))\n",
        "    n1 = len(meta) - n0\n",
        "    return n0, n1\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "    spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
        "    return {\n",
        "        'acc':  accuracy_score(y_true, y_pred),\n",
        "        'rec':  recall_score(y_true, y_pred, zero_division=0),  # sensitivity\n",
        "        'spec': spec,\n",
        "        'f1':   f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "def report(name, res):\n",
        "    print(f\"{name:>11s}: \"\n",
        "          f\"Accuracy={res['acc']:.4f} \"\n",
        "          f\"Sensitivity={res['rec']:.4f} \"\n",
        "          f\"Specificity={res['spec']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\")\n",
        "\n",
        "# ─── Load label metadata (same JSON your EEGformer uses) ────────\n",
        "with open(Path(DATA_DIR) / LABEL_FILE, 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type'] == 'train']\n",
        "test_within_meta = [d for d in all_meta if d['type'] == 'test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type'] == 'test_cross']\n",
        "\n",
        "class0, class1   = 'A', 'C'            # AD vs CN\n",
        "label_map        = {class0: 0, class1: 1}\n",
        "\n",
        "# Keep only F & C in evaluation splits\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "# ─── Balance training split (down‑sample larger class) ──────────\n",
        "data0 = [d for d in train_meta if d['label'] == class0]\n",
        "data1 = [d for d in train_meta if d['label'] == class1]\n",
        "k     = min(len(data0), len(data1))\n",
        "balanced_meta = random.sample(data0, k) + random.sample(data1, k)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]     # convert to int\n",
        "\n",
        "print(f\"\\n[Balanced] TRAIN  total={len(balanced_meta)} (AD={k}, CN={k})\")\n",
        "\n",
        "# ─── Convert each split to NumPy (X, y) -------------------------\n",
        "def make_numpy(metas):\n",
        "    raw = EEGDataset(DATA_DIR, metas)\n",
        "    ds  = FlatEEGDataset(raw, metas, label_map)\n",
        "    loader = DataLoader(ds, batch_size=128, shuffle=False,\n",
        "                        num_workers=NUM_WORKERS)\n",
        "    X, y = [], []\n",
        "    for xb, yb in loader:          # xb is *tensor* batch\n",
        "        X.append(xb)               # keep on CPU\n",
        "        y.append(yb)\n",
        "    return torch.cat(X).numpy(), torch.cat(y).numpy()\n",
        "\n",
        "X_train, y_train = make_numpy(balanced_meta)\n",
        "X_tw,    y_tw    = make_numpy(test_within_meta)\n",
        "X_tc,    y_tc    = make_numpy(test_cross_meta)\n",
        "\n",
        "print(f\"Input feature vector length per sample = {X_train.shape[1]}\")\n",
        "\n",
        "# ─── Define classical models ------------------------------------\n",
        "models = {\n",
        "    \"LightGBM\" : LGBMClassifier(\n",
        "        n_estimators=100, learning_rate=0.001,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    ),\n",
        "    \"SVM\"      : SVC(\n",
        "        kernel='rbf', probability=False,\n",
        "        class_weight='balanced', random_state=SEED, gamma='scale'\n",
        "    ),\n",
        "    \"KNN\"      : KNeighborsClassifier(\n",
        "        n_neighbors=7, weights='distance'\n",
        "    ),\n",
        "    \"MLP\"      : MLPClassifier(\n",
        "        hidden_layer_sizes=(256, 128), max_iter=200,\n",
        "        activation='relu', solver='adam', random_state=SEED\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=None,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# ─── Train & evaluate -------------------------------------------\n",
        "print(\"\\n=== Final Evaluation on Test Sets (Classical Baselines) ===\")\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf',    clf)\n",
        "    ])\n",
        "\n",
        "    t0 = time.time()\n",
        "    pipe.fit(X_train, y_train)\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    y_pred_tw = pipe.predict(X_tw)\n",
        "    y_pred_tc = pipe.predict(X_tc)\n",
        "\n",
        "    res_tw = metrics(y_tw, y_pred_tw)\n",
        "    res_tc = metrics(y_tc, y_pred_tc)\n",
        "\n",
        "    print(f\"\\n-- {name} (train time {train_time:.1f}s) --\")\n",
        "    print(f\"   test_within  total={len(y_tw)}\")\n",
        "    report(\"test_within\", res_tw)\n",
        "    print(f\"   test_cross   total={len(y_tc)}\")\n",
        "    report(\"test_cross\",  res_tc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "taV4vYVMSYp-",
        "outputId": "3f3d61ab-c32f-40df-db7a-7548fc9c11b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "[Balanced] TRAIN  total=2204 (AD=1102, CN=1102)\n",
            "Input feature vector length per sample = 27075\n",
            "\n",
            "=== Final Evaluation on Test Sets (Classical Baselines) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 1102, number of negative: 1102\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.845540 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6904125\n",
            "[LightGBM] [Info] Number of data points in the train set: 2204, number of used features: 27075\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- LightGBM (train time 83.5s) --\n",
            "   test_within  total=272\n",
            "test_within: Accuracy=0.5110 Sensitivity=0.4762 Specificity=0.5411 F1=0.4743\n",
            "   test_cross   total=626\n",
            " test_cross: Accuracy=0.4776 Sensitivity=0.4853 Specificity=0.4702 F1=0.4768\n",
            "\n",
            "-- SVM (train time 36.2s) --\n",
            "   test_within  total=272\n",
            "test_within: Accuracy=0.4779 Sensitivity=0.4841 Specificity=0.4726 F1=0.4621\n",
            "   test_cross   total=626\n",
            " test_cross: Accuracy=0.4521 Sensitivity=0.4560 Specificity=0.4483 F1=0.4494\n",
            "\n",
            "-- KNN (train time 0.7s) --\n",
            "   test_within  total=272\n",
            "test_within: Accuracy=0.5956 Sensitivity=0.5317 Specificity=0.6507 F1=0.5492\n",
            "   test_cross   total=626\n",
            " test_cross: Accuracy=0.4872 Sensitivity=0.4593 Specificity=0.5141 F1=0.4677\n",
            "\n",
            "-- MLP (train time 45.5s) --\n",
            "   test_within  total=272\n",
            "test_within: Accuracy=0.4963 Sensitivity=0.5000 Specificity=0.4932 F1=0.4791\n",
            "   test_cross   total=626\n",
            " test_cross: Accuracy=0.5000 Sensitivity=0.4919 Specificity=0.5078 F1=0.4911\n",
            "\n",
            "-- RandomForest (train time 42.7s) --\n",
            "   test_within  total=272\n",
            "test_within: Accuracy=0.5184 Sensitivity=0.5079 Specificity=0.5274 F1=0.4942\n",
            "   test_cross   total=626\n",
            " test_cross: Accuracy=0.4904 Sensitivity=0.4169 Specificity=0.5611 F1=0.4452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AD vs FTD Baseline Results"
      ],
      "metadata": {
        "id": "HVe4JFKzzIPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline_models.py  – Classical baselines for AD vs FTD\n",
        "# -------------------------------------------------------\n",
        "import os, json, random, time, copy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.pipeline       import Pipeline\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn.metrics        import (\n",
        "    recall_score, f1_score, accuracy_score, confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.svm            import SVC\n",
        "from sklearn.neighbors      import KNeighborsClassifier\n",
        "from sklearn.ensemble       import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm               import LGBMClassifier   # pip install lightgbm\n",
        "\n",
        "from eeg_dataset import EEGDataset                 # ← your existing loader\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ─── Paths & constants (adjust as needed) ───────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = 'labels.json'\n",
        "NUM_WORKERS = 4               # DataLoader workers\n",
        "\n",
        "# ─── Dataset: returns flat NumPy feature vector + label ─────────\n",
        "class FlatEEGDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps EEGDataset but outputs (1‑D float32 NumPy array, int label).\n",
        "    Works whether EEGDataset returns a tensor or an ndarray.\n",
        "    \"\"\"\n",
        "    def __init__(self, raw_ds, metas, label_map):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.labels = [label_map[d['label']] if isinstance(d['label'], str)\n",
        "                       else d['label'] for d in metas]\n",
        "\n",
        "    def __len__(self): return len(self.raw_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]                       # tensor OR ndarray\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x = x.flatten().cpu().numpy()\n",
        "        else:  # already ndarray\n",
        "            x = x.flatten()\n",
        "        return x.astype(np.float32), self.labels[idx]\n",
        "\n",
        "# ─── Utility functions ──────────────────────────────────────────\n",
        "def count_labels(meta, cls0='A', cls1='F'):\n",
        "    n0 = sum(1 for d in meta if d['label'] in (cls0, 0))\n",
        "    n1 = len(meta) - n0\n",
        "    return n0, n1\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "    spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
        "    return {\n",
        "        'acc':  accuracy_score(y_true, y_pred),\n",
        "        'rec':  recall_score(y_true, y_pred, zero_division=0),  # sensitivity\n",
        "        'spec': spec,\n",
        "        'f1':   f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "def report(name, res):\n",
        "    print(f\"{name:>11s}: \"\n",
        "          f\"Accuracy={res['acc']:.4f} \"\n",
        "          f\"Sensitivity={res['rec']:.4f} \"\n",
        "          f\"Specificity={res['spec']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\")\n",
        "\n",
        "# ─── Load label metadata (same JSON your EEGformer uses) ────────\n",
        "with open(Path(DATA_DIR) / LABEL_FILE, 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type'] == 'train']\n",
        "test_within_meta = [d for d in all_meta if d['type'] == 'test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type'] == 'test_cross']\n",
        "\n",
        "class0, class1   = 'A', 'F'            # AD vs FTD\n",
        "label_map        = {class0: 0, class1: 1}\n",
        "\n",
        "# Keep only A & F in evaluation splits\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "# ─── Balance training split (down‑sample larger class) ──────────\n",
        "data0 = [d for d in train_meta if d['label'] == class0]\n",
        "data1 = [d for d in train_meta if d['label'] == class1]\n",
        "k     = min(len(data0), len(data1))\n",
        "balanced_meta = random.sample(data0, k) + random.sample(data1, k)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]     # convert to int\n",
        "\n",
        "print(f\"\\n[Balanced] TRAIN  total={len(balanced_meta)} (AD={k}, FTD={k})\")\n",
        "\n",
        "# ─── Convert each split to NumPy (X, y) -------------------------\n",
        "def make_numpy(metas):\n",
        "    raw = EEGDataset(DATA_DIR, metas)\n",
        "    ds  = FlatEEGDataset(raw, metas, label_map)\n",
        "    loader = DataLoader(ds, batch_size=128, shuffle=False,\n",
        "                        num_workers=NUM_WORKERS)\n",
        "    X, y = [], []\n",
        "    for xb, yb in loader:          # xb is *tensor* batch\n",
        "        X.append(xb)               # keep on CPU\n",
        "        y.append(yb)\n",
        "    return torch.cat(X).numpy(), torch.cat(y).numpy()\n",
        "\n",
        "X_train, y_train = make_numpy(balanced_meta)\n",
        "X_tw,    y_tw    = make_numpy(test_within_meta)\n",
        "X_tc,    y_tc    = make_numpy(test_cross_meta)\n",
        "\n",
        "print(f\"Input feature vector length per sample = {X_train.shape[1]}\")\n",
        "\n",
        "# ─── Define classical models ------------------------------------\n",
        "models = {\n",
        "    \"LightGBM\" : LGBMClassifier(\n",
        "        n_estimators=100, learning_rate=0.001,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    ),\n",
        "    \"SVM\"      : SVC(\n",
        "        kernel='rbf', probability=False,\n",
        "        class_weight='balanced', random_state=SEED, gamma='scale'\n",
        "    ),\n",
        "    \"KNN\"      : KNeighborsClassifier(\n",
        "        n_neighbors=7, weights='distance'\n",
        "    ),\n",
        "    \"MLP\"      : MLPClassifier(\n",
        "        hidden_layer_sizes=(256, 128), max_iter=200,\n",
        "        activation='relu', solver='adam', random_state=SEED\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=None,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# ─── Train & evaluate -------------------------------------------\n",
        "print(\"\\n=== Final Evaluation on Test Sets (Classical Baselines) ===\")\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf',    clf)\n",
        "    ])\n",
        "\n",
        "    t0 = time.time()\n",
        "    pipe.fit(X_train, y_train)\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    y_pred_tw = pipe.predict(X_tw)\n",
        "    y_pred_tc = pipe.predict(X_tc)\n",
        "\n",
        "    res_tw = metrics(y_tw, y_pred_tw)\n",
        "    res_tc = metrics(y_tc, y_pred_tc)\n",
        "\n",
        "    print(f\"\\n-- {name} (train time {train_time:.1f}s) --\")\n",
        "    print(f\"   test_within  total={len(y_tw)}\")\n",
        "    report(\"test_within\", res_tw)\n",
        "    print(f\"   test_cross   total={len(y_tc)}\")\n",
        "    report(\"test_cross\",  res_tc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5RXWD0j2SYsd",
        "outputId": "48b79189-4f90-4133-c43e-00c777ebefb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Balanced] TRAIN  total=1458 (AD=729, FTD=729)\n",
            "Input feature vector length per sample = 27075\n",
            "\n",
            "=== Final Evaluation on Test Sets (Classical Baselines) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 729, number of negative: 729\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.868109 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6904125\n",
            "[LightGBM] [Info] Number of data points in the train set: 1458, number of used features: 27075\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- LightGBM (train time 71.4s) --\n",
            "   test_within  total=218\n",
            "test_within: Accuracy=0.5183 Sensitivity=0.4306 Specificity=0.5616 F1=0.3713\n",
            "   test_cross   total=566\n",
            " test_cross: Accuracy=0.4982 Sensitivity=0.4615 Specificity=0.5266 F1=0.4453\n",
            "\n",
            "-- SVM (train time 16.8s) --\n",
            "   test_within  total=218\n",
            "test_within: Accuracy=0.5505 Sensitivity=0.5972 Specificity=0.5274 F1=0.4674\n",
            "   test_cross   total=566\n",
            " test_cross: Accuracy=0.4770 Sensitivity=0.4777 Specificity=0.4765 F1=0.4436\n",
            "\n",
            "-- KNN (train time 0.5s) --\n",
            "   test_within  total=218\n",
            "test_within: Accuracy=0.5229 Sensitivity=0.5000 Specificity=0.5342 F1=0.4091\n",
            "   test_cross   total=566\n",
            " test_cross: Accuracy=0.4664 Sensitivity=0.4534 Specificity=0.4765 F1=0.4259\n",
            "\n",
            "-- MLP (train time 31.7s) --\n",
            "   test_within  total=218\n",
            "test_within: Accuracy=0.5505 Sensitivity=0.5556 Specificity=0.5479 F1=0.4494\n",
            "   test_cross   total=566\n",
            " test_cross: Accuracy=0.4859 Sensitivity=0.4899 Specificity=0.4828 F1=0.4540\n",
            "\n",
            "-- RandomForest (train time 23.9s) --\n",
            "   test_within  total=218\n",
            "test_within: Accuracy=0.5459 Sensitivity=0.4722 Specificity=0.5822 F1=0.4072\n",
            "   test_cross   total=566\n",
            " test_cross: Accuracy=0.4806 Sensitivity=0.4170 Specificity=0.5298 F1=0.4120\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2YtEZeOLvxDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Binary Classification\n",
        "- A = AD (= 0)\n",
        "- C = CN (= 1)\n",
        "- F = FTD (= 2)\n"
      ],
      "metadata": {
        "id": "TKz4VM5Erg1o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) AD vs CN Experiment"
      ],
      "metadata": {
        "id": "VwHcUyGE1oAf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Before using Optuna, try 5-fold cross val to see whether the model generalize well"
      ],
      "metadata": {
        "id": "ptxWdO30MCIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from wandb import Settings\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR     = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE   = 'labels.json'\n",
        "DEVICE       = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "LR           = 1e-3\n",
        "MAX_EPOCHS   = 100\n",
        "BATCH_SIZE   = 32\n",
        "NUM_WORKERS  = 4\n",
        "ES_PATIENCE  = 10    # Early Stopping patience\n",
        "\n",
        "# ─── Wrapper to override labels inside our experiment ────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        \"\"\"\n",
        "        raw_ds: instance of EEGDataset\n",
        "        metas: list of dicts with 'file_name' and mapped integer 'label'\n",
        "        \"\"\"\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]            # discard raw label\n",
        "        y     = self.metas[idx]['label']   # use our mapped 0/1 label\n",
        "        return x, y\n",
        "\n",
        "# ─── Which binary pairs to run ───────────────────────────────────\n",
        "binary_pairs = [\n",
        "    (\"A\", \"C\"),  # AD vs CN\n",
        "    #(\"F\", \"C\"),  # FTD vs CN\n",
        "    #(\"A\", \"F\"),  # AD vs FTD\n",
        "]\n",
        "\n",
        "random.seed(0)\n",
        "final_results = {}\n",
        "\n",
        "# ─── Load all train metadata ─────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta_all = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "\n",
        "for class0, class1 in binary_pairs:\n",
        "    print(f\"\\n=== Experiment: {class0} vs {class1} ===\")\n",
        "\n",
        "    # 1) filter for the two classes\n",
        "    data0 = [d for d in train_meta_all if d['label'] == class0]\n",
        "    data1 = [d for d in train_meta_all if d['label'] == class1]\n",
        "\n",
        "    # 2) decide undersample size (minority class)\n",
        "    min_count = min(len(data0), len(data1))\n",
        "\n",
        "    # 3) random undersample\n",
        "    samp0 = random.sample(data0, k=min_count)\n",
        "    samp1 = random.sample(data1, k=min_count)\n",
        "    balanced_meta = samp0 + samp1\n",
        "    random.shuffle(balanced_meta)\n",
        "\n",
        "    # 4) map labels to 0/1\n",
        "    label_map = {class0: 0, class1: 1}\n",
        "    for d in balanced_meta:\n",
        "        d['label'] = label_map[d['label']]\n",
        "\n",
        "    # 5) print balanced counts\n",
        "    counts = Counter(d['label'] for d in balanced_meta)\n",
        "    print(\"  Balanced counts:\", counts)\n",
        "\n",
        "    # 6) build raw EEGDataset and wrap it\n",
        "    raw_ds = EEGDataset(DATA_DIR, balanced_meta)\n",
        "    dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "    labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "    # 7) 5-Fold Stratified CV\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_accs, fold_precs, fold_recs, fold_f1s = [], [], [], []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(\n",
        "            skf.split(np.zeros(len(labels)), labels), start=1):\n",
        "\n",
        "        print(f\"\\n--- Fold {fold} ---\")\n",
        "        wandb.init(\n",
        "            project=\"eeg-binary-AD-CN-cv_1\",\n",
        "            name=f\"{class0}_vs_{class1}_fold{fold}\",\n",
        "            config={\n",
        "                \"lr\":           LR,\n",
        "                \"max_epochs\":   MAX_EPOCHS,\n",
        "                \"batch_size\":   BATCH_SIZE,\n",
        "                \"class_pair\":   f\"{class0} vs {class1}\"\n",
        "            },\n",
        "            settings=Settings(init_timeout=120)\n",
        "        )\n",
        "\n",
        "        train_ds = Subset(dataset, train_idx)\n",
        "        val_ds   = Subset(dataset, val_idx)\n",
        "        train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True,  num_workers=NUM_WORKERS)\n",
        "        val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE,\n",
        "                                  shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # 8) init model, optimizer, loss\n",
        "        input_len = dataset[0][0].shape[-1]\n",
        "        model     = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_len,\n",
        "            kernel_size=10,\n",
        "            num_filters=60,\n",
        "            num_heads=3,\n",
        "            num_blocks=1,\n",
        "            num_segments=5,\n",
        "            num_classes=2,\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=LR)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # ─── Early Stopping variables ────────────────────────────\n",
        "        best_val_loss = float(\"inf\")\n",
        "        es_count      = 0\n",
        "\n",
        "        # 9) epoch loop\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                tloss    += loss.item()\n",
        "                preds     = logits.argmax(1)\n",
        "                tcorrect += (preds == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            val_preds, val_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "\n",
        "                    preds = logits.argmax(1)\n",
        "                    vcorrect += (preds == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "\n",
        "                    val_preds.append(preds.cpu().numpy())\n",
        "                    val_labels.append(y.cpu().numpy())\n",
        "\n",
        "            val_preds  = np.concatenate(val_preds)\n",
        "            val_labels = np.concatenate(val_labels)\n",
        "            val_loss   = vloss / len(val_loader)\n",
        "            val_acc    = (val_preds == val_labels).mean()\n",
        "            precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "            recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "            f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "            elapsed    = time.time() - t0\n",
        "\n",
        "            # — terminal print & wandb log —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"prec={precision:.4f} rec={recall:.4f} f1={f1:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "                \"precision\":           precision,\n",
        "                \"recall\":              recall,\n",
        "                \"f1_score\":            f1\n",
        "            }, step=epoch)\n",
        "\n",
        "            # ─── Early Stopping check ─────────────────────────────\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                es_count = 0\n",
        "            else:\n",
        "                es_count += 1\n",
        "                if es_count >= ES_PATIENCE:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "        # record final metrics for this fold\n",
        "        fold_accs.append(val_acc)\n",
        "        fold_precs.append(precision)\n",
        "        fold_recs.append(recall)\n",
        "        fold_f1s.append(f1)\n",
        "\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 10) compute and print average metrics across folds\n",
        "    mean_acc  = np.mean(fold_accs)\n",
        "    mean_prec = np.mean(fold_precs)\n",
        "    mean_rec  = np.mean(fold_recs)\n",
        "    mean_f1   = np.mean(fold_f1s)\n",
        "    final_results[f\"{class0} vs {class1}\"] = {\n",
        "        \"accuracy\":  mean_acc,\n",
        "        \"precision\": mean_prec,\n",
        "        \"recall\":    mean_rec,\n",
        "        \"f1_score\":  mean_f1\n",
        "    }\n",
        "    print(\n",
        "        f\"\\n>>> Average for {class0} vs {class1}: \"\n",
        "        f\"acc={mean_acc:.4f}, \"\n",
        "        f\"prec={mean_prec:.4f}, \"\n",
        "        f\"rec={mean_rec:.4f}, \"\n",
        "        f\"f1={mean_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "# ─── Final summary ───────────────────────────────────────────────\n",
        "print(\"\\n=== Final Results ===\")\n",
        "for exp, metrics in final_results.items():\n",
        "    print(\n",
        "        f\"{exp}: \"\n",
        "        f\"acc={metrics['accuracy']:.4f}, \"\n",
        "        f\"prec={metrics['precision']:.4f}, \"\n",
        "        f\"rec={metrics['recall']:.4f}, \"\n",
        "        f\"f1={metrics['f1_score']:.4f}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LdJMnAf3rgGr",
        "outputId": "512f9f26-211e-498e-ba02-b898226db8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Experiment: A vs C ===\n",
            "  Balanced counts: Counter({1: 1102, 0: 1102})\n",
            "\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_063918-ux1fttxz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/ux1fttxz' target=\"_blank\">A_vs_C_fold1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/ux1fttxz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/ux1fttxz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7490 acc=0.5043 | val_loss=0.6942 acc=0.4921 | prec=0.4965 rec=0.9729 f1=0.6575 | time=126.3s\n",
            "Epoch 002 | train_loss=0.7162 acc=0.5003 | val_loss=0.6938 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=11.3s\n",
            "Epoch 003 | train_loss=0.7056 acc=0.5060 | val_loss=0.6941 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=11.5s\n",
            "Epoch 004 | train_loss=0.6953 acc=0.5264 | val_loss=0.6878 acc=0.6372 | prec=0.8020 rec=0.3665 f1=0.5031 | time=11.5s\n",
            "Epoch 005 | train_loss=0.6936 acc=0.5434 | val_loss=0.6920 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=11.2s\n",
            "Epoch 006 | train_loss=0.6608 acc=0.5814 | val_loss=0.5358 acc=0.7256 | prec=0.6953 rec=0.8054 f1=0.7463 | time=11.4s\n",
            "Epoch 007 | train_loss=0.5864 acc=0.7056 | val_loss=0.4862 acc=0.7823 | prec=0.8698 rec=0.6652 f1=0.7538 | time=11.5s\n",
            "Epoch 008 | train_loss=0.5406 acc=0.7351 | val_loss=0.5576 acc=0.6803 | prec=0.6143 rec=0.9729 f1=0.7531 | time=11.5s\n",
            "Epoch 009 | train_loss=0.5221 acc=0.7538 | val_loss=0.5568 acc=0.6871 | prec=0.6176 rec=0.9864 f1=0.7596 | time=11.2s\n",
            "Epoch 010 | train_loss=0.5003 acc=0.7521 | val_loss=0.4220 acc=0.8435 | prec=0.8089 rec=0.9005 f1=0.8522 | time=11.5s\n",
            "Epoch 011 | train_loss=0.4725 acc=0.7578 | val_loss=0.7425 acc=0.6644 | prec=0.6000 rec=0.9910 f1=0.7474 | time=11.6s\n",
            "Epoch 012 | train_loss=0.4405 acc=0.7828 | val_loss=0.3720 acc=0.8435 | prec=0.8838 rec=0.7919 f1=0.8353 | time=11.6s\n",
            "Epoch 013 | train_loss=0.4328 acc=0.7833 | val_loss=0.4106 acc=0.8367 | prec=0.8282 rec=0.8507 f1=0.8393 | time=11.5s\n",
            "Epoch 014 | train_loss=0.4269 acc=0.7975 | val_loss=0.3694 acc=0.8367 | prec=0.8860 rec=0.7738 f1=0.8261 | time=11.5s\n",
            "Epoch 015 | train_loss=0.4117 acc=0.7845 | val_loss=0.4393 acc=0.7868 | prec=0.7613 rec=0.8371 f1=0.7974 | time=11.6s\n",
            "Epoch 016 | train_loss=0.4015 acc=0.7913 | val_loss=0.4140 acc=0.8141 | prec=0.9112 rec=0.6968 f1=0.7897 | time=11.5s\n",
            "Epoch 017 | train_loss=0.3732 acc=0.8077 | val_loss=0.3785 acc=0.8299 | prec=0.9195 rec=0.7240 f1=0.8101 | time=11.3s\n",
            "Epoch 018 | train_loss=0.3579 acc=0.8151 | val_loss=0.4880 acc=0.8050 | prec=0.8814 rec=0.7059 f1=0.7839 | time=11.2s\n",
            "Epoch 019 | train_loss=0.3681 acc=0.8185 | val_loss=0.4540 acc=0.7959 | prec=0.9226 rec=0.6471 f1=0.7606 | time=11.2s\n",
            "Epoch 020 | train_loss=0.3377 acc=0.8208 | val_loss=0.3609 acc=0.8390 | prec=0.9213 rec=0.7421 f1=0.8221 | time=11.4s\n",
            "Epoch 021 | train_loss=0.3240 acc=0.8400 | val_loss=0.4138 acc=0.8163 | prec=0.8153 rec=0.8190 f1=0.8172 | time=11.5s\n",
            "Epoch 022 | train_loss=0.3243 acc=0.8383 | val_loss=0.5616 acc=0.7120 | prec=0.6577 rec=0.8869 f1=0.7553 | time=11.4s\n",
            "Epoch 023 | train_loss=0.2954 acc=0.8542 | val_loss=0.5934 acc=0.7642 | prec=0.7489 rec=0.7964 f1=0.7719 | time=11.6s\n",
            "Epoch 024 | train_loss=0.2800 acc=0.8531 | val_loss=0.5652 acc=0.7460 | prec=0.7224 rec=0.8009 f1=0.7597 | time=11.4s\n",
            "Epoch 025 | train_loss=0.2612 acc=0.8707 | val_loss=0.6855 acc=0.7528 | prec=0.8944 rec=0.5747 f1=0.6997 | time=11.5s\n",
            "Epoch 026 | train_loss=0.2759 acc=0.8559 | val_loss=0.4741 acc=0.7937 | prec=0.8385 rec=0.7285 f1=0.7797 | time=11.4s\n",
            "Epoch 027 | train_loss=0.2615 acc=0.8673 | val_loss=0.5633 acc=0.7778 | prec=0.7595 rec=0.8145 f1=0.7860 | time=11.7s\n",
            "Epoch 028 | train_loss=0.2640 acc=0.8599 | val_loss=0.6694 acc=0.7619 | prec=0.7522 rec=0.7828 f1=0.7672 | time=11.4s\n",
            "Epoch 029 | train_loss=0.2623 acc=0.8531 | val_loss=0.6488 acc=0.7800 | prec=0.7981 rec=0.7511 f1=0.7739 | time=11.6s\n",
            "Epoch 030 | train_loss=0.2700 acc=0.8491 | val_loss=0.6895 acc=0.7687 | prec=0.8563 rec=0.6471 f1=0.7371 | time=11.4s\n",
            "Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▄▄▄▁▄▆▆▆▆█▆██▇▇▇▇▇▆▇▇▆▆▆▅▇▇▆▆▆</td></tr><tr><td>precision</td><td>▁▁▁▆▁▄▇▃▃▆▃▇▆▇▅██▇██▆▄▅▅█▇▅▅▆▇</td></tr><tr><td>recall</td><td>███▁█▆▄██▇█▆▆▆▆▅▅▅▄▅▆▇▆▆▃▅▆▆▅▄</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▃▅▅▆▆▆▆▆▇▆▆▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▆▅▅▄▄▄▃▃▃▃▃▂▃▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▄▁▆▇▅▅█▄███▇▇█▇▇█▇▅▆▆▆▇▇▆▇▇</td></tr><tr><td>validation_loss</td><td>▇▇▇▇▇▄▃▅▅▂█▁▂▁▂▂▁▃▃▁▂▅▅▅▇▃▅▇▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>f1_score</td><td>0.73711</td></tr><tr><td>precision</td><td>0.85629</td></tr><tr><td>recall</td><td>0.64706</td></tr><tr><td>train_accuracy</td><td>0.84912</td></tr><tr><td>train_loss</td><td>0.26997</td></tr><tr><td>validation_accuracy</td><td>0.76871</td></tr><tr><td>validation_loss</td><td>0.68949</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">A_vs_C_fold1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/ux1fttxz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/ux1fttxz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_063918-ux1fttxz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_064659-jla39ejn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/jla39ejn' target=\"_blank\">A_vs_C_fold2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/jla39ejn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/jla39ejn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7349 acc=0.5054 | val_loss=0.6927 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=11.4s\n",
            "Epoch 002 | train_loss=0.7110 acc=0.4935 | val_loss=0.6924 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=11.5s\n",
            "Epoch 003 | train_loss=0.7167 acc=0.4816 | val_loss=0.6907 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=11.5s\n",
            "Epoch 004 | train_loss=0.7017 acc=0.5009 | val_loss=0.6744 acc=0.7392 | prec=0.8313 rec=0.6018 f1=0.6982 | time=11.6s\n",
            "Epoch 005 | train_loss=0.6734 acc=0.5842 | val_loss=0.5930 acc=0.7528 | prec=0.8333 rec=0.6335 f1=0.7198 | time=11.4s\n",
            "Epoch 006 | train_loss=0.6084 acc=0.6653 | val_loss=0.4823 acc=0.8186 | prec=0.8579 rec=0.7647 f1=0.8086 | time=11.7s\n",
            "Epoch 007 | train_loss=0.5546 acc=0.7096 | val_loss=0.4528 acc=0.8254 | prec=0.9286 rec=0.7059 f1=0.8021 | time=11.4s\n",
            "Epoch 008 | train_loss=0.5505 acc=0.7311 | val_loss=0.4094 acc=0.8322 | prec=0.7928 rec=0.9005 f1=0.8432 | time=11.3s\n",
            "Epoch 009 | train_loss=0.5213 acc=0.7345 | val_loss=0.4242 acc=0.8345 | prec=0.9253 rec=0.7285 f1=0.8152 | time=11.5s\n",
            "Epoch 010 | train_loss=0.4943 acc=0.7720 | val_loss=0.7714 acc=0.5873 | prec=0.5484 rec=1.0000 f1=0.7083 | time=11.5s\n",
            "Epoch 011 | train_loss=0.4653 acc=0.7822 | val_loss=0.3748 acc=0.8549 | prec=0.9110 rec=0.7873 f1=0.8447 | time=11.6s\n",
            "Epoch 012 | train_loss=0.4391 acc=0.7884 | val_loss=0.3567 acc=0.8730 | prec=0.8873 rec=0.8552 f1=0.8710 | time=11.4s\n",
            "Epoch 013 | train_loss=0.4376 acc=0.7964 | val_loss=0.3606 acc=0.8594 | prec=0.8597 rec=0.8597 f1=0.8597 | time=11.4s\n",
            "Epoch 014 | train_loss=0.4762 acc=0.7742 | val_loss=0.7821 acc=0.5805 | prec=0.5443 rec=1.0000 f1=0.7049 | time=11.4s\n",
            "Epoch 015 | train_loss=0.4366 acc=0.7992 | val_loss=0.4000 acc=0.8118 | prec=0.7464 rec=0.9457 f1=0.8343 | time=11.5s\n",
            "Epoch 016 | train_loss=0.3947 acc=0.8247 | val_loss=0.6415 acc=0.5510 | prec=0.5274 rec=1.0000 f1=0.6906 | time=11.6s\n",
            "Epoch 017 | train_loss=0.3942 acc=0.8247 | val_loss=0.4934 acc=0.7619 | prec=0.6835 rec=0.9774 f1=0.8045 | time=11.7s\n",
            "Epoch 018 | train_loss=0.4013 acc=0.8066 | val_loss=0.7123 acc=0.5125 | prec=0.5069 rec=1.0000 f1=0.6728 | time=11.2s\n",
            "Epoch 019 | train_loss=0.4126 acc=0.8009 | val_loss=0.3896 acc=0.8254 | prec=0.7687 rec=0.9321 f1=0.8425 | time=11.5s\n",
            "Epoch 020 | train_loss=0.3608 acc=0.8253 | val_loss=0.7304 acc=0.6236 | prec=0.5714 rec=0.9955 f1=0.7261 | time=11.4s\n",
            "Epoch 021 | train_loss=0.3430 acc=0.8412 | val_loss=0.3577 acc=0.8549 | prec=0.8398 rec=0.8778 f1=0.8584 | time=11.6s\n",
            "Epoch 022 | train_loss=0.3636 acc=0.8230 | val_loss=0.4394 acc=0.8095 | prec=0.7546 rec=0.9186 f1=0.8286 | time=11.6s\n",
            "Early stopping at epoch 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▇▇▇▇██▇███▇█▇▇▆█▇██</td></tr><tr><td>precision</td><td>▁▁▁▇▇▇█▇█▅██▇▅▇▅▆▅▇▅▇▇</td></tr><tr><td>recall</td><td>▁▁▁▅▅▆▆▇▆█▇▇▇███████▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▃▅▅▆▆▇▇▇▇▇▇██▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▆▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▅▆▇▇▇▇▃███▃▇▂▆▁▇▃█▇</td></tr><tr><td>validation_loss</td><td>▇▇▆▆▅▃▃▂▂█▁▁▁█▂▆▃▇▂▇▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>f1_score</td><td>0.82857</td></tr><tr><td>precision</td><td>0.75465</td></tr><tr><td>recall</td><td>0.91855</td></tr><tr><td>train_accuracy</td><td>0.82303</td></tr><tr><td>train_loss</td><td>0.3636</td></tr><tr><td>validation_accuracy</td><td>0.80952</td></tr><tr><td>validation_loss</td><td>0.43944</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">A_vs_C_fold2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/jla39ejn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/jla39ejn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_064659-jla39ejn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 3 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_065113-0d59345a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/0d59345a' target=\"_blank\">A_vs_C_fold3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/0d59345a' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/0d59345a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7406 acc=0.5116 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=11.4s\n",
            "Epoch 002 | train_loss=0.7287 acc=0.5133 | val_loss=0.6900 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=11.4s\n",
            "Epoch 003 | train_loss=0.6972 acc=0.5320 | val_loss=0.6414 acc=0.6417 | prec=0.8974 rec=0.3182 f1=0.4698 | time=11.4s\n",
            "Epoch 004 | train_loss=0.6229 acc=0.6568 | val_loss=0.6384 acc=0.6168 | prec=0.9811 rec=0.2364 f1=0.3810 | time=11.2s\n",
            "Epoch 005 | train_loss=0.5640 acc=0.7153 | val_loss=0.5536 acc=0.7392 | prec=0.8889 rec=0.5455 f1=0.6761 | time=11.3s\n",
            "Epoch 006 | train_loss=0.5270 acc=0.7459 | val_loss=0.4877 acc=0.7506 | prec=0.8986 rec=0.5636 f1=0.6927 | time=11.3s\n",
            "Epoch 007 | train_loss=0.5055 acc=0.7510 | val_loss=0.4173 acc=0.8254 | prec=0.8454 rec=0.7955 f1=0.8197 | time=11.6s\n",
            "Epoch 008 | train_loss=0.4957 acc=0.7499 | val_loss=0.4223 acc=0.8367 | prec=0.8426 rec=0.8273 f1=0.8349 | time=11.5s\n",
            "Epoch 009 | train_loss=0.4312 acc=0.7896 | val_loss=0.5513 acc=0.7370 | prec=0.9643 rec=0.4909 f1=0.6506 | time=11.8s\n",
            "Epoch 010 | train_loss=0.4509 acc=0.7794 | val_loss=0.3760 acc=0.8322 | prec=0.8578 rec=0.7955 f1=0.8255 | time=11.3s\n",
            "Epoch 011 | train_loss=0.4231 acc=0.8060 | val_loss=0.3873 acc=0.8481 | prec=0.8370 rec=0.8636 f1=0.8501 | time=11.5s\n",
            "Epoch 012 | train_loss=0.3793 acc=0.8145 | val_loss=0.4146 acc=0.8163 | prec=0.8757 rec=0.7364 f1=0.8000 | time=11.6s\n",
            "Epoch 013 | train_loss=0.3753 acc=0.8162 | val_loss=0.3947 acc=0.8299 | prec=0.8281 rec=0.8318 f1=0.8299 | time=11.5s\n",
            "Epoch 014 | train_loss=0.3705 acc=0.8225 | val_loss=0.3847 acc=0.8345 | prec=0.8517 rec=0.8091 f1=0.8298 | time=11.6s\n",
            "Epoch 015 | train_loss=0.3752 acc=0.8157 | val_loss=0.7563 acc=0.6916 | prec=0.9884 rec=0.3864 f1=0.5556 | time=11.2s\n",
            "Epoch 016 | train_loss=0.3613 acc=0.8270 | val_loss=0.4204 acc=0.8186 | prec=0.7941 rec=0.8591 f1=0.8253 | time=11.3s\n",
            "Epoch 017 | train_loss=0.4311 acc=0.7833 | val_loss=0.5331 acc=0.7392 | prec=0.9268 rec=0.5182 f1=0.6647 | time=11.5s\n",
            "Epoch 018 | train_loss=0.3860 acc=0.8123 | val_loss=0.4744 acc=0.8141 | prec=0.7717 rec=0.8909 f1=0.8270 | time=11.5s\n",
            "Epoch 019 | train_loss=0.3469 acc=0.8372 | val_loss=0.4113 acc=0.8163 | prec=0.8263 rec=0.8000 f1=0.8129 | time=11.7s\n",
            "Epoch 020 | train_loss=0.3349 acc=0.8429 | val_loss=0.5061 acc=0.7868 | prec=0.7838 rec=0.7909 f1=0.7873 | time=11.5s\n",
            "Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▅▄▇▇██▆█████▆█▆██▇</td></tr><tr><td>precision</td><td>▁▁▇█▇▇▇▇█▇▇▇▇▇█▇█▆▇▇</td></tr><tr><td>recall</td><td>▁▁▃▃▅▅▇▇▅▇█▇█▇▄█▅█▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▄▅▆▆▆▇▇▇▇▇█▇█▇▇██</td></tr><tr><td>train_loss</td><td>██▇▆▅▄▄▄▃▃▃▂▂▂▂▁▃▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▄▃▆▆██▆██▇██▅▇▆▇▇▇</td></tr><tr><td>validation_loss</td><td>▇▇▆▆▄▃▂▂▄▁▁▂▁▁█▂▄▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>f1_score</td><td>0.78733</td></tr><tr><td>precision</td><td>0.78378</td></tr><tr><td>recall</td><td>0.79091</td></tr><tr><td>train_accuracy</td><td>0.84288</td></tr><tr><td>train_loss</td><td>0.33488</td></tr><tr><td>validation_accuracy</td><td>0.78685</td></tr><tr><td>validation_loss</td><td>0.5061</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">A_vs_C_fold3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/0d59345a' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/0d59345a</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_065113-0d59345a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 4 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_065503-x81cpb81</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/x81cpb81' target=\"_blank\">A_vs_C_fold4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/x81cpb81' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/x81cpb81</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7451 acc=0.5054 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=11.5s\n",
            "Epoch 002 | train_loss=0.7267 acc=0.4850 | val_loss=0.6974 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=11.4s\n",
            "Epoch 003 | train_loss=0.7079 acc=0.5082 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=11.5s\n",
            "Epoch 004 | train_loss=0.7012 acc=0.5014 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=11.5s\n",
            "Epoch 005 | train_loss=0.7019 acc=0.5009 | val_loss=0.6917 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=11.2s\n",
            "Epoch 006 | train_loss=0.6941 acc=0.5167 | val_loss=0.6909 acc=0.4966 | prec=0.4977 rec=0.9955 f1=0.6636 | time=11.4s\n",
            "Epoch 007 | train_loss=0.6446 acc=0.6279 | val_loss=0.5883 acc=0.7642 | prec=0.7283 rec=0.8409 f1=0.7806 | time=11.5s\n",
            "Epoch 008 | train_loss=0.5807 acc=0.7033 | val_loss=0.6258 acc=0.5737 | prec=0.5392 rec=1.0000 f1=0.7006 | time=11.5s\n",
            "Epoch 009 | train_loss=0.5274 acc=0.7465 | val_loss=0.4484 acc=0.8209 | prec=0.8691 rec=0.7545 f1=0.8078 | time=11.6s\n",
            "Epoch 010 | train_loss=0.5420 acc=0.7357 | val_loss=0.4426 acc=0.8050 | prec=0.7991 rec=0.8136 f1=0.8063 | time=11.2s\n",
            "Epoch 011 | train_loss=0.5007 acc=0.7470 | val_loss=0.4886 acc=0.7370 | prec=0.6635 rec=0.9591 f1=0.7844 | time=11.9s\n",
            "Epoch 012 | train_loss=0.4745 acc=0.7714 | val_loss=0.4327 acc=0.8163 | prec=0.8717 rec=0.7409 f1=0.8010 | time=11.5s\n",
            "Epoch 013 | train_loss=0.4451 acc=0.7799 | val_loss=0.4675 acc=0.7642 | prec=0.6959 rec=0.9364 f1=0.7984 | time=11.7s\n",
            "Epoch 014 | train_loss=0.4472 acc=0.7805 | val_loss=0.4266 acc=0.8073 | prec=0.7974 rec=0.8227 f1=0.8098 | time=11.4s\n",
            "Epoch 015 | train_loss=0.4548 acc=0.7589 | val_loss=0.4198 acc=0.8231 | prec=0.8660 rec=0.7636 f1=0.8116 | time=11.4s\n",
            "Epoch 016 | train_loss=0.4126 acc=0.8088 | val_loss=0.5524 acc=0.7687 | prec=0.9275 rec=0.5818 f1=0.7151 | time=11.6s\n",
            "Epoch 017 | train_loss=0.4055 acc=0.7907 | val_loss=0.4443 acc=0.8163 | prec=0.8971 rec=0.7136 f1=0.7949 | time=11.5s\n",
            "Epoch 018 | train_loss=0.3700 acc=0.8287 | val_loss=0.4263 acc=0.8118 | prec=0.7729 rec=0.8818 f1=0.8238 | time=11.7s\n",
            "Epoch 019 | train_loss=0.3756 acc=0.8151 | val_loss=0.4738 acc=0.8027 | prec=0.9080 rec=0.6727 f1=0.7728 | time=11.5s\n",
            "Epoch 020 | train_loss=0.3543 acc=0.8236 | val_loss=0.4675 acc=0.7800 | prec=0.7338 rec=0.8773 f1=0.7992 | time=11.5s\n",
            "Epoch 021 | train_loss=0.3413 acc=0.8293 | val_loss=0.5475 acc=0.7868 | prec=0.8387 rec=0.7091 f1=0.7685 | time=11.5s\n",
            "Epoch 022 | train_loss=0.3364 acc=0.8469 | val_loss=0.6011 acc=0.7166 | prec=0.7987 rec=0.5773 f1=0.6702 | time=11.3s\n",
            "Epoch 023 | train_loss=0.3266 acc=0.8293 | val_loss=0.4950 acc=0.7778 | prec=0.7798 rec=0.7727 f1=0.7763 | time=11.4s\n",
            "Epoch 024 | train_loss=0.3228 acc=0.8383 | val_loss=0.5287 acc=0.7710 | prec=0.7554 rec=0.8000 f1=0.7770 | time=11.4s\n",
            "Epoch 025 | train_loss=0.3441 acc=0.8310 | val_loss=0.5548 acc=0.7483 | prec=0.6823 rec=0.9273 f1=0.7861 | time=11.3s\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▆▃▇▇▆▇▇▇▇▃▇█▆▇▆▁▆▆▆</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▅▂▇▆▄▇▄▆▇██▅█▅▇▆▆▅▄</td></tr><tr><td>recall</td><td>██████▅█▄▅▇▄▇▅▄▁▃▆▃▆▃▁▄▅▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▂▄▅▆▆▆▇▇▇▆▇▇█▇██████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▆▅▄▅▄▄▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▇▃██▆█▇██▇███▇▇▆▇▇▆</td></tr><tr><td>validation_loss</td><td>██████▅▆▂▂▃▁▂▁▁▄▂▁▂▂▄▆▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>f1_score</td><td>0.78613</td></tr><tr><td>precision</td><td>0.68227</td></tr><tr><td>recall</td><td>0.92727</td></tr><tr><td>train_accuracy</td><td>0.83097</td></tr><tr><td>train_loss</td><td>0.34412</td></tr><tr><td>validation_accuracy</td><td>0.7483</td></tr><tr><td>validation_loss</td><td>0.55484</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">A_vs_C_fold4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/x81cpb81' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/x81cpb81</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_065503-x81cpb81/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 5 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_065951-yjk0qgar</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/yjk0qgar' target=\"_blank\">A_vs_C_fold5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/yjk0qgar' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/yjk0qgar</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7487 acc=0.5079 | val_loss=0.7177 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=11.4s\n",
            "Epoch 002 | train_loss=0.7247 acc=0.4989 | val_loss=0.6912 acc=0.6523 | prec=0.6031 rec=0.8909 f1=0.7193 | time=11.5s\n",
            "Epoch 003 | train_loss=0.7036 acc=0.5181 | val_loss=0.7034 acc=0.5000 | prec=0.5000 rec=1.0000 f1=0.6667 | time=11.4s\n",
            "Epoch 004 | train_loss=0.7053 acc=0.5215 | val_loss=0.6239 acc=0.7591 | prec=0.8239 rec=0.6591 f1=0.7323 | time=11.4s\n",
            "Epoch 005 | train_loss=0.6541 acc=0.6066 | val_loss=0.6127 acc=0.7295 | prec=0.8855 rec=0.5273 f1=0.6610 | time=11.5s\n",
            "Epoch 006 | train_loss=0.6167 acc=0.6621 | val_loss=0.4855 acc=0.8455 | prec=0.8878 rec=0.7909 f1=0.8365 | time=11.4s\n",
            "Epoch 007 | train_loss=0.5764 acc=0.7035 | val_loss=0.5041 acc=0.7591 | prec=0.6900 rec=0.9409 f1=0.7962 | time=11.6s\n",
            "Epoch 008 | train_loss=0.5593 acc=0.7132 | val_loss=0.4215 acc=0.8523 | prec=0.8934 rec=0.8000 f1=0.8441 | time=11.3s\n",
            "Epoch 009 | train_loss=0.5269 acc=0.7432 | val_loss=0.3800 acc=0.8614 | prec=0.8664 rec=0.8545 f1=0.8604 | time=11.3s\n",
            "Epoch 010 | train_loss=0.5367 acc=0.7443 | val_loss=0.5018 acc=0.7659 | prec=0.9398 rec=0.5682 f1=0.7082 | time=11.0s\n",
            "Epoch 011 | train_loss=0.4984 acc=0.7551 | val_loss=0.4159 acc=0.8432 | prec=0.9467 rec=0.7273 f1=0.8226 | time=11.8s\n",
            "Epoch 012 | train_loss=0.4746 acc=0.7693 | val_loss=0.5408 acc=0.7432 | prec=0.9735 rec=0.5000 f1=0.6607 | time=11.7s\n",
            "Epoch 013 | train_loss=0.4416 acc=0.7823 | val_loss=0.3829 acc=0.8591 | prec=0.9293 rec=0.7773 f1=0.8465 | time=11.8s\n",
            "Epoch 014 | train_loss=0.4536 acc=0.7840 | val_loss=0.4897 acc=0.8114 | prec=0.8624 rec=0.7409 f1=0.7971 | time=12.5s\n",
            "Epoch 015 | train_loss=0.4433 acc=0.7925 | val_loss=0.3268 acc=0.8682 | prec=0.8821 rec=0.8500 f1=0.8657 | time=12.4s\n",
            "Epoch 016 | train_loss=0.4154 acc=0.8044 | val_loss=0.3849 acc=0.8273 | prec=0.9091 rec=0.7273 f1=0.8081 | time=12.2s\n",
            "Epoch 017 | train_loss=0.3881 acc=0.8044 | val_loss=0.3449 acc=0.8432 | prec=0.8612 rec=0.8182 f1=0.8392 | time=12.1s\n",
            "Epoch 018 | train_loss=0.3834 acc=0.8186 | val_loss=0.4076 acc=0.8091 | prec=0.9198 rec=0.6773 f1=0.7801 | time=12.4s\n",
            "Epoch 019 | train_loss=0.3555 acc=0.8158 | val_loss=0.3560 acc=0.8227 | prec=0.8777 rec=0.7500 f1=0.8088 | time=12.6s\n",
            "Epoch 020 | train_loss=0.3430 acc=0.8475 | val_loss=0.4298 acc=0.8250 | prec=0.7782 rec=0.9091 f1=0.8386 | time=12.4s\n",
            "Epoch 021 | train_loss=0.4126 acc=0.7925 | val_loss=0.5146 acc=0.7818 | prec=0.9493 rec=0.5955 f1=0.7318 | time=12.3s\n",
            "Epoch 022 | train_loss=0.3464 acc=0.8231 | val_loss=0.4994 acc=0.7909 | prec=0.7336 rec=0.9136 f1=0.8138 | time=12.3s\n",
            "Epoch 023 | train_loss=0.3535 acc=0.8175 | val_loss=0.4284 acc=0.8000 | prec=0.8267 rec=0.7591 f1=0.7915 | time=12.2s\n",
            "Epoch 024 | train_loss=0.3449 acc=0.8424 | val_loss=0.4031 acc=0.8500 | prec=0.8598 rec=0.8364 f1=0.8479 | time=12.4s\n",
            "Epoch 025 | train_loss=0.3115 acc=0.8379 | val_loss=0.4333 acc=0.8318 | prec=0.8724 rec=0.7773 f1=0.8221 | time=12.2s\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▇▆▇▆█▇██▇█▆█▇███▇██▇█▇██</td></tr><tr><td>precision</td><td>▁▅▅▇▇▇▆▇▇████▇▇█▇█▇▇█▆▇▇▇</td></tr><tr><td>recall</td><td>▁▇█▆▅▇█▇▇▅▆▅▆▆▇▆▇▆▆▇▅▇▆▇▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇█▇█▇██</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▅▅▄▅▄▄▃▃▃▃▂▂▂▂▃▂▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▄▁▆▅█▆██▆█▆█▇█▇█▇▇▇▆▇▇█▇</td></tr><tr><td>validation_loss</td><td>███▆▆▄▄▃▂▄▃▅▂▄▁▂▁▂▂▃▄▄▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>f1_score</td><td>0.82212</td></tr><tr><td>precision</td><td>0.87245</td></tr><tr><td>recall</td><td>0.77727</td></tr><tr><td>train_accuracy</td><td>0.83787</td></tr><tr><td>train_loss</td><td>0.31147</td></tr><tr><td>validation_accuracy</td><td>0.83182</td></tr><tr><td>validation_loss</td><td>0.43326</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">A_vs_C_fold5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/yjk0qgar' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1/runs/yjk0qgar</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-binary-AD-CN-cv_1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_065951-yjk0qgar/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Average for A vs C: acc=0.7890, prec=0.7899, rec=0.8122, f1=0.7923\n",
            "\n",
            "=== Final Results ===\n",
            "A vs C: acc=0.7890, prec=0.7899, rec=0.8122, f1=0.7923\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try Block = 1, Head = 3\n",
        "#### Use Optuna for hyperparameter tuning"
      ],
      "metadata": {
        "id": "OM3Pa-jrL5GO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 100\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Define Binary Dataset Wrapper ───────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare balanced metadata for AD vs CN ───────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# filter and undersample\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# map to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# raw and wrapped dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# stratified k-fold\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# ─── Optuna objective: minimize val_loss & maximize val_acc ───────────\n",
        "def objective(trial):\n",
        "    # Hyperparameter sampling\n",
        "    lr        = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    wd        = trial.suggest_float('wd', 1e-6, 1e-1, log=True)\n",
        "    pct_start = trial.suggest_float('pct_start', 0.1, 0.3)\n",
        "\n",
        "    print(f\"Trial {trial.number}: lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    fold_losses, fold_accs, fold_precs, fold_recs, fold_f1s = [], [], [], [], []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(\n",
        "            skf.split(np.zeros(len(labels)), labels), start=1):\n",
        "        print(f\"--- Fold {fold} ---\")\n",
        "        # initialize W&B run for each fold-trial\n",
        "        wandb.init(\n",
        "            project=\"eeg-optuna-AD-CN-optuna-search-1\",\n",
        "            name=f\"trial{trial.number}_fold{fold}\",\n",
        "            config={\"lr\": lr, \"wd\": wd, \"pct_start\": pct_start},\n",
        "            reinit=True\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(Subset(dataset, train_idx), batch_size=BATCH_SIZE,\n",
        "                                  shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader   = DataLoader(Subset(dataset, val_idx), batch_size=BATCH_SIZE,\n",
        "                                  shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # Model, optimizer, scheduler\n",
        "        input_len   = dataset[0][0].shape[-1]\n",
        "        model       = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_len,\n",
        "            kernel_size=10,\n",
        "            num_filters=120,\n",
        "            num_heads=3,\n",
        "            num_blocks=1,\n",
        "            num_segments=5,\n",
        "            num_classes=2\n",
        "        ).to(DEVICE)\n",
        "        optimizer   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "        total_steps = MAX_EPOCHS * len(train_loader)\n",
        "        scheduler   = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=lr,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=pct_start,\n",
        "            anneal_strategy='cos',\n",
        "            cycle_momentum=False\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_loss = float('inf')\n",
        "        es_count  = 0\n",
        "\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # Train\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                # Dynamic Weight Decay\n",
        "                cur_lr = optimizer.param_groups[0]['lr']\n",
        "                new_wd = wd * (cur_lr / lr)\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['weight_decay'] = new_wd\n",
        "                tloss    += loss.item()\n",
        "                preds     = logits.argmax(1)\n",
        "                tcorrect += (preds == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # Validate\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            val_preds, val_labels = [], []\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    preds   = logits.argmax(1)\n",
        "                    vcorrect += (preds == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "                    val_preds.append(preds.cpu().numpy())\n",
        "                    val_labels.append(y.cpu().numpy())\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_preds  = np.concatenate(val_preds)\n",
        "            val_labels = np.concatenate(val_labels)\n",
        "            val_acc    = (val_preds == val_labels).mean()\n",
        "            precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "            recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "            f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "            elapsed    = time.time() - t0\n",
        "\n",
        "            # Terminal output\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"prec={precision:.4f} rec={recall:.4f} f1={f1:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # W&B logging\n",
        "            wandb.log({\n",
        "                \"train_loss\": train_loss,\n",
        "                \"train_accuracy\": train_acc,\n",
        "                \"validation_loss\": val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "                \"precision\": precision,\n",
        "                \"recall\": recall,\n",
        "                \"f1_score\": f1\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Early Stopping by val_loss\n",
        "            if val_loss < best_loss:\n",
        "                best_loss = val_loss\n",
        "                es_count  = 0\n",
        "            else:\n",
        "                es_count += 1\n",
        "                if es_count >= ES_PATIENCE:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "        # collect metrics\n",
        "        fold_losses.append(best_loss)\n",
        "        fold_accs.append(val_acc)\n",
        "        fold_precs.append(precision)\n",
        "        fold_recs.append(recall)\n",
        "        fold_f1s.append(f1)\n",
        "\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    # 10) compute and print average metrics across folds\n",
        "    mean_acc  = np.mean(fold_accs)\n",
        "    mean_prec = np.mean(fold_precs)\n",
        "    mean_rec  = np.mean(fold_recs)\n",
        "    mean_f1   = np.mean(fold_f1s)\n",
        "    print(\n",
        "        f\"\\n>>> Average metrics across folds: \"\n",
        "        f\"acc={mean_acc:.4f}, \"\n",
        "        f\"prec={mean_prec:.4f}, \"\n",
        "        f\"rec={mean_rec:.4f}, \"\n",
        "        f\"f1={mean_f1:.4f}\"\n",
        "    )\n",
        "\n",
        "    return np.mean(fold_losses), np.mean(fold_accs)\n",
        "\n",
        "# ─── Run Optuna multi-objective study ────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize','maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "print('Best trial values (val_loss, val_acc):', study.best_trial.values)\n",
        "print('Best trial params:', study.best_trial.params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6ej288K-1cfW",
        "outputId": "ceefa179-0268-4128-a82e-f3684dc2d673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 07:24:19,147] A new study created in memory with name: no-name-a5c19af3-5512-4e1e-90ae-041bae786695\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: lr=3.95e-05, wd=4.65e-06, pct_start=0.26\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_072419-71huhgrt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/71huhgrt' target=\"_blank\">trial0_fold1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/71huhgrt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/71huhgrt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8009 acc=0.5128 | val_loss=0.7055 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=28.7s\n",
            "Epoch 002 | train_loss=0.7794 acc=0.5309 | val_loss=0.6995 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 003 | train_loss=0.8031 acc=0.5088 | val_loss=0.7018 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.2s\n",
            "Epoch 004 | train_loss=0.8076 acc=0.4957 | val_loss=0.7084 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.2s\n",
            "Epoch 005 | train_loss=0.7874 acc=0.4974 | val_loss=0.7075 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 006 | train_loss=0.7727 acc=0.5247 | val_loss=0.7048 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7619 acc=0.4946 | val_loss=0.6978 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7443 acc=0.5133 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7358 acc=0.5150 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7474 acc=0.5031 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 011 | train_loss=0.7459 acc=0.4940 | val_loss=0.6996 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7353 acc=0.4997 | val_loss=0.6956 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7532 acc=0.4969 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 014 | train_loss=0.7273 acc=0.5037 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7454 acc=0.4929 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7305 acc=0.4974 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7379 acc=0.5020 | val_loss=0.7016 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7363 acc=0.5003 | val_loss=0.6964 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7200 acc=0.5309 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7249 acc=0.5054 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 021 | train_loss=0.7260 acc=0.5031 | val_loss=0.6924 acc=0.5034 | prec=0.5011 rec=0.9955 f1=0.6667 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7207 acc=0.5094 | val_loss=0.6923 acc=0.4966 | prec=0.4977 rec=0.9955 f1=0.6636 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7207 acc=0.5224 | val_loss=0.6912 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 024 | train_loss=0.7140 acc=0.5196 | val_loss=0.6900 acc=0.6145 | prec=0.5702 rec=0.9227 f1=0.7049 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7176 acc=0.5082 | val_loss=0.6855 acc=0.5601 | prec=0.5323 rec=0.9727 f1=0.6881 | time=12.8s\n",
            "Epoch 026 | train_loss=0.7111 acc=0.5145 | val_loss=0.6783 acc=0.6009 | prec=0.5576 rec=0.9682 f1=0.7076 | time=12.8s\n",
            "Epoch 027 | train_loss=0.7015 acc=0.5343 | val_loss=0.6662 acc=0.6984 | prec=0.6526 rec=0.8455 f1=0.7366 | time=12.9s\n",
            "Epoch 028 | train_loss=0.6840 acc=0.5683 | val_loss=0.6283 acc=0.7551 | prec=0.7642 rec=0.7364 f1=0.7500 | time=12.7s\n",
            "Epoch 029 | train_loss=0.6660 acc=0.5978 | val_loss=0.6054 acc=0.7483 | prec=0.6899 rec=0.9000 f1=0.7811 | time=12.8s\n",
            "Epoch 030 | train_loss=0.6267 acc=0.6285 | val_loss=0.5779 acc=0.7710 | prec=0.7692 rec=0.7727 f1=0.7710 | time=12.9s\n",
            "Epoch 031 | train_loss=0.6060 acc=0.6512 | val_loss=0.5706 acc=0.7619 | prec=0.7003 rec=0.9136 f1=0.7929 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5756 acc=0.6829 | val_loss=0.5477 acc=0.7755 | prec=0.7354 rec=0.8591 f1=0.7925 | time=12.8s\n",
            "Epoch 033 | train_loss=0.5893 acc=0.6670 | val_loss=0.5242 acc=0.7982 | prec=0.8195 rec=0.7636 f1=0.7906 | time=12.8s\n",
            "Epoch 034 | train_loss=0.5874 acc=0.6897 | val_loss=0.5364 acc=0.7596 | prec=0.7065 rec=0.8864 f1=0.7863 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5438 acc=0.7119 | val_loss=0.5424 acc=0.7483 | prec=0.6799 rec=0.9364 f1=0.7878 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5457 acc=0.7153 | val_loss=0.5175 acc=0.7732 | prec=0.7128 rec=0.9136 f1=0.8008 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5366 acc=0.7272 | val_loss=0.5039 acc=0.8005 | prec=0.7870 rec=0.8227 f1=0.8044 | time=12.9s\n",
            "Epoch 038 | train_loss=0.5374 acc=0.7294 | val_loss=0.4619 acc=0.8050 | prec=0.8490 rec=0.7409 f1=0.7913 | time=12.8s\n",
            "Epoch 039 | train_loss=0.5533 acc=0.7096 | val_loss=0.4860 acc=0.8095 | prec=0.7931 rec=0.8364 f1=0.8142 | time=12.8s\n",
            "Epoch 040 | train_loss=0.5375 acc=0.7119 | val_loss=0.4881 acc=0.8050 | prec=0.7680 rec=0.8727 f1=0.8170 | time=12.7s\n",
            "Epoch 041 | train_loss=0.5262 acc=0.7283 | val_loss=0.4838 acc=0.8050 | prec=0.8073 rec=0.8000 f1=0.8037 | time=12.8s\n",
            "Epoch 042 | train_loss=0.5197 acc=0.7413 | val_loss=0.4750 acc=0.8095 | prec=0.7742 rec=0.8727 f1=0.8205 | time=12.9s\n",
            "Epoch 043 | train_loss=0.5188 acc=0.7317 | val_loss=0.4902 acc=0.7937 | prec=0.7471 rec=0.8864 f1=0.8108 | time=12.8s\n",
            "Epoch 044 | train_loss=0.4968 acc=0.7561 | val_loss=0.4940 acc=0.8027 | prec=0.7568 rec=0.8909 f1=0.8184 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4909 acc=0.7606 | val_loss=0.4711 acc=0.8141 | prec=0.7805 rec=0.8727 f1=0.8240 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4781 acc=0.7674 | val_loss=0.4660 acc=0.8141 | prec=0.7828 rec=0.8682 f1=0.8233 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4920 acc=0.7584 | val_loss=0.4593 acc=0.8118 | prec=0.7842 rec=0.8591 f1=0.8200 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4756 acc=0.7731 | val_loss=0.4549 acc=0.8141 | prec=0.7828 rec=0.8682 f1=0.8233 | time=12.9s\n",
            "Epoch 049 | train_loss=0.4854 acc=0.7589 | val_loss=0.4391 acc=0.8209 | prec=0.7925 rec=0.8682 f1=0.8286 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4725 acc=0.7862 | val_loss=0.4284 acc=0.8141 | prec=0.8108 rec=0.8182 f1=0.8145 | time=12.7s\n",
            "Epoch 051 | train_loss=0.4736 acc=0.7708 | val_loss=0.4348 acc=0.8163 | prec=0.8357 rec=0.7864 f1=0.8103 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4655 acc=0.7748 | val_loss=0.4336 acc=0.8163 | prec=0.8035 rec=0.8364 f1=0.8196 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4646 acc=0.7754 | val_loss=0.4454 acc=0.8050 | prec=0.8252 rec=0.7727 f1=0.7981 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4458 acc=0.7862 | val_loss=0.4247 acc=0.8231 | prec=0.8170 rec=0.8318 f1=0.8243 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4443 acc=0.7986 | val_loss=0.4204 acc=0.8254 | prec=0.8150 rec=0.8409 f1=0.8277 | time=12.9s\n",
            "Epoch 056 | train_loss=0.4422 acc=0.7782 | val_loss=0.4191 acc=0.8254 | prec=0.8705 rec=0.7636 f1=0.8136 | time=12.8s\n",
            "Epoch 057 | train_loss=0.4243 acc=0.8140 | val_loss=0.4102 acc=0.8277 | prec=0.8243 rec=0.8318 f1=0.8281 | time=12.7s\n",
            "Epoch 058 | train_loss=0.4329 acc=0.8032 | val_loss=0.4202 acc=0.8254 | prec=0.8783 rec=0.7545 f1=0.8117 | time=12.7s\n",
            "Epoch 059 | train_loss=0.4220 acc=0.8032 | val_loss=0.4119 acc=0.8390 | prec=0.8402 rec=0.8364 f1=0.8383 | time=12.8s\n",
            "Epoch 060 | train_loss=0.4206 acc=0.7992 | val_loss=0.4027 acc=0.8390 | prec=0.8066 rec=0.8909 f1=0.8467 | time=12.8s\n",
            "Epoch 061 | train_loss=0.4085 acc=0.8026 | val_loss=0.3960 acc=0.8322 | prec=0.8443 rec=0.8136 f1=0.8287 | time=12.9s\n",
            "Epoch 062 | train_loss=0.4384 acc=0.8015 | val_loss=0.3970 acc=0.8299 | prec=0.8166 rec=0.8500 f1=0.8330 | time=12.8s\n",
            "Epoch 063 | train_loss=0.4118 acc=0.8162 | val_loss=0.4118 acc=0.8299 | prec=0.8139 rec=0.8545 f1=0.8337 | time=12.8s\n",
            "Epoch 064 | train_loss=0.4108 acc=0.8037 | val_loss=0.4028 acc=0.8345 | prec=0.8155 rec=0.8636 f1=0.8389 | time=12.8s\n",
            "Epoch 065 | train_loss=0.4060 acc=0.8259 | val_loss=0.3952 acc=0.8345 | prec=0.8267 rec=0.8455 f1=0.8360 | time=12.9s\n",
            "Epoch 066 | train_loss=0.3959 acc=0.8247 | val_loss=0.3940 acc=0.8345 | prec=0.8621 rec=0.7955 f1=0.8274 | time=12.8s\n",
            "Epoch 067 | train_loss=0.4127 acc=0.8106 | val_loss=0.3892 acc=0.8277 | prec=0.8495 rec=0.7955 f1=0.8216 | time=12.8s\n",
            "Epoch 068 | train_loss=0.3970 acc=0.8208 | val_loss=0.3889 acc=0.8390 | prec=0.8225 rec=0.8636 f1=0.8426 | time=12.7s\n",
            "Epoch 069 | train_loss=0.4060 acc=0.8174 | val_loss=0.3814 acc=0.8345 | prec=0.8483 rec=0.8136 f1=0.8306 | time=12.8s\n",
            "Epoch 070 | train_loss=0.4316 acc=0.7986 | val_loss=0.4027 acc=0.8254 | prec=0.8357 rec=0.8091 f1=0.8222 | time=12.9s\n",
            "Epoch 071 | train_loss=0.3998 acc=0.8123 | val_loss=0.4094 acc=0.8367 | prec=0.8217 rec=0.8591 f1=0.8400 | time=12.9s\n",
            "Epoch 072 | train_loss=0.3999 acc=0.8145 | val_loss=0.4013 acc=0.8367 | prec=0.8136 rec=0.8727 f1=0.8421 | time=12.8s\n",
            "Epoch 073 | train_loss=0.4002 acc=0.8077 | val_loss=0.3992 acc=0.8254 | prec=0.8629 rec=0.7727 f1=0.8153 | time=12.9s\n",
            "Epoch 074 | train_loss=0.3862 acc=0.8230 | val_loss=0.3913 acc=0.8345 | prec=0.8296 rec=0.8409 f1=0.8352 | time=12.9s\n",
            "Epoch 075 | train_loss=0.4049 acc=0.8134 | val_loss=0.4049 acc=0.8345 | prec=0.8155 rec=0.8636 f1=0.8389 | time=12.9s\n",
            "Epoch 076 | train_loss=0.3850 acc=0.8219 | val_loss=0.3958 acc=0.8254 | prec=0.8206 rec=0.8318 f1=0.8262 | time=12.8s\n",
            "Epoch 077 | train_loss=0.4044 acc=0.8174 | val_loss=0.3965 acc=0.8345 | prec=0.8657 rec=0.7909 f1=0.8266 | time=13.0s\n",
            "Epoch 078 | train_loss=0.3895 acc=0.8145 | val_loss=0.3994 acc=0.8277 | prec=0.8214 rec=0.8364 f1=0.8288 | time=12.8s\n",
            "Epoch 079 | train_loss=0.4135 acc=0.8151 | val_loss=0.3969 acc=0.8390 | prec=0.8170 rec=0.8727 f1=0.8440 | time=12.8s\n",
            "Epoch 080 | train_loss=0.3813 acc=0.8213 | val_loss=0.4008 acc=0.8367 | prec=0.8058 rec=0.8864 f1=0.8442 | time=12.7s\n",
            "Epoch 081 | train_loss=0.3805 acc=0.8230 | val_loss=0.3944 acc=0.8413 | prec=0.8538 rec=0.8227 f1=0.8380 | time=12.8s\n",
            "Epoch 082 | train_loss=0.3893 acc=0.8157 | val_loss=0.3915 acc=0.8367 | prec=0.8558 rec=0.8091 f1=0.8318 | time=12.9s\n",
            "Epoch 083 | train_loss=0.3864 acc=0.8179 | val_loss=0.3937 acc=0.8413 | prec=0.8233 rec=0.8682 f1=0.8451 | time=12.8s\n",
            "Epoch 084 | train_loss=0.4005 acc=0.8100 | val_loss=0.3896 acc=0.8277 | prec=0.8673 rec=0.7727 f1=0.8173 | time=12.8s\n",
            "Early stopping at epoch 84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▃▆▆▆▆▆▆▆▇▇▇▇▇█▇██▇█▇██████▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▅▄▅▆▆▆▆▆▆▇▇▇█▇▇█▇▇▇▇█▇▇██</td></tr><tr><td>recall</td><td>██████████▇▁▅▂▆▆▆▃▄▃▅▅▃▂▄▄▂▁▃▄▄▃▃▃▄▄▄▄▃▂</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▂▁▁▁▁▁▂▁▁▂▁▄▅▅▆▆▇▇▇▇▇▇▇██▇███▇██████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▆▅▄▄▄▄▃▃▃▃▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▃▆▇▇▆▆▇▇▇▇▇▇▇▇█████████████</td></tr><tr><td>validation_loss</td><td>████████████▇▆▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81731</td></tr><tr><td>precision</td><td>0.86735</td></tr><tr><td>recall</td><td>0.77273</td></tr><tr><td>train_accuracy</td><td>0.80998</td></tr><tr><td>train_loss</td><td>0.40055</td></tr><tr><td>validation_accuracy</td><td>0.82766</td></tr><tr><td>validation_loss</td><td>0.38961</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0_fold1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/71huhgrt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/71huhgrt</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_072419-71huhgrt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fold 2 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_074236-mrwzcl0c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/mrwzcl0c' target=\"_blank\">trial0_fold2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/mrwzcl0c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/mrwzcl0c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7927 acc=0.4895 | val_loss=0.7131 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7870 acc=0.4770 | val_loss=0.7104 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7515 acc=0.4986 | val_loss=0.7022 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7752 acc=0.5043 | val_loss=0.6996 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7432 acc=0.5167 | val_loss=0.6986 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7357 acc=0.5094 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7528 acc=0.4827 | val_loss=0.7033 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7519 acc=0.4833 | val_loss=0.7091 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7344 acc=0.5196 | val_loss=0.7086 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7441 acc=0.4787 | val_loss=0.7004 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7258 acc=0.5088 | val_loss=0.7004 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7335 acc=0.5173 | val_loss=0.6961 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7409 acc=0.5111 | val_loss=0.7009 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7303 acc=0.5043 | val_loss=0.6982 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7330 acc=0.5196 | val_loss=0.7003 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7314 acc=0.5139 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7393 acc=0.4878 | val_loss=0.6969 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 018 | train_loss=0.7254 acc=0.5099 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 019 | train_loss=0.7366 acc=0.5082 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7153 acc=0.5241 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 021 | train_loss=0.7225 acc=0.5065 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7253 acc=0.5139 | val_loss=0.6922 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 023 | train_loss=0.7274 acc=0.5082 | val_loss=0.6975 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7108 acc=0.5150 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7135 acc=0.5179 | val_loss=0.6908 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 026 | train_loss=0.7152 acc=0.5213 | val_loss=0.6791 acc=0.7007 | prec=0.8188 rec=0.5136 f1=0.6313 | time=12.7s\n",
            "Epoch 027 | train_loss=0.7084 acc=0.5337 | val_loss=0.6670 acc=0.6122 | prec=0.8657 rec=0.2636 f1=0.4042 | time=12.7s\n",
            "Epoch 028 | train_loss=0.7005 acc=0.5440 | val_loss=0.6369 acc=0.7143 | prec=0.8456 rec=0.5227 f1=0.6461 | time=12.8s\n",
            "Epoch 029 | train_loss=0.6596 acc=0.6058 | val_loss=0.5942 acc=0.7800 | prec=0.8514 rec=0.6773 f1=0.7544 | time=12.8s\n",
            "Epoch 030 | train_loss=0.6351 acc=0.6449 | val_loss=0.5476 acc=0.8141 | prec=0.8450 rec=0.7682 f1=0.8048 | time=12.8s\n",
            "Epoch 031 | train_loss=0.6145 acc=0.6551 | val_loss=0.5304 acc=0.8073 | prec=0.7872 rec=0.8409 f1=0.8132 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5935 acc=0.6727 | val_loss=0.5288 acc=0.8141 | prec=0.7924 rec=0.8500 f1=0.8202 | time=12.7s\n",
            "Epoch 033 | train_loss=0.6053 acc=0.6619 | val_loss=0.5240 acc=0.7846 | prec=0.7341 rec=0.8909 f1=0.8049 | time=12.9s\n",
            "Epoch 034 | train_loss=0.5939 acc=0.6886 | val_loss=0.5142 acc=0.8141 | prec=0.8108 rec=0.8182 f1=0.8145 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5853 acc=0.6869 | val_loss=0.5112 acc=0.8095 | prec=0.7857 rec=0.8500 f1=0.8166 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5651 acc=0.7056 | val_loss=0.4990 acc=0.8141 | prec=0.8000 rec=0.8364 f1=0.8178 | time=12.8s\n",
            "Epoch 037 | train_loss=0.5483 acc=0.7266 | val_loss=0.4935 acc=0.8299 | prec=0.8469 rec=0.8045 f1=0.8252 | time=12.8s\n",
            "Epoch 038 | train_loss=0.5410 acc=0.7283 | val_loss=0.4882 acc=0.8390 | prec=0.8465 rec=0.8273 f1=0.8368 | time=12.7s\n",
            "Epoch 039 | train_loss=0.5413 acc=0.7209 | val_loss=0.4776 acc=0.8186 | prec=0.8500 rec=0.7727 f1=0.8095 | time=12.8s\n",
            "Epoch 040 | train_loss=0.5328 acc=0.7323 | val_loss=0.4803 acc=0.8186 | prec=0.8017 rec=0.8455 f1=0.8230 | time=12.8s\n",
            "Epoch 041 | train_loss=0.5377 acc=0.7408 | val_loss=0.4999 acc=0.7800 | prec=0.7158 rec=0.9273 f1=0.8079 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5256 acc=0.7476 | val_loss=0.4702 acc=0.8231 | prec=0.8817 rec=0.7455 f1=0.8079 | time=12.9s\n",
            "Epoch 043 | train_loss=0.5271 acc=0.7260 | val_loss=0.4796 acc=0.8254 | prec=0.8389 rec=0.8045 f1=0.8213 | time=12.7s\n",
            "Epoch 044 | train_loss=0.5274 acc=0.7374 | val_loss=0.4819 acc=0.8254 | prec=0.7826 rec=0.9000 f1=0.8372 | time=12.8s\n",
            "Epoch 045 | train_loss=0.5138 acc=0.7470 | val_loss=0.4821 acc=0.8027 | prec=0.7418 rec=0.9273 f1=0.8242 | time=12.9s\n",
            "Epoch 046 | train_loss=0.5150 acc=0.7499 | val_loss=0.4602 acc=0.8231 | prec=0.7958 rec=0.8682 f1=0.8304 | time=12.9s\n",
            "Epoch 047 | train_loss=0.4883 acc=0.7714 | val_loss=0.4531 acc=0.8345 | prec=0.8326 rec=0.8364 f1=0.8345 | time=12.8s\n",
            "Epoch 048 | train_loss=0.5015 acc=0.7476 | val_loss=0.4534 acc=0.8209 | prec=0.8052 rec=0.8455 f1=0.8248 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4759 acc=0.7867 | val_loss=0.4457 acc=0.8277 | prec=0.8364 rec=0.8136 f1=0.8249 | time=12.9s\n",
            "Epoch 050 | train_loss=0.4816 acc=0.7748 | val_loss=0.4455 acc=0.8299 | prec=0.8085 rec=0.8636 f1=0.8352 | time=12.9s\n",
            "Epoch 051 | train_loss=0.4560 acc=0.7822 | val_loss=0.4501 acc=0.8345 | prec=0.8075 rec=0.8773 f1=0.8410 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4690 acc=0.7788 | val_loss=0.4501 acc=0.8141 | prec=0.7717 rec=0.8909 f1=0.8270 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4856 acc=0.7822 | val_loss=0.4440 acc=0.8322 | prec=0.7874 rec=0.9091 f1=0.8439 | time=12.7s\n",
            "Epoch 054 | train_loss=0.4730 acc=0.7697 | val_loss=0.4540 acc=0.8209 | prec=0.7722 rec=0.9091 f1=0.8351 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4556 acc=0.7862 | val_loss=0.4480 acc=0.8367 | prec=0.8008 rec=0.8955 f1=0.8455 | time=12.8s\n",
            "Epoch 056 | train_loss=0.4838 acc=0.7828 | val_loss=0.4327 acc=0.8299 | prec=0.8033 rec=0.8727 f1=0.8366 | time=12.8s\n",
            "Epoch 057 | train_loss=0.4533 acc=0.7935 | val_loss=0.4290 acc=0.8254 | prec=0.8178 rec=0.8364 f1=0.8270 | time=12.9s\n",
            "Epoch 058 | train_loss=0.4466 acc=0.7833 | val_loss=0.4168 acc=0.8345 | prec=0.8483 rec=0.8136 f1=0.8306 | time=12.9s\n",
            "Epoch 059 | train_loss=0.4523 acc=0.7867 | val_loss=0.4427 acc=0.8299 | prec=0.7799 rec=0.9182 f1=0.8434 | time=12.9s\n",
            "Epoch 060 | train_loss=0.4381 acc=0.8009 | val_loss=0.4398 acc=0.8299 | prec=0.7866 rec=0.9045 f1=0.8414 | time=12.8s\n",
            "Epoch 061 | train_loss=0.4330 acc=0.7873 | val_loss=0.4258 acc=0.8367 | prec=0.8033 rec=0.8909 f1=0.8448 | time=12.8s\n",
            "Epoch 062 | train_loss=0.4440 acc=0.7901 | val_loss=0.4267 acc=0.8345 | prec=0.7952 rec=0.9000 f1=0.8443 | time=12.7s\n",
            "Epoch 063 | train_loss=0.4419 acc=0.7884 | val_loss=0.4261 acc=0.8299 | prec=0.7935 rec=0.8909 f1=0.8394 | time=12.8s\n",
            "Epoch 064 | train_loss=0.4379 acc=0.7901 | val_loss=0.4215 acc=0.8299 | prec=0.7959 rec=0.8864 f1=0.8387 | time=12.8s\n",
            "Epoch 065 | train_loss=0.4127 acc=0.7952 | val_loss=0.4432 acc=0.8299 | prec=0.7866 rec=0.9045 f1=0.8414 | time=12.8s\n",
            "Epoch 066 | train_loss=0.4243 acc=0.8054 | val_loss=0.4316 acc=0.8231 | prec=0.7795 rec=0.9000 f1=0.8354 | time=12.8s\n",
            "Epoch 067 | train_loss=0.4433 acc=0.8066 | val_loss=0.4149 acc=0.8254 | prec=0.7942 rec=0.8773 f1=0.8337 | time=12.8s\n",
            "Epoch 068 | train_loss=0.4259 acc=0.8117 | val_loss=0.4314 acc=0.8367 | prec=0.8333 rec=0.8409 f1=0.8371 | time=12.8s\n",
            "Epoch 069 | train_loss=0.4218 acc=0.8106 | val_loss=0.4437 acc=0.8277 | prec=0.7769 rec=0.9182 f1=0.8417 | time=12.8s\n",
            "Epoch 070 | train_loss=0.4267 acc=0.7907 | val_loss=0.4201 acc=0.8277 | prec=0.7903 rec=0.8909 f1=0.8376 | time=12.8s\n",
            "Epoch 071 | train_loss=0.4188 acc=0.8179 | val_loss=0.4151 acc=0.8322 | prec=0.7897 rec=0.9045 f1=0.8432 | time=12.9s\n",
            "Epoch 072 | train_loss=0.4198 acc=0.8168 | val_loss=0.4046 acc=0.8435 | prec=0.8268 rec=0.8682 f1=0.8470 | time=12.8s\n",
            "Epoch 073 | train_loss=0.4231 acc=0.8083 | val_loss=0.4228 acc=0.8277 | prec=0.7835 rec=0.9045 f1=0.8397 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3938 acc=0.8236 | val_loss=0.4137 acc=0.8390 | prec=0.8066 rec=0.8909 f1=0.8467 | time=12.8s\n",
            "Epoch 075 | train_loss=0.4100 acc=0.8071 | val_loss=0.4279 acc=0.8186 | prec=0.7632 rec=0.9227 f1=0.8354 | time=12.8s\n",
            "Epoch 076 | train_loss=0.4019 acc=0.8168 | val_loss=0.4085 acc=0.8367 | prec=0.7960 rec=0.9045 f1=0.8468 | time=12.9s\n",
            "Epoch 077 | train_loss=0.4176 acc=0.8202 | val_loss=0.4041 acc=0.8254 | prec=0.8295 rec=0.8182 f1=0.8238 | time=12.8s\n",
            "Epoch 078 | train_loss=0.4216 acc=0.8054 | val_loss=0.4078 acc=0.8367 | prec=0.8058 rec=0.8864 f1=0.8442 | time=12.9s\n",
            "Epoch 079 | train_loss=0.4155 acc=0.8123 | val_loss=0.4164 acc=0.8345 | prec=0.7905 rec=0.9091 f1=0.8457 | time=12.9s\n",
            "Epoch 080 | train_loss=0.4124 acc=0.8191 | val_loss=0.4029 acc=0.8322 | prec=0.7897 rec=0.9045 f1=0.8432 | time=12.8s\n",
            "Epoch 081 | train_loss=0.3959 acc=0.8100 | val_loss=0.3975 acc=0.8413 | prec=0.8348 rec=0.8500 f1=0.8423 | time=12.7s\n",
            "Epoch 082 | train_loss=0.3954 acc=0.8247 | val_loss=0.4157 acc=0.8345 | prec=0.7952 rec=0.9000 f1=0.8443 | time=12.8s\n",
            "Epoch 083 | train_loss=0.3886 acc=0.8361 | val_loss=0.4023 acc=0.8390 | prec=0.8041 rec=0.8955 f1=0.8473 | time=12.8s\n",
            "Epoch 084 | train_loss=0.4105 acc=0.8174 | val_loss=0.4066 acc=0.8277 | prec=0.7857 rec=0.9000 f1=0.8390 | time=12.9s\n",
            "Epoch 085 | train_loss=0.3968 acc=0.8378 | val_loss=0.3989 acc=0.8481 | prec=0.8255 rec=0.8818 f1=0.8527 | time=12.9s\n",
            "Epoch 086 | train_loss=0.3939 acc=0.8202 | val_loss=0.4009 acc=0.8390 | prec=0.7992 rec=0.9045 f1=0.8486 | time=12.8s\n",
            "Epoch 087 | train_loss=0.3902 acc=0.8134 | val_loss=0.4104 acc=0.8367 | prec=0.7984 rec=0.9000 f1=0.8462 | time=12.7s\n",
            "Epoch 088 | train_loss=0.4092 acc=0.8225 | val_loss=0.4082 acc=0.8231 | prec=0.7731 rec=0.9136 f1=0.8375 | time=12.8s\n",
            "Epoch 089 | train_loss=0.3999 acc=0.8128 | val_loss=0.4061 acc=0.8277 | prec=0.7857 rec=0.9000 f1=0.8390 | time=12.7s\n",
            "Epoch 090 | train_loss=0.3940 acc=0.8208 | val_loss=0.4125 acc=0.8390 | prec=0.8016 rec=0.9000 f1=0.8480 | time=12.8s\n",
            "Epoch 091 | train_loss=0.4033 acc=0.8140 | val_loss=0.4114 acc=0.8299 | prec=0.7866 rec=0.9045 f1=0.8414 | time=12.9s\n",
            "Epoch 092 | train_loss=0.4036 acc=0.8174 | val_loss=0.3945 acc=0.8435 | prec=0.8133 rec=0.8909 f1=0.8503 | time=12.8s\n",
            "Epoch 093 | train_loss=0.3929 acc=0.8191 | val_loss=0.4079 acc=0.8367 | prec=0.7984 rec=0.9000 f1=0.8462 | time=12.9s\n",
            "Epoch 094 | train_loss=0.3913 acc=0.8264 | val_loss=0.3924 acc=0.8413 | prec=0.8261 rec=0.8636 f1=0.8444 | time=12.7s\n",
            "Epoch 095 | train_loss=0.3883 acc=0.8213 | val_loss=0.4127 acc=0.8322 | prec=0.7897 rec=0.9045 f1=0.8432 | time=12.9s\n",
            "Epoch 096 | train_loss=0.3923 acc=0.8242 | val_loss=0.4067 acc=0.8345 | prec=0.7928 rec=0.9045 f1=0.8450 | time=12.8s\n",
            "Epoch 097 | train_loss=0.3912 acc=0.8191 | val_loss=0.4145 acc=0.8345 | prec=0.7952 rec=0.9000 f1=0.8443 | time=12.8s\n",
            "Epoch 098 | train_loss=0.3928 acc=0.8259 | val_loss=0.4097 acc=0.8231 | prec=0.7752 rec=0.9091 f1=0.8368 | time=12.9s\n",
            "Epoch 099 | train_loss=0.3862 acc=0.8219 | val_loss=0.4038 acc=0.8390 | prec=0.8016 rec=0.9000 f1=0.8480 | time=12.9s\n",
            "Epoch 100 | train_loss=0.4104 acc=0.8298 | val_loss=0.4078 acc=0.8277 | prec=0.7812 rec=0.9091 f1=0.8403 | time=12.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▆▄▆███████████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁██▇▇███▇▇▇█▇▇▇▇▇▇█▇▇█▇▇▇▇█▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅▇▇█▇███████▇█████████████</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▂▁▂▁▁▁▂▂▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▆▅▄▄▄▄▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▇▇▇▇▇▇█▇███▇▇████████████████</td></tr><tr><td>validation_loss</td><td>███████████████▄▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84034</td></tr><tr><td>precision</td><td>0.78125</td></tr><tr><td>recall</td><td>0.90909</td></tr><tr><td>train_accuracy</td><td>0.82984</td></tr><tr><td>train_loss</td><td>0.41041</td></tr><tr><td>validation_accuracy</td><td>0.82766</td></tr><tr><td>validation_loss</td><td>0.40785</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0_fold2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/mrwzcl0c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/mrwzcl0c</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_074236-mrwzcl0c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fold 3 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_080358-5wk7z64y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/5wk7z64y' target=\"_blank\">trial0_fold3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/5wk7z64y' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/5wk7z64y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8203 acc=0.4844 | val_loss=0.7081 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7810 acc=0.5037 | val_loss=0.6983 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7975 acc=0.4940 | val_loss=0.6998 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7953 acc=0.4895 | val_loss=0.6943 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7983 acc=0.5111 | val_loss=0.6958 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7737 acc=0.5133 | val_loss=0.6932 acc=0.4762 | prec=0.4675 rec=0.3258 f1=0.3840 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7799 acc=0.4929 | val_loss=0.6941 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7600 acc=0.5196 | val_loss=0.6939 acc=0.5011 | prec=0.5014 rec=0.8145 f1=0.6207 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7935 acc=0.4991 | val_loss=0.7058 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7643 acc=0.4923 | val_loss=0.6932 acc=0.4989 | prec=0.5000 rec=0.9955 f1=0.6657 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7609 acc=0.5111 | val_loss=0.6937 acc=0.4898 | prec=0.1667 rec=0.0045 f1=0.0088 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7546 acc=0.5082 | val_loss=0.6961 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 013 | train_loss=0.7608 acc=0.4952 | val_loss=0.6930 acc=0.5147 | prec=0.5113 rec=0.7195 f1=0.5977 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7551 acc=0.4986 | val_loss=0.6948 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7557 acc=0.5065 | val_loss=0.6986 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7633 acc=0.4940 | val_loss=0.6947 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7606 acc=0.4940 | val_loss=0.6974 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7510 acc=0.5071 | val_loss=0.6930 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7608 acc=0.4906 | val_loss=0.6927 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7439 acc=0.5014 | val_loss=0.6935 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.7s\n",
            "Epoch 021 | train_loss=0.7514 acc=0.4878 | val_loss=0.6942 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7479 acc=0.5009 | val_loss=0.6897 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 023 | train_loss=0.7338 acc=0.5213 | val_loss=0.6868 acc=0.6304 | prec=0.5948 rec=0.8235 f1=0.6907 | time=12.7s\n",
            "Epoch 024 | train_loss=0.7421 acc=0.4952 | val_loss=0.6823 acc=0.5442 | prec=0.7778 rec=0.1267 f1=0.2179 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7266 acc=0.5190 | val_loss=0.6683 acc=0.6621 | prec=0.7022 rec=0.5656 f1=0.6266 | time=12.8s\n",
            "Epoch 026 | train_loss=0.6969 acc=0.5536 | val_loss=0.6268 acc=0.7007 | prec=0.6862 rec=0.7421 f1=0.7130 | time=12.8s\n",
            "Epoch 027 | train_loss=0.6495 acc=0.5961 | val_loss=0.5940 acc=0.7052 | prec=0.7407 rec=0.6335 f1=0.6829 | time=12.7s\n",
            "Epoch 028 | train_loss=0.6183 acc=0.6495 | val_loss=0.5621 acc=0.7324 | prec=0.7641 rec=0.6742 f1=0.7163 | time=12.7s\n",
            "Epoch 029 | train_loss=0.6262 acc=0.6444 | val_loss=0.5396 acc=0.7483 | prec=0.7546 rec=0.7376 f1=0.7460 | time=12.8s\n",
            "Epoch 030 | train_loss=0.6008 acc=0.6704 | val_loss=0.5410 acc=0.7596 | prec=0.7291 rec=0.8281 f1=0.7754 | time=12.7s\n",
            "Epoch 031 | train_loss=0.5813 acc=0.6835 | val_loss=0.5177 acc=0.7800 | prec=0.7719 rec=0.7964 f1=0.7840 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5489 acc=0.7311 | val_loss=0.4902 acc=0.7868 | prec=0.8098 rec=0.7511 f1=0.7793 | time=12.8s\n",
            "Epoch 033 | train_loss=0.5558 acc=0.7102 | val_loss=0.4756 acc=0.7914 | prec=0.8241 rec=0.7421 f1=0.7810 | time=12.9s\n",
            "Epoch 034 | train_loss=0.5435 acc=0.7181 | val_loss=0.4765 acc=0.7937 | prec=0.7826 rec=0.8145 f1=0.7982 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5379 acc=0.7181 | val_loss=0.4717 acc=0.8095 | prec=0.8157 rec=0.8009 f1=0.8082 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5162 acc=0.7504 | val_loss=0.4614 acc=0.8118 | prec=0.8080 rec=0.8190 f1=0.8135 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5288 acc=0.7408 | val_loss=0.4674 acc=0.7982 | prec=0.7640 rec=0.8643 f1=0.8110 | time=12.9s\n",
            "Epoch 038 | train_loss=0.5244 acc=0.7379 | val_loss=0.4533 acc=0.8050 | prec=0.7922 rec=0.8281 f1=0.8097 | time=12.8s\n",
            "Epoch 039 | train_loss=0.5041 acc=0.7504 | val_loss=0.4525 acc=0.8118 | prec=0.8286 rec=0.7873 f1=0.8074 | time=12.8s\n",
            "Epoch 040 | train_loss=0.5044 acc=0.7493 | val_loss=0.4595 acc=0.8163 | prec=0.8017 rec=0.8416 f1=0.8212 | time=12.8s\n",
            "Epoch 041 | train_loss=0.5093 acc=0.7606 | val_loss=0.4466 acc=0.8231 | prec=0.8421 rec=0.7964 f1=0.8186 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5040 acc=0.7623 | val_loss=0.4422 acc=0.8209 | prec=0.8515 rec=0.7783 f1=0.8132 | time=12.9s\n",
            "Epoch 043 | train_loss=0.5007 acc=0.7561 | val_loss=0.4302 acc=0.8231 | prec=0.8326 rec=0.8100 f1=0.8211 | time=12.9s\n",
            "Epoch 044 | train_loss=0.4871 acc=0.7589 | val_loss=0.4424 acc=0.8231 | prec=0.8783 rec=0.7511 f1=0.8098 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4806 acc=0.7657 | val_loss=0.4238 acc=0.8322 | prec=0.8419 rec=0.8190 f1=0.8303 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4864 acc=0.7623 | val_loss=0.4528 acc=0.8095 | prec=0.8870 rec=0.7104 f1=0.7889 | time=12.7s\n",
            "Epoch 047 | train_loss=0.4693 acc=0.7742 | val_loss=0.4238 acc=0.8367 | prec=0.8634 rec=0.8009 f1=0.8310 | time=13.0s\n",
            "Epoch 048 | train_loss=0.4808 acc=0.7691 | val_loss=0.4352 acc=0.8254 | prec=0.7927 rec=0.8824 f1=0.8351 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4827 acc=0.7623 | val_loss=0.4286 acc=0.8345 | prec=0.8627 rec=0.7964 f1=0.8282 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4760 acc=0.7589 | val_loss=0.4226 acc=0.8390 | prec=0.8750 rec=0.7919 f1=0.8314 | time=12.8s\n",
            "Epoch 051 | train_loss=0.4829 acc=0.7714 | val_loss=0.4269 acc=0.8390 | prec=0.8713 rec=0.7964 f1=0.8322 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4574 acc=0.7742 | val_loss=0.4288 acc=0.8231 | prec=0.8783 rec=0.7511 f1=0.8098 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4726 acc=0.7748 | val_loss=0.4162 acc=0.8277 | prec=0.8718 rec=0.7692 f1=0.8173 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4615 acc=0.7896 | val_loss=0.4274 acc=0.8299 | prec=0.8967 rec=0.7466 f1=0.8148 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4414 acc=0.7930 | val_loss=0.4127 acc=0.8481 | prec=0.8500 rec=0.8462 f1=0.8481 | time=12.8s\n",
            "Epoch 056 | train_loss=0.4464 acc=0.7674 | val_loss=0.4144 acc=0.8322 | prec=0.8808 rec=0.7692 f1=0.8213 | time=12.9s\n",
            "Epoch 057 | train_loss=0.4316 acc=0.8015 | val_loss=0.4359 acc=0.8141 | prec=0.8927 rec=0.7149 f1=0.7940 | time=12.9s\n",
            "Epoch 058 | train_loss=0.4334 acc=0.8009 | val_loss=0.4424 acc=0.8005 | prec=0.8982 rec=0.6787 f1=0.7732 | time=12.9s\n",
            "Epoch 059 | train_loss=0.4236 acc=0.7981 | val_loss=0.4319 acc=0.8299 | prec=0.8967 rec=0.7466 f1=0.8148 | time=12.8s\n",
            "Epoch 060 | train_loss=0.4333 acc=0.7930 | val_loss=0.4358 acc=0.8186 | prec=0.9075 rec=0.7104 f1=0.7970 | time=12.9s\n",
            "Epoch 061 | train_loss=0.4229 acc=0.7998 | val_loss=0.4078 acc=0.8390 | prec=0.8750 rec=0.7919 f1=0.8314 | time=12.8s\n",
            "Epoch 062 | train_loss=0.4189 acc=0.7918 | val_loss=0.4025 acc=0.8549 | prec=0.8618 rec=0.8462 f1=0.8539 | time=12.9s\n",
            "Epoch 063 | train_loss=0.4207 acc=0.8134 | val_loss=0.4351 acc=0.8118 | prec=0.9012 rec=0.7014 f1=0.7888 | time=12.8s\n",
            "Epoch 064 | train_loss=0.4173 acc=0.8071 | val_loss=0.4342 acc=0.8027 | prec=0.8941 rec=0.6878 f1=0.7775 | time=12.7s\n",
            "Epoch 065 | train_loss=0.4340 acc=0.7992 | val_loss=0.4556 acc=0.7891 | prec=0.9051 rec=0.6471 f1=0.7546 | time=12.9s\n",
            "Epoch 066 | train_loss=0.4160 acc=0.8054 | val_loss=0.4244 acc=0.8163 | prec=0.8933 rec=0.7195 f1=0.7970 | time=12.9s\n",
            "Epoch 067 | train_loss=0.4125 acc=0.8106 | val_loss=0.4162 acc=0.8254 | prec=0.8830 rec=0.7511 f1=0.8117 | time=12.7s\n",
            "Epoch 068 | train_loss=0.4028 acc=0.8060 | val_loss=0.4387 acc=0.8095 | prec=0.9006 rec=0.6968 f1=0.7857 | time=12.9s\n",
            "Epoch 069 | train_loss=0.4112 acc=0.8134 | val_loss=0.4186 acc=0.8163 | prec=0.8933 rec=0.7195 f1=0.7970 | time=12.7s\n",
            "Epoch 070 | train_loss=0.3958 acc=0.8140 | val_loss=0.4173 acc=0.8277 | prec=0.8919 rec=0.7466 f1=0.8128 | time=12.9s\n",
            "Epoch 071 | train_loss=0.4083 acc=0.8060 | val_loss=0.4355 acc=0.8005 | prec=0.9030 rec=0.6742 f1=0.7720 | time=12.9s\n",
            "Epoch 072 | train_loss=0.4069 acc=0.8009 | val_loss=0.4434 acc=0.8005 | prec=0.9030 rec=0.6742 f1=0.7720 | time=12.8s\n",
            "Epoch 073 | train_loss=0.3932 acc=0.8083 | val_loss=0.4115 acc=0.8435 | prec=0.8762 rec=0.8009 f1=0.8369 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3894 acc=0.8213 | val_loss=0.4212 acc=0.8186 | prec=0.8939 rec=0.7240 f1=0.8000 | time=12.9s\n",
            "Epoch 075 | train_loss=0.3860 acc=0.8179 | val_loss=0.4496 acc=0.7891 | prec=0.9000 rec=0.6516 f1=0.7559 | time=12.9s\n",
            "Epoch 076 | train_loss=0.3901 acc=0.8083 | val_loss=0.4284 acc=0.8254 | prec=0.9045 rec=0.7285 f1=0.8070 | time=12.9s\n",
            "Epoch 077 | train_loss=0.3877 acc=0.8208 | val_loss=0.4335 acc=0.8050 | prec=0.9042 rec=0.6833 f1=0.7784 | time=12.8s\n",
            "Early stopping at epoch 77\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▆▁▁▁▁▁▇▁▇▃▇▇▇▇██████████████▇▇█▇███</td></tr><tr><td>precision</td><td>▁▁▅▅▂▁▁▅▁▅▆▆▇▇▇▇▇▇▇▇██▇█████████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▃▁█▁▆▁▁▁█▁▅▅▆▇▇▇▇▇▇▇▆▇▇▆▆▇▆▆▇▆▆▆▆▆▆▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▁▁▁▁▁▁▁▂▁▂▃▄▅▆▆▇▇▇▇▇▇▇▇██▇█████████</td></tr><tr><td>train_loss</td><td>███▇█▇▇▇▇▇▇▇▅▅▅▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▄▂▅▇▇▇▇▇▇▇███▇██▇█▇██▇▇▇▇█▇</td></tr><tr><td>validation_loss</td><td>██████████▇▇▆▂▂▂▂▂▂▂▁▂▂▂▁▁▂▁▂▂▂▁▁▂▂▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.77835</td></tr><tr><td>precision</td><td>0.90419</td></tr><tr><td>recall</td><td>0.68326</td></tr><tr><td>train_accuracy</td><td>0.82076</td></tr><tr><td>train_loss</td><td>0.3877</td></tr><tr><td>validation_accuracy</td><td>0.80499</td></tr><tr><td>validation_loss</td><td>0.43354</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0_fold3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/5wk7z64y' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/5wk7z64y</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_080358-5wk7z64y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fold 4 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_082027-fqx21z0c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/fqx21z0c' target=\"_blank\">trial0_fold4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/fqx21z0c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/fqx21z0c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8055 acc=0.4974 | val_loss=0.7461 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 002 | train_loss=0.8293 acc=0.4889 | val_loss=0.7392 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 003 | train_loss=0.8069 acc=0.4884 | val_loss=0.7342 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7613 acc=0.5099 | val_loss=0.7237 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7829 acc=0.4952 | val_loss=0.7152 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7763 acc=0.4918 | val_loss=0.7115 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7760 acc=0.5088 | val_loss=0.7078 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7643 acc=0.5060 | val_loss=0.7095 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.7s\n",
            "Epoch 009 | train_loss=0.7473 acc=0.5332 | val_loss=0.7042 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7862 acc=0.4872 | val_loss=0.7052 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7710 acc=0.4923 | val_loss=0.7044 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7618 acc=0.5026 | val_loss=0.7028 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7581 acc=0.4986 | val_loss=0.7031 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7567 acc=0.5003 | val_loss=0.6973 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7535 acc=0.4867 | val_loss=0.7042 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7462 acc=0.5184 | val_loss=0.7079 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7387 acc=0.5184 | val_loss=0.6995 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7310 acc=0.5218 | val_loss=0.6996 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.7s\n",
            "Epoch 019 | train_loss=0.7714 acc=0.4759 | val_loss=0.7001 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7245 acc=0.5292 | val_loss=0.7038 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7357 acc=0.5235 | val_loss=0.6970 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7327 acc=0.4963 | val_loss=0.6981 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7291 acc=0.5179 | val_loss=0.6913 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7219 acc=0.5150 | val_loss=0.6950 acc=0.5011 | prec=0.5011 rec=1.0000 f1=0.6677 | time=12.7s\n",
            "Epoch 025 | train_loss=0.7151 acc=0.5258 | val_loss=0.6828 acc=0.4989 | prec=0.5000 rec=0.9955 f1=0.6657 | time=12.8s\n",
            "Epoch 026 | train_loss=0.7022 acc=0.5400 | val_loss=0.6573 acc=0.5918 | prec=0.5544 rec=0.9457 f1=0.6990 | time=12.9s\n",
            "Epoch 027 | train_loss=0.6808 acc=0.5752 | val_loss=0.6069 acc=0.7596 | prec=0.7556 rec=0.7692 f1=0.7623 | time=12.7s\n",
            "Epoch 028 | train_loss=0.6455 acc=0.6200 | val_loss=0.5692 acc=0.7642 | prec=0.7721 rec=0.7511 f1=0.7615 | time=12.9s\n",
            "Epoch 029 | train_loss=0.6182 acc=0.6393 | val_loss=0.5840 acc=0.7188 | prec=0.6580 rec=0.9140 f1=0.7652 | time=12.8s\n",
            "Epoch 030 | train_loss=0.5895 acc=0.6716 | val_loss=0.5298 acc=0.7732 | prec=0.7430 rec=0.8371 f1=0.7872 | time=12.8s\n",
            "Epoch 031 | train_loss=0.5816 acc=0.6659 | val_loss=0.5251 acc=0.7778 | prec=0.7595 rec=0.8145 f1=0.7860 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5579 acc=0.6960 | val_loss=0.5017 acc=0.7937 | prec=0.7928 rec=0.7964 f1=0.7946 | time=12.9s\n",
            "Epoch 033 | train_loss=0.5871 acc=0.6926 | val_loss=0.4878 acc=0.7937 | prec=0.7851 rec=0.8100 f1=0.7973 | time=12.7s\n",
            "Epoch 034 | train_loss=0.5421 acc=0.7147 | val_loss=0.5017 acc=0.7937 | prec=0.7851 rec=0.8100 f1=0.7973 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5465 acc=0.7192 | val_loss=0.4897 acc=0.7732 | prec=0.7391 rec=0.8462 f1=0.7890 | time=12.9s\n",
            "Epoch 036 | train_loss=0.5357 acc=0.7311 | val_loss=0.4781 acc=0.7891 | prec=0.7645 rec=0.8371 f1=0.7991 | time=12.8s\n",
            "Epoch 037 | train_loss=0.5118 acc=0.7362 | val_loss=0.4779 acc=0.8231 | prec=0.8295 rec=0.8145 f1=0.8219 | time=12.9s\n",
            "Epoch 038 | train_loss=0.5124 acc=0.7476 | val_loss=0.4765 acc=0.7959 | prec=0.7718 rec=0.8416 f1=0.8052 | time=13.0s\n",
            "Epoch 039 | train_loss=0.5121 acc=0.7584 | val_loss=0.4644 acc=0.8163 | prec=0.8043 rec=0.8371 f1=0.8204 | time=12.8s\n",
            "Epoch 040 | train_loss=0.4917 acc=0.7538 | val_loss=0.4676 acc=0.8050 | prec=0.8771 rec=0.7104 f1=0.7850 | time=13.0s\n",
            "Epoch 041 | train_loss=0.4890 acc=0.7601 | val_loss=0.4568 acc=0.8005 | prec=0.7649 rec=0.8688 f1=0.8136 | time=12.8s\n",
            "Epoch 042 | train_loss=0.4888 acc=0.7493 | val_loss=0.4613 acc=0.8231 | prec=0.8095 rec=0.8462 f1=0.8274 | time=12.9s\n",
            "Epoch 043 | train_loss=0.4924 acc=0.7635 | val_loss=0.4506 acc=0.8231 | prec=0.8454 rec=0.7919 f1=0.8178 | time=12.8s\n",
            "Epoch 044 | train_loss=0.5052 acc=0.7731 | val_loss=0.4512 acc=0.8073 | prec=0.7906 rec=0.8371 f1=0.8132 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4613 acc=0.7811 | val_loss=0.4536 acc=0.8141 | prec=0.8174 rec=0.8100 f1=0.8136 | time=12.7s\n",
            "Epoch 046 | train_loss=0.4715 acc=0.7839 | val_loss=0.4589 acc=0.8050 | prec=0.7689 rec=0.8733 f1=0.8178 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4715 acc=0.7623 | val_loss=0.4740 acc=0.7755 | prec=0.7243 rec=0.8914 f1=0.7992 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4362 acc=0.8071 | val_loss=0.4719 acc=0.7732 | prec=0.7354 rec=0.8552 f1=0.7908 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4565 acc=0.7941 | val_loss=0.4607 acc=0.7823 | prec=0.7490 rec=0.8507 f1=0.7966 | time=12.7s\n",
            "Epoch 050 | train_loss=0.4437 acc=0.8077 | val_loss=0.4501 acc=0.7891 | prec=0.7623 rec=0.8416 f1=0.8000 | time=12.7s\n",
            "Epoch 051 | train_loss=0.4565 acc=0.7845 | val_loss=0.4927 acc=0.7506 | prec=0.6934 rec=0.9005 f1=0.7835 | time=12.9s\n",
            "Epoch 052 | train_loss=0.4588 acc=0.7720 | val_loss=0.4477 acc=0.7914 | prec=0.7633 rec=0.8462 f1=0.8026 | time=12.9s\n",
            "Epoch 053 | train_loss=0.4389 acc=0.8015 | val_loss=0.4635 acc=0.7891 | prec=0.7424 rec=0.8869 f1=0.8082 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4426 acc=0.7986 | val_loss=0.4948 acc=0.7551 | prec=0.6955 rec=0.9095 f1=0.7882 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4137 acc=0.8020 | val_loss=0.4434 acc=0.8050 | prec=0.7897 rec=0.8326 f1=0.8106 | time=12.7s\n",
            "Epoch 056 | train_loss=0.4352 acc=0.8003 | val_loss=0.4356 acc=0.8163 | prec=0.8465 rec=0.7738 f1=0.8085 | time=12.7s\n",
            "Epoch 057 | train_loss=0.4367 acc=0.7952 | val_loss=0.4594 acc=0.7800 | prec=0.7331 rec=0.8824 f1=0.8008 | time=12.8s\n",
            "Epoch 058 | train_loss=0.4256 acc=0.7952 | val_loss=0.4725 acc=0.7551 | prec=0.6969 rec=0.9050 f1=0.7874 | time=12.8s\n",
            "Epoch 059 | train_loss=0.4495 acc=0.7901 | val_loss=0.4583 acc=0.7823 | prec=0.7341 rec=0.8869 f1=0.8033 | time=12.8s\n",
            "Epoch 060 | train_loss=0.4447 acc=0.7907 | val_loss=0.4752 acc=0.7574 | prec=0.7007 rec=0.9005 f1=0.7881 | time=12.8s\n",
            "Epoch 061 | train_loss=0.4181 acc=0.8054 | val_loss=0.4408 acc=0.7959 | prec=0.7529 rec=0.8824 f1=0.8125 | time=12.7s\n",
            "Epoch 062 | train_loss=0.4179 acc=0.8049 | val_loss=0.4432 acc=0.7823 | prec=0.7376 rec=0.8778 f1=0.8017 | time=12.8s\n",
            "Epoch 063 | train_loss=0.4194 acc=0.8032 | val_loss=0.4487 acc=0.7800 | prec=0.7331 rec=0.8824 f1=0.8008 | time=12.9s\n",
            "Epoch 064 | train_loss=0.4059 acc=0.8213 | val_loss=0.4539 acc=0.7800 | prec=0.7296 rec=0.8914 f1=0.8024 | time=12.7s\n",
            "Epoch 065 | train_loss=0.4143 acc=0.8037 | val_loss=0.4468 acc=0.7846 | prec=0.7423 rec=0.8733 f1=0.8025 | time=12.8s\n",
            "Epoch 066 | train_loss=0.4100 acc=0.8066 | val_loss=0.4661 acc=0.7642 | prec=0.7097 rec=0.8959 f1=0.7920 | time=12.8s\n",
            "Epoch 067 | train_loss=0.3998 acc=0.8128 | val_loss=0.4667 acc=0.7664 | prec=0.7122 rec=0.8959 f1=0.7936 | time=12.8s\n",
            "Epoch 068 | train_loss=0.3969 acc=0.8242 | val_loss=0.4802 acc=0.7460 | prec=0.6847 rec=0.9140 f1=0.7829 | time=12.8s\n",
            "Epoch 069 | train_loss=0.3854 acc=0.8225 | val_loss=0.4559 acc=0.7710 | prec=0.7256 rec=0.8733 f1=0.7926 | time=12.8s\n",
            "Epoch 070 | train_loss=0.3972 acc=0.8202 | val_loss=0.4432 acc=0.7891 | prec=0.7481 rec=0.8733 f1=0.8058 | time=12.8s\n",
            "Epoch 071 | train_loss=0.4000 acc=0.8134 | val_loss=0.4391 acc=0.7914 | prec=0.7529 rec=0.8688 f1=0.8067 | time=12.8s\n",
            "Early stopping at epoch 71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▅▅▆▇▇█▇▇██▇█▆▇▇▆▇▇▇▇▆▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▆▄▆▆▆▆▇█▇▇▆▅▅▆▆▆▅▅▆▇▅▅▅▆▅▅▅▅▅▆</td></tr><tr><td>recall</td><td>███████████████▇▂▂▄▃▄▄▄▄▁▄▄▅▄▄▄▆▄▅▅▅▅▅▆▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▁▁▁▂▂▁▂▂▂▃▄▅▅▆▆▇▇▇▇▇▇▇▇███▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▆▆▆▆▆▅▄▄▄▃▃▃▂▂▃▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▇▆▇▇▇▇██████▇▇▇▆▇█▇▇▇▇▇▇▇▆▇</td></tr><tr><td>validation_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▇▄▃▃▂▂▂▂▂▂▂▁▁▁▂▂▁▂▂▁▂▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.80672</td></tr><tr><td>precision</td><td>0.75294</td></tr><tr><td>recall</td><td>0.86878</td></tr><tr><td>train_accuracy</td><td>0.81339</td></tr><tr><td>train_loss</td><td>0.39998</td></tr><tr><td>validation_accuracy</td><td>0.79138</td></tr><tr><td>validation_loss</td><td>0.43911</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0_fold4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/fqx21z0c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/fqx21z0c</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_082027-fqx21z0c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Fold 5 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_083538-r4mbomvh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/r4mbomvh' target=\"_blank\">trial0_fold5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/r4mbomvh' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/r4mbomvh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7786 acc=0.4864 | val_loss=0.6928 acc=0.5273 | prec=0.5577 rec=0.2636 f1=0.3580 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7680 acc=0.4870 | val_loss=0.6929 acc=0.5000 | prec=0.5000 rec=0.9591 f1=0.6573 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7601 acc=0.5079 | val_loss=0.6931 acc=0.5023 | prec=0.5015 rec=0.7636 f1=0.6054 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7467 acc=0.5125 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=0.9955 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7440 acc=0.4994 | val_loss=0.6933 acc=0.4818 | prec=0.4898 rec=0.8727 f1=0.6275 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7638 acc=0.4853 | val_loss=0.6930 acc=0.5045 | prec=0.5024 rec=0.9364 f1=0.6540 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7372 acc=0.5119 | val_loss=0.6930 acc=0.4886 | prec=0.4941 rec=0.9500 f1=0.6501 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7490 acc=0.4966 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=1.0000 f1=0.6667 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7406 acc=0.5023 | val_loss=0.6931 acc=0.5205 | prec=0.5517 rec=0.2182 f1=0.3127 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7312 acc=0.5040 | val_loss=0.6928 acc=0.5000 | prec=0.5000 rec=1.0000 f1=0.6667 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7314 acc=0.5011 | val_loss=0.6931 acc=0.5023 | prec=0.5011 rec=0.9955 f1=0.6667 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7300 acc=0.5000 | val_loss=0.6931 acc=0.5045 | prec=0.6250 rec=0.0227 f1=0.0439 | time=13.2s\n",
            "Epoch 013 | train_loss=0.7253 acc=0.5164 | val_loss=0.6938 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 014 | train_loss=0.7174 acc=0.5170 | val_loss=0.6948 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7326 acc=0.4977 | val_loss=0.6930 acc=0.5159 | prec=0.5086 rec=0.9364 f1=0.6592 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7344 acc=0.4932 | val_loss=0.6956 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7379 acc=0.4824 | val_loss=0.6951 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7247 acc=0.4904 | val_loss=0.6949 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7174 acc=0.5011 | val_loss=0.6945 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7243 acc=0.4915 | val_loss=0.6952 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7233 acc=0.5091 | val_loss=0.6950 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7185 acc=0.4994 | val_loss=0.6944 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7274 acc=0.5023 | val_loss=0.6936 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 024 | train_loss=0.7173 acc=0.5153 | val_loss=0.6944 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7199 acc=0.5028 | val_loss=0.6924 acc=0.5159 | prec=0.8889 rec=0.0364 f1=0.0699 | time=12.9s\n",
            "Epoch 026 | train_loss=0.7309 acc=0.4841 | val_loss=0.6923 acc=0.5091 | prec=0.8333 rec=0.0227 f1=0.0442 | time=12.9s\n",
            "Epoch 027 | train_loss=0.7197 acc=0.5028 | val_loss=0.6930 acc=0.5000 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 028 | train_loss=0.7178 acc=0.5136 | val_loss=0.6906 acc=0.6705 | prec=0.6415 rec=0.7727 f1=0.7010 | time=12.9s\n",
            "Epoch 029 | train_loss=0.7159 acc=0.4983 | val_loss=0.6905 acc=0.5000 | prec=0.5000 rec=1.0000 f1=0.6667 | time=12.8s\n",
            "Epoch 030 | train_loss=0.7119 acc=0.5176 | val_loss=0.6899 acc=0.5500 | prec=0.5274 rec=0.9636 f1=0.6817 | time=12.9s\n",
            "Epoch 031 | train_loss=0.7178 acc=0.5091 | val_loss=0.6894 acc=0.5318 | prec=1.0000 rec=0.0636 f1=0.1197 | time=12.8s\n",
            "Epoch 032 | train_loss=0.7167 acc=0.4904 | val_loss=0.6884 acc=0.5023 | prec=1.0000 rec=0.0045 f1=0.0090 | time=12.8s\n",
            "Epoch 033 | train_loss=0.7120 acc=0.5238 | val_loss=0.6827 acc=0.5364 | prec=0.5191 rec=0.9864 f1=0.6803 | time=12.8s\n",
            "Epoch 034 | train_loss=0.7119 acc=0.5034 | val_loss=0.6744 acc=0.6182 | prec=0.8611 rec=0.2818 f1=0.4247 | time=12.7s\n",
            "Epoch 035 | train_loss=0.6994 acc=0.5300 | val_loss=0.6633 acc=0.6955 | prec=0.8468 rec=0.4773 f1=0.6105 | time=12.9s\n",
            "Epoch 036 | train_loss=0.7000 acc=0.5210 | val_loss=0.6470 acc=0.6841 | prec=0.8716 rec=0.4318 f1=0.5775 | time=12.8s\n",
            "Epoch 037 | train_loss=0.6874 acc=0.5482 | val_loss=0.6328 acc=0.7114 | prec=0.8394 rec=0.5227 f1=0.6443 | time=12.9s\n",
            "Epoch 038 | train_loss=0.6664 acc=0.5901 | val_loss=0.6076 acc=0.7750 | prec=0.8135 rec=0.7136 f1=0.7603 | time=12.8s\n",
            "Epoch 039 | train_loss=0.6438 acc=0.6207 | val_loss=0.5794 acc=0.7955 | prec=0.8351 rec=0.7364 f1=0.7826 | time=12.9s\n",
            "Epoch 040 | train_loss=0.6177 acc=0.6525 | val_loss=0.5427 acc=0.7909 | prec=0.7936 rec=0.7864 f1=0.7900 | time=12.8s\n",
            "Epoch 041 | train_loss=0.5912 acc=0.6797 | val_loss=0.5343 acc=0.7977 | prec=0.7787 rec=0.8318 f1=0.8044 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5923 acc=0.6888 | val_loss=0.5043 acc=0.8091 | prec=0.8908 rec=0.7045 f1=0.7868 | time=12.8s\n",
            "Epoch 043 | train_loss=0.5715 acc=0.6865 | val_loss=0.4857 acc=0.8182 | prec=0.8933 rec=0.7227 f1=0.7990 | time=12.9s\n",
            "Epoch 044 | train_loss=0.5638 acc=0.7120 | val_loss=0.4717 acc=0.8159 | prec=0.8357 rec=0.7864 f1=0.8103 | time=12.9s\n",
            "Epoch 045 | train_loss=0.5570 acc=0.7154 | val_loss=0.4747 acc=0.8273 | prec=0.8396 rec=0.8091 f1=0.8241 | time=12.8s\n",
            "Epoch 046 | train_loss=0.5481 acc=0.7217 | val_loss=0.4657 acc=0.8227 | prec=0.8287 rec=0.8136 f1=0.8211 | time=12.8s\n",
            "Epoch 047 | train_loss=0.5371 acc=0.7239 | val_loss=0.4685 acc=0.8295 | prec=0.8404 rec=0.8136 f1=0.8268 | time=12.8s\n",
            "Epoch 048 | train_loss=0.5123 acc=0.7466 | val_loss=0.4595 acc=0.8091 | prec=0.8696 rec=0.7273 f1=0.7921 | time=12.9s\n",
            "Epoch 049 | train_loss=0.5251 acc=0.7324 | val_loss=0.4663 acc=0.8205 | prec=0.8052 rec=0.8455 f1=0.8248 | time=12.8s\n",
            "Epoch 050 | train_loss=0.5230 acc=0.7307 | val_loss=0.4538 acc=0.8227 | prec=0.8034 rec=0.8545 f1=0.8282 | time=12.9s\n",
            "Epoch 051 | train_loss=0.5183 acc=0.7455 | val_loss=0.4548 acc=0.8318 | prec=0.8443 rec=0.8136 f1=0.8287 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4979 acc=0.7545 | val_loss=0.4492 acc=0.8295 | prec=0.8085 rec=0.8636 f1=0.8352 | time=12.8s\n",
            "Epoch 053 | train_loss=0.5087 acc=0.7704 | val_loss=0.4752 acc=0.8068 | prec=0.7606 rec=0.8955 f1=0.8225 | time=12.9s\n",
            "Epoch 054 | train_loss=0.5027 acc=0.7596 | val_loss=0.4503 acc=0.8250 | prec=0.8069 rec=0.8545 f1=0.8300 | time=12.9s\n",
            "Epoch 055 | train_loss=0.5050 acc=0.7551 | val_loss=0.4396 acc=0.8295 | prec=0.8680 rec=0.7773 f1=0.8201 | time=12.8s\n",
            "Epoch 056 | train_loss=0.4864 acc=0.7698 | val_loss=0.4455 acc=0.8341 | prec=0.8296 rec=0.8409 f1=0.8352 | time=12.7s\n",
            "Epoch 057 | train_loss=0.4866 acc=0.7761 | val_loss=0.4459 acc=0.8250 | prec=0.8865 rec=0.7455 f1=0.8099 | time=12.9s\n",
            "Epoch 058 | train_loss=0.4715 acc=0.7874 | val_loss=0.4415 acc=0.8455 | prec=0.8276 rec=0.8727 f1=0.8496 | time=13.1s\n",
            "Epoch 059 | train_loss=0.4680 acc=0.7817 | val_loss=0.4305 acc=0.8318 | prec=0.8476 rec=0.8091 f1=0.8279 | time=13.0s\n",
            "Epoch 060 | train_loss=0.4743 acc=0.7800 | val_loss=0.4266 acc=0.8318 | prec=0.8883 rec=0.7591 f1=0.8186 | time=12.8s\n",
            "Epoch 061 | train_loss=0.4647 acc=0.7795 | val_loss=0.4388 acc=0.8136 | prec=0.8632 rec=0.7455 f1=0.8000 | time=13.0s\n",
            "Epoch 062 | train_loss=0.4567 acc=0.7925 | val_loss=0.4398 acc=0.8250 | prec=0.8783 rec=0.7545 f1=0.8117 | time=13.0s\n",
            "Epoch 063 | train_loss=0.4527 acc=0.7976 | val_loss=0.4266 acc=0.8341 | prec=0.8769 rec=0.7773 f1=0.8241 | time=13.1s\n",
            "Epoch 064 | train_loss=0.4862 acc=0.7851 | val_loss=0.4250 acc=0.8295 | prec=0.8962 rec=0.7455 f1=0.8139 | time=12.9s\n",
            "Epoch 065 | train_loss=0.4590 acc=0.7988 | val_loss=0.4291 acc=0.8318 | prec=0.8411 rec=0.8182 f1=0.8295 | time=12.8s\n",
            "Epoch 066 | train_loss=0.4699 acc=0.7817 | val_loss=0.4240 acc=0.8341 | prec=0.8483 rec=0.8136 f1=0.8306 | time=12.9s\n",
            "Epoch 067 | train_loss=0.4780 acc=0.7846 | val_loss=0.4355 acc=0.8318 | prec=0.7874 rec=0.9091 f1=0.8439 | time=13.1s\n",
            "Epoch 068 | train_loss=0.4468 acc=0.7942 | val_loss=0.4330 acc=0.8295 | prec=0.8718 rec=0.7727 f1=0.8193 | time=13.0s\n",
            "Epoch 069 | train_loss=0.4282 acc=0.8112 | val_loss=0.4251 acc=0.8295 | prec=0.8962 rec=0.7455 f1=0.8139 | time=13.0s\n",
            "Epoch 070 | train_loss=0.4759 acc=0.7783 | val_loss=0.4219 acc=0.8273 | prec=0.8871 rec=0.7500 f1=0.8128 | time=13.0s\n",
            "Epoch 071 | train_loss=0.4463 acc=0.7942 | val_loss=0.4054 acc=0.8432 | prec=0.8416 rec=0.8455 f1=0.8435 | time=12.9s\n",
            "Epoch 072 | train_loss=0.4404 acc=0.7897 | val_loss=0.4066 acc=0.8364 | prec=0.8394 rec=0.8318 f1=0.8356 | time=13.0s\n",
            "Epoch 073 | train_loss=0.4337 acc=0.8152 | val_loss=0.4199 acc=0.8386 | prec=0.8433 rec=0.8318 f1=0.8375 | time=12.7s\n",
            "Epoch 074 | train_loss=0.4501 acc=0.7976 | val_loss=0.4367 acc=0.8318 | prec=0.8842 rec=0.7636 f1=0.8195 | time=13.0s\n",
            "Epoch 075 | train_loss=0.4415 acc=0.8033 | val_loss=0.4186 acc=0.8295 | prec=0.8502 rec=0.8000 f1=0.8244 | time=12.9s\n",
            "Epoch 076 | train_loss=0.4487 acc=0.8005 | val_loss=0.4278 acc=0.8273 | prec=0.8750 rec=0.7636 f1=0.8155 | time=12.8s\n",
            "Epoch 077 | train_loss=0.4321 acc=0.7942 | val_loss=0.4127 acc=0.8386 | prec=0.8465 rec=0.8273 f1=0.8368 | time=12.8s\n",
            "Epoch 078 | train_loss=0.4198 acc=0.8044 | val_loss=0.4123 acc=0.8432 | prec=0.8647 rec=0.8136 f1=0.8384 | time=12.9s\n",
            "Epoch 079 | train_loss=0.4559 acc=0.8010 | val_loss=0.4145 acc=0.8341 | prec=0.8419 rec=0.8227 f1=0.8322 | time=12.9s\n",
            "Epoch 080 | train_loss=0.4424 acc=0.7931 | val_loss=0.4121 acc=0.8318 | prec=0.8380 rec=0.8227 f1=0.8303 | time=12.8s\n",
            "Epoch 081 | train_loss=0.4185 acc=0.8061 | val_loss=0.4284 acc=0.8227 | prec=0.8777 rec=0.7500 f1=0.8088 | time=12.8s\n",
            "Epoch 082 | train_loss=0.4308 acc=0.7993 | val_loss=0.4157 acc=0.8273 | prec=0.8564 rec=0.7864 f1=0.8199 | time=12.8s\n",
            "Epoch 083 | train_loss=0.4161 acc=0.8095 | val_loss=0.4285 acc=0.8273 | prec=0.8673 rec=0.7727 f1=0.8173 | time=12.8s\n",
            "Epoch 084 | train_loss=0.4226 acc=0.8061 | val_loss=0.4182 acc=0.8273 | prec=0.8564 rec=0.7864 f1=0.8199 | time=12.8s\n",
            "Epoch 085 | train_loss=0.4096 acc=0.8090 | val_loss=0.4183 acc=0.8341 | prec=0.8517 rec=0.8091 f1=0.8298 | time=12.8s\n",
            "Epoch 086 | train_loss=0.3999 acc=0.8277 | val_loss=0.4218 acc=0.8250 | prec=0.8522 rec=0.7864 f1=0.8180 | time=12.8s\n",
            "Early stopping at epoch 86\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▄▆▆▆▆▆▄▆▆▁▁▁▁▁▁▁▆▇▁▇▇███████████████████</td></tr><tr><td>precision</td><td>▅▅▄▅▅▁▁▅▁▁▁▇▁▅▅█▅▇▇▇▇▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▃█▇▃█▁▁█▁▁▁▁▁█▃▅▆▇▇▇▇▇▇▇▆▇▆▆▆▇▇▆▇▆▇▇▆▇▆▇</td></tr><tr><td>train_accuracy</td><td>▁▂▂▁▁▁▁▁▁▁▂▂▁▁▁▂▂▂▃▅▅▆▆▇▆▇▇▇▇█▇▇████████</td></tr><tr><td>train_loss</td><td>███▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▄▄▄▄▃▃▂▂▂▃▂▂▂▂▂▂▁▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▁▂▃▅▇▇▇▇███▇███████████████</td></tr><tr><td>validation_loss</td><td>██████████████████▇▆▃▃▂▂▂▂▂▂▁▂▁▁▁▂▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81797</td></tr><tr><td>precision</td><td>0.85222</td></tr><tr><td>recall</td><td>0.78636</td></tr><tr><td>train_accuracy</td><td>0.82766</td></tr><tr><td>train_loss</td><td>0.39992</td></tr><tr><td>validation_accuracy</td><td>0.825</td></tr><tr><td>validation_loss</td><td>0.42185</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0_fold5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/r4mbomvh' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/r4mbomvh</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_083538-r4mbomvh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 08:54:06,623] Trial 0 finished with values: [0.40346492316041677, 0.8153401360544217] and parameters: {'lr': 3.948422459576768e-05, 'wd': 4.653871115001634e-06, 'pct_start': 0.25905355008255493}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            ">>> Average metrics across folds: acc=0.8153, prec=0.8316, rec=0.8040, f1=0.8121\n",
            "Trial 1: lr=3.31e-03, wd=6.27e-05, pct_start=0.28\n",
            "--- Fold 1 ---\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_085406-brmq5g9x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/brmq5g9x' target=\"_blank\">trial1_fold1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/brmq5g9x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-optuna-search-1/runs/brmq5g9x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7570 acc=0.4906 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7577 acc=0.4929 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7538 acc=0.4810 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-05-04 08:54:50,056] Trial 1 failed with parameters: {'lr': 0.0033052112326828124, 'wd': 6.27199998744308e-05, 'pct_start': 0.2849821528541324} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-7-836a626fa4aa>\", line 140, in objective\n",
            "    tloss    += loss.item()\n",
            "                ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-04 08:54:50,058] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-836a626fa4aa>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;31m# ─── Run Optuna multi-objective study ────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best trial values (val_loss, val_acc):'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-836a626fa4aa>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m                     \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_wd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                 \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m                 \u001b[0mpreds\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5 Fold Cross Validation (Results)\n",
        "1] Trial 0: lr=3.95e-05, wd=4.65e-06, pct_start=0.26\n",
        "* Epoch 084 | train_loss=0.4005 acc=0.8100 | val_loss=0.3896 acc=0.8277 | prec=0.8673 rec=0.7727 f1=0.8173 | time=12.8s\n",
        "Early stopping at epoch 84"
      ],
      "metadata": {
        "id": "ImtOyUpXc9oU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Try single fold instead of 5 fold to speed up the searching hyperparameters\n"
      ],
      "metadata": {
        "id": "ZBTANFRTZ98M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 100\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr        = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    wd        = trial.suggest_float('wd', 1e-6, 1e-1, log=True)\n",
        "    pct_start = trial.suggest_float('pct_start', 0.1, 0.3)\n",
        "\n",
        "    print(f\"Trial {trial.number}: lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':pct_start},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len   = dataset[0][0].shape[-1]\n",
        "    model       = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=120,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "    optimizer   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler   = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0  # reset counter\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # Train\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # Dynamic WD\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr/lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds==y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss/len(train_loader)\n",
        "        train_acc  = tcorrect/ttotal\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss += loss.item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds==y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss/len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds==val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "        elapsed    = time.time()-t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "              f\"prec={precision:.4f} rec={recall:.4f} f1={f1:.4f} | time={elapsed:.1f}s\")\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0  # reset on improvement\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize','maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ─── Print Pareto-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uamy_FZLZ9H0",
        "outputId": "36fcb7dd-cfb8-418a-82cf-057da6874895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 09:07:13,940] A new study created in memory with name: no-name-f1673775-135c-4ddf-b68a-74cb6ae01bb6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: lr=4.87e-03, wd=1.46e-06, pct_start=0.28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_090713-dl406lu2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dl406lu2' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dl406lu2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dl406lu2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7341 acc=0.5071 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=14.4s\n",
            "Epoch 002 | train_loss=0.7315 acc=0.5116 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7191 acc=0.4742 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7146 acc=0.4901 | val_loss=0.6984 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7151 acc=0.4963 | val_loss=0.6927 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7249 acc=0.5082 | val_loss=0.6920 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7085 acc=0.5082 | val_loss=0.6900 acc=0.6077 | prec=0.5662 rec=0.9136 f1=0.6991 | time=12.9s\n",
            "Epoch 008 | train_loss=0.6904 acc=0.5349 | val_loss=0.6723 acc=0.7528 | prec=0.7846 rec=0.6955 f1=0.7373 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7036 acc=0.5139 | val_loss=0.6910 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7000 acc=0.5292 | val_loss=0.6783 acc=0.6395 | prec=0.5869 rec=0.9364 f1=0.7215 | time=12.9s\n",
            "Epoch 011 | train_loss=0.6489 acc=0.6171 | val_loss=0.6138 acc=0.6440 | prec=0.5887 rec=0.9500 f1=0.7270 | time=12.9s\n",
            "Epoch 012 | train_loss=0.6079 acc=0.6846 | val_loss=0.7105 acc=0.5306 | prec=0.5152 rec=1.0000 f1=0.6801 | time=12.9s\n",
            "Epoch 013 | train_loss=0.5512 acc=0.7124 | val_loss=1.2707 acc=0.5011 | prec=0.5000 rec=1.0000 f1=0.6667 | time=12.9s\n",
            "Epoch 014 | train_loss=0.5093 acc=0.7504 | val_loss=1.0504 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 015 | train_loss=0.5575 acc=0.7016 | val_loss=0.8684 acc=0.6735 | prec=0.9524 rec=0.3636 f1=0.5263 | time=13.0s\n",
            "Epoch 016 | train_loss=0.5041 acc=0.7482 | val_loss=0.5647 acc=0.6553 | prec=0.5929 rec=0.9864 f1=0.7406 | time=13.2s\n",
            "Epoch 017 | train_loss=0.4985 acc=0.7408 | val_loss=1.5015 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 018 | train_loss=0.4799 acc=0.7754 | val_loss=0.4849 acc=0.7098 | prec=0.6420 rec=0.9455 f1=0.7647 | time=13.1s\n",
            "Epoch 019 | train_loss=0.4783 acc=0.7612 | val_loss=0.4812 acc=0.7302 | prec=0.6603 rec=0.9455 f1=0.7776 | time=13.1s\n",
            "Epoch 020 | train_loss=0.4664 acc=0.7640 | val_loss=0.8304 acc=0.6190 | prec=0.9643 rec=0.2455 f1=0.3913 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4772 acc=0.7703 | val_loss=0.7831 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 022 | train_loss=0.4666 acc=0.7748 | val_loss=0.4607 acc=0.7687 | prec=0.7007 rec=0.9364 f1=0.8016 | time=13.1s\n",
            "Epoch 023 | train_loss=0.4620 acc=0.7737 | val_loss=0.8178 acc=0.7324 | prec=0.9474 rec=0.4909 f1=0.6467 | time=13.1s\n",
            "Epoch 024 | train_loss=0.4346 acc=0.7839 | val_loss=0.4453 acc=0.7778 | prec=0.7075 rec=0.9455 f1=0.8093 | time=13.2s\n",
            "Epoch 025 | train_loss=0.4276 acc=0.7924 | val_loss=0.4521 acc=0.8141 | prec=0.8382 rec=0.7773 f1=0.8066 | time=13.5s\n",
            "Epoch 026 | train_loss=0.3967 acc=0.7896 | val_loss=0.6167 acc=0.7166 | prec=0.6480 rec=0.9455 f1=0.7689 | time=13.1s\n",
            "Epoch 027 | train_loss=0.4275 acc=0.7918 | val_loss=0.4402 acc=0.7891 | prec=0.7343 rec=0.9045 f1=0.8106 | time=13.3s\n",
            "Epoch 028 | train_loss=0.4226 acc=0.7998 | val_loss=0.5118 acc=0.7188 | prec=0.6429 rec=0.9818 f1=0.7770 | time=13.2s\n",
            "Epoch 029 | train_loss=0.4307 acc=0.7884 | val_loss=0.5703 acc=0.5782 | prec=0.5423 rec=0.9909 f1=0.7010 | time=13.1s\n",
            "Epoch 030 | train_loss=0.4014 acc=0.8100 | val_loss=1.1258 acc=0.5782 | prec=0.5419 rec=1.0000 f1=0.7029 | time=13.4s\n",
            "Epoch 031 | train_loss=0.4056 acc=0.7890 | val_loss=0.5642 acc=0.8254 | prec=0.8950 rec=0.7364 f1=0.8080 | time=13.1s\n",
            "Epoch 032 | train_loss=0.3711 acc=0.8264 | val_loss=0.4816 acc=0.8322 | prec=0.8687 rec=0.7818 f1=0.8230 | time=13.3s\n",
            "Epoch 033 | train_loss=0.3447 acc=0.8191 | val_loss=0.5166 acc=0.8299 | prec=0.8680 rec=0.7773 f1=0.8201 | time=13.1s\n",
            "Epoch 034 | train_loss=0.3842 acc=0.8032 | val_loss=0.4550 acc=0.7823 | prec=0.7296 rec=0.8955 f1=0.8041 | time=13.1s\n",
            "Epoch 035 | train_loss=0.3435 acc=0.8253 | val_loss=0.5647 acc=0.6372 | prec=0.5829 rec=0.9591 f1=0.7251 | time=13.1s\n",
            "Epoch 036 | train_loss=0.3680 acc=0.8168 | val_loss=0.7756 acc=0.8050 | prec=0.8895 rec=0.6955 f1=0.7806 | time=13.1s\n",
            "Epoch 037 | train_loss=0.3955 acc=0.8083 | val_loss=0.6633 acc=0.5170 | prec=1.0000 rec=0.0318 f1=0.0617 | time=13.0s\n",
            "Epoch 038 | train_loss=0.4889 acc=0.7691 | val_loss=1.2266 acc=0.5624 | prec=1.0000 rec=0.1227 f1=0.2186 | time=13.1s\n",
            "Epoch 039 | train_loss=0.3433 acc=0.8225 | val_loss=0.4808 acc=0.8163 | prec=0.8062 rec=0.8318 f1=0.8188 | time=13.0s\n",
            "Epoch 040 | train_loss=0.3370 acc=0.8151 | val_loss=1.2479 acc=0.7120 | prec=0.9697 rec=0.4364 f1=0.6019 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3337 acc=0.8383 | val_loss=0.8206 acc=0.6463 | prec=0.9848 rec=0.2955 f1=0.4545 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3655 acc=0.8060 | val_loss=0.5296 acc=0.7778 | prec=0.7089 rec=0.9409 f1=0.8086 | time=13.0s\n",
            "Early stopping at epoch 42\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▇▇▁▇▇▇▇▇▇▇▇▇▅▇▇██▄█▇█████▇▇████▇█▂▃█▆█</td></tr><tr><td>precision</td><td>▁▁▄▄▁▄▅▆▄▅▅▅▅▄█▅▄▅▆█▆█▆▇▆▆▅▅▅▇▇▇▆▅▇██▇█▆</td></tr><tr><td>recall</td><td>▁▁██▁█▇▆██████▄████▃█▄█▆█▇███▆▆▆▇█▆▁▂▇▄█</td></tr><tr><td>train_accuracy</td><td>▂▂▁▁▁▂▂▂▂▂▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇██████▇███</td></tr><tr><td>train_loss</td><td>███████▇▇▇▆▆▅▄▅▄▄▄▃▃▃▃▃▃▂▃▃▃▂▂▂▁▂▁▂▂▄▁▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▃▆▁▄▄▂▁▁▅▄▁▅▆▄▇▆▇█▆▇▆▃▃███▇▄▇▁▂█▅▇</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▃▃▃▃▂▃▆▅▄▂█▁▁▄▁▃▁▁▂▁▁▂▆▂▁▂▁▂▃▂▆▁▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.80859</td></tr><tr><td>precision</td><td>0.7089</td></tr><tr><td>recall</td><td>0.94091</td></tr><tr><td>train_accuracy</td><td>0.80601</td></tr><tr><td>train_loss</td><td>0.36545</td></tr><tr><td>validation_accuracy</td><td>0.77778</td></tr><tr><td>validation_loss</td><td>0.52964</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dl406lu2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dl406lu2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_090713-dl406lu2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 09:16:26,240] Trial 0 finished with values: [0.529642933181354, 0.7777777777777778] and parameters: {'lr': 0.004871404808428789, 'wd': 1.4620507613533937e-06, 'pct_start': 0.2808408733366775}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: lr=4.60e-05, wd=1.68e-02, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_091626-mdl7z6z1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/mdl7z6z1' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/mdl7z6z1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/mdl7z6z1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7664 acc=0.5094 | val_loss=0.6977 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7875 acc=0.5009 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7663 acc=0.5122 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7903 acc=0.4838 | val_loss=0.6925 acc=0.5011 | prec=0.5000 rec=0.8500 f1=0.6296 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7772 acc=0.4895 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7712 acc=0.4844 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7687 acc=0.5060 | val_loss=0.6957 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7496 acc=0.5014 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7444 acc=0.4974 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7389 acc=0.5105 | val_loss=0.6930 acc=0.5125 | prec=0.5066 rec=0.8773 f1=0.6423 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7438 acc=0.5082 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7441 acc=0.5071 | val_loss=0.6934 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7406 acc=0.5054 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7471 acc=0.4878 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7341 acc=0.5128 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7258 acc=0.5116 | val_loss=0.6931 acc=0.5147 | prec=0.8750 rec=0.0318 f1=0.0614 | time=12.7s\n",
            "Epoch 017 | train_loss=0.7385 acc=0.4957 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7451 acc=0.4918 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7417 acc=0.5037 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>███████████▁▁▁█▂██▁</td></tr><tr><td>precision</td><td>▅▅▅▅▅▅▅▅▅▅▅▁▁▁▅█▅▅▁</td></tr><tr><td>recall</td><td>███▇█████▇█▁▁▁█▁██▁</td></tr><tr><td>train_accuracy</td><td>▇▅█▁▂▁▆▅▄▇▇▇▆▂██▄▃▆</td></tr><tr><td>train_loss</td><td>▅█▅█▇▆▆▄▃▂▃▃▃▃▂▁▂▃▃</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▂▁▁▁▁▁▇▁▁▂▂▁█▁▁▂</td></tr><tr><td>validation_loss</td><td>█▅▃▁▂▄▅▃▃▂▃▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.50369</td></tr><tr><td>train_loss</td><td>0.74166</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.6929</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/mdl7z6z1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/mdl7z6z1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_091626-mdl7z6z1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 09:20:31,268] Trial 1 finished with values: [0.6928977583135877, 0.5011337868480725] and parameters: {'lr': 4.604240150543454e-05, 'wd': 0.016781362552932243, 'pct_start': 0.23080672422716378}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: lr=2.04e-04, wd=1.74e-02, pct_start=0.27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_092031-dllp9aye</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dllp9aye' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dllp9aye' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dllp9aye</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7502 acc=0.5060 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7563 acc=0.4912 | val_loss=0.7085 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7298 acc=0.5167 | val_loss=0.7128 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7295 acc=0.5065 | val_loss=0.7166 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7350 acc=0.4906 | val_loss=0.7019 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7259 acc=0.4816 | val_loss=0.7043 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 007 | train_loss=0.7230 acc=0.5088 | val_loss=0.7000 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7268 acc=0.5003 | val_loss=0.6961 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7234 acc=0.5065 | val_loss=0.6965 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7162 acc=0.5077 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7296 acc=0.4906 | val_loss=0.6911 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 012 | train_loss=0.7195 acc=0.5026 | val_loss=0.6914 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7149 acc=0.4986 | val_loss=0.6879 acc=0.5147 | prec=1.0000 rec=0.0273 f1=0.0531 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7078 acc=0.5133 | val_loss=0.6781 acc=0.5306 | prec=0.9333 rec=0.0636 f1=0.1191 | time=13.0s\n",
            "Epoch 015 | train_loss=0.6741 acc=0.5803 | val_loss=0.6313 acc=0.7188 | prec=0.8934 rec=0.4955 f1=0.6374 | time=12.8s\n",
            "Epoch 016 | train_loss=0.6208 acc=0.6495 | val_loss=0.5568 acc=0.7823 | prec=0.7366 rec=0.8773 f1=0.8008 | time=12.9s\n",
            "Epoch 017 | train_loss=0.5737 acc=0.6858 | val_loss=0.4937 acc=0.7891 | prec=0.7592 rec=0.8455 f1=0.8000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.5360 acc=0.7334 | val_loss=0.4782 acc=0.8435 | prec=0.8719 rec=0.8045 f1=0.8369 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5101 acc=0.7413 | val_loss=0.4809 acc=0.7732 | prec=0.7128 rec=0.9136 f1=0.8008 | time=12.9s\n",
            "Epoch 020 | train_loss=0.4961 acc=0.7623 | val_loss=0.4413 acc=0.8231 | prec=0.7958 rec=0.8682 f1=0.8304 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4957 acc=0.7584 | val_loss=0.4342 acc=0.8390 | prec=0.8706 rec=0.7955 f1=0.8314 | time=12.9s\n",
            "Epoch 022 | train_loss=0.4842 acc=0.7777 | val_loss=0.4091 acc=0.8322 | prec=0.8614 rec=0.7909 f1=0.8246 | time=12.9s\n",
            "Epoch 023 | train_loss=0.4541 acc=0.7833 | val_loss=0.4256 acc=0.8481 | prec=0.8312 rec=0.8727 f1=0.8514 | time=13.0s\n",
            "Epoch 024 | train_loss=0.4306 acc=0.7981 | val_loss=0.4092 acc=0.8481 | prec=0.9322 rec=0.7500 f1=0.8312 | time=12.8s\n",
            "Epoch 025 | train_loss=0.4228 acc=0.7918 | val_loss=0.4717 acc=0.7256 | prec=0.6523 rec=0.9636 f1=0.7780 | time=12.9s\n",
            "Epoch 026 | train_loss=0.4006 acc=0.8225 | val_loss=0.3664 acc=0.8639 | prec=0.8738 rec=0.8500 f1=0.8618 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4060 acc=0.8009 | val_loss=0.3965 acc=0.8549 | prec=0.8421 rec=0.8727 f1=0.8571 | time=13.0s\n",
            "Epoch 028 | train_loss=0.3728 acc=0.8298 | val_loss=0.3616 acc=0.8639 | prec=0.8636 rec=0.8636 f1=0.8636 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3859 acc=0.8219 | val_loss=0.3805 acc=0.8526 | prec=0.8414 rec=0.8682 f1=0.8546 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4267 acc=0.7952 | val_loss=0.3848 acc=0.8571 | prec=0.8985 rec=0.8045 f1=0.8489 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3647 acc=0.8259 | val_loss=0.3744 acc=0.8345 | prec=0.8050 rec=0.8818 f1=0.8416 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3365 acc=0.8429 | val_loss=0.4283 acc=0.7959 | prec=0.7444 rec=0.9000 f1=0.8148 | time=12.8s\n",
            "Epoch 033 | train_loss=0.3465 acc=0.8406 | val_loss=0.5623 acc=0.7029 | prec=0.6361 rec=0.9455 f1=0.7605 | time=12.9s\n",
            "Epoch 034 | train_loss=0.3238 acc=0.8508 | val_loss=0.4120 acc=0.8141 | prec=0.7738 rec=0.8864 f1=0.8263 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3112 acc=0.8423 | val_loss=0.4067 acc=0.8163 | prec=0.7769 rec=0.8864 f1=0.8280 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3025 acc=0.8599 | val_loss=0.5680 acc=0.7506 | prec=0.6797 rec=0.9455 f1=0.7909 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2988 acc=0.8559 | val_loss=0.3983 acc=0.8435 | prec=0.8356 rec=0.8545 f1=0.8449 | time=12.9s\n",
            "Epoch 038 | train_loss=0.3256 acc=0.8417 | val_loss=0.5966 acc=0.7370 | prec=0.6656 rec=0.9500 f1=0.7828 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2852 acc=0.8605 | val_loss=0.4984 acc=0.7846 | prec=0.7256 rec=0.9136 f1=0.8089 | time=12.8s\n",
            "Epoch 040 | train_loss=0.2779 acc=0.8525 | val_loss=0.3715 acc=0.8435 | prec=0.8479 rec=0.8364 f1=0.8421 | time=13.0s\n",
            "Epoch 041 | train_loss=0.2623 acc=0.8707 | val_loss=0.3754 acc=0.8458 | prec=0.8423 rec=0.8500 f1=0.8462 | time=12.8s\n",
            "Epoch 042 | train_loss=0.2742 acc=0.8684 | val_loss=0.3625 acc=0.8413 | prec=0.8178 rec=0.8773 f1=0.8465 | time=13.0s\n",
            "Epoch 043 | train_loss=0.2527 acc=0.8826 | val_loss=0.3873 acc=0.8481 | prec=0.8341 rec=0.8682 f1=0.8508 | time=12.9s\n",
            "Early stopping at epoch 43\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▆█▇███████▇██████▇██▇█▇████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁█▇▆▆▇▆▇▇▇▇█▆▇▇▇▇▇▆▅▆▆▆▇▆▆▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▇▇▇█▇▇▇▇▆█▇▇▇▇▇██▇▇█▇██▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▁▁▁▁▁▁▁▁▁▃▄▅▅▆▆▆▆▆▇▆▇▇▇▆▇▇▇▇▇██▇█▇██</td></tr><tr><td>train_loss</td><td>█████████▇█▇▇▇▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▇█▆▇█▇██▅████▇▇▅▇▇▆█▆▆███</td></tr><tr><td>validation_loss</td><td>██████████▇▇▇▆▅▄▃▃▂▂▂▂▂▃▁▂▁▁▁▂▅▂▂▅▂▆▄▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85078</td></tr><tr><td>precision</td><td>0.83406</td></tr><tr><td>recall</td><td>0.86818</td></tr><tr><td>train_accuracy</td><td>0.88259</td></tr><tr><td>train_loss</td><td>0.25265</td></tr><tr><td>validation_accuracy</td><td>0.84807</td></tr><tr><td>validation_loss</td><td>0.38729</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dllp9aye' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/dllp9aye</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_092031-dllp9aye/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 09:29:45,553] Trial 2 finished with values: [0.3872918284365109, 0.8480725623582767] and parameters: {'lr': 0.00020393996770545606, 'wd': 0.017367391720262873, 'pct_start': 0.2730983746217761}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: lr=8.76e-05, wd=1.30e-05, pct_start=0.29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_092945-wh3xjvni</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/wh3xjvni' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/wh3xjvni' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/wh3xjvni</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7815 acc=0.5026 | val_loss=0.6944 acc=0.5057 | prec=0.5714 rec=0.0364 f1=0.0684 | time=13.6s\n",
            "Epoch 002 | train_loss=0.7572 acc=0.5099 | val_loss=0.6940 acc=0.4762 | prec=0.4574 rec=0.2682 f1=0.3381 | time=13.6s\n",
            "Epoch 003 | train_loss=0.7398 acc=0.5099 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.7s\n",
            "Epoch 004 | train_loss=0.7496 acc=0.4878 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.7s\n",
            "Epoch 005 | train_loss=0.7341 acc=0.5060 | val_loss=0.6939 acc=0.5102 | prec=0.6250 rec=0.0455 f1=0.0847 | time=13.7s\n",
            "Epoch 006 | train_loss=0.7282 acc=0.5128 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.7s\n",
            "Epoch 007 | train_loss=0.7272 acc=0.5218 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.7s\n",
            "Epoch 008 | train_loss=0.7328 acc=0.4963 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.8s\n",
            "Epoch 009 | train_loss=0.7318 acc=0.5133 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.9s\n",
            "Epoch 010 | train_loss=0.7308 acc=0.5043 | val_loss=0.6950 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.8s\n",
            "Epoch 011 | train_loss=0.7300 acc=0.4850 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.7s\n",
            "Epoch 012 | train_loss=0.7308 acc=0.5031 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7196 acc=0.5201 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7270 acc=0.4946 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7237 acc=0.5014 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7242 acc=0.4929 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7287 acc=0.5020 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7219 acc=0.4918 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7217 acc=0.4918 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7138 acc=0.4991 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7113 acc=0.5009 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7113 acc=0.5048 | val_loss=0.6928 acc=0.5918 | prec=0.7128 rec=0.3045 f1=0.4268 | time=12.9s\n",
            "Epoch 023 | train_loss=0.7097 acc=0.4980 | val_loss=0.6926 acc=0.6145 | prec=0.6126 rec=0.6182 f1=0.6154 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7136 acc=0.4969 | val_loss=0.6922 acc=0.6213 | prec=0.6137 rec=0.6500 f1=0.6313 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7144 acc=0.4906 | val_loss=0.6922 acc=0.6553 | prec=0.7931 rec=0.4182 f1=0.5476 | time=12.9s\n",
            "Epoch 026 | train_loss=0.7167 acc=0.4663 | val_loss=0.6922 acc=0.5533 | prec=0.7805 rec=0.1455 f1=0.2452 | time=12.9s\n",
            "Epoch 027 | train_loss=0.7116 acc=0.5020 | val_loss=0.6925 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=12.8s\n",
            "Epoch 028 | train_loss=0.7027 acc=0.5173 | val_loss=0.6914 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 029 | train_loss=0.7130 acc=0.4867 | val_loss=0.6924 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 030 | train_loss=0.7036 acc=0.5071 | val_loss=0.6878 acc=0.6961 | prec=0.6822 rec=0.7318 f1=0.7061 | time=12.9s\n",
            "Epoch 031 | train_loss=0.7034 acc=0.5037 | val_loss=0.6833 acc=0.5488 | prec=0.5258 rec=0.9727 f1=0.6826 | time=12.8s\n",
            "Epoch 032 | train_loss=0.6949 acc=0.5451 | val_loss=0.6653 acc=0.7438 | prec=0.7662 rec=0.7000 f1=0.7316 | time=12.8s\n",
            "Epoch 033 | train_loss=0.6868 acc=0.5451 | val_loss=0.6554 acc=0.7052 | prec=0.9018 rec=0.4591 f1=0.6084 | time=12.9s\n",
            "Epoch 034 | train_loss=0.6531 acc=0.6149 | val_loss=0.6239 acc=0.7596 | prec=0.8800 rec=0.6000 f1=0.7135 | time=12.9s\n",
            "Epoch 035 | train_loss=0.6266 acc=0.6370 | val_loss=0.5718 acc=0.8005 | prec=0.8626 rec=0.7136 f1=0.7811 | time=12.8s\n",
            "Epoch 036 | train_loss=0.6038 acc=0.6795 | val_loss=0.5185 acc=0.8118 | prec=0.8477 rec=0.7591 f1=0.8010 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5922 acc=0.7056 | val_loss=0.5301 acc=0.7460 | prec=0.7000 rec=0.8591 f1=0.7714 | time=12.8s\n",
            "Epoch 038 | train_loss=0.5637 acc=0.6960 | val_loss=0.5056 acc=0.8231 | prec=0.8622 rec=0.7682 f1=0.8125 | time=12.7s\n",
            "Epoch 039 | train_loss=0.5695 acc=0.7209 | val_loss=0.5070 acc=0.8209 | prec=0.8279 rec=0.8091 f1=0.8184 | time=12.9s\n",
            "Epoch 040 | train_loss=0.5318 acc=0.7289 | val_loss=0.4875 acc=0.8186 | prec=0.8182 rec=0.8182 f1=0.8182 | time=12.9s\n",
            "Epoch 041 | train_loss=0.5312 acc=0.7419 | val_loss=0.4906 acc=0.8095 | prec=0.7810 rec=0.8591 f1=0.8182 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5250 acc=0.7425 | val_loss=0.5041 acc=0.7755 | prec=0.7184 rec=0.9045 f1=0.8008 | time=12.9s\n",
            "Epoch 043 | train_loss=0.4989 acc=0.7567 | val_loss=0.4707 acc=0.8186 | prec=0.8365 rec=0.7909 f1=0.8131 | time=12.9s\n",
            "Epoch 044 | train_loss=0.5008 acc=0.7516 | val_loss=0.4413 acc=0.8231 | prec=0.8586 rec=0.7727 f1=0.8134 | time=13.0s\n",
            "Epoch 045 | train_loss=0.4889 acc=0.7731 | val_loss=0.4457 acc=0.8163 | prec=0.8294 rec=0.7955 f1=0.8121 | time=12.8s\n",
            "Epoch 046 | train_loss=0.4700 acc=0.7930 | val_loss=0.4715 acc=0.7937 | prec=0.7490 rec=0.8818 f1=0.8100 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4685 acc=0.7822 | val_loss=0.4554 acc=0.8118 | prec=0.7773 rec=0.8727 f1=0.8223 | time=12.9s\n",
            "Epoch 048 | train_loss=0.4509 acc=0.7890 | val_loss=0.4321 acc=0.8095 | prec=0.8119 rec=0.8045 f1=0.8082 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4364 acc=0.7947 | val_loss=0.4469 acc=0.8141 | prec=0.8026 rec=0.8318 f1=0.8170 | time=12.9s\n",
            "Epoch 050 | train_loss=0.4417 acc=0.7856 | val_loss=0.4498 acc=0.8186 | prec=0.7756 rec=0.8955 f1=0.8312 | time=12.9s\n",
            "Epoch 051 | train_loss=0.4323 acc=0.7975 | val_loss=0.4381 acc=0.8118 | prec=0.8549 rec=0.7500 f1=0.7990 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4366 acc=0.8054 | val_loss=0.4895 acc=0.7687 | prec=0.7034 rec=0.9273 f1=0.8000 | time=13.0s\n",
            "Epoch 053 | train_loss=0.4227 acc=0.8106 | val_loss=0.4518 acc=0.8095 | prec=0.7764 rec=0.8682 f1=0.8197 | time=12.9s\n",
            "Epoch 054 | train_loss=0.4104 acc=0.7969 | val_loss=0.4941 acc=0.7687 | prec=0.6993 rec=0.9409 f1=0.8023 | time=12.9s\n",
            "Epoch 055 | train_loss=0.4010 acc=0.8247 | val_loss=0.4423 acc=0.8163 | prec=0.7663 rec=0.9091 f1=0.8316 | time=12.9s\n",
            "Epoch 056 | train_loss=0.3954 acc=0.8281 | val_loss=0.5452 acc=0.7302 | prec=0.6593 rec=0.9500 f1=0.7784 | time=12.8s\n",
            "Epoch 057 | train_loss=0.4171 acc=0.8191 | val_loss=0.5156 acc=0.7619 | prec=0.6936 rec=0.9364 f1=0.7969 | time=12.9s\n",
            "Epoch 058 | train_loss=0.3949 acc=0.8247 | val_loss=0.4904 acc=0.7551 | prec=0.6931 rec=0.9136 f1=0.7882 | time=12.8s\n",
            "Epoch 059 | train_loss=0.3788 acc=0.8185 | val_loss=0.5089 acc=0.7460 | prec=0.6765 rec=0.9409 f1=0.7871 | time=12.8s\n",
            "Epoch 060 | train_loss=0.3940 acc=0.8145 | val_loss=0.4163 acc=0.8254 | prec=0.8122 rec=0.8455 f1=0.8285 | time=12.9s\n",
            "Epoch 061 | train_loss=0.3895 acc=0.8157 | val_loss=0.4498 acc=0.8005 | prec=0.7463 rec=0.9091 f1=0.8197 | time=13.0s\n",
            "Epoch 062 | train_loss=0.3582 acc=0.8355 | val_loss=0.4331 acc=0.8095 | prec=0.7656 rec=0.8909 f1=0.8235 | time=12.9s\n",
            "Epoch 063 | train_loss=0.3620 acc=0.8276 | val_loss=0.4161 acc=0.8141 | prec=0.7738 rec=0.8864 f1=0.8263 | time=12.9s\n",
            "Epoch 064 | train_loss=0.3679 acc=0.8321 | val_loss=0.4255 acc=0.8141 | prec=0.7717 rec=0.8909 f1=0.8270 | time=12.9s\n",
            "Epoch 065 | train_loss=0.3896 acc=0.8100 | val_loss=0.4665 acc=0.7868 | prec=0.7234 rec=0.9273 f1=0.8127 | time=12.9s\n",
            "Epoch 066 | train_loss=0.3430 acc=0.8474 | val_loss=0.4683 acc=0.7800 | prec=0.7189 rec=0.9182 f1=0.8064 | time=12.9s\n",
            "Epoch 067 | train_loss=0.3527 acc=0.8383 | val_loss=0.4008 acc=0.8277 | prec=0.8025 rec=0.8682 f1=0.8341 | time=13.0s\n",
            "Epoch 068 | train_loss=0.3590 acc=0.8412 | val_loss=0.4309 acc=0.8118 | prec=0.7625 rec=0.9045 f1=0.8274 | time=12.8s\n",
            "Epoch 069 | train_loss=0.3398 acc=0.8412 | val_loss=0.4649 acc=0.7846 | prec=0.7193 rec=0.9318 f1=0.8119 | time=12.9s\n",
            "Epoch 070 | train_loss=0.3482 acc=0.8423 | val_loss=0.4873 acc=0.7687 | prec=0.7034 rec=0.9273 f1=0.8000 | time=12.8s\n",
            "Epoch 071 | train_loss=0.3462 acc=0.8463 | val_loss=0.4506 acc=0.7868 | prec=0.7266 rec=0.9182 f1=0.8112 | time=12.9s\n",
            "Epoch 072 | train_loss=0.3364 acc=0.8412 | val_loss=0.4376 acc=0.8027 | prec=0.7491 rec=0.9091 f1=0.8214 | time=12.8s\n",
            "Epoch 073 | train_loss=0.3525 acc=0.8429 | val_loss=0.4265 acc=0.8163 | prec=0.7704 rec=0.9000 f1=0.8302 | time=13.0s\n",
            "Epoch 074 | train_loss=0.3301 acc=0.8559 | val_loss=0.4490 acc=0.7959 | prec=0.7425 rec=0.9045 f1=0.8156 | time=12.8s\n",
            "Epoch 075 | train_loss=0.3203 acc=0.8520 | val_loss=0.4683 acc=0.7732 | prec=0.7113 rec=0.9182 f1=0.8016 | time=12.9s\n",
            "Epoch 076 | train_loss=0.3470 acc=0.8452 | val_loss=0.4749 acc=0.7800 | prec=0.7158 rec=0.9273 f1=0.8079 | time=12.8s\n",
            "Epoch 077 | train_loss=0.3340 acc=0.8383 | val_loss=0.4638 acc=0.7868 | prec=0.7333 rec=0.9000 f1=0.8082 | time=13.0s\n",
            "Epoch 078 | train_loss=0.3213 acc=0.8514 | val_loss=0.5015 acc=0.7664 | prec=0.7038 rec=0.9182 f1=0.7968 | time=12.9s\n",
            "Epoch 079 | train_loss=0.3033 acc=0.8565 | val_loss=0.4675 acc=0.7868 | prec=0.7316 rec=0.9045 f1=0.8089 | time=12.9s\n",
            "Epoch 080 | train_loss=0.3355 acc=0.8321 | val_loss=0.4942 acc=0.7732 | prec=0.7098 rec=0.9227 f1=0.8024 | time=13.0s\n",
            "Epoch 081 | train_loss=0.3113 acc=0.8644 | val_loss=0.4696 acc=0.7868 | prec=0.7266 rec=0.9182 f1=0.8112 | time=12.9s\n",
            "Epoch 082 | train_loss=0.3372 acc=0.8315 | val_loss=0.4566 acc=0.7914 | prec=0.7370 rec=0.9045 f1=0.8122 | time=12.9s\n",
            "Early stopping at epoch 82\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▄▁▁▁▁▁▁▁▅▆▁▇▇▇▇█████████████████████████</td></tr><tr><td>precision</td><td>▆▁▁▁▁▁▁▁▁▁▆▆▅▅▅███▇█▇▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▃▅▄▆█▆▄▆▆▇▇▇▇▇▇▇▇▇▆███▇▇█████▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▂▁▁▂▄▅▅▅▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>██▇█▇▇▇▇▇▇▇▇▇▇▇▆▅▅▅▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▂▁▁▂▁▁▁▁▁▁▁▁▄▅▃▁▅▂▆▆▆████▆▆█▇███▇▇█▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>███████████████▇▅▃▃▃▂▃▂▂▂▃▃▄▄▁▁▃▁▃▂▃▃▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81224</td></tr><tr><td>precision</td><td>0.73704</td></tr><tr><td>recall</td><td>0.90455</td></tr><tr><td>train_accuracy</td><td>0.83154</td></tr><tr><td>train_loss</td><td>0.33723</td></tr><tr><td>validation_accuracy</td><td>0.79138</td></tr><tr><td>validation_loss</td><td>0.45657</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/wh3xjvni' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/wh3xjvni</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_092945-wh3xjvni/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 09:47:32,396] Trial 3 finished with values: [0.4565716130392892, 0.7913832199546486] and parameters: {'lr': 8.756715655652909e-05, 'wd': 1.3049838917966441e-05, 'pct_start': 0.2866765615386274}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: lr=3.22e-04, wd=5.35e-02, pct_start=0.25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_094732-1zqw93rv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/1zqw93rv' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/1zqw93rv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/1zqw93rv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7851 acc=0.5071 | val_loss=0.6994 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7494 acc=0.4946 | val_loss=0.6995 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7241 acc=0.5099 | val_loss=0.7081 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7404 acc=0.4901 | val_loss=0.6963 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7252 acc=0.4918 | val_loss=0.6946 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7162 acc=0.5077 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7235 acc=0.5060 | val_loss=0.6957 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7240 acc=0.4923 | val_loss=0.6951 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7113 acc=0.5122 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7153 acc=0.4946 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7134 acc=0.4952 | val_loss=0.6916 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7086 acc=0.5122 | val_loss=0.6885 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 013 | train_loss=0.6854 acc=0.5479 | val_loss=0.6464 acc=0.5125 | prec=0.5058 rec=0.9909 f1=0.6697 | time=12.8s\n",
            "Epoch 014 | train_loss=0.6684 acc=0.5837 | val_loss=0.5786 acc=0.7279 | prec=0.6701 rec=0.8955 f1=0.7665 | time=12.8s\n",
            "Epoch 015 | train_loss=0.6276 acc=0.6200 | val_loss=0.5474 acc=0.7800 | prec=0.7617 rec=0.8136 f1=0.7868 | time=12.9s\n",
            "Epoch 016 | train_loss=0.5889 acc=0.6665 | val_loss=0.5449 acc=0.7551 | prec=0.7014 rec=0.8864 f1=0.7831 | time=12.9s\n",
            "Epoch 017 | train_loss=0.5599 acc=0.7028 | val_loss=0.5014 acc=0.8050 | prec=0.7792 rec=0.8500 f1=0.8130 | time=12.8s\n",
            "Epoch 018 | train_loss=0.5226 acc=0.7311 | val_loss=0.5501 acc=0.6780 | prec=0.6089 rec=0.9909 f1=0.7543 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5052 acc=0.7487 | val_loss=0.4285 acc=0.8367 | prec=0.8333 rec=0.8409 f1=0.8371 | time=12.8s\n",
            "Epoch 020 | train_loss=0.4769 acc=0.7652 | val_loss=0.4022 acc=0.8413 | prec=0.9121 rec=0.7545 f1=0.8259 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4584 acc=0.7771 | val_loss=0.3855 acc=0.8481 | prec=0.9091 rec=0.7727 f1=0.8354 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4623 acc=0.7901 | val_loss=0.4384 acc=0.8345 | prec=0.9106 rec=0.7409 f1=0.8170 | time=12.7s\n",
            "Epoch 023 | train_loss=0.4636 acc=0.7771 | val_loss=0.3971 acc=0.8526 | prec=0.8112 rec=0.9182 f1=0.8614 | time=12.8s\n",
            "Epoch 024 | train_loss=0.4229 acc=0.7958 | val_loss=0.4247 acc=0.8141 | prec=0.9157 rec=0.6909 f1=0.7876 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3756 acc=0.8321 | val_loss=0.3854 acc=0.8571 | prec=0.8257 rec=0.9045 f1=0.8633 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4023 acc=0.8134 | val_loss=0.3924 acc=0.8322 | prec=0.8147 rec=0.8591 f1=0.8363 | time=12.8s\n",
            "Epoch 027 | train_loss=0.3519 acc=0.8344 | val_loss=0.3857 acc=0.8390 | prec=0.8170 rec=0.8727 f1=0.8440 | time=13.0s\n",
            "Epoch 028 | train_loss=0.3538 acc=0.8304 | val_loss=0.3813 acc=0.8277 | prec=0.7857 rec=0.9000 f1=0.8390 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3504 acc=0.8202 | val_loss=0.3497 acc=0.8526 | prec=0.8298 rec=0.8864 f1=0.8571 | time=12.9s\n",
            "Epoch 030 | train_loss=0.3241 acc=0.8542 | val_loss=0.3678 acc=0.8435 | prec=0.8356 rec=0.8545 f1=0.8449 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3317 acc=0.8310 | val_loss=0.3742 acc=0.8435 | prec=0.8545 rec=0.8273 f1=0.8406 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3336 acc=0.8446 | val_loss=0.3537 acc=0.8571 | prec=0.8458 rec=0.8727 f1=0.8591 | time=12.8s\n",
            "Epoch 033 | train_loss=0.3084 acc=0.8452 | val_loss=0.3635 acc=0.8481 | prec=0.8732 rec=0.8136 f1=0.8424 | time=13.0s\n",
            "Epoch 034 | train_loss=0.2999 acc=0.8463 | val_loss=0.3776 acc=0.8413 | prec=0.8641 rec=0.8091 f1=0.8357 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3133 acc=0.8520 | val_loss=0.4595 acc=0.7959 | prec=0.8916 rec=0.6727 f1=0.7668 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4264 acc=0.7992 | val_loss=0.4527 acc=0.7914 | prec=0.9384 rec=0.6227 f1=0.7486 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3804 acc=0.8060 | val_loss=0.4279 acc=0.8073 | prec=0.8771 rec=0.7136 f1=0.7870 | time=12.9s\n",
            "Epoch 038 | train_loss=0.3211 acc=0.8378 | val_loss=0.3926 acc=0.8299 | prec=0.8194 rec=0.8455 f1=0.8322 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2851 acc=0.8514 | val_loss=0.3785 acc=0.8481 | prec=0.8493 rec=0.8455 f1=0.8474 | time=12.7s\n",
            "Epoch 040 | train_loss=0.3051 acc=0.8514 | val_loss=0.3768 acc=0.8050 | prec=0.7538 rec=0.9045 f1=0.8223 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3008 acc=0.8446 | val_loss=0.4182 acc=0.7868 | prec=0.7250 rec=0.9227 f1=0.8120 | time=13.0s\n",
            "Epoch 042 | train_loss=0.2690 acc=0.8724 | val_loss=0.3519 acc=0.8549 | prec=0.8482 rec=0.8636 f1=0.8559 | time=12.9s\n",
            "Epoch 043 | train_loss=0.2938 acc=0.8429 | val_loss=0.3729 acc=0.8186 | prec=0.7756 rec=0.8955 f1=0.8312 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2647 acc=0.8622 | val_loss=0.4014 acc=0.8390 | prec=0.8143 rec=0.8773 f1=0.8446 | time=12.7s\n",
            "Early stopping at epoch 44\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▆▄▇▇▇█▅█▇▇▇█▇▇█▇▅▄▅▇▇▇▆█▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▅▄▅▃▆██▆█▆▆▆▆▆▆▇▇▇▇█▇▆▇▅▅▇▆</td></tr><tr><td>recall</td><td>████████████▆▅▆▅█▅▃▄▆▂▆▅▆▆▆▅▅▆▄▂▁▃▅▅▆▇▅▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▅▅▆▆▆▆▇▇▇▇▇▇█▇▇██▇▇▇██▇██</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▇▇▇▆▆▅▅▄▄▄▄▄▃▂▃▂▂▂▂▂▂▁▂▃▃▂▁▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▆▆▇▅████▇███▇█████▇▇▇▇█▇▇██</td></tr><tr><td>validation_loss</td><td>███████████▇▅▅▅▄▅▃▂▂▂▂▂▂▂▂▁▁▁▁▂▃▃▃▂▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84464</td></tr><tr><td>precision</td><td>0.81435</td></tr><tr><td>recall</td><td>0.87727</td></tr><tr><td>train_accuracy</td><td>0.86217</td></tr><tr><td>train_loss</td><td>0.2647</td></tr><tr><td>validation_accuracy</td><td>0.839</td></tr><tr><td>validation_loss</td><td>0.40139</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/1zqw93rv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/1zqw93rv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_094732-1zqw93rv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 09:56:59,531] Trial 4 finished with values: [0.40139150832380566, 0.8390022675736961] and parameters: {'lr': 0.0003222868081728424, 'wd': 0.053488518887033966, 'pct_start': 0.246242511281258}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: lr=1.55e-05, wd=3.46e-03, pct_start=0.18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_095659-60ic6bht</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/60ic6bht' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/60ic6bht' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/60ic6bht</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7331 acc=0.4918 | val_loss=0.6936 acc=0.4830 | prec=0.4899 rec=0.8864 f1=0.6311 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7255 acc=0.5184 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7345 acc=0.4980 | val_loss=0.6935 acc=0.4717 | prec=0.4733 rec=0.5227 f1=0.4968 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7293 acc=0.5111 | val_loss=0.6936 acc=0.4943 | prec=0.4000 rec=0.0273 f1=0.0511 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7399 acc=0.4867 | val_loss=0.6937 acc=0.4943 | prec=0.4966 rec=0.9909 f1=0.6616 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7317 acc=0.5031 | val_loss=0.6936 acc=0.4943 | prec=0.4966 rec=0.9864 f1=0.6606 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7294 acc=0.5088 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7294 acc=0.4799 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7255 acc=0.4997 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7337 acc=0.4611 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7217 acc=0.5014 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7373 acc=0.4787 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7234 acc=0.4889 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7212 acc=0.5179 | val_loss=0.6931 acc=0.4966 | prec=0.4977 rec=0.9773 f1=0.6595 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7205 acc=0.5031 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7064 acc=0.5190 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 017 | train_loss=0.7117 acc=0.5116 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7092 acc=0.5167 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7280 acc=0.4850 | val_loss=0.6928 acc=0.5215 | prec=0.5106 rec=0.9818 f1=0.6719 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7085 acc=0.5179 | val_loss=0.6927 acc=0.5533 | prec=0.5545 rec=0.5318 f1=0.5429 | time=12.8s\n",
            "Epoch 021 | train_loss=0.7087 acc=0.5043 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7160 acc=0.4980 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 023 | train_loss=0.7243 acc=0.4878 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 024 | train_loss=0.7131 acc=0.5105 | val_loss=0.6927 acc=0.5102 | prec=0.5047 rec=0.9864 f1=0.6677 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7274 acc=0.4850 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 026 | train_loss=0.7228 acc=0.4838 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 027 | train_loss=0.7121 acc=0.5133 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 028 | train_loss=0.7175 acc=0.4969 | val_loss=0.6924 acc=0.5147 | prec=0.7500 rec=0.0409 f1=0.0776 | time=12.9s\n",
            "Epoch 029 | train_loss=0.7138 acc=0.5082 | val_loss=0.6922 acc=0.5057 | prec=0.5023 rec=0.9909 f1=0.6667 | time=12.7s\n",
            "Epoch 030 | train_loss=0.7176 acc=0.4952 | val_loss=0.6914 acc=0.6168 | prec=0.6024 rec=0.6818 f1=0.6397 | time=12.9s\n",
            "Epoch 031 | train_loss=0.7221 acc=0.4838 | val_loss=0.6909 acc=0.5442 | prec=0.8519 rec=0.1045 f1=0.1862 | time=12.8s\n",
            "Epoch 032 | train_loss=0.7080 acc=0.5218 | val_loss=0.6920 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 033 | train_loss=0.7200 acc=0.5088 | val_loss=0.6917 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=12.9s\n",
            "Epoch 034 | train_loss=0.7032 acc=0.5196 | val_loss=0.6890 acc=0.5714 | prec=0.8605 rec=0.1682 f1=0.2814 | time=12.8s\n",
            "Epoch 035 | train_loss=0.7127 acc=0.5105 | val_loss=0.6892 acc=0.6372 | prec=0.5949 rec=0.8545 f1=0.7015 | time=12.9s\n",
            "Epoch 036 | train_loss=0.7062 acc=0.5048 | val_loss=0.6874 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 037 | train_loss=0.7108 acc=0.5037 | val_loss=0.6861 acc=0.6689 | prec=0.7984 rec=0.4500 f1=0.5756 | time=12.9s\n",
            "Epoch 038 | train_loss=0.6978 acc=0.5235 | val_loss=0.6745 acc=0.5646 | prec=0.9375 rec=0.1364 f1=0.2381 | time=12.9s\n",
            "Epoch 039 | train_loss=0.7056 acc=0.5162 | val_loss=0.6675 acc=0.6803 | prec=0.8160 rec=0.4636 f1=0.5913 | time=12.8s\n",
            "Epoch 040 | train_loss=0.6952 acc=0.5337 | val_loss=0.6632 acc=0.6576 | prec=0.8286 rec=0.3955 f1=0.5354 | time=12.9s\n",
            "Epoch 041 | train_loss=0.6766 acc=0.5587 | val_loss=0.6459 acc=0.6757 | prec=0.8348 rec=0.4364 f1=0.5731 | time=12.8s\n",
            "Epoch 042 | train_loss=0.6714 acc=0.5774 | val_loss=0.6394 acc=0.6939 | prec=0.8346 rec=0.4818 f1=0.6110 | time=12.9s\n",
            "Epoch 043 | train_loss=0.6612 acc=0.6018 | val_loss=0.6237 acc=0.7619 | prec=0.8010 rec=0.6955 f1=0.7445 | time=12.8s\n",
            "Epoch 044 | train_loss=0.6524 acc=0.5786 | val_loss=0.6260 acc=0.7664 | prec=0.7826 rec=0.7364 f1=0.7588 | time=13.0s\n",
            "Epoch 045 | train_loss=0.6557 acc=0.6115 | val_loss=0.6266 acc=0.7755 | prec=0.7840 rec=0.7591 f1=0.7714 | time=12.8s\n",
            "Epoch 046 | train_loss=0.6199 acc=0.6534 | val_loss=0.6004 acc=0.7755 | prec=0.7619 rec=0.8000 f1=0.7805 | time=12.9s\n",
            "Epoch 047 | train_loss=0.6157 acc=0.6444 | val_loss=0.5750 acc=0.7732 | prec=0.8061 rec=0.7182 f1=0.7596 | time=12.8s\n",
            "Epoch 048 | train_loss=0.6113 acc=0.6699 | val_loss=0.5843 acc=0.7619 | prec=0.7076 rec=0.8909 f1=0.7887 | time=13.0s\n",
            "Epoch 049 | train_loss=0.5959 acc=0.6778 | val_loss=0.5629 acc=0.7868 | prec=0.7669 rec=0.8227 f1=0.7939 | time=12.9s\n",
            "Epoch 050 | train_loss=0.5972 acc=0.6846 | val_loss=0.5460 acc=0.7891 | prec=0.7679 rec=0.8273 f1=0.7965 | time=12.9s\n",
            "Epoch 051 | train_loss=0.5987 acc=0.6642 | val_loss=0.5626 acc=0.7823 | prec=0.7500 rec=0.8455 f1=0.7949 | time=12.8s\n",
            "Epoch 052 | train_loss=0.5907 acc=0.6767 | val_loss=0.5549 acc=0.7846 | prec=0.7572 rec=0.8364 f1=0.7948 | time=12.9s\n",
            "Epoch 053 | train_loss=0.6011 acc=0.6710 | val_loss=0.5439 acc=0.7846 | prec=0.7395 rec=0.8773 f1=0.8025 | time=12.8s\n",
            "Epoch 054 | train_loss=0.5792 acc=0.6835 | val_loss=0.5424 acc=0.7846 | prec=0.7451 rec=0.8636 f1=0.8000 | time=12.9s\n",
            "Epoch 055 | train_loss=0.5829 acc=0.6801 | val_loss=0.5544 acc=0.7823 | prec=0.7366 rec=0.8773 f1=0.8008 | time=12.8s\n",
            "Epoch 056 | train_loss=0.5718 acc=0.6914 | val_loss=0.5295 acc=0.7868 | prec=0.7461 rec=0.8682 f1=0.8025 | time=12.8s\n",
            "Epoch 057 | train_loss=0.5806 acc=0.6875 | val_loss=0.5335 acc=0.7846 | prec=0.7341 rec=0.8909 f1=0.8049 | time=12.9s\n",
            "Epoch 058 | train_loss=0.5654 acc=0.7005 | val_loss=0.5427 acc=0.7891 | prec=0.7471 rec=0.8727 f1=0.8050 | time=12.9s\n",
            "Epoch 059 | train_loss=0.5911 acc=0.6863 | val_loss=0.5353 acc=0.7687 | prec=0.7138 rec=0.8955 f1=0.7944 | time=12.8s\n",
            "Epoch 060 | train_loss=0.5786 acc=0.6994 | val_loss=0.5345 acc=0.7982 | prec=0.7673 rec=0.8545 f1=0.8086 | time=12.9s\n",
            "Epoch 061 | train_loss=0.5748 acc=0.7119 | val_loss=0.5283 acc=0.7823 | prec=0.7385 rec=0.8727 f1=0.8000 | time=12.8s\n",
            "Epoch 062 | train_loss=0.5639 acc=0.7039 | val_loss=0.5325 acc=0.7959 | prec=0.7642 rec=0.8545 f1=0.8069 | time=13.0s\n",
            "Epoch 063 | train_loss=0.5579 acc=0.7056 | val_loss=0.5321 acc=0.7914 | prec=0.7540 rec=0.8636 f1=0.8051 | time=12.9s\n",
            "Epoch 064 | train_loss=0.5653 acc=0.7090 | val_loss=0.5292 acc=0.8027 | prec=0.7854 rec=0.8318 f1=0.8079 | time=12.8s\n",
            "Epoch 065 | train_loss=0.5575 acc=0.7215 | val_loss=0.5145 acc=0.8027 | prec=0.7879 rec=0.8273 f1=0.8071 | time=12.8s\n",
            "Epoch 066 | train_loss=0.5543 acc=0.7158 | val_loss=0.5280 acc=0.7982 | prec=0.7741 rec=0.8409 f1=0.8061 | time=12.8s\n",
            "Epoch 067 | train_loss=0.5569 acc=0.7056 | val_loss=0.5200 acc=0.7846 | prec=0.7358 rec=0.8864 f1=0.8041 | time=12.9s\n",
            "Epoch 068 | train_loss=0.5594 acc=0.7011 | val_loss=0.5216 acc=0.7846 | prec=0.7376 rec=0.8818 f1=0.8033 | time=12.8s\n",
            "Epoch 069 | train_loss=0.5601 acc=0.7056 | val_loss=0.5294 acc=0.7823 | prec=0.7296 rec=0.8955 f1=0.8041 | time=12.9s\n",
            "Epoch 070 | train_loss=0.5505 acc=0.7187 | val_loss=0.5230 acc=0.8027 | prec=0.7759 rec=0.8500 f1=0.8113 | time=12.8s\n",
            "Epoch 071 | train_loss=0.5630 acc=0.7141 | val_loss=0.5187 acc=0.7846 | prec=0.7395 rec=0.8773 f1=0.8025 | time=12.8s\n",
            "Epoch 072 | train_loss=0.5643 acc=0.6903 | val_loss=0.5175 acc=0.7868 | prec=0.7386 rec=0.8864 f1=0.8058 | time=12.8s\n",
            "Epoch 073 | train_loss=0.5495 acc=0.7209 | val_loss=0.5320 acc=0.7914 | prec=0.7500 rec=0.8727 f1=0.8067 | time=12.9s\n",
            "Epoch 074 | train_loss=0.5539 acc=0.7209 | val_loss=0.5044 acc=0.7982 | prec=0.7811 rec=0.8273 f1=0.8035 | time=12.8s\n",
            "Epoch 075 | train_loss=0.5515 acc=0.7215 | val_loss=0.5112 acc=0.7982 | prec=0.7610 rec=0.8682 f1=0.8110 | time=12.8s\n",
            "Epoch 076 | train_loss=0.5562 acc=0.7232 | val_loss=0.5044 acc=0.8005 | prec=0.7895 rec=0.8182 f1=0.8036 | time=12.8s\n",
            "Epoch 077 | train_loss=0.5533 acc=0.7266 | val_loss=0.5066 acc=0.8027 | prec=0.7930 rec=0.8182 f1=0.8054 | time=12.8s\n",
            "Epoch 078 | train_loss=0.5506 acc=0.7153 | val_loss=0.5147 acc=0.7800 | prec=0.7338 rec=0.8773 f1=0.7992 | time=12.8s\n",
            "Epoch 079 | train_loss=0.5731 acc=0.7102 | val_loss=0.5001 acc=0.8073 | prec=0.8111 rec=0.8000 f1=0.8055 | time=12.9s\n",
            "Epoch 080 | train_loss=0.5392 acc=0.7402 | val_loss=0.5032 acc=0.7937 | prec=0.7745 rec=0.8273 f1=0.8000 | time=12.8s\n",
            "Epoch 081 | train_loss=0.5682 acc=0.7056 | val_loss=0.5022 acc=0.8095 | prec=0.8091 rec=0.8091 f1=0.8091 | time=12.9s\n",
            "Epoch 082 | train_loss=0.5557 acc=0.7158 | val_loss=0.5133 acc=0.7937 | prec=0.7768 rec=0.8227 f1=0.7991 | time=12.9s\n",
            "Epoch 083 | train_loss=0.5578 acc=0.7050 | val_loss=0.5197 acc=0.8027 | prec=0.7692 rec=0.8636 f1=0.8137 | time=12.8s\n",
            "Epoch 084 | train_loss=0.5560 acc=0.7113 | val_loss=0.5148 acc=0.7937 | prec=0.7722 rec=0.8318 f1=0.8009 | time=12.9s\n",
            "Epoch 085 | train_loss=0.5426 acc=0.7317 | val_loss=0.5133 acc=0.8027 | prec=0.7714 rec=0.8591 f1=0.8129 | time=12.9s\n",
            "Epoch 086 | train_loss=0.5489 acc=0.7215 | val_loss=0.5058 acc=0.7891 | prec=0.7679 rec=0.8273 f1=0.7965 | time=13.0s\n",
            "Epoch 087 | train_loss=0.5511 acc=0.7249 | val_loss=0.5193 acc=0.7914 | prec=0.7481 rec=0.8773 f1=0.8075 | time=12.8s\n",
            "Epoch 088 | train_loss=0.5411 acc=0.7067 | val_loss=0.5150 acc=0.8050 | prec=0.7724 rec=0.8636 f1=0.8155 | time=12.8s\n",
            "Epoch 089 | train_loss=0.5564 acc=0.7090 | val_loss=0.5049 acc=0.8050 | prec=0.8045 rec=0.8045 f1=0.8045 | time=12.9s\n",
            "Epoch 090 | train_loss=0.5367 acc=0.7300 | val_loss=0.5069 acc=0.7982 | prec=0.7835 rec=0.8227 f1=0.8027 | time=12.9s\n",
            "Epoch 091 | train_loss=0.5544 acc=0.7141 | val_loss=0.5117 acc=0.7914 | prec=0.7689 rec=0.8318 f1=0.7991 | time=12.8s\n",
            "Epoch 092 | train_loss=0.5428 acc=0.7243 | val_loss=0.5053 acc=0.8005 | prec=0.7895 rec=0.8182 f1=0.8036 | time=12.8s\n",
            "Epoch 093 | train_loss=0.5587 acc=0.7011 | val_loss=0.5202 acc=0.8050 | prec=0.7724 rec=0.8636 f1=0.8155 | time=12.9s\n",
            "Epoch 094 | train_loss=0.5474 acc=0.7334 | val_loss=0.5013 acc=0.8073 | prec=0.8082 rec=0.8045 f1=0.8064 | time=12.9s\n",
            "Early stopping at epoch 94\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▇▅▁▇▇▇▇▇▇▆▇▇▇▇▇▃▆▃▅▆▇██████████████████</td></tr><tr><td>precision</td><td>▄▄▄▄▄▄▄▅▄▄▅▁█▇▄█▇▇▆▆▆▆▆▆▆▆▆▆▆▆▇▆▇▆▆▆▆▇▆▆</td></tr><tr><td>recall</td><td>█▅██████▅▁███▆▂█▄▄▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▁▂▂▂▁▂▁▂▁▂▂▂▂▂▂▄▅▆▆▆▆▇▇▇▇▇▇█▇▇█▇▇▇██</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▇▇▇▇▇▇▆▅▅▅▄▄▃▃▂▂▂▂▂▁▂▂▁▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▃▁▁▂▂▂▁▁▃▃▅▆▇▇▇███▇▇█████▇████████</td></tr><tr><td>validation_loss</td><td>████████████████████▆▅▄▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.80638</td></tr><tr><td>precision</td><td>0.80822</td></tr><tr><td>recall</td><td>0.80455</td></tr><tr><td>train_accuracy</td><td>0.73341</td></tr><tr><td>train_loss</td><td>0.54735</td></tr><tr><td>validation_accuracy</td><td>0.80726</td></tr><tr><td>validation_loss</td><td>0.50128</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/60ic6bht' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/60ic6bht</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_095659-60ic6bht/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 10:17:09,660] Trial 5 finished with values: [0.5012795520680291, 0.8072562358276644] and parameters: {'lr': 1.5549106681660494e-05, 'wd': 0.003464327980925034, 'pct_start': 0.1819914573493272}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: lr=2.54e-03, wd=3.04e-04, pct_start=0.15\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_101709-v1zr4zvm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/v1zr4zvm' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/v1zr4zvm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/v1zr4zvm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7767 acc=0.4946 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7459 acc=0.4872 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7520 acc=0.5150 | val_loss=0.6925 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7457 acc=0.4867 | val_loss=0.6906 acc=0.5034 | prec=0.5011 rec=0.9955 f1=0.6667 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7260 acc=0.4833 | val_loss=0.6923 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7129 acc=0.5218 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7163 acc=0.4889 | val_loss=0.6846 acc=0.6145 | prec=0.8378 rec=0.2818 f1=0.4218 | time=12.9s\n",
            "Epoch 008 | train_loss=0.6656 acc=0.5831 | val_loss=0.6483 acc=0.5283 | prec=0.5140 rec=1.0000 f1=0.6790 | time=12.9s\n",
            "Epoch 009 | train_loss=0.5595 acc=0.7102 | val_loss=0.7736 acc=0.5918 | prec=0.9545 rec=0.1909 f1=0.3182 | time=12.9s\n",
            "Epoch 010 | train_loss=0.5413 acc=0.7419 | val_loss=0.5674 acc=0.6689 | prec=0.6063 rec=0.9591 f1=0.7430 | time=13.0s\n",
            "Epoch 011 | train_loss=0.5121 acc=0.7413 | val_loss=0.4581 acc=0.8299 | prec=0.8919 rec=0.7500 f1=0.8148 | time=12.9s\n",
            "Epoch 012 | train_loss=0.5275 acc=0.7300 | val_loss=0.4085 acc=0.8186 | prec=0.8723 rec=0.7455 f1=0.8039 | time=12.8s\n",
            "Epoch 013 | train_loss=0.4966 acc=0.7482 | val_loss=0.4564 acc=0.8254 | prec=0.8950 rec=0.7364 f1=0.8080 | time=13.0s\n",
            "Epoch 014 | train_loss=0.4562 acc=0.7839 | val_loss=0.6418 acc=0.6236 | prec=0.5703 rec=0.9955 f1=0.7252 | time=13.0s\n",
            "Epoch 015 | train_loss=0.4690 acc=0.7760 | val_loss=1.0407 acc=0.5193 | prec=0.5093 rec=1.0000 f1=0.6748 | time=13.0s\n",
            "Epoch 016 | train_loss=0.5435 acc=0.7147 | val_loss=0.3975 acc=0.8141 | prec=0.7634 rec=0.9091 f1=0.8299 | time=12.9s\n",
            "Epoch 017 | train_loss=0.4585 acc=0.7777 | val_loss=0.4246 acc=0.8254 | prec=0.7871 rec=0.8909 f1=0.8358 | time=13.1s\n",
            "Epoch 018 | train_loss=0.4376 acc=0.7862 | val_loss=0.6984 acc=0.7755 | prec=0.9618 rec=0.5727 f1=0.7179 | time=13.1s\n",
            "Epoch 019 | train_loss=0.4756 acc=0.7521 | val_loss=0.5804 acc=0.7188 | prec=0.6569 rec=0.9136 f1=0.7643 | time=13.1s\n",
            "Epoch 020 | train_loss=0.4030 acc=0.8140 | val_loss=0.5148 acc=0.8322 | prec=0.8925 rec=0.7545 f1=0.8177 | time=13.1s\n",
            "Epoch 021 | train_loss=0.4169 acc=0.7924 | val_loss=0.4330 acc=0.7914 | prec=0.7353 rec=0.9091 f1=0.8130 | time=13.2s\n",
            "Epoch 022 | train_loss=0.3931 acc=0.8151 | val_loss=0.6129 acc=0.8163 | prec=0.9112 rec=0.7000 f1=0.7918 | time=13.0s\n",
            "Epoch 023 | train_loss=0.3921 acc=0.8111 | val_loss=0.5906 acc=0.7120 | prec=0.6566 rec=0.8864 f1=0.7544 | time=13.2s\n",
            "Epoch 024 | train_loss=0.4063 acc=0.8179 | val_loss=0.7366 acc=0.8186 | prec=0.9070 rec=0.7091 f1=0.7959 | time=13.0s\n",
            "Epoch 025 | train_loss=0.4189 acc=0.8315 | val_loss=0.5116 acc=0.8095 | prec=0.8400 rec=0.7636 f1=0.8000 | time=13.0s\n",
            "Epoch 026 | train_loss=0.3991 acc=0.8123 | val_loss=0.6735 acc=0.7052 | prec=0.6355 rec=0.9591 f1=0.7645 | time=13.0s\n",
            "Epoch 027 | train_loss=0.3466 acc=0.8321 | val_loss=0.4462 acc=0.8163 | prec=0.8601 rec=0.7545 f1=0.8039 | time=13.1s\n",
            "Epoch 028 | train_loss=0.3294 acc=0.8395 | val_loss=0.5298 acc=0.7574 | prec=0.7025 rec=0.8909 f1=0.7856 | time=13.2s\n",
            "Epoch 029 | train_loss=0.3201 acc=0.8293 | val_loss=0.9720 acc=0.6145 | prec=0.5676 rec=0.9545 f1=0.7119 | time=13.1s\n",
            "Epoch 030 | train_loss=0.3936 acc=0.8088 | val_loss=0.6497 acc=0.7483 | prec=0.6940 rec=0.8864 f1=0.7784 | time=13.2s\n",
            "Epoch 031 | train_loss=0.3259 acc=0.8599 | val_loss=1.0851 acc=0.6576 | prec=0.5977 rec=0.9591 f1=0.7365 | time=13.2s\n",
            "Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▇▁▁▅▇▄▇███▇▇██▇▇███▇██▇██▇█▇</td></tr><tr><td>precision</td><td>▁▁▁▅▁▁▇▅█▅▇▇█▅▅▇▇█▆▇▆█▆█▇▆▇▆▅▆▅</td></tr><tr><td>recall</td><td>▁▁▁█▁▁▃█▂█▆▆▆██▇▇▅▇▆▇▆▇▆▆█▆▇█▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▁▂▁▃▅▆▆▆▆▇▆▅▆▇▆▇▇▇▇▇▇▇▇█▇▇█</td></tr><tr><td>train_loss</td><td>████▇▇▇▆▅▄▄▄▄▃▃▄▃▃▃▂▂▂▂▂▃▂▁▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▃▂▃▅███▄▁██▇▆█▇█▅██▅█▆▃▆▄</td></tr><tr><td>validation_loss</td><td>▄▄▄▄▄▄▄▄▅▃▂▁▂▃█▁▁▄▃▂▁▃▃▄▂▄▁▂▇▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.73647</td></tr><tr><td>precision</td><td>0.59773</td></tr><tr><td>recall</td><td>0.95909</td></tr><tr><td>train_accuracy</td><td>0.8599</td></tr><tr><td>train_loss</td><td>0.32594</td></tr><tr><td>validation_accuracy</td><td>0.6576</td></tr><tr><td>validation_loss</td><td>1.08512</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/v1zr4zvm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/v1zr4zvm</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_101709-v1zr4zvm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 10:23:53,819] Trial 6 finished with values: [1.0851248502731323, 0.6575963718820862] and parameters: {'lr': 0.002535373942163694, 'wd': 0.0003043640651593832, 'pct_start': 0.1456722334320794}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: lr=7.13e-05, wd=1.30e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_102353-zdr1xydp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/zdr1xydp' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/zdr1xydp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/zdr1xydp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7731 acc=0.5043 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7808 acc=0.5150 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7581 acc=0.4986 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7563 acc=0.5009 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7510 acc=0.4906 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7442 acc=0.5105 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7456 acc=0.5077 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7547 acc=0.4991 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7404 acc=0.5105 | val_loss=0.6927 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7440 acc=0.4929 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 011 | train_loss=0.7444 acc=0.5003 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7275 acc=0.5105 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7409 acc=0.4957 | val_loss=0.6937 acc=0.4898 | prec=0.4943 rec=0.9818 f1=0.6575 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7337 acc=0.5020 | val_loss=0.6946 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 015 | train_loss=0.7247 acc=0.4929 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7175 acc=0.5184 | val_loss=0.6931 acc=0.4966 | prec=0.4977 rec=0.9909 f1=0.6626 | time=13.0s\n",
            "Epoch 017 | train_loss=0.7317 acc=0.5054 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 018 | train_loss=0.7220 acc=0.4940 | val_loss=0.6940 acc=0.4694 | prec=0.4833 rec=0.9227 f1=0.6344 | time=13.0s\n",
            "Epoch 019 | train_loss=0.7229 acc=0.5060 | val_loss=0.6926 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7204 acc=0.5009 | val_loss=0.6881 acc=0.5329 | prec=0.5171 rec=0.9636 f1=0.6730 | time=12.9s\n",
            "Epoch 021 | train_loss=0.6946 acc=0.5383 | val_loss=0.6673 acc=0.6168 | prec=0.8696 rec=0.2727 f1=0.4152 | time=12.9s\n",
            "Epoch 022 | train_loss=0.6842 acc=0.5570 | val_loss=0.6196 acc=0.7211 | prec=0.8170 rec=0.5682 f1=0.6702 | time=13.0s\n",
            "Epoch 023 | train_loss=0.6538 acc=0.6092 | val_loss=0.5895 acc=0.7710 | prec=0.7990 rec=0.7227 f1=0.7589 | time=12.9s\n",
            "Epoch 024 | train_loss=0.6058 acc=0.6478 | val_loss=0.5610 acc=0.8095 | prec=0.8656 rec=0.7318 f1=0.7931 | time=13.0s\n",
            "Epoch 025 | train_loss=0.5905 acc=0.6642 | val_loss=0.5055 acc=0.8073 | prec=0.8358 rec=0.7636 f1=0.7981 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5836 acc=0.6761 | val_loss=0.5063 acc=0.8141 | prec=0.8224 rec=0.8000 f1=0.8111 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5774 acc=0.6920 | val_loss=0.4899 acc=0.8163 | prec=0.8677 rec=0.7455 f1=0.8020 | time=12.8s\n",
            "Epoch 028 | train_loss=0.5657 acc=0.7011 | val_loss=0.5102 acc=0.8209 | prec=0.8439 rec=0.7864 f1=0.8141 | time=13.0s\n",
            "Epoch 029 | train_loss=0.5453 acc=0.7107 | val_loss=0.4998 acc=0.8095 | prec=0.9000 rec=0.6955 f1=0.7846 | time=12.9s\n",
            "Epoch 030 | train_loss=0.5315 acc=0.7294 | val_loss=0.5035 acc=0.7800 | prec=0.7057 rec=0.9591 f1=0.8131 | time=13.0s\n",
            "Epoch 031 | train_loss=0.5260 acc=0.7266 | val_loss=0.4795 acc=0.8277 | prec=0.8750 rec=0.7636 f1=0.8155 | time=12.9s\n",
            "Epoch 032 | train_loss=0.5221 acc=0.7317 | val_loss=0.4526 acc=0.8367 | prec=0.8592 rec=0.8045 f1=0.8310 | time=12.9s\n",
            "Epoch 033 | train_loss=0.5146 acc=0.7379 | val_loss=0.4645 acc=0.8390 | prec=0.8744 rec=0.7909 f1=0.8305 | time=12.9s\n",
            "Epoch 034 | train_loss=0.5017 acc=0.7402 | val_loss=0.4548 acc=0.8299 | prec=0.8251 rec=0.8364 f1=0.8307 | time=13.0s\n",
            "Epoch 035 | train_loss=0.4707 acc=0.7635 | val_loss=0.4348 acc=0.8435 | prec=0.8832 rec=0.7909 f1=0.8345 | time=13.0s\n",
            "Epoch 036 | train_loss=0.4976 acc=0.7425 | val_loss=0.4626 acc=0.8209 | prec=0.8939 rec=0.7273 f1=0.8020 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4740 acc=0.7629 | val_loss=0.4258 acc=0.8503 | prec=0.8500 rec=0.8500 f1=0.8500 | time=13.0s\n",
            "Epoch 038 | train_loss=0.4752 acc=0.7612 | val_loss=0.4720 acc=0.8027 | prec=0.9290 rec=0.6545 f1=0.7680 | time=13.0s\n",
            "Epoch 039 | train_loss=0.4549 acc=0.7805 | val_loss=0.4101 acc=0.8549 | prec=0.8861 rec=0.8136 f1=0.8483 | time=12.8s\n",
            "Epoch 040 | train_loss=0.4490 acc=0.7918 | val_loss=0.4088 acc=0.8503 | prec=0.8080 rec=0.9182 f1=0.8596 | time=13.0s\n",
            "Epoch 041 | train_loss=0.4304 acc=0.7952 | val_loss=0.4655 acc=0.7800 | prec=0.7043 rec=0.9636 f1=0.8138 | time=12.9s\n",
            "Epoch 042 | train_loss=0.4467 acc=0.7941 | val_loss=0.4305 acc=0.8594 | prec=0.8950 rec=0.8136 f1=0.8524 | time=13.0s\n",
            "Epoch 043 | train_loss=0.4271 acc=0.8054 | val_loss=0.3701 acc=0.8662 | prec=0.8966 rec=0.8273 f1=0.8605 | time=13.0s\n",
            "Epoch 044 | train_loss=0.4256 acc=0.7947 | val_loss=0.4371 acc=0.7846 | prec=0.9310 rec=0.6136 f1=0.7397 | time=13.0s\n",
            "Epoch 045 | train_loss=0.4373 acc=0.7913 | val_loss=0.4182 acc=0.8367 | prec=0.7721 rec=0.9545 f1=0.8537 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4159 acc=0.8020 | val_loss=0.4544 acc=0.7891 | prec=0.9441 rec=0.6136 f1=0.7438 | time=12.9s\n",
            "Epoch 047 | train_loss=0.4738 acc=0.7657 | val_loss=0.4311 acc=0.8186 | prec=0.7482 rec=0.9591 f1=0.8406 | time=12.9s\n",
            "Epoch 048 | train_loss=0.4046 acc=0.8043 | val_loss=0.3856 acc=0.8435 | prec=0.7871 rec=0.9409 f1=0.8571 | time=12.9s\n",
            "Epoch 049 | train_loss=0.4101 acc=0.8117 | val_loss=0.3801 acc=0.8345 | prec=0.7860 rec=0.9182 f1=0.8470 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3951 acc=0.8003 | val_loss=0.3435 acc=0.8685 | prec=0.8821 rec=0.8500 f1=0.8657 | time=12.9s\n",
            "Epoch 051 | train_loss=0.3840 acc=0.8196 | val_loss=0.3635 acc=0.8549 | prec=0.8095 rec=0.9273 f1=0.8644 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3735 acc=0.8287 | val_loss=0.3524 acc=0.8730 | prec=0.8981 rec=0.8409 f1=0.8685 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3790 acc=0.8293 | val_loss=0.3413 acc=0.8639 | prec=0.8448 rec=0.8909 f1=0.8673 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3752 acc=0.8230 | val_loss=0.3401 acc=0.8503 | prec=0.8156 rec=0.9045 f1=0.8578 | time=13.1s\n",
            "Epoch 055 | train_loss=0.3623 acc=0.8395 | val_loss=0.3644 acc=0.8322 | prec=0.7874 rec=0.9091 f1=0.8439 | time=12.9s\n",
            "Epoch 056 | train_loss=0.3612 acc=0.8332 | val_loss=0.3434 acc=0.8730 | prec=0.8694 rec=0.8773 f1=0.8733 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3640 acc=0.8355 | val_loss=0.3477 acc=0.8730 | prec=0.8796 rec=0.8636 f1=0.8716 | time=13.1s\n",
            "Epoch 058 | train_loss=0.3664 acc=0.8253 | val_loss=0.3472 acc=0.8481 | prec=0.8122 rec=0.9045 f1=0.8559 | time=12.9s\n",
            "Epoch 059 | train_loss=0.3573 acc=0.8315 | val_loss=0.3831 acc=0.8231 | prec=0.7649 rec=0.9318 f1=0.8402 | time=13.0s\n",
            "Epoch 060 | train_loss=0.3674 acc=0.8230 | val_loss=0.3743 acc=0.8254 | prec=0.7719 rec=0.9227 f1=0.8406 | time=12.9s\n",
            "Epoch 061 | train_loss=0.3422 acc=0.8429 | val_loss=0.3502 acc=0.8571 | prec=0.8230 rec=0.9091 f1=0.8639 | time=12.9s\n",
            "Epoch 062 | train_loss=0.3442 acc=0.8389 | val_loss=0.3432 acc=0.8685 | prec=0.8491 rec=0.8955 f1=0.8717 | time=13.0s\n",
            "Epoch 063 | train_loss=0.3462 acc=0.8452 | val_loss=0.4578 acc=0.7823 | prec=0.7109 rec=0.9500 f1=0.8132 | time=13.2s\n",
            "Epoch 064 | train_loss=0.3637 acc=0.8236 | val_loss=0.3417 acc=0.8730 | prec=0.8905 rec=0.8500 f1=0.8698 | time=13.0s\n",
            "Epoch 065 | train_loss=0.3341 acc=0.8423 | val_loss=0.3578 acc=0.8503 | prec=0.8182 rec=0.9000 f1=0.8571 | time=12.9s\n",
            "Epoch 066 | train_loss=0.3279 acc=0.8559 | val_loss=0.3446 acc=0.8662 | prec=0.8610 rec=0.8727 f1=0.8668 | time=13.0s\n",
            "Epoch 067 | train_loss=0.3270 acc=0.8372 | val_loss=0.3975 acc=0.8231 | prec=0.7731 rec=0.9136 f1=0.8375 | time=12.9s\n",
            "Epoch 068 | train_loss=0.3324 acc=0.8503 | val_loss=0.3375 acc=0.8685 | prec=0.8649 rec=0.8727 f1=0.8688 | time=12.9s\n",
            "Epoch 069 | train_loss=0.3416 acc=0.8338 | val_loss=0.3599 acc=0.8617 | prec=0.8383 rec=0.8955 f1=0.8659 | time=12.8s\n",
            "Epoch 070 | train_loss=0.3356 acc=0.8554 | val_loss=0.4582 acc=0.7755 | prec=0.7065 rec=0.9409 f1=0.8070 | time=13.0s\n",
            "Epoch 071 | train_loss=0.2961 acc=0.8627 | val_loss=0.3447 acc=0.8639 | prec=0.8540 rec=0.8773 f1=0.8655 | time=13.0s\n",
            "Epoch 072 | train_loss=0.3220 acc=0.8429 | val_loss=0.3401 acc=0.8662 | prec=0.8546 rec=0.8818 f1=0.8680 | time=13.0s\n",
            "Epoch 073 | train_loss=0.3114 acc=0.8542 | val_loss=0.3320 acc=0.8776 | prec=0.9069 rec=0.8409 f1=0.8726 | time=12.9s\n",
            "Epoch 074 | train_loss=0.3246 acc=0.8440 | val_loss=0.3422 acc=0.8707 | prec=0.8590 rec=0.8864 f1=0.8725 | time=12.9s\n",
            "Epoch 075 | train_loss=0.3183 acc=0.8542 | val_loss=0.3428 acc=0.8707 | prec=0.8590 rec=0.8864 f1=0.8725 | time=12.8s\n",
            "Epoch 076 | train_loss=0.3098 acc=0.8537 | val_loss=0.3567 acc=0.8503 | prec=0.8208 rec=0.8955 f1=0.8565 | time=13.0s\n",
            "Epoch 077 | train_loss=0.3126 acc=0.8610 | val_loss=0.3570 acc=0.8481 | prec=0.8148 rec=0.9000 f1=0.8553 | time=12.9s\n",
            "Epoch 078 | train_loss=0.3015 acc=0.8661 | val_loss=0.3534 acc=0.8571 | prec=0.8398 rec=0.8818 f1=0.8603 | time=12.9s\n",
            "Epoch 079 | train_loss=0.3393 acc=0.8531 | val_loss=0.3402 acc=0.8730 | prec=0.8942 rec=0.8455 f1=0.8692 | time=13.0s\n",
            "Epoch 080 | train_loss=0.3057 acc=0.8474 | val_loss=0.3919 acc=0.8231 | prec=0.7731 rec=0.9136 f1=0.8375 | time=13.0s\n",
            "Epoch 081 | train_loss=0.3020 acc=0.8457 | val_loss=0.3645 acc=0.8481 | prec=0.8122 rec=0.9045 f1=0.8559 | time=12.9s\n",
            "Epoch 082 | train_loss=0.2966 acc=0.8639 | val_loss=0.3413 acc=0.8730 | prec=0.8832 rec=0.8591 f1=0.8710 | time=12.8s\n",
            "Epoch 083 | train_loss=0.3139 acc=0.8486 | val_loss=0.3765 acc=0.8322 | prec=0.7920 rec=0.9000 f1=0.8426 | time=12.9s\n",
            "Epoch 084 | train_loss=0.3089 acc=0.8644 | val_loss=0.3422 acc=0.8639 | prec=0.8636 rec=0.8636 f1=0.8636 | time=12.9s\n",
            "Epoch 085 | train_loss=0.3079 acc=0.8644 | val_loss=0.4018 acc=0.8277 | prec=0.7769 rec=0.9182 f1=0.8417 | time=13.0s\n",
            "Epoch 086 | train_loss=0.2939 acc=0.8627 | val_loss=0.3494 acc=0.8685 | prec=0.8785 rec=0.8545 f1=0.8664 | time=12.9s\n",
            "Epoch 087 | train_loss=0.3007 acc=0.8514 | val_loss=0.3680 acc=0.8390 | prec=0.8041 rec=0.8955 f1=0.8473 | time=12.8s\n",
            "Epoch 088 | train_loss=0.2949 acc=0.8525 | val_loss=0.3569 acc=0.8458 | prec=0.8220 rec=0.8818 f1=0.8509 | time=12.9s\n",
            "Early stopping at epoch 88\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▁▁▆▆▁▆▆▁▇▇████▇██▇██████████▇█████████</td></tr><tr><td>precision</td><td>▁▁▁▁█▁▄▅▇▇▇▇▇▇▇▇█▇▇▇█▆▇▇▇▇▆▇▆▇▇▇▇▇▆▇▇▇▆▇</td></tr><tr><td>recall</td><td>▁▁▁▁█▁██▆▆▇▆█▆▇▇▆▇▇▇███▇▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▄▆▆▇▇▇▇▇▇▇▇██▇████████████</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▇▇▇▇▇▅▅▄▄▄▄▄▃▃▄▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▂▅▇▇▇▇▆▇█▇██▆█▆▇▇▇███▇██████████▇</td></tr><tr><td>validation_loss</td><td>██████████▅▄▄▄▄▄▃▂▃▃▂▁▂▁▁▁▁▂▁▃▁▁▁▁▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85088</td></tr><tr><td>precision</td><td>0.82203</td></tr><tr><td>recall</td><td>0.88182</td></tr><tr><td>train_accuracy</td><td>0.85252</td></tr><tr><td>train_loss</td><td>0.29492</td></tr><tr><td>validation_accuracy</td><td>0.8458</td></tr><tr><td>validation_loss</td><td>0.35685</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/zdr1xydp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/zdr1xydp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_102353-zdr1xydp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 10:42:52,928] Trial 7 finished with values: [0.3568546644278935, 0.8458049886621315] and parameters: {'lr': 7.130176486506828e-05, 'wd': 1.3018139682194866e-05, 'pct_start': 0.23447989527306254}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: lr=3.44e-05, wd=3.04e-06, pct_start=0.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_104252-gal2qsvp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/gal2qsvp' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/gal2qsvp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/gal2qsvp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7716 acc=0.4986 | val_loss=0.7028 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7620 acc=0.5043 | val_loss=0.7026 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7566 acc=0.4946 | val_loss=0.7033 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7434 acc=0.4991 | val_loss=0.6981 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7522 acc=0.4986 | val_loss=0.7034 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7461 acc=0.4861 | val_loss=0.7062 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7478 acc=0.4923 | val_loss=0.6971 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7445 acc=0.5003 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7425 acc=0.5009 | val_loss=0.6991 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7544 acc=0.4963 | val_loss=0.7014 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7339 acc=0.5082 | val_loss=0.7010 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 012 | train_loss=0.7505 acc=0.4691 | val_loss=0.7020 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7337 acc=0.4997 | val_loss=0.7027 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7354 acc=0.5014 | val_loss=0.7020 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7253 acc=0.5082 | val_loss=0.6980 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 016 | train_loss=0.7188 acc=0.5099 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 017 | train_loss=0.7195 acc=0.5003 | val_loss=0.6913 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 018 | train_loss=0.7237 acc=0.5037 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7264 acc=0.5043 | val_loss=0.6864 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7355 acc=0.5009 | val_loss=0.6879 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7106 acc=0.5224 | val_loss=0.6855 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 022 | train_loss=0.7043 acc=0.5417 | val_loss=0.6749 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 023 | train_loss=0.7163 acc=0.5337 | val_loss=0.6706 acc=0.5556 | prec=0.5294 rec=0.9818 f1=0.6879 | time=13.0s\n",
            "Epoch 024 | train_loss=0.6963 acc=0.5683 | val_loss=0.6372 acc=0.7234 | prec=0.6713 rec=0.8727 f1=0.7589 | time=12.9s\n",
            "Epoch 025 | train_loss=0.6550 acc=0.6166 | val_loss=0.6155 acc=0.7664 | prec=0.7388 rec=0.8227 f1=0.7785 | time=12.9s\n",
            "Epoch 026 | train_loss=0.6519 acc=0.6285 | val_loss=0.5895 acc=0.7732 | prec=0.8191 rec=0.7000 f1=0.7549 | time=12.9s\n",
            "Epoch 027 | train_loss=0.6248 acc=0.6619 | val_loss=0.5776 acc=0.7778 | prec=0.7585 rec=0.8136 f1=0.7851 | time=12.9s\n",
            "Epoch 028 | train_loss=0.6099 acc=0.6642 | val_loss=0.5479 acc=0.8073 | prec=0.8230 rec=0.7818 f1=0.8019 | time=12.9s\n",
            "Epoch 029 | train_loss=0.6043 acc=0.6824 | val_loss=0.5215 acc=0.8050 | prec=0.8418 rec=0.7500 f1=0.7933 | time=12.9s\n",
            "Epoch 030 | train_loss=0.6107 acc=0.6875 | val_loss=0.5222 acc=0.7868 | prec=0.7500 rec=0.8591 f1=0.8008 | time=12.9s\n",
            "Epoch 031 | train_loss=0.5837 acc=0.6943 | val_loss=0.4943 acc=0.8073 | prec=0.8054 rec=0.8091 f1=0.8073 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5735 acc=0.7113 | val_loss=0.5158 acc=0.7800 | prec=0.7393 rec=0.8636 f1=0.7966 | time=12.9s\n",
            "Epoch 033 | train_loss=0.5604 acc=0.7238 | val_loss=0.5218 acc=0.7800 | prec=0.7338 rec=0.8773 f1=0.7992 | time=12.9s\n",
            "Epoch 034 | train_loss=0.5554 acc=0.7170 | val_loss=0.5007 acc=0.7846 | prec=0.7593 rec=0.8318 f1=0.7939 | time=12.9s\n",
            "Epoch 035 | train_loss=0.5371 acc=0.7391 | val_loss=0.5143 acc=0.7755 | prec=0.7283 rec=0.8773 f1=0.7959 | time=12.9s\n",
            "Epoch 036 | train_loss=0.5389 acc=0.7317 | val_loss=0.5150 acc=0.7596 | prec=0.6979 rec=0.9136 f1=0.7913 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5317 acc=0.7396 | val_loss=0.4860 acc=0.8095 | prec=0.8148 rec=0.8000 f1=0.8073 | time=12.9s\n",
            "Epoch 038 | train_loss=0.5305 acc=0.7448 | val_loss=0.4702 acc=0.7914 | prec=0.7667 rec=0.8364 f1=0.8000 | time=12.9s\n",
            "Epoch 039 | train_loss=0.5259 acc=0.7561 | val_loss=0.4757 acc=0.8050 | prec=0.7888 rec=0.8318 f1=0.8097 | time=12.8s\n",
            "Epoch 040 | train_loss=0.5185 acc=0.7482 | val_loss=0.4855 acc=0.7937 | prec=0.7570 rec=0.8636 f1=0.8068 | time=12.9s\n",
            "Epoch 041 | train_loss=0.5176 acc=0.7652 | val_loss=0.4911 acc=0.7914 | prec=0.7500 rec=0.8727 f1=0.8067 | time=12.8s\n",
            "Epoch 042 | train_loss=0.5136 acc=0.7482 | val_loss=0.4986 acc=0.7619 | prec=0.6990 rec=0.9182 f1=0.7937 | time=12.9s\n",
            "Epoch 043 | train_loss=0.4916 acc=0.7612 | val_loss=0.4639 acc=0.8231 | prec=0.8287 rec=0.8136 f1=0.8211 | time=12.9s\n",
            "Epoch 044 | train_loss=0.4982 acc=0.7533 | val_loss=0.4712 acc=0.8186 | prec=0.8017 rec=0.8455 f1=0.8230 | time=12.9s\n",
            "Epoch 045 | train_loss=0.4993 acc=0.7572 | val_loss=0.4697 acc=0.8073 | prec=0.7778 rec=0.8591 f1=0.8164 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4875 acc=0.7618 | val_loss=0.4768 acc=0.8141 | prec=0.7949 rec=0.8455 f1=0.8194 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4735 acc=0.7850 | val_loss=0.4563 acc=0.8141 | prec=0.8000 rec=0.8364 f1=0.8178 | time=12.9s\n",
            "Epoch 048 | train_loss=0.5052 acc=0.7652 | val_loss=0.4529 acc=0.8050 | prec=0.7659 rec=0.8773 f1=0.8178 | time=12.9s\n",
            "Epoch 049 | train_loss=0.5100 acc=0.7584 | val_loss=0.4803 acc=0.7937 | prec=0.7345 rec=0.9182 f1=0.8162 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4801 acc=0.7737 | val_loss=0.4596 acc=0.8073 | prec=0.7547 rec=0.9091 f1=0.8247 | time=12.9s\n",
            "Epoch 051 | train_loss=0.4756 acc=0.7799 | val_loss=0.4485 acc=0.8163 | prec=0.7704 rec=0.9000 f1=0.8302 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4666 acc=0.7969 | val_loss=0.4364 acc=0.8027 | prec=0.7830 rec=0.8364 f1=0.8088 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4692 acc=0.7992 | val_loss=0.4603 acc=0.8073 | prec=0.8000 rec=0.8182 f1=0.8090 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4672 acc=0.7822 | val_loss=0.4412 acc=0.8073 | prec=0.7733 rec=0.8682 f1=0.8180 | time=13.0s\n",
            "Epoch 055 | train_loss=0.4612 acc=0.7964 | val_loss=0.4441 acc=0.8027 | prec=0.7628 rec=0.8773 f1=0.8161 | time=12.9s\n",
            "Epoch 056 | train_loss=0.4578 acc=0.7879 | val_loss=0.4490 acc=0.8005 | prec=0.7558 rec=0.8864 f1=0.8159 | time=13.0s\n",
            "Epoch 057 | train_loss=0.4761 acc=0.7805 | val_loss=0.4447 acc=0.8050 | prec=0.7445 rec=0.9273 f1=0.8259 | time=12.9s\n",
            "Epoch 058 | train_loss=0.4455 acc=0.7992 | val_loss=0.4603 acc=0.8005 | prec=0.7538 rec=0.8909 f1=0.8167 | time=12.9s\n",
            "Epoch 059 | train_loss=0.4432 acc=0.7884 | val_loss=0.4386 acc=0.8050 | prec=0.7724 rec=0.8636 f1=0.8155 | time=12.8s\n",
            "Epoch 060 | train_loss=0.4506 acc=0.7913 | val_loss=0.4349 acc=0.8050 | prec=0.7557 rec=0.9000 f1=0.8216 | time=13.0s\n",
            "Epoch 061 | train_loss=0.4371 acc=0.7952 | val_loss=0.4423 acc=0.8027 | prec=0.7649 rec=0.8727 f1=0.8153 | time=13.0s\n",
            "Epoch 062 | train_loss=0.4559 acc=0.7805 | val_loss=0.4365 acc=0.8027 | prec=0.7588 rec=0.8864 f1=0.8176 | time=12.9s\n",
            "Epoch 063 | train_loss=0.4434 acc=0.7947 | val_loss=0.4228 acc=0.8367 | prec=0.8458 rec=0.8227 f1=0.8341 | time=13.0s\n",
            "Epoch 064 | train_loss=0.4236 acc=0.8157 | val_loss=0.4363 acc=0.8005 | prec=0.7558 rec=0.8864 f1=0.8159 | time=13.0s\n",
            "Epoch 065 | train_loss=0.4356 acc=0.7958 | val_loss=0.4242 acc=0.8299 | prec=0.8166 rec=0.8500 f1=0.8330 | time=12.8s\n",
            "Epoch 066 | train_loss=0.4221 acc=0.8009 | val_loss=0.4432 acc=0.8118 | prec=0.7605 rec=0.9091 f1=0.8282 | time=12.8s\n",
            "Epoch 067 | train_loss=0.4357 acc=0.8015 | val_loss=0.4590 acc=0.8027 | prec=0.7401 rec=0.9318 f1=0.8249 | time=12.8s\n",
            "Epoch 068 | train_loss=0.4146 acc=0.8196 | val_loss=0.4386 acc=0.8141 | prec=0.7738 rec=0.8864 f1=0.8263 | time=12.9s\n",
            "Epoch 069 | train_loss=0.4275 acc=0.8032 | val_loss=0.4261 acc=0.8277 | prec=0.8000 rec=0.8727 f1=0.8348 | time=12.9s\n",
            "Epoch 070 | train_loss=0.4191 acc=0.8128 | val_loss=0.4411 acc=0.8073 | prec=0.7567 rec=0.9045 f1=0.8240 | time=12.9s\n",
            "Epoch 071 | train_loss=0.4424 acc=0.8026 | val_loss=0.4373 acc=0.8095 | prec=0.7556 rec=0.9136 f1=0.8272 | time=12.8s\n",
            "Epoch 072 | train_loss=0.4311 acc=0.8060 | val_loss=0.4249 acc=0.8186 | prec=0.7778 rec=0.8909 f1=0.8305 | time=12.9s\n",
            "Epoch 073 | train_loss=0.4202 acc=0.8106 | val_loss=0.4402 acc=0.8163 | prec=0.7725 rec=0.8955 f1=0.8295 | time=12.8s\n",
            "Epoch 074 | train_loss=0.4191 acc=0.8088 | val_loss=0.4103 acc=0.8322 | prec=0.8380 rec=0.8227 f1=0.8303 | time=12.8s\n",
            "Epoch 075 | train_loss=0.4268 acc=0.8117 | val_loss=0.4186 acc=0.8322 | prec=0.8093 rec=0.8682 f1=0.8377 | time=13.0s\n",
            "Epoch 076 | train_loss=0.4091 acc=0.8270 | val_loss=0.4323 acc=0.8277 | prec=0.7927 rec=0.8864 f1=0.8369 | time=12.9s\n",
            "Epoch 077 | train_loss=0.4192 acc=0.8225 | val_loss=0.4212 acc=0.8299 | prec=0.8008 rec=0.8773 f1=0.8373 | time=12.8s\n",
            "Epoch 078 | train_loss=0.4207 acc=0.8185 | val_loss=0.4365 acc=0.8231 | prec=0.7731 rec=0.9136 f1=0.8375 | time=13.0s\n",
            "Epoch 079 | train_loss=0.4016 acc=0.8230 | val_loss=0.4215 acc=0.8254 | prec=0.8122 rec=0.8455 f1=0.8285 | time=12.9s\n",
            "Epoch 080 | train_loss=0.4313 acc=0.8123 | val_loss=0.4290 acc=0.8254 | prec=0.8017 rec=0.8636 f1=0.8315 | time=12.9s\n",
            "Epoch 081 | train_loss=0.4186 acc=0.8060 | val_loss=0.4252 acc=0.8277 | prec=0.7927 rec=0.8864 f1=0.8369 | time=12.9s\n",
            "Epoch 082 | train_loss=0.4159 acc=0.8100 | val_loss=0.4280 acc=0.8163 | prec=0.7725 rec=0.8955 f1=0.8295 | time=12.8s\n",
            "Epoch 083 | train_loss=0.4020 acc=0.8281 | val_loss=0.4316 acc=0.8209 | prec=0.7787 rec=0.8955 f1=0.8330 | time=13.0s\n",
            "Epoch 084 | train_loss=0.4110 acc=0.8213 | val_loss=0.4261 acc=0.8277 | prec=0.7951 rec=0.8818 f1=0.8362 | time=12.9s\n",
            "Epoch 085 | train_loss=0.4083 acc=0.8196 | val_loss=0.4285 acc=0.8254 | prec=0.7942 rec=0.8773 f1=0.8337 | time=12.9s\n",
            "Epoch 086 | train_loss=0.3885 acc=0.8349 | val_loss=0.4223 acc=0.8231 | prec=0.8008 rec=0.8591 f1=0.8289 | time=12.8s\n",
            "Epoch 087 | train_loss=0.4031 acc=0.8185 | val_loss=0.4436 acc=0.8095 | prec=0.7556 rec=0.9136 f1=0.8272 | time=12.8s\n",
            "Epoch 088 | train_loss=0.4159 acc=0.8174 | val_loss=0.4341 acc=0.8277 | prec=0.7951 rec=0.8818 f1=0.8362 | time=12.9s\n",
            "Epoch 089 | train_loss=0.4120 acc=0.8100 | val_loss=0.4381 acc=0.8186 | prec=0.7734 rec=0.9000 f1=0.8319 | time=13.0s\n",
            "Early stopping at epoch 89\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▅▆▅▆▇▆▆▆▇▇▇█▇▇▇▇█▇▇███████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁█▆█▇▆▅▇▆▆▅▇▇▆▆▆▇▇▆▆▇▆▆▆▆▇▆▇▇█▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>██████████▃▂▁▄▃▅▂▄▃▄▃▅▅▄▅▅▄▅▄▄▆▅▃▅▅▄▄▅▅▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▂▃▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>██▇█▇▇█▇▇▇▅▅▅▅▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▇▇▇██▇▇▇▇███▇▇▇▇▇▇▇██████████</td></tr><tr><td>validation_loss</td><td>█████▇▇▆▄▄▄▃▄▂▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83193</td></tr><tr><td>precision</td><td>0.77344</td></tr><tr><td>recall</td><td>0.9</td></tr><tr><td>train_accuracy</td><td>0.80998</td></tr><tr><td>train_loss</td><td>0.41204</td></tr><tr><td>validation_accuracy</td><td>0.81859</td></tr><tr><td>validation_loss</td><td>0.43814</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/gal2qsvp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/gal2qsvp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_104252-gal2qsvp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 11:02:02,697] Trial 8 finished with values: [0.4381433491195951, 0.81859410430839] and parameters: {'lr': 3.444615567428066e-05, 'wd': 3.0432311486384725e-06, 'pct_start': 0.1421653397551902}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: lr=5.22e-04, wd=1.99e-04, pct_start=0.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_110202-d87jnmzu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/d87jnmzu' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/d87jnmzu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/d87jnmzu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7615 acc=0.5054 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7512 acc=0.4935 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7357 acc=0.4974 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7431 acc=0.4861 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7250 acc=0.5003 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7212 acc=0.4935 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7103 acc=0.5060 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7069 acc=0.5082 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7302 acc=0.5213 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7266 acc=0.4867 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7092 acc=0.5105 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7174 acc=0.4833 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 013 | train_loss=0.6989 acc=0.5173 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7137 acc=0.4878 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7050 acc=0.5014 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7040 acc=0.5026 | val_loss=0.6925 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.6934 acc=0.5156 | val_loss=0.6895 acc=0.6281 | prec=0.9000 rec=0.2864 f1=0.4345 | time=12.8s\n",
            "Epoch 018 | train_loss=0.6945 acc=0.5156 | val_loss=0.6602 acc=0.6848 | prec=0.8462 rec=0.4500 f1=0.5875 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6706 acc=0.5865 | val_loss=0.6090 acc=0.7120 | prec=0.9043 rec=0.4727 f1=0.6209 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6225 acc=0.6784 | val_loss=0.6035 acc=0.6984 | prec=0.6306 rec=0.9545 f1=0.7595 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5806 acc=0.7368 | val_loss=0.4950 acc=0.8209 | prec=0.8052 rec=0.8455 f1=0.8248 | time=12.8s\n",
            "Epoch 022 | train_loss=0.5573 acc=0.7442 | val_loss=0.5323 acc=0.7982 | prec=0.7251 rec=0.9591 f1=0.8258 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5229 acc=0.7567 | val_loss=0.4528 acc=0.8118 | prec=0.7665 rec=0.8955 f1=0.8260 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4910 acc=0.7663 | val_loss=0.3900 acc=0.8617 | prec=0.8841 rec=0.8318 f1=0.8571 | time=12.9s\n",
            "Epoch 025 | train_loss=0.4827 acc=0.7828 | val_loss=0.4412 acc=0.8367 | prec=0.7803 rec=0.9364 f1=0.8512 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4504 acc=0.8003 | val_loss=0.3814 acc=0.8617 | prec=0.8299 rec=0.9091 f1=0.8677 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4171 acc=0.8032 | val_loss=0.3614 acc=0.8707 | prec=0.8410 rec=0.9136 f1=0.8758 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4104 acc=0.8100 | val_loss=0.5024 acc=0.7211 | prec=0.6474 rec=0.9682 f1=0.7760 | time=12.8s\n",
            "Epoch 029 | train_loss=0.3913 acc=0.8281 | val_loss=0.4544 acc=0.7664 | prec=0.6906 rec=0.9636 f1=0.8046 | time=12.9s\n",
            "Epoch 030 | train_loss=0.3580 acc=0.8366 | val_loss=0.4008 acc=0.8186 | prec=0.7555 rec=0.9409 f1=0.8381 | time=13.0s\n",
            "Epoch 031 | train_loss=0.3813 acc=0.8304 | val_loss=0.4027 acc=0.8163 | prec=0.7491 rec=0.9500 f1=0.8377 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3437 acc=0.8463 | val_loss=0.3529 acc=0.8549 | prec=0.8333 rec=0.8864 f1=0.8590 | time=12.9s\n",
            "Epoch 033 | train_loss=0.3366 acc=0.8497 | val_loss=0.3506 acc=0.8458 | prec=0.8167 rec=0.8909 f1=0.8522 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3747 acc=0.8389 | val_loss=0.3610 acc=0.8594 | prec=0.8496 rec=0.8727 f1=0.8610 | time=13.0s\n",
            "Epoch 035 | train_loss=0.3493 acc=0.8310 | val_loss=0.3735 acc=0.8458 | prec=0.8220 rec=0.8818 f1=0.8509 | time=12.9s\n",
            "Epoch 036 | train_loss=0.3168 acc=0.8491 | val_loss=0.3718 acc=0.8390 | prec=0.8170 rec=0.8727 f1=0.8440 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3003 acc=0.8508 | val_loss=0.4238 acc=0.8163 | prec=0.7683 rec=0.9045 f1=0.8309 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3087 acc=0.8639 | val_loss=0.5893 acc=0.7619 | prec=0.6898 rec=0.9500 f1=0.7992 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2871 acc=0.8707 | val_loss=0.6747 acc=0.7211 | prec=0.6456 rec=0.9773 f1=0.7776 | time=12.9s\n",
            "Epoch 040 | train_loss=0.3127 acc=0.8497 | val_loss=0.6113 acc=0.7415 | prec=0.6688 rec=0.9545 f1=0.7865 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3110 acc=0.8474 | val_loss=0.4877 acc=0.8141 | prec=0.7654 rec=0.9045 f1=0.8292 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3006 acc=0.8622 | val_loss=0.5602 acc=0.7460 | prec=0.6742 rec=0.9500 f1=0.7887 | time=12.9s\n",
            "Epoch 043 | train_loss=0.2983 acc=0.8633 | val_loss=0.4007 acc=0.8685 | prec=0.8649 rec=0.8727 f1=0.8688 | time=13.0s\n",
            "Epoch 044 | train_loss=0.3402 acc=0.8361 | val_loss=0.6428 acc=0.6644 | prec=0.6000 rec=0.9818 f1=0.7448 | time=12.7s\n",
            "Epoch 045 | train_loss=0.2928 acc=0.8599 | val_loss=0.5729 acc=0.7755 | prec=0.7037 rec=0.9500 f1=0.8085 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3074 acc=0.8565 | val_loss=0.5968 acc=0.7710 | prec=0.7017 rec=0.9409 f1=0.8039 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3287 acc=0.8576 | val_loss=0.4341 acc=0.8027 | prec=0.7472 rec=0.9136 f1=0.8221 | time=13.0s\n",
            "Epoch 048 | train_loss=0.2716 acc=0.8707 | val_loss=0.5010 acc=0.8118 | prec=0.7546 rec=0.9227 f1=0.8303 | time=12.9s\n",
            "Early stopping at epoch 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▆▁▁▆▁▆▆▆▆▁▁▆▆▄▆▇██████▇▇██████▇▇▇██▇▇▇█</td></tr><tr><td>precision</td><td>▁▅▁▁▅▁▅▅▅▅▁▁▅▅██▆▇▇▇▇▇█▆▆▇▇▇█▇▇▆▆▆▇█▆▆▆▇</td></tr><tr><td>recall</td><td>▁█▁▁█▁████▁▁██▃▄█▇█▇█▇▇███▇▇▇▇▇███▇▇███▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▁▁▂▁▁▁▂▃▅▆▆▆▆▇▇▇▇▇██▇▇██████▇███</td></tr><tr><td>train_loss</td><td>████▇▇▇██▇▇▇▇▇▇▇▆▅▅▅▄▄▃▃▃▃▂▂▂▂▁▂▁▂▂▁▂▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▅▅▇▇▇▇██▅▆▇████▇▆▅▆▇█▄▆▆▇</td></tr><tr><td>validation_loss</td><td>███████████████▆▆▄▅▃▃▂▁▄▃▂▁▁▁▁▂▆█▆▄▂▇▆▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83027</td></tr><tr><td>precision</td><td>0.75465</td></tr><tr><td>recall</td><td>0.92273</td></tr><tr><td>train_accuracy</td><td>0.87067</td></tr><tr><td>train_loss</td><td>0.27157</td></tr><tr><td>validation_accuracy</td><td>0.81179</td></tr><tr><td>validation_loss</td><td>0.50101</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/d87jnmzu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold/runs/d87jnmzu</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_110202-d87jnmzu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 11:12:22,166] Trial 9 finished with values: [0.5010083009089742, 0.8117913832199547] and parameters: {'lr': 0.0005219080039578175, 'wd': 0.00019917379546773365, 'pct_start': 0.1229693696038243}.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "A single best trial cannot be retrieved from a multi-objective study. Consider using Study.best_trials to retrieve a list containing the best trials.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-15949d359e4c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[0mstudy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptuna\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_study\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'minimize'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'maximize'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best values:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best params:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36mbest_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_multi_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m             raise RuntimeError(\n\u001b[0m\u001b[1;32m    158\u001b[0m                 \u001b[0;34m\"A single best trial cannot be retrieved from a multi-objective study. Consider \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;34m\"using Study.best_trials to retrieve a list containing the best trials.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: A single best trial cannot be retrieved from a multi-objective study. Consider using Study.best_trials to retrieve a list containing the best trials."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block = 1, Head = 3 (Best Trial)\n",
        "1] Trial 7: lr=7.13e-05, wd=1.30e-05, pct_start=0.23\n",
        "- Epoch 088 | train_loss=0.2949 acc=0.8525 | val_loss=0.3569 acc=0.8458 | prec=0.8220 rec=0.8818 f1=0.8509 | time=12.9s\n",
        "Early stopping at epoch 88\n",
        "\n",
        "2] Trial 8: lr=3.44e-05, wd=3.04e-06, pct_start=0.14\n",
        "- Epoch 089 | train_loss=0.4120 acc=0.8100 | val_loss=0.4381 acc=0.8186 | prec=0.7734 rec=0.9000 f1=0.8319 | time=13.0s\n",
        "Early stopping at epoch 89"
      ],
      "metadata": {
        "id": "2CyULgjqbTOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Increase the model complexity\n",
        "#### Try Block = 2, Head = 2 experiment\n",
        "- Training Loss -> 0 should be confirmed\n",
        "- Observe Validation Loss & Validation Accuracy & F-1 Score"
      ],
      "metadata": {
        "id": "5tBmFz9WLxxa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 100\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr        = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    wd        = trial.suggest_float('wd', 1e-6, 1e-1, log=True)\n",
        "    pct_start = trial.suggest_float('pct_start', 0.1, 0.3)\n",
        "\n",
        "    print(f\"Trial {trial.number}: lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold-2',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':pct_start},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len   = dataset[0][0].shape[-1]\n",
        "    model       = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=120,\n",
        "        num_heads=2,\n",
        "        num_blocks=2,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "    optimizer   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler   = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0  # reset counter\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # Train\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # Dynamic WD\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr/lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds==y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss/len(train_loader)\n",
        "        train_acc  = tcorrect/ttotal\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss += loss.item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds==y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss/len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds==val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "        elapsed    = time.time()-t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "              f\"prec={precision:.4f} rec={recall:.4f} f1={f1:.4f} | time={elapsed:.1f}s\")\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0  # reset on improvement\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize','maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ─── Print Pareto-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZCiBGVOhLvq0",
        "outputId": "63de62f7-2182-4180-9618-314307e04478"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 18:50:32,727] A new study created in memory with name: no-name-33a7aece-c5fa-4fab-847e-74d125e76683\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: lr=4.20e-03, wd=8.05e-06, pct_start=0.26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_185032-le51kl46</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/le51kl46' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/le51kl46' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/le51kl46</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7283 acc=0.5179 | val_loss=0.6924 acc=0.5102 | prec=0.5046 rec=1.0000 f1=0.6707 | time=142.5s\n",
            "Epoch 002 | train_loss=0.7269 acc=0.5065 | val_loss=0.7036 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 003 | train_loss=0.7325 acc=0.5054 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 004 | train_loss=0.7144 acc=0.5167 | val_loss=0.7042 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 005 | train_loss=0.7172 acc=0.4957 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 006 | train_loss=0.6995 acc=0.5286 | val_loss=0.6931 acc=0.5669 | prec=0.6706 rec=0.2591 f1=0.3738 | time=16.8s\n",
            "Epoch 007 | train_loss=0.7087 acc=0.4889 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=17.0s\n",
            "Epoch 008 | train_loss=0.7069 acc=0.4986 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 009 | train_loss=0.6932 acc=0.5009 | val_loss=0.6924 acc=0.5102 | prec=1.0000 rec=0.0182 f1=0.0357 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7059 acc=0.4952 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 011 | train_loss=0.6945 acc=0.5077 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 012 | train_loss=0.6967 acc=0.5031 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 013 | train_loss=0.6993 acc=0.4782 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 014 | train_loss=0.6983 acc=0.4997 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 015 | train_loss=0.6970 acc=0.4952 | val_loss=0.6883 acc=0.5283 | prec=1.0000 rec=0.0545 f1=0.1034 | time=16.8s\n",
            "Epoch 016 | train_loss=0.6467 acc=0.6018 | val_loss=0.6577 acc=0.5215 | prec=0.5104 rec=1.0000 f1=0.6759 | time=16.9s\n",
            "Epoch 017 | train_loss=0.6785 acc=0.5564 | val_loss=0.6924 acc=0.6372 | prec=0.7459 rec=0.4136 f1=0.5322 | time=16.9s\n",
            "Epoch 018 | train_loss=0.7053 acc=0.5026 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=17.0s\n",
            "Epoch 019 | train_loss=0.7043 acc=0.5003 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 020 | train_loss=0.6977 acc=0.5179 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 021 | train_loss=0.6955 acc=0.5099 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 022 | train_loss=0.6854 acc=0.5445 | val_loss=0.7003 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 023 | train_loss=0.6987 acc=0.4974 | val_loss=0.6899 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 024 | train_loss=0.6742 acc=0.5774 | val_loss=0.5990 acc=0.7052 | prec=0.8169 rec=0.5273 f1=0.6409 | time=17.0s\n",
            "Epoch 025 | train_loss=0.6281 acc=0.6347 | val_loss=0.6309 acc=0.7098 | prec=0.8286 rec=0.5273 f1=0.6444 | time=17.3s\n",
            "Epoch 026 | train_loss=0.5749 acc=0.6892 | val_loss=0.5343 acc=0.7528 | prec=0.7284 rec=0.8045 f1=0.7646 | time=17.0s\n",
            "Epoch 027 | train_loss=0.5554 acc=0.7016 | val_loss=0.4854 acc=0.7755 | prec=0.7788 rec=0.7682 f1=0.7735 | time=17.0s\n",
            "Epoch 028 | train_loss=0.5500 acc=0.7175 | val_loss=0.5246 acc=0.7211 | prec=0.9145 rec=0.4864 f1=0.6350 | time=17.2s\n",
            "Epoch 029 | train_loss=0.5408 acc=0.7102 | val_loss=0.6805 acc=0.5306 | prec=0.5152 rec=1.0000 f1=0.6801 | time=17.0s\n",
            "Epoch 030 | train_loss=0.5412 acc=0.7243 | val_loss=0.6603 acc=0.6553 | prec=0.9722 rec=0.3182 f1=0.4795 | time=17.0s\n",
            "Epoch 031 | train_loss=0.5488 acc=0.7192 | val_loss=0.6206 acc=0.6871 | prec=0.9271 rec=0.4045 f1=0.5633 | time=17.2s\n",
            "Epoch 032 | train_loss=0.5013 acc=0.7493 | val_loss=0.5512 acc=0.6190 | prec=0.5677 rec=0.9909 f1=0.7219 | time=17.0s\n",
            "Epoch 033 | train_loss=0.5060 acc=0.7425 | val_loss=0.5388 acc=0.7211 | prec=0.9217 rec=0.4818 f1=0.6328 | time=17.1s\n",
            "Epoch 034 | train_loss=0.4571 acc=0.7521 | val_loss=0.4220 acc=0.8118 | prec=0.8408 rec=0.7682 f1=0.8029 | time=17.2s\n",
            "Epoch 035 | train_loss=0.4783 acc=0.7714 | val_loss=0.5681 acc=0.7052 | prec=0.9500 rec=0.4318 f1=0.5938 | time=17.0s\n",
            "Epoch 036 | train_loss=0.4873 acc=0.7431 | val_loss=0.4216 acc=0.8095 | prec=0.9096 rec=0.6864 f1=0.7824 | time=17.0s\n",
            "Epoch 037 | train_loss=0.4771 acc=0.7612 | val_loss=0.4038 acc=0.7982 | prec=0.7764 rec=0.8364 f1=0.8053 | time=17.1s\n",
            "Epoch 038 | train_loss=0.4350 acc=0.7799 | val_loss=1.2878 acc=0.5420 | prec=0.5214 rec=0.9955 f1=0.6844 | time=17.0s\n",
            "Epoch 039 | train_loss=0.4261 acc=0.7981 | val_loss=0.5959 acc=0.7279 | prec=0.9464 rec=0.4818 f1=0.6386 | time=16.9s\n",
            "Epoch 040 | train_loss=0.4647 acc=0.7742 | val_loss=0.5554 acc=0.7483 | prec=0.9431 rec=0.5273 f1=0.6764 | time=17.2s\n",
            "Epoch 041 | train_loss=0.4939 acc=0.7742 | val_loss=0.4136 acc=0.7800 | prec=0.7128 rec=0.9364 f1=0.8094 | time=16.9s\n",
            "Epoch 042 | train_loss=0.4298 acc=0.7890 | val_loss=0.5703 acc=0.7891 | prec=0.9441 rec=0.6136 f1=0.7438 | time=17.0s\n",
            "Epoch 043 | train_loss=0.4490 acc=0.7924 | val_loss=0.7886 acc=0.5714 | prec=0.5383 rec=0.9909 f1=0.6976 | time=17.0s\n",
            "Epoch 044 | train_loss=0.4241 acc=0.7924 | val_loss=0.4308 acc=0.8027 | prec=0.7509 rec=0.9045 f1=0.8206 | time=17.0s\n",
            "Epoch 045 | train_loss=0.4503 acc=0.7862 | val_loss=0.5164 acc=0.7891 | prec=0.9150 rec=0.6364 f1=0.7507 | time=16.9s\n",
            "Epoch 046 | train_loss=0.4497 acc=0.7896 | val_loss=0.3976 acc=0.8299 | prec=0.8962 rec=0.7455 f1=0.8139 | time=17.1s\n",
            "Epoch 047 | train_loss=0.4106 acc=0.7986 | val_loss=0.4515 acc=0.7619 | prec=0.9197 rec=0.5727 f1=0.7059 | time=16.9s\n",
            "Epoch 048 | train_loss=0.4059 acc=0.8066 | val_loss=0.4085 acc=0.8209 | prec=0.9172 rec=0.7045 f1=0.7969 | time=17.0s\n",
            "Epoch 049 | train_loss=0.3918 acc=0.8026 | val_loss=0.4464 acc=0.8050 | prec=0.7481 rec=0.9182 f1=0.8245 | time=17.1s\n",
            "Epoch 050 | train_loss=0.4206 acc=0.7896 | val_loss=0.5827 acc=0.7120 | prec=0.6422 rec=0.9545 f1=0.7678 | time=17.0s\n",
            "Epoch 051 | train_loss=0.4043 acc=0.7998 | val_loss=0.4118 acc=0.8345 | prec=0.9455 rec=0.7091 f1=0.8104 | time=16.9s\n",
            "Epoch 052 | train_loss=0.4123 acc=0.8196 | val_loss=0.3786 acc=0.8413 | prec=0.8750 rec=0.7955 f1=0.8333 | time=17.1s\n",
            "Epoch 053 | train_loss=0.3837 acc=0.8066 | val_loss=0.4379 acc=0.7755 | prec=0.7051 rec=0.9455 f1=0.8078 | time=16.8s\n",
            "Epoch 054 | train_loss=0.3955 acc=0.8111 | val_loss=0.4681 acc=0.8163 | prec=0.9317 rec=0.6818 f1=0.7874 | time=16.9s\n",
            "Epoch 055 | train_loss=0.3538 acc=0.8213 | val_loss=0.4538 acc=0.8390 | prec=0.9461 rec=0.7182 f1=0.8165 | time=17.1s\n",
            "Epoch 056 | train_loss=0.3605 acc=0.8151 | val_loss=0.3653 acc=0.8526 | prec=0.8818 rec=0.8136 f1=0.8463 | time=16.9s\n",
            "Epoch 057 | train_loss=0.4053 acc=0.7918 | val_loss=0.3723 acc=0.8231 | prec=0.8087 rec=0.8455 f1=0.8267 | time=16.9s\n",
            "Epoch 058 | train_loss=0.3520 acc=0.8213 | val_loss=0.4048 acc=0.8141 | prec=0.7782 rec=0.8773 f1=0.8248 | time=17.1s\n",
            "Epoch 059 | train_loss=0.3869 acc=0.8134 | val_loss=0.3830 acc=0.8299 | prec=0.8341 rec=0.8227 f1=0.8284 | time=16.9s\n",
            "Epoch 060 | train_loss=0.3513 acc=0.8236 | val_loss=0.3974 acc=0.8073 | prec=0.7733 rec=0.8682 f1=0.8180 | time=16.9s\n",
            "Epoch 061 | train_loss=0.3443 acc=0.8236 | val_loss=0.3694 acc=0.8322 | prec=0.8259 rec=0.8409 f1=0.8333 | time=17.2s\n",
            "Epoch 062 | train_loss=0.3596 acc=0.8486 | val_loss=0.4497 acc=0.7959 | prec=0.7481 rec=0.8909 f1=0.8133 | time=16.9s\n",
            "Epoch 063 | train_loss=0.3458 acc=0.8395 | val_loss=0.4419 acc=0.8390 | prec=0.8225 rec=0.8636 f1=0.8426 | time=16.9s\n",
            "Epoch 064 | train_loss=0.4300 acc=0.7833 | val_loss=0.7595 acc=0.6757 | prec=0.9753 rec=0.3591 f1=0.5249 | time=17.1s\n",
            "Epoch 065 | train_loss=0.3579 acc=0.8191 | val_loss=0.3835 acc=0.8186 | prec=0.8302 rec=0.8000 f1=0.8148 | time=17.0s\n",
            "Epoch 066 | train_loss=0.3646 acc=0.8083 | val_loss=0.5557 acc=0.7800 | prec=0.9695 rec=0.5773 f1=0.7236 | time=17.0s\n",
            "Epoch 067 | train_loss=0.3331 acc=0.8253 | val_loss=0.5423 acc=0.7914 | prec=0.9776 rec=0.5955 f1=0.7401 | time=17.6s\n",
            "Epoch 068 | train_loss=0.3205 acc=0.8298 | val_loss=0.3707 acc=0.8390 | prec=0.8531 rec=0.8182 f1=0.8353 | time=17.4s\n",
            "Epoch 069 | train_loss=0.3144 acc=0.8480 | val_loss=0.4083 acc=0.8231 | prec=0.8777 rec=0.7500 f1=0.8088 | time=17.8s\n",
            "Epoch 070 | train_loss=0.3609 acc=0.8389 | val_loss=0.4186 acc=0.8322 | prec=0.8925 rec=0.7545 f1=0.8177 | time=17.5s\n",
            "Epoch 071 | train_loss=0.3092 acc=0.8383 | val_loss=0.4249 acc=0.7937 | prec=0.7345 rec=0.9182 f1=0.8162 | time=16.9s\n",
            "Early stopping at epoch 71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▇▇▇▇▇▁▁▁▁▁▅▇▁▁▁▆▆▇▆▅▇▆██▆▇█▇███████▅█▇██</td></tr><tr><td>precision</td><td>▅▄▄▄▆█▁▁▁▁▆▄▁▁▁▇▆▇▅▇▇█▆█▅▇▇▅█▇█▇▆▇▆▇█▇▇▆</td></tr><tr><td>recall</td><td>████▃▁▁▁▁▁█▄▁▁▁▇▄▃▄█▇█▅█▇▅▆▇█▆▆▇▇▇▇▇▇▄▅▆</td></tr><tr><td>train_accuracy</td><td>▂▁▂▁▁▁▁▁▁▃▁▁▁▂▁▄▅▅▅▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇██▇██</td></tr><tr><td>train_loss</td><td>███████▇▇▇▇█▇▇▇▅▅▅▅▄▄▄▃▃▃▃▃▂▃▃▂▂▂▃▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▂▁▁▁▁▂▁▁▆▇▂▄▃▆▇▇▂▂▇█▆█▅██▇██▇██▇▅█▇██</td></tr><tr><td>validation_loss</td><td>▃▄▃▄▃▃▃▃▃▃▃▃▃▃▃▂▂▃▃▃▁▃▁▁█▄▁▂▁▁▁▂▂▁▁▁▁▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81616</td></tr><tr><td>precision</td><td>0.73455</td></tr><tr><td>recall</td><td>0.91818</td></tr><tr><td>train_accuracy</td><td>0.83834</td></tr><tr><td>train_loss</td><td>0.30918</td></tr><tr><td>validation_accuracy</td><td>0.79365</td></tr><tr><td>validation_loss</td><td>0.42489</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/le51kl46' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/le51kl46</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_185032-le51kl46/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 19:13:01,490] Trial 0 finished with values: [0.42488924733230043, 0.7936507936507936] and parameters: {'lr': 0.004200672431740192, 'wd': 8.054570809194863e-06, 'pct_start': 0.2590797714249703}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: lr=8.40e-04, wd=7.06e-06, pct_start=0.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_191301-0820fkj2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/0820fkj2' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/0820fkj2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/0820fkj2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7751 acc=0.5014 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 002 | train_loss=0.7499 acc=0.5099 | val_loss=0.6967 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 003 | train_loss=0.7556 acc=0.4810 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 004 | train_loss=0.7309 acc=0.5179 | val_loss=0.6930 acc=0.5057 | prec=0.5024 rec=0.9591 f1=0.6594 | time=16.8s\n",
            "Epoch 005 | train_loss=0.7340 acc=0.5026 | val_loss=0.6912 acc=0.5125 | prec=0.5061 rec=0.9364 f1=0.6571 | time=16.8s\n",
            "Epoch 006 | train_loss=0.7323 acc=0.4929 | val_loss=0.6932 acc=0.5057 | prec=0.5023 rec=1.0000 f1=0.6687 | time=16.7s\n",
            "Epoch 007 | train_loss=0.7277 acc=0.5281 | val_loss=0.6946 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 008 | train_loss=0.7187 acc=0.5201 | val_loss=0.6894 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 009 | train_loss=0.7293 acc=0.4816 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7081 acc=0.5150 | val_loss=0.6960 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.6s\n",
            "Epoch 011 | train_loss=0.7023 acc=0.4991 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 012 | train_loss=0.6997 acc=0.4946 | val_loss=0.6923 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 013 | train_loss=0.6903 acc=0.5241 | val_loss=0.6866 acc=0.6576 | prec=0.6273 rec=0.7727 f1=0.6925 | time=16.7s\n",
            "Epoch 014 | train_loss=0.6703 acc=0.5910 | val_loss=0.6235 acc=0.6893 | prec=0.8673 rec=0.4455 f1=0.5886 | time=16.8s\n",
            "Epoch 015 | train_loss=0.6625 acc=0.5933 | val_loss=0.5938 acc=0.7506 | prec=0.7895 rec=0.6818 f1=0.7317 | time=16.6s\n",
            "Epoch 016 | train_loss=0.5870 acc=0.7067 | val_loss=0.5012 acc=0.7937 | prec=0.8274 rec=0.7409 f1=0.7818 | time=16.7s\n",
            "Epoch 017 | train_loss=0.5552 acc=0.7221 | val_loss=0.4728 acc=0.7846 | prec=0.8453 rec=0.6955 f1=0.7631 | time=16.8s\n",
            "Epoch 018 | train_loss=0.5185 acc=0.7379 | val_loss=0.5852 acc=0.6508 | prec=0.5917 rec=0.9682 f1=0.7345 | time=16.9s\n",
            "Epoch 019 | train_loss=0.4993 acc=0.7499 | val_loss=0.4755 acc=0.7710 | prec=0.9281 rec=0.5864 f1=0.7187 | time=16.8s\n",
            "Epoch 020 | train_loss=0.4866 acc=0.7674 | val_loss=0.5126 acc=0.7574 | prec=0.9248 rec=0.5591 f1=0.6969 | time=16.9s\n",
            "Epoch 021 | train_loss=0.4692 acc=0.7674 | val_loss=0.8440 acc=0.6871 | prec=0.9362 rec=0.4000 f1=0.5605 | time=16.8s\n",
            "Epoch 022 | train_loss=0.4304 acc=0.7788 | val_loss=0.6198 acc=0.5896 | prec=0.5489 rec=0.9955 f1=0.7076 | time=16.8s\n",
            "Epoch 023 | train_loss=0.4090 acc=0.8037 | val_loss=0.3860 acc=0.8503 | prec=0.9231 rec=0.7636 f1=0.8358 | time=16.8s\n",
            "Epoch 024 | train_loss=0.4039 acc=0.8077 | val_loss=0.6445 acc=0.6190 | prec=0.5684 rec=0.9818 f1=0.7200 | time=16.9s\n",
            "Epoch 025 | train_loss=0.4669 acc=0.7669 | val_loss=0.4060 acc=0.8186 | prec=0.8398 rec=0.7864 f1=0.8122 | time=16.8s\n",
            "Epoch 026 | train_loss=0.4118 acc=0.8157 | val_loss=0.5276 acc=0.7868 | prec=0.7218 rec=0.9318 f1=0.8135 | time=16.9s\n",
            "Epoch 027 | train_loss=0.3851 acc=0.8111 | val_loss=0.4909 acc=0.7347 | prec=0.9402 rec=0.5000 f1=0.6528 | time=16.9s\n",
            "Epoch 028 | train_loss=0.3727 acc=0.8270 | val_loss=0.4289 acc=0.7959 | prec=0.7444 rec=0.9000 f1=0.8148 | time=16.8s\n",
            "Epoch 029 | train_loss=0.3411 acc=0.8480 | val_loss=0.4924 acc=0.7982 | prec=0.8876 rec=0.6818 f1=0.7712 | time=16.8s\n",
            "Epoch 030 | train_loss=0.3573 acc=0.8434 | val_loss=0.6153 acc=0.7324 | prec=0.6614 rec=0.9500 f1=0.7799 | time=16.8s\n",
            "Epoch 031 | train_loss=0.3401 acc=0.8355 | val_loss=0.5142 acc=0.7937 | prec=0.9006 rec=0.6591 f1=0.7612 | time=16.9s\n",
            "Epoch 032 | train_loss=0.3026 acc=0.8531 | val_loss=0.4089 acc=0.8254 | prec=0.8150 rec=0.8409 f1=0.8277 | time=16.8s\n",
            "Epoch 033 | train_loss=0.2804 acc=0.8565 | val_loss=0.4116 acc=0.8390 | prec=0.8371 rec=0.8409 f1=0.8390 | time=16.8s\n",
            "Epoch 034 | train_loss=0.2600 acc=0.8780 | val_loss=0.3947 acc=0.8277 | prec=0.8564 rec=0.7864 f1=0.8199 | time=16.8s\n",
            "Epoch 035 | train_loss=0.2796 acc=0.8508 | val_loss=0.4992 acc=0.7710 | prec=0.7164 rec=0.8955 f1=0.7960 | time=16.8s\n",
            "Epoch 036 | train_loss=0.2655 acc=0.8695 | val_loss=0.5251 acc=0.7211 | prec=0.9145 rec=0.4864 f1=0.6350 | time=16.8s\n",
            "Epoch 037 | train_loss=0.2568 acc=0.8622 | val_loss=0.5497 acc=0.7868 | prec=0.7500 rec=0.8591 f1=0.8008 | time=16.8s\n",
            "Epoch 038 | train_loss=0.2459 acc=0.8724 | val_loss=0.6646 acc=0.7619 | prec=0.7032 rec=0.9045 f1=0.7913 | time=16.8s\n",
            "Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▇▁▇▆▇▇▇▇▁▁▁▇▆▇█▇▇▇▇▆▇█▇██▆█▇█▇████▆██</td></tr><tr><td>precision</td><td>▁▅▁▅▅▅▅▅▅▁▁▁▆▇▇▇▇▅███▅█▅▇▆█▇█▆█▇▇▇▆█▇▆</td></tr><tr><td>recall</td><td>▁█▁██████▁▁▁▆▄▆▆▆█▅▅▄█▆█▇█▅▇▆█▆▇▇▇▇▄▇▇</td></tr><tr><td>train_accuracy</td><td>▁▂▁▂▁▁▂▂▁▂▁▁▂▃▃▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▇▆▅▅▄▄▄▃▃▃▄▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▅▆▇▇▄▆▆▅▃█▃▇▇▆▇▇▆▇███▆▅▇▆</td></tr><tr><td>validation_loss</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▅▄▃▂▄▂▃█▅▁▅▁▃▃▂▃▅▃▁▁▁▃▃▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.79125</td></tr><tr><td>precision</td><td>0.70318</td></tr><tr><td>recall</td><td>0.90455</td></tr><tr><td>train_accuracy</td><td>0.87238</td></tr><tr><td>train_loss</td><td>0.24588</td></tr><tr><td>validation_accuracy</td><td>0.7619</td></tr><tr><td>validation_loss</td><td>0.66456</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/0820fkj2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/0820fkj2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_191301-0820fkj2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 19:23:40,676] Trial 1 finished with values: [0.6645568204777581, 0.7619047619047619] and parameters: {'lr': 0.0008395191170339473, 'wd': 7.0601203612974644e-06, 'pct_start': 0.11064205481520246}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: lr=3.37e-03, wd=1.73e-06, pct_start=0.29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_192340-s7908ycs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/s7908ycs' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/s7908ycs' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/s7908ycs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7828 acc=0.4997 | val_loss=0.7040 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 002 | train_loss=0.7501 acc=0.5105 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 003 | train_loss=0.7311 acc=0.5241 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 004 | train_loss=0.7308 acc=0.4974 | val_loss=0.6919 acc=0.5079 | prec=1.0000 rec=0.0136 f1=0.0269 | time=16.9s\n",
            "Epoch 005 | train_loss=0.7648 acc=0.4895 | val_loss=0.6921 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 006 | train_loss=0.7483 acc=0.5116 | val_loss=0.6906 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 007 | train_loss=0.7459 acc=0.4810 | val_loss=0.6832 acc=0.5125 | prec=0.5057 rec=1.0000 f1=0.6718 | time=17.0s\n",
            "Epoch 008 | train_loss=0.7228 acc=0.5099 | val_loss=0.6923 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 009 | train_loss=0.7076 acc=0.4974 | val_loss=0.6921 acc=0.5646 | prec=0.5348 rec=0.9773 f1=0.6913 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7131 acc=0.4810 | val_loss=0.6972 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 011 | train_loss=0.7081 acc=0.4833 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 012 | train_loss=0.7011 acc=0.4974 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 013 | train_loss=0.6959 acc=0.5071 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 014 | train_loss=0.6994 acc=0.4816 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 015 | train_loss=0.6951 acc=0.5048 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 016 | train_loss=0.6967 acc=0.4918 | val_loss=0.6956 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 017 | train_loss=0.6925 acc=0.5457 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 018 | train_loss=0.6956 acc=0.5031 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 019 | train_loss=0.6978 acc=0.5014 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 020 | train_loss=0.6991 acc=0.4816 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 021 | train_loss=0.6933 acc=0.5179 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 022 | train_loss=0.6992 acc=0.4804 | val_loss=0.6920 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=16.8s\n",
            "Early stopping at epoch 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>██▁▁▁▁█▁█▁▁▁▁▁██▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▄▄▁█▁▁▅▁▅▁▁▁▁▁▄▄▁▁▁▁▁█</td></tr><tr><td>recall</td><td>██▁▁▁▁█▁█▁▁▁▁▁██▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▃▄▆▃▂▄▁▄▃▁▁▃▄▁▄▂█▃▃▁▅▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▇▅▅▃▂▃▂▂▁▂▁▁▁▁▁▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▂▁▁▂▁█▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▅▅▄▄▄▁▄▄▆▅▄▄▄▅▅▄▅▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.00905</td></tr><tr><td>precision</td><td>1</td></tr><tr><td>recall</td><td>0.00455</td></tr><tr><td>train_accuracy</td><td>0.48043</td></tr><tr><td>train_loss</td><td>0.6992</td></tr><tr><td>validation_accuracy</td><td>0.5034</td></tr><tr><td>validation_loss</td><td>0.69201</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/s7908ycs' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/s7908ycs</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_192340-s7908ycs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 19:29:51,670] Trial 2 finished with values: [0.6920116203171867, 0.5034013605442177] and parameters: {'lr': 0.0033651470378963182, 'wd': 1.7252113705444655e-06, 'pct_start': 0.2875866507633068}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: lr=3.08e-03, wd=9.84e-05, pct_start=0.14\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_192951-kxpiyi00</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/kxpiyi00' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/kxpiyi00' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/kxpiyi00</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7372 acc=0.5088 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 002 | train_loss=0.7321 acc=0.4935 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 003 | train_loss=0.7235 acc=0.4770 | val_loss=0.6984 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 004 | train_loss=0.7125 acc=0.4969 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 005 | train_loss=0.7099 acc=0.5088 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 006 | train_loss=0.7223 acc=0.4918 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 007 | train_loss=0.7050 acc=0.4918 | val_loss=0.6931 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=16.8s\n",
            "Epoch 008 | train_loss=0.6962 acc=0.5184 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 009 | train_loss=0.6996 acc=0.4969 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 010 | train_loss=0.6975 acc=0.4946 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 011 | train_loss=0.6933 acc=0.5020 | val_loss=0.6950 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.6s\n",
            "Epoch 012 | train_loss=0.6980 acc=0.5139 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 013 | train_loss=0.6994 acc=0.4878 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=17.0s\n",
            "Epoch 014 | train_loss=0.6989 acc=0.4850 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 015 | train_loss=0.6978 acc=0.4884 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 016 | train_loss=0.6955 acc=0.4833 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 017 | train_loss=0.6978 acc=0.5003 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Early stopping at epoch 17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>███▁█▁▁██▁▁▁▁▁███</td></tr><tr><td>precision</td><td>▄▄▄▁▄▁█▄▄▁▁▁▁▁▄▄▄</td></tr><tr><td>recall</td><td>███▁█▁▁██▁▁▁▁▁███</td></tr><tr><td>train_accuracy</td><td>▆▄▁▄▆▃▃█▄▄▅▇▃▂▃▂▅</td></tr><tr><td>train_loss</td><td>█▇▆▄▄▆▃▁▂▂▁▂▂▂▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▃▁▃█▁▁▃▃▃▃▃▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▁█▁▁▃▁▁▂▂▄▆▃▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.66566</td></tr><tr><td>precision</td><td>0.49887</td></tr><tr><td>recall</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.50028</td></tr><tr><td>train_loss</td><td>0.69781</td></tr><tr><td>validation_accuracy</td><td>0.49887</td></tr><tr><td>validation_loss</td><td>0.69372</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/kxpiyi00' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/kxpiyi00</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_192951-kxpiyi00/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 19:34:37,789] Trial 3 finished with values: [0.6937152658190046, 0.4988662131519274] and parameters: {'lr': 0.0030753629109466817, 'wd': 9.844829583938918e-05, 'pct_start': 0.13602013408175923}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: lr=3.65e-03, wd=2.36e-03, pct_start=0.26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_193437-9snz5a2x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/9snz5a2x' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/9snz5a2x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/9snz5a2x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7317 acc=0.5230 | val_loss=0.7157 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 002 | train_loss=0.7443 acc=0.4895 | val_loss=0.7046 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 003 | train_loss=0.7291 acc=0.4986 | val_loss=0.6900 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 004 | train_loss=0.7375 acc=0.5207 | val_loss=0.6972 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 005 | train_loss=0.7255 acc=0.5031 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 006 | train_loss=0.6765 acc=0.5649 | val_loss=0.6371 acc=0.6372 | prec=0.5857 rec=0.9318 f1=0.7193 | time=16.7s\n",
            "Epoch 007 | train_loss=0.6195 acc=0.6563 | val_loss=0.6085 acc=0.6122 | prec=0.5653 rec=0.9636 f1=0.7126 | time=16.7s\n",
            "Epoch 008 | train_loss=0.5595 acc=0.7124 | val_loss=0.5421 acc=0.7188 | prec=0.6558 rec=0.9182 f1=0.7652 | time=16.8s\n",
            "Epoch 009 | train_loss=0.5491 acc=0.7340 | val_loss=0.5642 acc=0.6621 | prec=0.5989 rec=0.9773 f1=0.7427 | time=16.7s\n",
            "Epoch 010 | train_loss=0.5195 acc=0.7300 | val_loss=0.5344 acc=0.7392 | prec=0.9070 rec=0.5318 f1=0.6705 | time=16.7s\n",
            "Epoch 011 | train_loss=0.5012 acc=0.7493 | val_loss=0.5475 acc=0.6508 | prec=0.5932 rec=0.9545 f1=0.7317 | time=16.7s\n",
            "Epoch 012 | train_loss=0.5010 acc=0.7578 | val_loss=0.4888 acc=0.7392 | prec=0.6688 rec=0.9455 f1=0.7834 | time=16.7s\n",
            "Epoch 013 | train_loss=0.4535 acc=0.7760 | val_loss=1.2365 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 014 | train_loss=0.5283 acc=0.7351 | val_loss=0.6010 acc=0.6485 | prec=0.9333 rec=0.3182 f1=0.4746 | time=16.8s\n",
            "Epoch 015 | train_loss=0.4428 acc=0.7805 | val_loss=0.5607 acc=0.6939 | prec=0.6261 rec=0.9591 f1=0.7576 | time=16.9s\n",
            "Epoch 016 | train_loss=0.4497 acc=0.7952 | val_loss=0.5349 acc=0.7868 | prec=0.9200 rec=0.6273 f1=0.7459 | time=16.8s\n",
            "Epoch 017 | train_loss=0.4553 acc=0.7862 | val_loss=0.7797 acc=0.6349 | prec=0.5782 rec=0.9909 f1=0.7303 | time=16.8s\n",
            "Epoch 018 | train_loss=0.4742 acc=0.7629 | val_loss=0.6773 acc=0.6599 | prec=0.6357 rec=0.7455 f1=0.6862 | time=17.1s\n",
            "Epoch 019 | train_loss=0.4411 acc=0.7981 | val_loss=0.9588 acc=0.6485 | prec=0.9114 rec=0.3273 f1=0.4816 | time=16.9s\n",
            "Epoch 020 | train_loss=0.4317 acc=0.7958 | val_loss=1.7577 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=17.0s\n",
            "Epoch 021 | train_loss=0.5640 acc=0.7164 | val_loss=0.9477 acc=0.6122 | prec=0.9455 rec=0.2364 f1=0.3782 | time=17.2s\n",
            "Epoch 022 | train_loss=0.5275 acc=0.7289 | val_loss=0.5024 acc=0.7778 | prec=0.9122 rec=0.6136 f1=0.7337 | time=17.0s\n",
            "Epoch 023 | train_loss=0.4349 acc=0.7884 | val_loss=2.6175 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=17.2s\n",
            "Epoch 024 | train_loss=0.5328 acc=0.7158 | val_loss=0.4192 acc=0.8095 | prec=0.8208 rec=0.7909 f1=0.8056 | time=17.1s\n",
            "Epoch 025 | train_loss=0.4431 acc=0.7657 | val_loss=0.4545 acc=0.7914 | prec=0.8951 rec=0.6591 f1=0.7592 | time=17.1s\n",
            "Epoch 026 | train_loss=0.4338 acc=0.7941 | val_loss=0.7846 acc=0.5624 | prec=0.5328 rec=0.9955 f1=0.6941 | time=17.2s\n",
            "Epoch 027 | train_loss=0.3918 acc=0.8037 | val_loss=0.5704 acc=0.7755 | prec=0.8380 rec=0.6818 f1=0.7519 | time=17.1s\n",
            "Epoch 028 | train_loss=0.4285 acc=0.8054 | val_loss=0.6095 acc=0.6644 | prec=0.6034 rec=0.9545 f1=0.7394 | time=17.0s\n",
            "Epoch 029 | train_loss=0.3886 acc=0.8174 | val_loss=0.5598 acc=0.6508 | prec=0.5902 rec=0.9818 f1=0.7372 | time=17.2s\n",
            "Epoch 030 | train_loss=0.3845 acc=0.8094 | val_loss=0.5440 acc=0.7256 | prec=0.6774 rec=0.8591 f1=0.7575 | time=17.1s\n",
            "Epoch 031 | train_loss=0.4174 acc=0.7850 | val_loss=0.4832 acc=0.8027 | prec=0.8182 rec=0.7773 f1=0.7972 | time=17.0s\n",
            "Epoch 032 | train_loss=0.4076 acc=0.7964 | val_loss=0.5418 acc=0.7642 | prec=0.8625 rec=0.6273 f1=0.7263 | time=17.3s\n",
            "Epoch 033 | train_loss=0.3738 acc=0.8054 | val_loss=0.6680 acc=0.6735 | prec=0.6105 rec=0.9545 f1=0.7447 | time=17.0s\n",
            "Epoch 034 | train_loss=0.3814 acc=0.8043 | val_loss=0.4645 acc=0.8027 | prec=0.8182 rec=0.7773 f1=0.7972 | time=17.1s\n",
            "Epoch 035 | train_loss=0.3639 acc=0.8219 | val_loss=0.6113 acc=0.7483 | prec=0.8707 rec=0.5818 f1=0.6975 | time=17.3s\n",
            "Epoch 036 | train_loss=0.3935 acc=0.8185 | val_loss=1.9276 acc=0.6145 | prec=0.9808 rec=0.2318 f1=0.3750 | time=17.0s\n",
            "Epoch 037 | train_loss=0.3876 acc=0.7941 | val_loss=0.6619 acc=0.7937 | prec=0.8686 rec=0.6909 f1=0.7696 | time=17.1s\n",
            "Epoch 038 | train_loss=0.3437 acc=0.8264 | val_loss=0.6601 acc=0.7460 | prec=0.7061 rec=0.8409 f1=0.7676 | time=17.1s\n",
            "Epoch 039 | train_loss=0.3505 acc=0.8225 | val_loss=0.4844 acc=0.8050 | prec=0.8641 rec=0.7227 f1=0.7871 | time=17.0s\n",
            "Early stopping at epoch 39\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▇▇▇▇▇▇▇█▇▇▇█▁▅█▇▇▇▅▇▄▇▇██▇█▇▇██▇▇█▇▄███</td></tr><tr><td>precision</td><td>▅▅▅▅▅▅▅▆▅▇▅▆▁█▅█▅▆█▅██▅▇▇▅▇▅▅▆▇▇▅▇▇█▇▆▇</td></tr><tr><td>recall</td><td>███████▇█▅██▁▃█▅█▆▃█▃▅█▇▆█▆██▇▆▅█▆▅▃▆▇▆</td></tr><tr><td>train_accuracy</td><td>▂▁▁▂▁▃▄▆▆▆▆▇▇▆▇▇▇▇▇▇▆▆▇▆▇▇████▇▇████▇██</td></tr><tr><td>train_loss</td><td>█████▇▆▅▅▄▄▄▃▄▃▃▃▃▃▃▅▄▃▄▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▄▄▆▅▆▄▆▁▄▅▇▄▅▄▁▄▇▁██▂▇▅▄▆█▇▅█▇▄█▇█</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▂▁▁▁▁▁▄▂▁▁▂▂▃▅▃▁█▁▁▂▁▂▁▁▁▁▂▁▂▆▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78713</td></tr><tr><td>precision</td><td>0.86413</td></tr><tr><td>recall</td><td>0.72273</td></tr><tr><td>train_accuracy</td><td>0.82246</td></tr><tr><td>train_loss</td><td>0.35049</td></tr><tr><td>validation_accuracy</td><td>0.80499</td></tr><tr><td>validation_loss</td><td>0.48441</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/9snz5a2x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/9snz5a2x</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_193437-9snz5a2x/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 19:45:39,879] Trial 4 finished with values: [0.4844085787023817, 0.8049886621315193] and parameters: {'lr': 0.0036466844800439376, 'wd': 0.002362616064213731, 'pct_start': 0.25599762193132525}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: lr=2.14e-05, wd=1.09e-05, pct_start=0.29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_194539-meuqjaft</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/meuqjaft' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/meuqjaft' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/meuqjaft</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8317 acc=0.4946 | val_loss=0.7527 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 002 | train_loss=0.8098 acc=0.4935 | val_loss=0.7436 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 003 | train_loss=0.7924 acc=0.5031 | val_loss=0.7234 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 004 | train_loss=0.7806 acc=0.5037 | val_loss=0.7087 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 005 | train_loss=0.7594 acc=0.5133 | val_loss=0.6982 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 006 | train_loss=0.7796 acc=0.5026 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 007 | train_loss=0.7629 acc=0.5054 | val_loss=0.6929 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=16.7s\n",
            "Epoch 008 | train_loss=0.7576 acc=0.4980 | val_loss=0.6932 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=17.0s\n",
            "Epoch 009 | train_loss=0.7469 acc=0.5014 | val_loss=0.6929 acc=0.5374 | prec=0.5317 rec=0.6091 f1=0.5678 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7431 acc=0.5082 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 011 | train_loss=0.7422 acc=0.4878 | val_loss=0.6931 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=16.9s\n",
            "Epoch 012 | train_loss=0.7475 acc=0.4957 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 013 | train_loss=0.7451 acc=0.4844 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=0.9955 f1=0.6646 | time=16.6s\n",
            "Epoch 014 | train_loss=0.7355 acc=0.4963 | val_loss=0.6964 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 015 | train_loss=0.7643 acc=0.4742 | val_loss=0.6930 acc=0.4875 | prec=0.4915 rec=0.7864 f1=0.6049 | time=16.8s\n",
            "Epoch 016 | train_loss=0.7323 acc=0.5133 | val_loss=0.6930 acc=0.5102 | prec=0.5088 rec=0.5227 f1=0.5157 | time=16.8s\n",
            "Epoch 017 | train_loss=0.7348 acc=0.5014 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 018 | train_loss=0.7475 acc=0.4969 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 019 | train_loss=0.7401 acc=0.5031 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 020 | train_loss=0.7284 acc=0.5088 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 021 | train_loss=0.7426 acc=0.4963 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 022 | train_loss=0.7271 acc=0.4969 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 023 | train_loss=0.7361 acc=0.4867 | val_loss=0.6929 acc=0.5011 | prec=0.5000 rec=0.9955 f1=0.6657 | time=16.6s\n",
            "Epoch 024 | train_loss=0.7304 acc=0.4918 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 025 | train_loss=0.7299 acc=0.4923 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 026 | train_loss=0.7284 acc=0.5003 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 027 | train_loss=0.7370 acc=0.4697 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 028 | train_loss=0.7349 acc=0.5003 | val_loss=0.6929 acc=0.5306 | prec=0.5253 rec=0.6136 f1=0.5660 | time=16.8s\n",
            "Epoch 029 | train_loss=0.7287 acc=0.4963 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=0.9955 f1=0.6646 | time=16.7s\n",
            "Epoch 030 | train_loss=0.7271 acc=0.5014 | val_loss=0.6929 acc=0.4966 | prec=0.4977 rec=0.9909 f1=0.6626 | time=16.9s\n",
            "Epoch 031 | train_loss=0.7339 acc=0.4935 | val_loss=0.6930 acc=0.5102 | prec=0.5526 rec=0.0955 f1=0.1628 | time=16.8s\n",
            "Epoch 032 | train_loss=0.7297 acc=0.4940 | val_loss=0.6931 acc=0.5079 | prec=0.6364 rec=0.0318 f1=0.0606 | time=16.7s\n",
            "Epoch 033 | train_loss=0.7195 acc=0.5179 | val_loss=0.6934 acc=0.4989 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 034 | train_loss=0.7082 acc=0.5332 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 035 | train_loss=0.7135 acc=0.5196 | val_loss=0.6929 acc=0.5147 | prec=0.5750 rec=0.1045 f1=0.1769 | time=16.6s\n",
            "Epoch 036 | train_loss=0.7247 acc=0.4861 | val_loss=0.6929 acc=0.5125 | prec=0.5714 rec=0.0909 f1=0.1569 | time=16.8s\n",
            "Epoch 037 | train_loss=0.7126 acc=0.5235 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 038 | train_loss=0.7267 acc=0.4991 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>██████▁▁▇█▁▁██▇▆██████████▁▇██▃▂▁█▃▃█▁</td></tr><tr><td>precision</td><td>▄▄▄▄▄▄██▅▄█▁▄▄▄▅▄▄▄▄▄▄▅▄▄▄▁▅▄▄▅▅▁▄▅▅▄▁</td></tr><tr><td>recall</td><td>██████▁▁▅█▁▁██▇▅██████████▁▅██▂▁▁█▂▂█▁</td></tr><tr><td>train_accuracy</td><td>▄▄▅▅▆▅▅▄▅▅▃▄▃▄▂▆▅▄▅▅▄▄▃▃▃▄▁▄▄▅▄▄▆█▆▃▇▄</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▅▄▄▃▃▃▃▃▃▄▂▃▃▃▂▃▂▃▂▂▂▃▃▂▂▂▂▂▁▁▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▃▃▃█▃▄▃▃▃▁▄▃▃▃▃▃▃▃▃▃▃▃▇▃▂▄▄▃▃▅▅▃▃</td></tr><tr><td>validation_loss</td><td>█▇▅▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.49915</td></tr><tr><td>train_loss</td><td>0.72671</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69312</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/meuqjaft' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/meuqjaft</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_194539-meuqjaft/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 19:56:18,379] Trial 5 finished with values: [0.6931181337152209, 0.5011337868480725] and parameters: {'lr': 2.1363256090034322e-05, 'wd': 1.0873031815175781e-05, 'pct_start': 0.29161276165305083}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: lr=3.75e-04, wd=6.89e-02, pct_start=0.19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_195618-lkyx1z6v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/lkyx1z6v' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/lkyx1z6v' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/lkyx1z6v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8190 acc=0.4957 | val_loss=0.6938 acc=0.4853 | prec=0.4915 rec=0.9227 f1=0.6414 | time=17.0s\n",
            "Epoch 002 | train_loss=0.7918 acc=0.4884 | val_loss=0.7083 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 003 | train_loss=0.7605 acc=0.5201 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 004 | train_loss=0.7515 acc=0.5122 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 005 | train_loss=0.7686 acc=0.4765 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 006 | train_loss=0.7418 acc=0.5269 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 007 | train_loss=0.7487 acc=0.4940 | val_loss=0.6981 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 008 | train_loss=0.7615 acc=0.4986 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 009 | train_loss=0.7335 acc=0.5037 | val_loss=0.6926 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 010 | train_loss=0.7364 acc=0.4940 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 011 | train_loss=0.7303 acc=0.4872 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 012 | train_loss=0.7200 acc=0.5139 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 013 | train_loss=0.7627 acc=0.4731 | val_loss=0.6927 acc=0.5011 | prec=0.5000 rec=0.0045 f1=0.0090 | time=16.7s\n",
            "Epoch 014 | train_loss=0.7409 acc=0.5071 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 015 | train_loss=0.7381 acc=0.5088 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 016 | train_loss=0.7379 acc=0.5094 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 017 | train_loss=0.7344 acc=0.4759 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 018 | train_loss=0.7208 acc=0.4969 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 019 | train_loss=0.7135 acc=0.4855 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 020 | train_loss=0.7089 acc=0.5088 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 021 | train_loss=0.7149 acc=0.4963 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.6s\n",
            "Epoch 022 | train_loss=0.7181 acc=0.4782 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 023 | train_loss=0.7010 acc=0.5026 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 024 | train_loss=0.6992 acc=0.5173 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>████████████▁███████▁███</td></tr><tr><td>precision</td><td>████████████████████▁███</td></tr><tr><td>recall</td><td>▇███████████▁███████▁███</td></tr><tr><td>train_accuracy</td><td>▄▃▇▆▁█▄▄▅▄▃▆▁▅▆▆▁▄▃▆▄▂▅▇</td></tr><tr><td>train_loss</td><td>█▆▅▄▅▃▄▅▃▃▃▂▅▃▃▃▃▂▂▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>validation_loss</td><td>▂█▂▂▂▂▃▂▁▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.66566</td></tr><tr><td>precision</td><td>0.49887</td></tr><tr><td>recall</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.5173</td></tr><tr><td>train_loss</td><td>0.69919</td></tr><tr><td>validation_accuracy</td><td>0.49887</td></tr><tr><td>validation_loss</td><td>0.69366</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/lkyx1z6v' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/lkyx1z6v</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_195618-lkyx1z6v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 20:03:02,208] Trial 6 finished with values: [0.6936610043048859, 0.4988662131519274] and parameters: {'lr': 0.00037484033346192407, 'wd': 0.06892841907306128, 'pct_start': 0.19007078282928078}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: lr=1.10e-05, wd=7.88e-04, pct_start=0.28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_200302-wu1sn7zg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/wu1sn7zg' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/wu1sn7zg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/wu1sn7zg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7838 acc=0.5190 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 002 | train_loss=0.7709 acc=0.5173 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 003 | train_loss=0.7950 acc=0.4872 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 004 | train_loss=0.7980 acc=0.4957 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 005 | train_loss=0.7910 acc=0.5043 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 006 | train_loss=0.8118 acc=0.4674 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 007 | train_loss=0.7498 acc=0.5315 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 008 | train_loss=0.7712 acc=0.4940 | val_loss=0.6929 acc=0.5306 | prec=0.5253 rec=0.6136 f1=0.5660 | time=16.7s\n",
            "Epoch 009 | train_loss=0.7651 acc=0.5071 | val_loss=0.6928 acc=0.5306 | prec=0.5489 rec=0.3318 f1=0.4136 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7832 acc=0.5128 | val_loss=0.6927 acc=0.5238 | prec=0.5439 rec=0.2818 f1=0.3713 | time=16.7s\n",
            "Epoch 011 | train_loss=0.7659 acc=0.5014 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 012 | train_loss=0.7667 acc=0.4901 | val_loss=0.6931 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=16.9s\n",
            "Epoch 013 | train_loss=0.7572 acc=0.5071 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 014 | train_loss=0.7455 acc=0.5054 | val_loss=0.6929 acc=0.4921 | prec=0.4938 rec=0.7273 f1=0.5882 | time=16.8s\n",
            "Epoch 015 | train_loss=0.7512 acc=0.4935 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 016 | train_loss=0.7442 acc=0.4974 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 017 | train_loss=0.7273 acc=0.5105 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 018 | train_loss=0.7397 acc=0.5026 | val_loss=0.6930 acc=0.5261 | prec=0.5385 rec=0.3500 f1=0.4242 | time=16.8s\n",
            "Epoch 019 | train_loss=0.7279 acc=0.5190 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 020 | train_loss=0.7436 acc=0.4997 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 021 | train_loss=0.7220 acc=0.5167 | val_loss=0.6932 acc=0.5034 | prec=0.6667 rec=0.0091 f1=0.0179 | time=16.9s\n",
            "Epoch 022 | train_loss=0.7307 acc=0.4872 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 023 | train_loss=0.7530 acc=0.4861 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 024 | train_loss=0.7186 acc=0.5094 | val_loss=0.6931 acc=0.5147 | prec=0.5176 rec=0.4000 f1=0.4513 | time=16.8s\n",
            "Epoch 025 | train_loss=0.7310 acc=0.4878 | val_loss=0.6931 acc=0.4875 | prec=0.4926 rec=0.9091 f1=0.6390 | time=16.8s\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>███████▇▅▅▁▁▁▇▁█▁▅█▁▁▁▁▆█</td></tr><tr><td>precision</td><td>▄▄▄▄▄▄▄▅▅▅▁█▁▄▁▄▁▅▄▁▆▁▁▅▄</td></tr><tr><td>recall</td><td>███████▅▃▃▁▁▁▆▁█▁▃█▁▁▁▁▄▇</td></tr><tr><td>train_accuracy</td><td>▇▆▃▄▅▁█▄▅▆▅▃▅▅▄▄▆▅▇▅▆▃▃▆▃</td></tr><tr><td>train_loss</td><td>▆▅▇▇▆█▃▅▄▆▅▅▄▃▃▃▂▃▂▃▁▂▄▁▂</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▃▃██▇▃▄▃▂▃▃▃▇▃▃▄▃▃▅▁</td></tr><tr><td>validation_loss</td><td>▅▇▆██▄▃▁▁▁▃▂▃▂▄▃▃▂▄▃▃▃▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.63898</td></tr><tr><td>precision</td><td>0.49261</td></tr><tr><td>recall</td><td>0.90909</td></tr><tr><td>train_accuracy</td><td>0.4878</td></tr><tr><td>train_loss</td><td>0.73102</td></tr><tr><td>validation_accuracy</td><td>0.48753</td></tr><tr><td>validation_loss</td><td>0.69314</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/wu1sn7zg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/wu1sn7zg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_200302-wu1sn7zg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 20:10:02,683] Trial 7 finished with values: [0.693137845822743, 0.4875283446712018] and parameters: {'lr': 1.1029092745969571e-05, 'wd': 0.0007878234979375977, 'pct_start': 0.2844365139790766}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: lr=6.50e-05, wd=4.72e-04, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_201002-rmhh7c07</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/rmhh7c07' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/rmhh7c07' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/rmhh7c07</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7764 acc=0.5179 | val_loss=0.7020 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 002 | train_loss=0.7662 acc=0.5190 | val_loss=0.7022 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 003 | train_loss=0.7879 acc=0.4804 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.6s\n",
            "Epoch 004 | train_loss=0.7590 acc=0.5054 | val_loss=0.7055 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 005 | train_loss=0.7764 acc=0.4923 | val_loss=0.7033 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 006 | train_loss=0.7869 acc=0.4855 | val_loss=0.7086 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 007 | train_loss=0.7623 acc=0.5133 | val_loss=0.7089 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 008 | train_loss=0.7700 acc=0.4844 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 009 | train_loss=0.7618 acc=0.4782 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7531 acc=0.4980 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 011 | train_loss=0.7729 acc=0.4731 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 012 | train_loss=0.7380 acc=0.5020 | val_loss=0.6972 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 013 | train_loss=0.7460 acc=0.4867 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 014 | train_loss=0.7128 acc=0.5275 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.6s\n",
            "Epoch 015 | train_loss=0.7401 acc=0.4940 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 016 | train_loss=0.7349 acc=0.5116 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.6s\n",
            "Epoch 017 | train_loss=0.7472 acc=0.4969 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 018 | train_loss=0.7454 acc=0.5037 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.9s\n",
            "Epoch 019 | train_loss=0.7589 acc=0.4787 | val_loss=0.7011 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.5s\n",
            "Epoch 020 | train_loss=0.7216 acc=0.5077 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 021 | train_loss=0.7296 acc=0.4997 | val_loss=0.6911 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.8s\n",
            "Epoch 022 | train_loss=0.7160 acc=0.5156 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=16.7s\n",
            "Epoch 023 | train_loss=0.7072 acc=0.5417 | val_loss=0.6786 acc=0.5556 | prec=0.9286 rec=0.1182 f1=0.2097 | time=16.7s\n",
            "Epoch 024 | train_loss=0.6953 acc=0.5655 | val_loss=0.6469 acc=0.7120 | prec=0.7719 rec=0.6000 f1=0.6752 | time=16.8s\n",
            "Epoch 025 | train_loss=0.6603 acc=0.5922 | val_loss=0.5982 acc=0.7415 | prec=0.7704 rec=0.6864 f1=0.7260 | time=16.8s\n",
            "Epoch 026 | train_loss=0.6315 acc=0.6234 | val_loss=0.5772 acc=0.7211 | prec=0.8392 rec=0.5455 f1=0.6612 | time=16.7s\n",
            "Epoch 027 | train_loss=0.6023 acc=0.6704 | val_loss=0.5354 acc=0.7710 | prec=0.7902 rec=0.7364 f1=0.7624 | time=16.8s\n",
            "Epoch 028 | train_loss=0.5914 acc=0.7073 | val_loss=0.5579 acc=0.7234 | prec=0.8769 rec=0.5182 f1=0.6514 | time=16.8s\n",
            "Epoch 029 | train_loss=0.5806 acc=0.6892 | val_loss=0.5355 acc=0.7483 | prec=0.7072 rec=0.8455 f1=0.7702 | time=16.7s\n",
            "Epoch 030 | train_loss=0.5646 acc=0.7107 | val_loss=0.5064 acc=0.7868 | prec=0.8088 rec=0.7500 f1=0.7783 | time=16.8s\n",
            "Epoch 031 | train_loss=0.5523 acc=0.7408 | val_loss=0.4878 acc=0.7959 | prec=0.8533 rec=0.7136 f1=0.7772 | time=16.8s\n",
            "Epoch 032 | train_loss=0.5370 acc=0.7379 | val_loss=0.4919 acc=0.8005 | prec=0.8367 rec=0.7455 f1=0.7885 | time=16.6s\n",
            "Epoch 033 | train_loss=0.5383 acc=0.7323 | val_loss=0.4812 acc=0.8073 | prec=0.8358 rec=0.7636 f1=0.7981 | time=16.8s\n",
            "Epoch 034 | train_loss=0.5047 acc=0.7487 | val_loss=0.5088 acc=0.7642 | prec=0.7266 rec=0.8455 f1=0.7815 | time=16.9s\n",
            "Epoch 035 | train_loss=0.5095 acc=0.7629 | val_loss=0.5006 acc=0.7732 | prec=0.9110 rec=0.6045 f1=0.7268 | time=16.8s\n",
            "Epoch 036 | train_loss=0.4967 acc=0.7635 | val_loss=0.5021 acc=0.7574 | prec=0.7025 rec=0.8909 f1=0.7856 | time=16.7s\n",
            "Epoch 037 | train_loss=0.5072 acc=0.7618 | val_loss=0.5058 acc=0.7370 | prec=0.6818 rec=0.8864 f1=0.7708 | time=16.8s\n",
            "Epoch 038 | train_loss=0.4755 acc=0.7748 | val_loss=0.4688 acc=0.8095 | prec=0.8208 rec=0.7909 f1=0.8056 | time=16.7s\n",
            "Epoch 039 | train_loss=0.4824 acc=0.7686 | val_loss=0.4631 acc=0.8027 | prec=0.8634 rec=0.7182 f1=0.7841 | time=16.6s\n",
            "Epoch 040 | train_loss=0.4719 acc=0.7731 | val_loss=0.4820 acc=0.7732 | prec=0.7419 rec=0.8364 f1=0.7863 | time=16.9s\n",
            "Epoch 041 | train_loss=0.4442 acc=0.7901 | val_loss=0.4642 acc=0.8050 | prec=0.8807 rec=0.7045 f1=0.7828 | time=16.7s\n",
            "Epoch 042 | train_loss=0.4349 acc=0.8100 | val_loss=0.4816 acc=0.7732 | prec=0.7679 rec=0.7818 f1=0.7748 | time=16.8s\n",
            "Epoch 043 | train_loss=0.4564 acc=0.7805 | val_loss=0.4660 acc=0.7982 | prec=0.8359 rec=0.7409 f1=0.7855 | time=16.9s\n",
            "Epoch 044 | train_loss=0.4374 acc=0.8140 | val_loss=0.4496 acc=0.7937 | prec=0.8177 rec=0.7545 f1=0.7849 | time=16.7s\n",
            "Epoch 045 | train_loss=0.4312 acc=0.7930 | val_loss=0.4727 acc=0.7732 | prec=0.7290 rec=0.8682 f1=0.7925 | time=16.8s\n",
            "Epoch 046 | train_loss=0.3996 acc=0.8230 | val_loss=0.4778 acc=0.7732 | prec=0.7500 rec=0.8182 f1=0.7826 | time=16.9s\n",
            "Epoch 047 | train_loss=0.4236 acc=0.8117 | val_loss=0.4901 acc=0.7823 | prec=0.7385 rec=0.8727 f1=0.8000 | time=16.7s\n",
            "Epoch 048 | train_loss=0.3849 acc=0.8417 | val_loss=0.4535 acc=0.7982 | prec=0.8134 rec=0.7727 f1=0.7925 | time=16.7s\n",
            "Epoch 049 | train_loss=0.3987 acc=0.8236 | val_loss=0.4862 acc=0.7710 | prec=0.7409 rec=0.8318 f1=0.7837 | time=16.9s\n",
            "Epoch 050 | train_loss=0.4018 acc=0.8157 | val_loss=0.4810 acc=0.7778 | prec=0.8720 rec=0.6500 f1=0.7448 | time=16.8s\n",
            "Epoch 051 | train_loss=0.4291 acc=0.8083 | val_loss=0.4752 acc=0.7732 | prec=0.7586 rec=0.8000 f1=0.7788 | time=16.7s\n",
            "Epoch 052 | train_loss=0.4167 acc=0.8049 | val_loss=0.5137 acc=0.7619 | prec=0.7170 rec=0.8636 f1=0.7835 | time=16.7s\n",
            "Epoch 053 | train_loss=0.3917 acc=0.8242 | val_loss=0.4754 acc=0.7800 | prec=0.7733 rec=0.7909 f1=0.7820 | time=16.8s\n",
            "Epoch 054 | train_loss=0.3800 acc=0.8293 | val_loss=0.4746 acc=0.7778 | prec=0.7585 rec=0.8136 f1=0.7851 | time=16.8s\n",
            "Epoch 055 | train_loss=0.3927 acc=0.8230 | val_loss=0.4769 acc=0.7778 | prec=0.8144 rec=0.7182 f1=0.7633 | time=16.7s\n",
            "Epoch 056 | train_loss=0.3783 acc=0.8355 | val_loss=0.5075 acc=0.7551 | prec=0.7205 rec=0.8318 f1=0.7722 | time=16.8s\n",
            "Epoch 057 | train_loss=0.3806 acc=0.8293 | val_loss=0.5283 acc=0.7483 | prec=0.6968 rec=0.8773 f1=0.7767 | time=16.7s\n",
            "Epoch 058 | train_loss=0.3679 acc=0.8372 | val_loss=0.5120 acc=0.7619 | prec=0.7291 rec=0.8318 f1=0.7771 | time=16.8s\n",
            "Epoch 059 | train_loss=0.3919 acc=0.8264 | val_loss=0.4719 acc=0.7846 | prec=0.7706 rec=0.8091 f1=0.7894 | time=17.0s\n",
            "Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▇▁▁▁▁▇▁▁▁▇▇▇█████▇████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▅▁▁▁█▇▇▇█▆▇▇▇▆█▆▆█▇▇▇▆▇▇█▇▆▇▇▆▆▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▂▆▅▆▅▆▆▆▆▇▇▆▇▆▆▇▇▇▆▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▂▁▂▁▂▁▁▂▁▁▁▂▂▂▂▄▅▅▅▆▆▇▇▆▇▇▇▇▇▇███▇▇█████</td></tr><tr><td>train_loss</td><td>█████▇█▇▇▇▇▇▇█▇▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▁▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▆▆▇▆███▇▇█▇█▇█▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>██████████████▇▄▃▄▃▂▂▂▃▂▂▁▁▂▂▁▂▂▁▂▂▃▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78936</td></tr><tr><td>precision</td><td>0.77056</td></tr><tr><td>recall</td><td>0.80909</td></tr><tr><td>train_accuracy</td><td>0.82643</td></tr><tr><td>train_loss</td><td>0.39192</td></tr><tr><td>validation_accuracy</td><td>0.78458</td></tr><tr><td>validation_loss</td><td>0.47194</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/rmhh7c07' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/rmhh7c07</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_201002-rmhh7c07/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 20:26:32,820] Trial 8 finished with values: [0.4719352764742715, 0.7845804988662132] and parameters: {'lr': 6.499684966379918e-05, 'wd': 0.0004715924792286669, 'pct_start': 0.2306706241454232}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: lr=1.83e-04, wd=5.80e-03, pct_start=0.29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_202632-bbs6bq8s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/bbs6bq8s' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/bbs6bq8s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/bbs6bq8s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7475 acc=0.4901 | val_loss=0.7048 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 002 | train_loss=0.7315 acc=0.5128 | val_loss=0.6970 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 003 | train_loss=0.7135 acc=0.5037 | val_loss=0.6932 acc=0.5170 | prec=0.5202 rec=0.4091 f1=0.4580 | time=16.7s\n",
            "Epoch 004 | train_loss=0.7262 acc=0.4991 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 005 | train_loss=0.7317 acc=0.4850 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 006 | train_loss=0.7100 acc=0.5162 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 007 | train_loss=0.7215 acc=0.5111 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 008 | train_loss=0.7349 acc=0.4844 | val_loss=0.7001 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 009 | train_loss=0.7163 acc=0.5116 | val_loss=0.6932 acc=0.5034 | prec=0.5098 rec=0.1182 f1=0.1919 | time=16.7s\n",
            "Epoch 010 | train_loss=0.7247 acc=0.4969 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 011 | train_loss=0.7250 acc=0.4957 | val_loss=0.6951 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 012 | train_loss=0.7084 acc=0.5077 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 013 | train_loss=0.7181 acc=0.4816 | val_loss=0.6928 acc=0.5238 | prec=0.5321 rec=0.3773 f1=0.4415 | time=16.7s\n",
            "Epoch 014 | train_loss=0.7133 acc=0.4991 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 015 | train_loss=0.7224 acc=0.4872 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.9s\n",
            "Epoch 016 | train_loss=0.7194 acc=0.4878 | val_loss=0.6906 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.8s\n",
            "Epoch 017 | train_loss=0.7030 acc=0.5167 | val_loss=0.6925 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=16.7s\n",
            "Epoch 018 | train_loss=0.7051 acc=0.5252 | val_loss=0.6823 acc=0.5170 | prec=0.5081 rec=1.0000 f1=0.6738 | time=16.8s\n",
            "Epoch 019 | train_loss=0.6770 acc=0.5655 | val_loss=0.6229 acc=0.6939 | prec=0.6308 rec=0.9318 f1=0.7523 | time=16.8s\n",
            "Epoch 020 | train_loss=0.6189 acc=0.6614 | val_loss=0.5360 acc=0.7642 | prec=0.7990 rec=0.7045 f1=0.7488 | time=16.7s\n",
            "Epoch 021 | train_loss=0.5867 acc=0.7079 | val_loss=0.5576 acc=0.7596 | prec=0.8608 rec=0.6182 f1=0.7196 | time=16.7s\n",
            "Epoch 022 | train_loss=0.5652 acc=0.7022 | val_loss=0.5318 acc=0.7415 | prec=0.6767 rec=0.9227 f1=0.7808 | time=16.8s\n",
            "Epoch 023 | train_loss=0.5180 acc=0.7402 | val_loss=0.5346 acc=0.7234 | prec=0.6581 rec=0.9273 f1=0.7698 | time=16.9s\n",
            "Epoch 024 | train_loss=0.5285 acc=0.7385 | val_loss=0.5552 acc=0.6916 | prec=0.6273 rec=0.9409 f1=0.7527 | time=16.8s\n",
            "Epoch 025 | train_loss=0.4914 acc=0.7561 | val_loss=0.4580 acc=0.8163 | prec=0.8424 rec=0.7773 f1=0.8085 | time=16.7s\n",
            "Epoch 026 | train_loss=0.4895 acc=0.7589 | val_loss=0.5165 acc=0.7188 | prec=0.6481 rec=0.9545 f1=0.7721 | time=16.8s\n",
            "Epoch 027 | train_loss=0.4554 acc=0.7879 | val_loss=0.4572 acc=0.8141 | prec=0.8416 rec=0.7727 f1=0.8057 | time=16.9s\n",
            "Epoch 028 | train_loss=0.4327 acc=0.7986 | val_loss=0.4246 acc=0.8254 | prec=0.8178 rec=0.8364 f1=0.8270 | time=16.8s\n",
            "Epoch 029 | train_loss=0.4251 acc=0.8276 | val_loss=0.4423 acc=0.8186 | prec=0.8763 rec=0.7409 f1=0.8030 | time=16.7s\n",
            "Epoch 030 | train_loss=0.4143 acc=0.8094 | val_loss=0.4770 acc=0.7914 | prec=0.8951 rec=0.6591 f1=0.7592 | time=16.7s\n",
            "Epoch 031 | train_loss=0.4099 acc=0.8123 | val_loss=0.5166 acc=0.7347 | prec=0.6667 rec=0.9364 f1=0.7788 | time=16.9s\n",
            "Epoch 032 | train_loss=0.4181 acc=0.7941 | val_loss=0.4560 acc=0.7710 | prec=0.7148 rec=0.9000 f1=0.7968 | time=16.7s\n",
            "Epoch 033 | train_loss=0.3691 acc=0.8276 | val_loss=0.4991 acc=0.7528 | prec=0.6894 rec=0.9182 f1=0.7875 | time=16.7s\n",
            "Epoch 034 | train_loss=0.3682 acc=0.8196 | val_loss=0.4382 acc=0.8050 | prec=0.8350 rec=0.7591 f1=0.7952 | time=16.9s\n",
            "Epoch 035 | train_loss=0.3542 acc=0.8349 | val_loss=0.4225 acc=0.8073 | prec=0.7922 rec=0.8318 f1=0.8115 | time=16.7s\n",
            "Epoch 036 | train_loss=0.3518 acc=0.8452 | val_loss=0.4284 acc=0.8050 | prec=0.8284 rec=0.7682 f1=0.7972 | time=16.6s\n",
            "Epoch 037 | train_loss=0.3436 acc=0.8423 | val_loss=0.4280 acc=0.8209 | prec=0.8133 rec=0.8318 f1=0.8225 | time=16.9s\n",
            "Epoch 038 | train_loss=0.3247 acc=0.8593 | val_loss=0.4519 acc=0.7959 | prec=0.7876 rec=0.8091 f1=0.7982 | time=16.8s\n",
            "Epoch 039 | train_loss=0.3138 acc=0.8605 | val_loss=0.4418 acc=0.8050 | prec=0.8526 rec=0.7364 f1=0.7902 | time=16.7s\n",
            "Epoch 040 | train_loss=0.3285 acc=0.8434 | val_loss=0.4658 acc=0.7868 | prec=0.7669 rec=0.8227 f1=0.7939 | time=16.8s\n",
            "Epoch 041 | train_loss=0.3293 acc=0.8452 | val_loss=0.4737 acc=0.7914 | prec=0.8636 rec=0.6909 f1=0.7677 | time=16.7s\n",
            "Epoch 042 | train_loss=0.2984 acc=0.8571 | val_loss=0.5498 acc=0.7846 | prec=0.8788 rec=0.6591 f1=0.7532 | time=16.8s\n",
            "Epoch 043 | train_loss=0.3151 acc=0.8542 | val_loss=0.4447 acc=0.8163 | prec=0.8233 rec=0.8045 f1=0.8138 | time=16.7s\n",
            "Epoch 044 | train_loss=0.2747 acc=0.8684 | val_loss=0.5277 acc=0.7937 | prec=0.8772 rec=0.6818 f1=0.7673 | time=16.8s\n",
            "Epoch 045 | train_loss=0.2759 acc=0.8718 | val_loss=0.4907 acc=0.7868 | prec=0.7917 rec=0.7773 f1=0.7844 | time=16.7s\n",
            "Epoch 046 | train_loss=0.2920 acc=0.8826 | val_loss=0.4955 acc=0.7891 | prec=0.8470 rec=0.7045 f1=0.7692 | time=16.8s\n",
            "Epoch 047 | train_loss=0.3061 acc=0.8417 | val_loss=0.4514 acc=0.8027 | prec=0.8122 rec=0.7864 f1=0.7991 | time=16.8s\n",
            "Epoch 048 | train_loss=0.2929 acc=0.8763 | val_loss=0.5640 acc=0.7619 | prec=0.8808 rec=0.6045 f1=0.7170 | time=16.7s\n",
            "Epoch 049 | train_loss=0.3003 acc=0.8605 | val_loss=0.4676 acc=0.7959 | prec=0.7851 rec=0.8136 f1=0.7991 | time=16.8s\n",
            "Epoch 050 | train_loss=0.2896 acc=0.8661 | val_loss=0.4530 acc=0.7778 | prec=0.7346 rec=0.8682 f1=0.7958 | time=16.8s\n",
            "Early stopping at epoch 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▄▆▆▆▆▁▆▆▄▆▆▆▆▇▇▇▇▇█▇██▇███████▇▇██▇█▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▃▇█▄▄▇▄▇▇█▅▇▆▇▇▇▆██▇▆▇▇█▅</td></tr><tr><td>recall</td><td>██▃████▁██▃████▆▅▇▇██▆▇▆▅▇▇▆▇▆▆▆▇▆▅▅▆▆▆▇</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▁▂▁▂▁▁▁▁▁▁▂▂▄▅▅▆▆▆▆▇▇▇▆▇▇▇██▇████▇██</td></tr><tr><td>train_loss</td><td>██▇████████▇██▇▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▅▇▇▆▆█▆███▆▇████▇▇▇█▇▇█▇▇</td></tr><tr><td>validation_loss</td><td>██████████████▇▄▄▄▄▄▃▂▁▁▂▂▃▁▁▁▁▂▂▄▂▃▃▂▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.79583</td></tr><tr><td>precision</td><td>0.73462</td></tr><tr><td>recall</td><td>0.86818</td></tr><tr><td>train_accuracy</td><td>0.86614</td></tr><tr><td>train_loss</td><td>0.28958</td></tr><tr><td>validation_accuracy</td><td>0.77778</td></tr><tr><td>validation_loss</td><td>0.453</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/bbs6bq8s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2/runs/bbs6bq8s</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_202632-bbs6bq8s/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 20:40:32,569] Trial 9 finished with values: [0.4529983890908105, 0.7777777777777778] and parameters: {'lr': 0.00018265124173547746, 'wd': 0.0058040494218769565, 'pct_start': 0.28932639472707466}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pareto-optimal trials (val_loss, val_acc) and their params:\n",
            " Trial #0: values=[0.42488924733230043, 0.7936507936507936]\n",
            "              params={'lr': 0.004200672431740192, 'wd': 8.054570809194863e-06, 'pct_start': 0.2590797714249703}\n",
            " Trial #4: values=[0.4844085787023817, 0.8049886621315193]\n",
            "              params={'lr': 0.0036466844800439376, 'wd': 0.002362616064213731, 'pct_start': 0.25599762193132525}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mid-Results Report (Best Trial)\n",
        "\n",
        "#### [Block = 1, Head = 3] 5 Fold Cross Validation (Results)\n",
        "1] Trial 0: lr=3.95e-05, wd=4.65e-06, pct_start=0.26\n",
        "* Epoch 084 | train_loss=0.4005 acc=0.8100 | val_loss=0.3896 acc=0.8277 | prec=0.8673 rec=0.7727 f1=0.8173 | time=12.8s\n",
        "Early stopping at epoch 84\n",
        "\n",
        "#### Block = 1, Head = 3 (Best Trial)\n",
        "1] Trial 7: lr=7.13e-05, wd=1.30e-05, pct_start=0.23\n",
        "- Epoch 088 | train_loss=0.2949 acc=0.8525 | val_loss=0.3569 acc=0.8458 | prec=0.8220 rec=0.8818 f1=0.8509 | time=12.9s\n",
        "Early stopping at epoch 88\n",
        "\n",
        "2] Trial 8: lr=3.44e-05, wd=3.04e-06, pct_start=0.14\n",
        "- Epoch 089 | train_loss=0.4120 acc=0.8100 | val_loss=0.4381 acc=0.8186 | prec=0.7734 rec=0.9000 f1=0.8319 | time=13.0s\n",
        "Early stopping at epoch 89\n",
        "\n",
        "#### [Block = 2, Head = 2] (Best Trial)\n",
        "\n",
        "1) Trial 0: lr=4.20e-03, wd=8.05e-06, pct_start=0.26\n",
        "\n",
        "* Epoch 071 | train_loss=0.3092 acc=0.8383 | val_loss=0.4249 acc=0.7937 | prec=0.7345 rec=0.9182 f1=0.8162 | time=16.9s\n",
        "* Early stopping at epoch 71\n",
        "\n",
        "2) Trial 9: lr=1.83e-04, wd=5.80e-03, pct_start=0.29\n",
        "\n",
        "* Epoch 050 | train_loss=0.2896 acc=0.8661 | val_loss=0.4530 acc=0.7778 | prec=0.7346 rec=0.8682 f1=0.7958 | time=16.8s\n",
        "* Early stopping at epoch 50"
      ],
      "metadata": {
        "id": "ndVUnvN0dJQw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Return to Block = 1, Head = 3 and increase the filter size to increase the model complexity\n",
        "- Since we found out that the increasing the block size does not increase the validation accuracy and decrease the validation loss, we decided to decrease the Block size as original and maintain the head size as 3. This means that increasing the block size does not generalize well.\n",
        "- Block = 2, Head = 2 performance shows less performance compare to Block = 1, Head = 3\n",
        "- Increase the num_filters = 120 -> 180\n",
        "- Filter Ablation Studies = [120, 150, 180]\n"
      ],
      "metadata": {
        "id": "7d8wXR4qYY6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 100\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr        = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    wd        = trial.suggest_float('wd', 1e-6, 1e-1, log=True)\n",
        "    pct_start = 0.23\n",
        "    NUM_FILTERS = trial.suggest_categorical('num_filters', [120,150,180])\n",
        "\n",
        "    print(f\"Trial {trial.number}: lr={lr:.2e}, wd={wd:.2e}, filters = {NUM_FILTERS}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold-4',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr':lr, 'wd':wd, 'filters':NUM_FILTERS},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len   = dataset[0][0].shape[-1]\n",
        "    model       = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=NUM_FILTERS,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "    optimizer   = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler   = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0  # reset counter\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # Train\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # Dynamic WD\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr/lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds==y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss/len(train_loader)\n",
        "        train_acc  = tcorrect/ttotal\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss += loss.item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds==y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss/len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds==val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "        elapsed    = time.time()-t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "              f\"prec={precision:.4f} rec={recall:.4f} f1={f1:.4f} | time={elapsed:.1f}s\")\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0  # reset on improvement\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize','maximize'])\n",
        "study.optimize(objective, n_trials=15)\n",
        "\n",
        "# ─── Print Pareto-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "9e0ITLJ2YYSH",
        "outputId": "a51df2ea-0d0b-4982-90d0-9f1f5956a015"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 22:56:14,213] A new study created in memory with name: no-name-9393b890-7744-4790-a374-2ad58b1bbbf1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: lr=1.99e-04, wd=1.25e-03, filters = 180\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_225614-edawukao</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/edawukao' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/edawukao' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/edawukao</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7693 acc=0.5116 | val_loss=0.6975 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=24.6s\n",
            "Epoch 002 | train_loss=0.7537 acc=0.5026 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 003 | train_loss=0.7501 acc=0.4776 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 004 | train_loss=0.7319 acc=0.5105 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 005 | train_loss=0.7425 acc=0.4980 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 006 | train_loss=0.7423 acc=0.5150 | val_loss=0.6963 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.3s\n",
            "Epoch 007 | train_loss=0.7429 acc=0.5077 | val_loss=0.7010 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 008 | train_loss=0.7369 acc=0.5031 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 009 | train_loss=0.7326 acc=0.4923 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 010 | train_loss=0.7319 acc=0.5003 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 011 | train_loss=0.7286 acc=0.5060 | val_loss=0.6906 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=23.6s\n",
            "Epoch 012 | train_loss=0.7243 acc=0.4997 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 013 | train_loss=0.7213 acc=0.5037 | val_loss=0.6916 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 014 | train_loss=0.7213 acc=0.5184 | val_loss=0.6813 acc=0.6621 | prec=0.8901 rec=0.3682 f1=0.5209 | time=23.6s\n",
            "Epoch 015 | train_loss=0.6926 acc=0.5598 | val_loss=0.5932 acc=0.7914 | prec=0.8855 rec=0.6682 f1=0.7617 | time=23.7s\n",
            "Epoch 016 | train_loss=0.6521 acc=0.5984 | val_loss=0.5827 acc=0.7959 | prec=0.9062 rec=0.6591 f1=0.7632 | time=23.5s\n",
            "Epoch 017 | train_loss=0.6266 acc=0.6358 | val_loss=0.5602 acc=0.8073 | prec=0.7755 rec=0.8636 f1=0.8172 | time=23.4s\n",
            "Epoch 018 | train_loss=0.6072 acc=0.6478 | val_loss=0.5950 acc=0.7619 | prec=0.9259 rec=0.5682 f1=0.7042 | time=23.4s\n",
            "Epoch 019 | train_loss=0.5896 acc=0.6750 | val_loss=0.5760 acc=0.7937 | prec=0.9272 rec=0.6364 f1=0.7547 | time=23.5s\n",
            "Epoch 020 | train_loss=0.5650 acc=0.6948 | val_loss=0.5359 acc=0.8367 | prec=0.9353 rec=0.7227 f1=0.8154 | time=23.5s\n",
            "Epoch 021 | train_loss=0.5618 acc=0.7050 | val_loss=0.5306 acc=0.8503 | prec=0.9425 rec=0.7455 f1=0.8325 | time=23.5s\n",
            "Epoch 022 | train_loss=0.5692 acc=0.6858 | val_loss=0.4982 acc=0.8662 | prec=0.8676 rec=0.8636 f1=0.8656 | time=23.6s\n",
            "Epoch 023 | train_loss=0.5327 acc=0.7215 | val_loss=0.4979 acc=0.8571 | prec=0.9243 rec=0.7773 f1=0.8444 | time=23.5s\n",
            "Epoch 024 | train_loss=0.5123 acc=0.7408 | val_loss=0.4436 acc=0.8753 | prec=0.8947 rec=0.8500 f1=0.8718 | time=23.5s\n",
            "Epoch 025 | train_loss=0.5010 acc=0.7493 | val_loss=0.5295 acc=0.7551 | prec=0.6806 rec=0.9591 f1=0.7962 | time=23.5s\n",
            "Epoch 026 | train_loss=0.4676 acc=0.7589 | val_loss=0.4773 acc=0.8458 | prec=0.9419 rec=0.7364 f1=0.8265 | time=23.5s\n",
            "Epoch 027 | train_loss=0.4481 acc=0.7907 | val_loss=0.4526 acc=0.8435 | prec=0.9037 rec=0.7682 f1=0.8305 | time=23.6s\n",
            "Epoch 028 | train_loss=0.4271 acc=0.7828 | val_loss=0.3982 acc=0.8458 | prec=0.8689 rec=0.8136 f1=0.8404 | time=23.6s\n",
            "Epoch 029 | train_loss=0.4190 acc=0.8088 | val_loss=0.4196 acc=0.7959 | prec=0.7241 rec=0.9545 f1=0.8235 | time=23.8s\n",
            "Epoch 030 | train_loss=0.3979 acc=0.7958 | val_loss=0.4028 acc=0.8594 | prec=0.8435 rec=0.8818 f1=0.8622 | time=23.5s\n",
            "Epoch 031 | train_loss=0.4055 acc=0.8134 | val_loss=0.4112 acc=0.8345 | prec=0.7976 rec=0.8955 f1=0.8437 | time=23.6s\n",
            "Epoch 032 | train_loss=0.3949 acc=0.8145 | val_loss=0.3825 acc=0.8526 | prec=0.8744 rec=0.8227 f1=0.8478 | time=23.5s\n",
            "Epoch 033 | train_loss=0.3929 acc=0.8026 | val_loss=0.4396 acc=0.7551 | prec=0.6931 rec=0.9136 f1=0.7882 | time=23.7s\n",
            "Epoch 034 | train_loss=0.4054 acc=0.7867 | val_loss=0.4180 acc=0.8549 | prec=0.8305 rec=0.8909 f1=0.8596 | time=23.6s\n",
            "Epoch 035 | train_loss=0.3708 acc=0.8281 | val_loss=0.4268 acc=0.7868 | prec=0.7172 rec=0.9455 f1=0.8157 | time=23.5s\n",
            "Epoch 036 | train_loss=0.3670 acc=0.8338 | val_loss=0.3987 acc=0.8526 | prec=0.8818 rec=0.8136 f1=0.8463 | time=23.5s\n",
            "Epoch 037 | train_loss=0.3996 acc=0.8003 | val_loss=0.3856 acc=0.8254 | prec=0.8069 rec=0.8545 f1=0.8300 | time=23.4s\n",
            "Epoch 038 | train_loss=0.3556 acc=0.8259 | val_loss=0.3987 acc=0.8571 | prec=0.8458 rec=0.8727 f1=0.8591 | time=23.5s\n",
            "Epoch 039 | train_loss=0.3307 acc=0.8423 | val_loss=0.3903 acc=0.8413 | prec=0.8074 rec=0.8955 f1=0.8491 | time=23.5s\n",
            "Epoch 040 | train_loss=0.3316 acc=0.8457 | val_loss=0.3928 acc=0.8163 | prec=0.7546 rec=0.9364 f1=0.8357 | time=23.5s\n",
            "Epoch 041 | train_loss=0.3076 acc=0.8588 | val_loss=0.3907 acc=0.8277 | prec=0.7835 rec=0.9045 f1=0.8397 | time=23.5s\n",
            "Epoch 042 | train_loss=0.3177 acc=0.8327 | val_loss=0.4001 acc=0.8141 | prec=0.9157 rec=0.6909 f1=0.7876 | time=23.5s\n",
            "Epoch 043 | train_loss=0.3479 acc=0.8588 | val_loss=0.4337 acc=0.7823 | prec=0.7138 rec=0.9409 f1=0.8118 | time=23.6s\n",
            "Epoch 044 | train_loss=0.3201 acc=0.8514 | val_loss=0.4312 acc=0.8005 | prec=0.7391 rec=0.9273 f1=0.8226 | time=23.6s\n",
            "Epoch 045 | train_loss=0.3001 acc=0.8508 | val_loss=0.3653 acc=0.8481 | prec=0.8883 rec=0.7955 f1=0.8393 | time=23.6s\n",
            "Epoch 046 | train_loss=0.3364 acc=0.8537 | val_loss=0.3716 acc=0.8390 | prec=0.8091 rec=0.8864 f1=0.8460 | time=23.5s\n",
            "Epoch 047 | train_loss=0.3005 acc=0.8469 | val_loss=0.3692 acc=0.8481 | prec=0.8660 rec=0.8227 f1=0.8438 | time=23.5s\n",
            "Epoch 048 | train_loss=0.2805 acc=0.8605 | val_loss=0.3903 acc=0.8458 | prec=0.8333 rec=0.8636 f1=0.8482 | time=23.6s\n",
            "Epoch 049 | train_loss=0.2779 acc=0.8639 | val_loss=0.4173 acc=0.8118 | prec=0.7566 rec=0.9182 f1=0.8296 | time=23.6s\n",
            "Epoch 050 | train_loss=0.2619 acc=0.8746 | val_loss=0.4024 acc=0.8367 | prec=0.7846 rec=0.9273 f1=0.8500 | time=23.7s\n",
            "Epoch 051 | train_loss=0.2820 acc=0.8724 | val_loss=0.4095 acc=0.8390 | prec=0.7876 rec=0.9273 f1=0.8518 | time=23.7s\n",
            "Epoch 052 | train_loss=0.2703 acc=0.8780 | val_loss=0.3948 acc=0.8345 | prec=0.8451 rec=0.8182 f1=0.8314 | time=23.6s\n",
            "Epoch 053 | train_loss=0.2670 acc=0.8661 | val_loss=0.4006 acc=0.8277 | prec=0.8364 rec=0.8136 f1=0.8249 | time=23.5s\n",
            "Epoch 054 | train_loss=0.2217 acc=0.8803 | val_loss=0.4048 acc=0.8231 | prec=0.8060 rec=0.8500 f1=0.8274 | time=23.7s\n",
            "Epoch 055 | train_loss=0.2443 acc=0.8792 | val_loss=0.4146 acc=0.8367 | prec=0.8109 rec=0.8773 f1=0.8428 | time=23.6s\n",
            "Epoch 056 | train_loss=0.2305 acc=0.8877 | val_loss=0.4538 acc=0.8299 | prec=0.8436 rec=0.8091 f1=0.8260 | time=23.7s\n",
            "Epoch 057 | train_loss=0.2355 acc=0.8837 | val_loss=0.4464 acc=0.8163 | prec=0.7908 rec=0.8591 f1=0.8235 | time=23.7s\n",
            "Epoch 058 | train_loss=0.2354 acc=0.8832 | val_loss=0.4604 acc=0.7982 | prec=0.7348 rec=0.9318 f1=0.8216 | time=23.7s\n",
            "Epoch 059 | train_loss=0.2428 acc=0.8627 | val_loss=0.3986 acc=0.8277 | prec=0.8303 rec=0.8227 f1=0.8265 | time=23.6s\n",
            "Epoch 060 | train_loss=0.2256 acc=0.8866 | val_loss=0.4457 acc=0.8299 | prec=0.8139 rec=0.8545 f1=0.8337 | time=23.7s\n",
            "Early stopping at epoch 60\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▁▆▁▆▁▁▆▅█▇▇████▇██████████████████████</td></tr><tr><td>precision</td><td>▄▄▄▁▄▁▄█▁▄▇▇▆▇▇█▇▇▆█▇▇▇▇▆▇▇▇▇▆▆▇▇▆▇▇▇▇▇▆</td></tr><tr><td>recall</td><td>███▁█▁▁█▁▁▄▆▆▇▅▆▆▆█▆█▇▇▇▇█▇▇▇▇█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▂▁▁▂▁▂▁▁▁▁▂▂▃▄▅▅▅▆▆▆▇▇▇▆▇▇▇▇█▇▇▇████████</td></tr><tr><td>train_loss</td><td>███████▇▇▇▆▆▆▅▅▅▅▅▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▄▇▇▇▇███▆███▇▇█▆█▆▇█▇▇▇▇▇██▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>██████████▆▅▆▅▅▄▄▃▄▃▂▂▁▃▂▁▂▂▂▂▂▁▁▁▂▂▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.8337</td></tr><tr><td>precision</td><td>0.81385</td></tr><tr><td>recall</td><td>0.85455</td></tr><tr><td>train_accuracy</td><td>0.88656</td></tr><tr><td>train_loss</td><td>0.22561</td></tr><tr><td>validation_accuracy</td><td>0.82993</td></tr><tr><td>validation_loss</td><td>0.44574</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/edawukao' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/edawukao</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_225614-edawukao/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 23:19:52,021] Trial 0 finished with values: [0.44574250813041416, 0.8299319727891157] and parameters: {'lr': 0.00019898762102533166, 'wd': 0.0012504934210175447, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: lr=1.17e-05, wd=3.26e-03, filters = 120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_231952-c2fvdzsq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/c2fvdzsq' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/c2fvdzsq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/c2fvdzsq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7545 acc=0.5037 | val_loss=0.6923 acc=0.5351 | prec=0.5261 rec=0.6864 f1=0.5957 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7497 acc=0.4929 | val_loss=0.6924 acc=0.5034 | prec=0.5714 rec=0.0182 f1=0.0352 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7429 acc=0.5201 | val_loss=0.6921 acc=0.5397 | prec=0.5279 rec=0.7318 f1=0.6133 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7329 acc=0.5167 | val_loss=0.6927 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7595 acc=0.4833 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7448 acc=0.4821 | val_loss=0.6923 acc=0.5283 | prec=0.5227 rec=0.6273 f1=0.5702 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7487 acc=0.4765 | val_loss=0.6922 acc=0.5306 | prec=0.5448 rec=0.3591 f1=0.4329 | time=13.2s\n",
            "Epoch 008 | train_loss=0.7329 acc=0.5003 | val_loss=0.6925 acc=0.4966 | prec=0.2500 rec=0.0045 f1=0.0089 | time=13.1s\n",
            "Epoch 009 | train_loss=0.7297 acc=0.5230 | val_loss=0.6928 acc=0.5125 | prec=0.5373 rec=0.1636 f1=0.2509 | time=13.1s\n",
            "Epoch 010 | train_loss=0.7428 acc=0.5082 | val_loss=0.6926 acc=0.4966 | prec=0.4977 rec=0.9909 f1=0.6626 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7331 acc=0.5020 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7438 acc=0.5077 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7355 acc=0.5167 | val_loss=0.6923 acc=0.5488 | prec=0.5301 rec=0.8409 f1=0.6503 | time=13.0s\n",
            "Epoch 014 | train_loss=0.7292 acc=0.5133 | val_loss=0.6926 acc=0.5374 | prec=0.5412 rec=0.4773 f1=0.5072 | time=13.1s\n",
            "Epoch 015 | train_loss=0.7265 acc=0.4901 | val_loss=0.6927 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=13.2s\n",
            "Epoch 016 | train_loss=0.7442 acc=0.4821 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 017 | train_loss=0.7303 acc=0.4821 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 018 | train_loss=0.7315 acc=0.4872 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Early stopping at epoch 18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▇▁▇▁▁▇▆▁▄█▁▁█▆▁▁▁▁</td></tr><tr><td>precision</td><td>▅▅▅▁▁▅▅▃▅▄▁▁▅▅█▁▁▁</td></tr><tr><td>recall</td><td>▆▁▆▁▁▅▄▁▂█▁▁▇▄▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▅▃█▇▂▂▁▅█▆▅▆▇▇▃▂▂▃</td></tr><tr><td>train_loss</td><td>▇▆▄▂█▅▆▂▂▄▂▅▃▂▁▅▂▂</td></tr><tr><td>validation_accuracy</td><td>▆▂▇▂▂▅▆▁▃▁▂▂█▆▂▂▂▂</td></tr><tr><td>validation_loss</td><td>▁▂▁▃▅▂▁▂▃▃▄▅▁▃▃▇▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.48724</td></tr><tr><td>train_loss</td><td>0.73146</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69402</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/c2fvdzsq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/c2fvdzsq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_231952-c2fvdzsq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 23:23:46,792] Trial 1 finished with values: [0.6940225873674665, 0.5011337868480725] and parameters: {'lr': 1.1731738194941784e-05, 'wd': 0.003262368225542386, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: lr=4.98e-05, wd=3.48e-03, filters = 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_232346-sh0jw09i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/sh0jw09i' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/sh0jw09i' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/sh0jw09i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7671 acc=0.4878 | val_loss=0.6934 acc=0.4966 | prec=0.4977 rec=0.9955 f1=0.6636 | time=18.2s\n",
            "Epoch 002 | train_loss=0.7638 acc=0.5167 | val_loss=0.6933 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=18.1s\n",
            "Epoch 003 | train_loss=0.7681 acc=0.4957 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.3s\n",
            "Epoch 004 | train_loss=0.7367 acc=0.5150 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 005 | train_loss=0.7344 acc=0.4963 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 006 | train_loss=0.7427 acc=0.4838 | val_loss=0.6932 acc=0.5057 | prec=0.7500 rec=0.0136 f1=0.0268 | time=18.1s\n",
            "Epoch 007 | train_loss=0.7400 acc=0.5082 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 008 | train_loss=0.7422 acc=0.4906 | val_loss=0.6931 acc=0.5193 | prec=0.5238 rec=0.4000 f1=0.4536 | time=18.1s\n",
            "Epoch 009 | train_loss=0.7325 acc=0.4974 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 010 | train_loss=0.7380 acc=0.4929 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 011 | train_loss=0.7279 acc=0.5167 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 012 | train_loss=0.7165 acc=0.4963 | val_loss=0.6976 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 013 | train_loss=0.7207 acc=0.5190 | val_loss=0.6980 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 014 | train_loss=0.7322 acc=0.4901 | val_loss=0.6963 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 015 | train_loss=0.7363 acc=0.4770 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 016 | train_loss=0.7154 acc=0.5111 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 017 | train_loss=0.7147 acc=0.5167 | val_loss=0.6965 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.3s\n",
            "Epoch 018 | train_loss=0.7170 acc=0.5235 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 019 | train_loss=0.7319 acc=0.4759 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 020 | train_loss=0.7205 acc=0.5128 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 021 | train_loss=0.7248 acc=0.4974 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.0s\n",
            "Epoch 022 | train_loss=0.7160 acc=0.4963 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.3s\n",
            "Epoch 023 | train_loss=0.7115 acc=0.5116 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>█▁▁▁▁▁▁▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▄█▁▁▁▆▁▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall</td><td>█▁▁▁▁▁▁▄▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▃▇▄▇▄▂▆▃▄▃▇▄▇▃▁▆▇█▁▆▄▄▆</td></tr><tr><td>train_loss</td><td>█▇█▄▄▅▅▅▄▄▃▂▂▄▄▁▁▂▄▂▃▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▄▂▂▂▄▂█▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>validation_loss</td><td>▂▁▃▂▂▁▂▁▃▁▄██▆▃▃▆▂▂▄▄▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.51163</td></tr><tr><td>train_loss</td><td>0.71148</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69455</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/sh0jw09i' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/sh0jw09i</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_232346-sh0jw09i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 23:30:45,679] Trial 2 finished with values: [0.6945507568972451, 0.5011337868480725] and parameters: {'lr': 4.978000458908573e-05, 'wd': 0.0034788999348974348, 'num_filters': 150}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: lr=7.29e-04, wd=1.88e-06, filters = 120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_233045-5tgdjvpq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/5tgdjvpq' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/5tgdjvpq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/5tgdjvpq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7866 acc=0.5213 | val_loss=0.7003 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7474 acc=0.5190 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7497 acc=0.5020 | val_loss=0.6979 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7413 acc=0.5071 | val_loss=0.7012 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7685 acc=0.4736 | val_loss=0.6975 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7486 acc=0.4963 | val_loss=0.6998 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7598 acc=0.4895 | val_loss=0.7036 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7325 acc=0.5150 | val_loss=0.7066 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7353 acc=0.5258 | val_loss=0.6508 acc=0.6825 | prec=0.8846 rec=0.4182 f1=0.5679 | time=12.8s\n",
            "Epoch 010 | train_loss=0.6961 acc=0.5638 | val_loss=0.5752 acc=0.8118 | prec=0.8914 rec=0.7091 f1=0.7899 | time=12.8s\n",
            "Epoch 011 | train_loss=0.6371 acc=0.6353 | val_loss=0.5363 acc=0.8299 | prec=0.8404 rec=0.8136 f1=0.8268 | time=12.9s\n",
            "Epoch 012 | train_loss=0.6066 acc=0.6687 | val_loss=0.5456 acc=0.8141 | prec=0.9259 rec=0.6818 f1=0.7853 | time=12.6s\n",
            "Epoch 013 | train_loss=0.5747 acc=0.6977 | val_loss=0.4387 acc=0.8345 | prec=0.7905 rec=0.9091 f1=0.8457 | time=12.9s\n",
            "Epoch 014 | train_loss=0.5500 acc=0.7085 | val_loss=0.4320 acc=0.8345 | prec=0.9153 rec=0.7364 f1=0.8161 | time=12.8s\n",
            "Epoch 015 | train_loss=0.5139 acc=0.7527 | val_loss=0.3914 acc=0.8549 | prec=0.8750 rec=0.8273 f1=0.8505 | time=12.8s\n",
            "Epoch 016 | train_loss=0.4908 acc=0.7431 | val_loss=0.3850 acc=0.8594 | prec=0.8527 rec=0.8682 f1=0.8604 | time=13.2s\n",
            "Epoch 017 | train_loss=0.4766 acc=0.7516 | val_loss=0.4216 acc=0.8322 | prec=0.7786 rec=0.9273 f1=0.8465 | time=13.3s\n",
            "Epoch 018 | train_loss=0.4486 acc=0.7879 | val_loss=0.4008 acc=0.8299 | prec=0.9677 rec=0.6818 f1=0.8000 | time=13.0s\n",
            "Epoch 019 | train_loss=0.4163 acc=0.7964 | val_loss=0.4345 acc=0.8073 | prec=0.9787 rec=0.6273 f1=0.7645 | time=13.0s\n",
            "Epoch 020 | train_loss=0.3963 acc=0.7935 | val_loss=0.3814 acc=0.8413 | prec=0.7953 rec=0.9182 f1=0.8523 | time=13.5s\n",
            "Epoch 021 | train_loss=0.4376 acc=0.7794 | val_loss=0.3362 acc=0.8503 | prec=0.8377 rec=0.8682 f1=0.8527 | time=13.7s\n",
            "Epoch 022 | train_loss=0.4008 acc=0.7845 | val_loss=0.3418 acc=0.8617 | prec=0.9077 rec=0.8045 f1=0.8530 | time=13.7s\n",
            "Epoch 023 | train_loss=0.3882 acc=0.7924 | val_loss=0.3853 acc=0.8367 | prec=0.9512 rec=0.7091 f1=0.8125 | time=13.8s\n",
            "Epoch 024 | train_loss=0.3655 acc=0.8185 | val_loss=0.7200 acc=0.6372 | prec=0.5798 rec=0.9909 f1=0.7315 | time=13.8s\n",
            "Epoch 025 | train_loss=0.3448 acc=0.8276 | val_loss=0.3373 acc=0.8662 | prec=0.9171 rec=0.8045 f1=0.8571 | time=13.7s\n",
            "Epoch 026 | train_loss=0.6052 acc=0.6744 | val_loss=0.3455 acc=0.8639 | prec=0.9000 rec=0.8182 f1=0.8571 | time=13.7s\n",
            "Epoch 027 | train_loss=0.3897 acc=0.8060 | val_loss=0.4190 acc=0.8571 | prec=0.8985 rec=0.8045 f1=0.8489 | time=13.6s\n",
            "Epoch 028 | train_loss=0.3266 acc=0.8446 | val_loss=0.4360 acc=0.7914 | prec=0.7936 rec=0.7864 f1=0.7900 | time=13.6s\n",
            "Epoch 029 | train_loss=0.3118 acc=0.8338 | val_loss=0.4574 acc=0.7415 | prec=0.6710 rec=0.9455 f1=0.7849 | time=13.9s\n",
            "Epoch 030 | train_loss=0.3189 acc=0.8542 | val_loss=0.4879 acc=0.7823 | prec=0.7214 rec=0.9182 f1=0.8080 | time=13.6s\n",
            "Epoch 031 | train_loss=0.3022 acc=0.8480 | val_loss=0.4299 acc=0.7642 | prec=0.6921 rec=0.9500 f1=0.8008 | time=13.7s\n",
            "Epoch 032 | train_loss=0.3196 acc=0.8525 | val_loss=0.6587 acc=0.7029 | prec=0.6378 rec=0.9364 f1=0.7587 | time=13.6s\n",
            "Epoch 033 | train_loss=0.3032 acc=0.8491 | val_loss=0.3993 acc=0.8299 | prec=0.8436 rec=0.8091 f1=0.8260 | time=13.7s\n",
            "Epoch 034 | train_loss=0.3975 acc=0.7941 | val_loss=0.4300 acc=0.7914 | prec=0.7581 rec=0.8545 f1=0.8034 | time=13.2s\n",
            "Epoch 035 | train_loss=0.2784 acc=0.8656 | val_loss=0.4807 acc=0.7710 | prec=0.7228 rec=0.8773 f1=0.7926 | time=12.9s\n",
            "Epoch 036 | train_loss=0.2683 acc=0.8633 | val_loss=0.5991 acc=0.7506 | prec=0.6884 rec=0.9136 f1=0.7852 | time=12.8s\n",
            "Early stopping at epoch 36\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▆▇█▇██████▇████▇███▇▇██▇██▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▇▇▇█▇█▇▇▇██▇▇▇█▅█▇▇▇▆▆▆▆▇▆▆▆</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▄▆▇▆▇▆▇▇█▆▅▇▇▇▆█▇▇▇▇█▇██▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▂▂▂▂▁▁▁▂▂▃▄▄▅▅▆▆▆▇▇▇▆▇▇▇▇▅▇█▇████▇██</td></tr><tr><td>train_loss</td><td>█▇█▇█▇█▇▇▇▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▆▃▂▂▂▁▂▁▃▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▄▇▇▇▇▇██▇▇▇███▇▄███▇▆▆▆▅▇▇▆▆</td></tr><tr><td>validation_loss</td><td>████████▇▅▅▅▃▃▂▂▃▂▃▂▁▁▂█▁▁▃▃▃▄▃▇▂▃▄▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78516</td></tr><tr><td>precision</td><td>0.68836</td></tr><tr><td>recall</td><td>0.91364</td></tr><tr><td>train_accuracy</td><td>0.8633</td></tr><tr><td>train_loss</td><td>0.26833</td></tr><tr><td>validation_accuracy</td><td>0.75057</td></tr><tr><td>validation_loss</td><td>0.59911</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/5tgdjvpq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/5tgdjvpq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_233045-5tgdjvpq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-04 23:38:42,625] Trial 3 finished with values: [0.599111237696239, 0.7505668934240363] and parameters: {'lr': 0.0007286820565059034, 'wd': 1.882039890429645e-06, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: lr=3.90e-05, wd=1.17e-04, filters = 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_233842-3i2pq2w4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/3i2pq2w4' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/3i2pq2w4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/3i2pq2w4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7864 acc=0.4963 | val_loss=0.7041 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.1s\n",
            "Epoch 002 | train_loss=0.7724 acc=0.5009 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 003 | train_loss=0.7580 acc=0.5065 | val_loss=0.6936 acc=0.4921 | prec=0.4954 rec=0.9864 f1=0.6596 | time=18.3s\n",
            "Epoch 004 | train_loss=0.7521 acc=0.4969 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.0s\n",
            "Epoch 005 | train_loss=0.7567 acc=0.5077 | val_loss=0.6941 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.4s\n",
            "Epoch 006 | train_loss=0.7551 acc=0.4912 | val_loss=0.6932 acc=0.4966 | prec=0.4977 rec=0.9773 f1=0.6595 | time=18.5s\n",
            "Epoch 007 | train_loss=0.7617 acc=0.4929 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 008 | train_loss=0.7553 acc=0.5026 | val_loss=0.6932 acc=0.5011 | prec=0.5000 rec=0.9955 f1=0.6657 | time=18.1s\n",
            "Epoch 009 | train_loss=0.7546 acc=0.5122 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.0s\n",
            "Epoch 010 | train_loss=0.7422 acc=0.4929 | val_loss=0.6932 acc=0.4943 | prec=0.4964 rec=0.9364 f1=0.6488 | time=18.3s\n",
            "Epoch 011 | train_loss=0.7142 acc=0.5366 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 012 | train_loss=0.7527 acc=0.4725 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 013 | train_loss=0.7513 acc=0.4986 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.3s\n",
            "Epoch 014 | train_loss=0.7305 acc=0.5065 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 015 | train_loss=0.7343 acc=0.4957 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 016 | train_loss=0.7397 acc=0.5054 | val_loss=0.6924 acc=0.5079 | prec=0.5034 rec=0.9955 f1=0.6687 | time=18.1s\n",
            "Epoch 017 | train_loss=0.7240 acc=0.5139 | val_loss=0.6923 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.3s\n",
            "Epoch 018 | train_loss=0.7419 acc=0.4884 | val_loss=0.6941 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.0s\n",
            "Epoch 019 | train_loss=0.7318 acc=0.4884 | val_loss=0.6920 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 020 | train_loss=0.7369 acc=0.4901 | val_loss=0.6910 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 021 | train_loss=0.7174 acc=0.5417 | val_loss=0.6876 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=18.2s\n",
            "Epoch 022 | train_loss=0.7059 acc=0.5337 | val_loss=0.6813 acc=0.7460 | prec=0.7596 rec=0.7182 f1=0.7383 | time=18.1s\n",
            "Epoch 023 | train_loss=0.7144 acc=0.5281 | val_loss=0.6652 acc=0.7619 | prec=0.7347 rec=0.8182 f1=0.7742 | time=18.0s\n",
            "Epoch 024 | train_loss=0.6759 acc=0.5876 | val_loss=0.6119 acc=0.7846 | prec=0.8834 rec=0.6545 f1=0.7520 | time=18.1s\n",
            "Epoch 025 | train_loss=0.6488 acc=0.6177 | val_loss=0.5824 acc=0.6916 | prec=0.6257 rec=0.9500 f1=0.7545 | time=18.1s\n",
            "Epoch 026 | train_loss=0.6319 acc=0.6296 | val_loss=0.5309 acc=0.8231 | prec=0.8901 rec=0.7364 f1=0.8060 | time=18.1s\n",
            "Epoch 027 | train_loss=0.6113 acc=0.6591 | val_loss=0.5430 acc=0.7710 | prec=0.9048 rec=0.6045 f1=0.7248 | time=18.0s\n",
            "Epoch 028 | train_loss=0.5815 acc=0.6892 | val_loss=0.5064 acc=0.8345 | prec=0.8848 rec=0.7682 f1=0.8224 | time=18.0s\n",
            "Epoch 029 | train_loss=0.5825 acc=0.7062 | val_loss=0.4858 acc=0.8322 | prec=0.8967 rec=0.7500 f1=0.8168 | time=18.1s\n",
            "Epoch 030 | train_loss=0.5659 acc=0.7204 | val_loss=0.4788 acc=0.8435 | prec=0.8447 rec=0.8409 f1=0.8428 | time=18.2s\n",
            "Epoch 031 | train_loss=0.5596 acc=0.7277 | val_loss=0.4744 acc=0.8458 | prec=0.8551 rec=0.8318 f1=0.8433 | time=18.2s\n",
            "Epoch 032 | train_loss=0.5241 acc=0.7476 | val_loss=0.5058 acc=0.7732 | prec=0.9478 rec=0.5773 f1=0.7175 | time=18.0s\n",
            "Epoch 033 | train_loss=0.5218 acc=0.7402 | val_loss=0.4650 acc=0.8367 | prec=0.9205 rec=0.7364 f1=0.8182 | time=18.3s\n",
            "Epoch 034 | train_loss=0.5128 acc=0.7482 | val_loss=0.4444 acc=0.8707 | prec=0.8937 rec=0.8409 f1=0.8665 | time=18.2s\n",
            "Epoch 035 | train_loss=0.5207 acc=0.7476 | val_loss=0.4515 acc=0.8594 | prec=0.8591 rec=0.8591 f1=0.8591 | time=18.0s\n",
            "Epoch 036 | train_loss=0.5173 acc=0.7533 | val_loss=0.4482 acc=0.8322 | prec=0.8042 rec=0.8773 f1=0.8391 | time=18.1s\n",
            "Epoch 037 | train_loss=0.4996 acc=0.7697 | val_loss=0.4463 acc=0.8639 | prec=0.9000 rec=0.8182 f1=0.8571 | time=18.1s\n",
            "Epoch 038 | train_loss=0.4945 acc=0.7788 | val_loss=0.4500 acc=0.8594 | prec=0.8990 rec=0.8091 f1=0.8517 | time=18.2s\n",
            "Epoch 039 | train_loss=0.4888 acc=0.7725 | val_loss=0.4574 acc=0.8571 | prec=0.9538 rec=0.7500 f1=0.8397 | time=18.2s\n",
            "Epoch 040 | train_loss=0.4777 acc=0.7708 | val_loss=0.4265 acc=0.8685 | prec=0.9263 rec=0.8000 f1=0.8585 | time=18.3s\n",
            "Epoch 041 | train_loss=0.4615 acc=0.7862 | val_loss=0.4446 acc=0.8594 | prec=0.9438 rec=0.7636 f1=0.8442 | time=18.3s\n",
            "Epoch 042 | train_loss=0.4697 acc=0.7663 | val_loss=0.4291 acc=0.8254 | prec=0.8150 rec=0.8409 f1=0.8277 | time=18.2s\n",
            "Epoch 043 | train_loss=0.4765 acc=0.7703 | val_loss=0.4521 acc=0.8435 | prec=0.9415 rec=0.7318 f1=0.8235 | time=18.2s\n",
            "Epoch 044 | train_loss=0.4682 acc=0.7788 | val_loss=0.4336 acc=0.8594 | prec=0.9158 rec=0.7909 f1=0.8488 | time=18.2s\n",
            "Epoch 045 | train_loss=0.4550 acc=0.7998 | val_loss=0.4220 acc=0.8413 | prec=0.8606 rec=0.8136 f1=0.8364 | time=18.3s\n",
            "Epoch 046 | train_loss=0.4418 acc=0.8049 | val_loss=0.4292 acc=0.8526 | prec=0.9379 rec=0.7545 f1=0.8363 | time=18.1s\n",
            "Epoch 047 | train_loss=0.4398 acc=0.7952 | val_loss=0.4248 acc=0.8639 | prec=0.9348 rec=0.7818 f1=0.8515 | time=18.2s\n",
            "Epoch 048 | train_loss=0.4369 acc=0.8003 | val_loss=0.4545 acc=0.7959 | prec=0.9710 rec=0.6091 f1=0.7486 | time=18.2s\n",
            "Epoch 049 | train_loss=0.4309 acc=0.8100 | val_loss=0.4256 acc=0.8367 | prec=0.8700 rec=0.7909 f1=0.8286 | time=18.1s\n",
            "Epoch 050 | train_loss=0.4402 acc=0.7935 | val_loss=0.4280 acc=0.8322 | prec=0.9294 rec=0.7182 f1=0.8103 | time=18.1s\n",
            "Epoch 051 | train_loss=0.4302 acc=0.8071 | val_loss=0.4159 acc=0.8435 | prec=0.8479 rec=0.8364 f1=0.8421 | time=18.1s\n",
            "Epoch 052 | train_loss=0.4243 acc=0.8026 | val_loss=0.4064 acc=0.8435 | prec=0.9171 rec=0.7545 f1=0.8279 | time=18.0s\n",
            "Epoch 053 | train_loss=0.4146 acc=0.8083 | val_loss=0.4339 acc=0.8118 | prec=0.9363 rec=0.6682 f1=0.7798 | time=18.1s\n",
            "Epoch 054 | train_loss=0.4137 acc=0.8128 | val_loss=0.4117 acc=0.8549 | prec=0.9333 rec=0.7636 f1=0.8400 | time=18.1s\n",
            "Epoch 055 | train_loss=0.4302 acc=0.8123 | val_loss=0.4032 acc=0.8481 | prec=0.9135 rec=0.7682 f1=0.8346 | time=18.1s\n",
            "Epoch 056 | train_loss=0.4182 acc=0.8032 | val_loss=0.4230 acc=0.8209 | prec=0.9172 rec=0.7045 f1=0.7969 | time=18.1s\n",
            "Epoch 057 | train_loss=0.4167 acc=0.8066 | val_loss=0.3900 acc=0.8481 | prec=0.8400 rec=0.8591 f1=0.8494 | time=18.1s\n",
            "Epoch 058 | train_loss=0.4145 acc=0.8077 | val_loss=0.4059 acc=0.8367 | prec=0.8162 rec=0.8682 f1=0.8414 | time=18.2s\n",
            "Epoch 059 | train_loss=0.3915 acc=0.8264 | val_loss=0.4501 acc=0.7868 | prec=0.9565 rec=0.6000 f1=0.7374 | time=18.4s\n",
            "Epoch 060 | train_loss=0.3941 acc=0.8208 | val_loss=0.3992 acc=0.8481 | prec=0.8883 rec=0.7955 f1=0.8393 | time=17.9s\n",
            "Epoch 061 | train_loss=0.3777 acc=0.8429 | val_loss=0.3925 acc=0.8571 | prec=0.9026 rec=0.8000 f1=0.8482 | time=18.2s\n",
            "Epoch 062 | train_loss=0.4018 acc=0.8270 | val_loss=0.4300 acc=0.8118 | prec=0.9536 rec=0.6545 f1=0.7763 | time=18.2s\n",
            "Epoch 063 | train_loss=0.4026 acc=0.8270 | val_loss=0.3925 acc=0.8526 | prec=0.9235 rec=0.7682 f1=0.8387 | time=18.2s\n",
            "Epoch 064 | train_loss=0.3966 acc=0.8361 | val_loss=0.3773 acc=0.8571 | prec=0.8584 rec=0.8545 f1=0.8565 | time=18.1s\n",
            "Epoch 065 | train_loss=0.3934 acc=0.8247 | val_loss=0.4040 acc=0.8549 | prec=0.9194 rec=0.7773 f1=0.8424 | time=18.1s\n",
            "Epoch 066 | train_loss=0.3787 acc=0.8355 | val_loss=0.3954 acc=0.8503 | prec=0.9140 rec=0.7727 f1=0.8374 | time=18.2s\n",
            "Epoch 067 | train_loss=0.3821 acc=0.8259 | val_loss=0.4001 acc=0.8503 | prec=0.9053 rec=0.7818 f1=0.8390 | time=18.1s\n",
            "Epoch 068 | train_loss=0.3710 acc=0.8253 | val_loss=0.3958 acc=0.8435 | prec=0.9126 rec=0.7591 f1=0.8288 | time=18.3s\n",
            "Epoch 069 | train_loss=0.3723 acc=0.8196 | val_loss=0.3935 acc=0.8549 | prec=0.8861 rec=0.8136 f1=0.8483 | time=18.0s\n",
            "Epoch 070 | train_loss=0.3801 acc=0.8276 | val_loss=0.4018 acc=0.8413 | prec=0.9167 rec=0.7500 f1=0.8250 | time=18.0s\n",
            "Epoch 071 | train_loss=0.3691 acc=0.8281 | val_loss=0.3933 acc=0.8458 | prec=0.9086 rec=0.7682 f1=0.8325 | time=18.2s\n",
            "Epoch 072 | train_loss=0.3668 acc=0.8497 | val_loss=0.3817 acc=0.8458 | prec=0.9086 rec=0.7682 f1=0.8325 | time=18.1s\n",
            "Epoch 073 | train_loss=0.3736 acc=0.8344 | val_loss=0.3850 acc=0.8435 | prec=0.8912 rec=0.7818 f1=0.8329 | time=18.3s\n",
            "Epoch 074 | train_loss=0.3656 acc=0.8327 | val_loss=0.3907 acc=0.8435 | prec=0.8995 rec=0.7727 f1=0.8313 | time=18.1s\n",
            "Epoch 075 | train_loss=0.3594 acc=0.8383 | val_loss=0.4149 acc=0.8322 | prec=0.9345 rec=0.7136 f1=0.8093 | time=18.3s\n",
            "Epoch 076 | train_loss=0.3695 acc=0.8349 | val_loss=0.3894 acc=0.8458 | prec=0.9043 rec=0.7727 f1=0.8333 | time=18.1s\n",
            "Epoch 077 | train_loss=0.3608 acc=0.8412 | val_loss=0.3945 acc=0.8413 | prec=0.9076 rec=0.7591 f1=0.8267 | time=18.1s\n",
            "Epoch 078 | train_loss=0.3707 acc=0.8417 | val_loss=0.3830 acc=0.8435 | prec=0.9171 rec=0.7545 f1=0.8279 | time=18.2s\n",
            "Epoch 079 | train_loss=0.3613 acc=0.8361 | val_loss=0.3856 acc=0.8390 | prec=0.9162 rec=0.7455 f1=0.8221 | time=18.1s\n",
            "Early stopping at epoch 79\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▆▁▆▆▇▇▇█▇██████████▇█████▇▇██████████</td></tr><tr><td>precision</td><td>▅▅▅▅▅▁▅▅▅▅▅▅▅▅▆███▇▇███▇██▇█▇███████████</td></tr><tr><td>recall</td><td>██████▁████▆█▅▆▇▅▆▇▇▆▆▇▇▇▆▆▇▅▇▆▆▇▆▆▆▆▆▆▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▁▁▂▂▃▄▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▆▅▅▅▅▄▃▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▆▆▅▇▇▇▇████▇██▇██▇▇▇▆▇██████▇</td></tr><tr><td>validation_loss</td><td>██████████████▇▄▄▃▃▃▂▃▂▂▂▂▂▂▂▁▂▁▁▁▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82206</td></tr><tr><td>precision</td><td>0.9162</td></tr><tr><td>recall</td><td>0.74545</td></tr><tr><td>train_accuracy</td><td>0.83607</td></tr><tr><td>train_loss</td><td>0.36126</td></tr><tr><td>validation_accuracy</td><td>0.839</td></tr><tr><td>validation_loss</td><td>0.38563</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/3i2pq2w4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/3i2pq2w4</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_233842-3i2pq2w4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 00:02:38,426] Trial 4 finished with values: [0.38563031596796854, 0.8390022675736961] and parameters: {'lr': 3.897432619701595e-05, 'wd': 0.00011659053590836514, 'num_filters': 150}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: lr=8.10e-03, wd=3.12e-05, filters = 120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_000238-w6xu1z9y</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/w6xu1z9y' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/w6xu1z9y' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/w6xu1z9y</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7461 acc=0.5099 | val_loss=0.6972 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7353 acc=0.5020 | val_loss=0.6969 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7318 acc=0.5099 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7266 acc=0.4833 | val_loss=0.6931 acc=0.5578 | prec=0.5899 rec=0.3727 f1=0.4568 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7076 acc=0.5218 | val_loss=0.6914 acc=0.6327 | prec=0.9531 rec=0.2773 f1=0.4296 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7034 acc=0.5201 | val_loss=0.6962 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7032 acc=0.5077 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7013 acc=0.5026 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 009 | train_loss=0.6894 acc=0.5156 | val_loss=0.6603 acc=0.7619 | prec=0.7602 rec=0.7636 f1=0.7619 | time=13.1s\n",
            "Epoch 010 | train_loss=0.6505 acc=0.6296 | val_loss=0.6801 acc=0.5238 | prec=1.0000 rec=0.0455 f1=0.0870 | time=13.1s\n",
            "Epoch 011 | train_loss=0.5724 acc=0.6897 | val_loss=0.4736 acc=0.7959 | prec=0.9276 rec=0.6409 f1=0.7581 | time=13.1s\n",
            "Epoch 012 | train_loss=0.5486 acc=0.7102 | val_loss=0.4744 acc=0.7687 | prec=0.9214 rec=0.5864 f1=0.7167 | time=13.3s\n",
            "Epoch 013 | train_loss=0.5596 acc=0.6994 | val_loss=0.4972 acc=0.7642 | prec=0.9603 rec=0.5500 f1=0.6994 | time=13.1s\n",
            "Epoch 014 | train_loss=0.5386 acc=0.7300 | val_loss=0.8750 acc=0.5760 | prec=0.5409 rec=0.9909 f1=0.6998 | time=13.0s\n",
            "Epoch 015 | train_loss=0.5462 acc=0.7033 | val_loss=0.8595 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 016 | train_loss=0.5335 acc=0.7204 | val_loss=0.4656 acc=0.8118 | prec=0.9536 rec=0.6545 f1=0.7763 | time=13.0s\n",
            "Epoch 017 | train_loss=0.5263 acc=0.7272 | val_loss=0.4561 acc=0.8299 | prec=0.9143 rec=0.7273 f1=0.8101 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5190 acc=0.7221 | val_loss=0.7232 acc=0.6939 | prec=0.9775 rec=0.3955 f1=0.5631 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5286 acc=0.7232 | val_loss=0.9582 acc=0.5873 | prec=0.9750 rec=0.1773 f1=0.3000 | time=13.0s\n",
            "Epoch 020 | train_loss=0.6453 acc=0.6393 | val_loss=0.5179 acc=0.7914 | prec=0.7462 rec=0.8818 f1=0.8083 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5531 acc=0.7056 | val_loss=0.5563 acc=0.8345 | prec=0.8050 rec=0.8818 f1=0.8416 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5264 acc=0.7215 | val_loss=0.5558 acc=0.6644 | prec=0.9865 rec=0.3318 f1=0.4966 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5412 acc=0.7226 | val_loss=0.4208 acc=0.8073 | prec=0.9355 rec=0.6591 f1=0.7733 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4870 acc=0.7482 | val_loss=0.4016 acc=0.8231 | prec=0.9383 rec=0.6909 f1=0.7958 | time=12.9s\n",
            "Epoch 025 | train_loss=0.4837 acc=0.7703 | val_loss=0.4805 acc=0.8073 | prec=0.7586 rec=0.9000 f1=0.8233 | time=13.0s\n",
            "Epoch 026 | train_loss=0.4750 acc=0.7601 | val_loss=1.0138 acc=0.5578 | prec=0.5301 rec=1.0000 f1=0.6929 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4563 acc=0.7822 | val_loss=1.9786 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5294 acc=0.7226 | val_loss=0.4498 acc=0.8027 | prec=0.7401 rec=0.9318 f1=0.8249 | time=12.9s\n",
            "Epoch 029 | train_loss=0.4303 acc=0.7691 | val_loss=0.4205 acc=0.8005 | prec=0.9714 rec=0.6182 f1=0.7556 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4601 acc=0.7550 | val_loss=0.3997 acc=0.8798 | prec=0.8711 rec=0.8909 f1=0.8809 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4226 acc=0.7884 | val_loss=0.4013 acc=0.8503 | prec=0.8889 rec=0.8000 f1=0.8421 | time=13.0s\n",
            "Epoch 032 | train_loss=0.3898 acc=0.8060 | val_loss=0.4422 acc=0.8254 | prec=0.7638 rec=0.9409 f1=0.8432 | time=13.0s\n",
            "Epoch 033 | train_loss=0.3965 acc=0.8049 | val_loss=0.5941 acc=0.7778 | prec=0.9621 rec=0.5773 f1=0.7216 | time=12.9s\n",
            "Epoch 034 | train_loss=0.3729 acc=0.7998 | val_loss=0.4547 acc=0.7642 | prec=0.6933 rec=0.9455 f1=0.8000 | time=13.1s\n",
            "Epoch 035 | train_loss=0.3620 acc=0.8219 | val_loss=0.4553 acc=0.7483 | prec=0.9431 rec=0.5273 f1=0.6764 | time=13.0s\n",
            "Epoch 036 | train_loss=0.3542 acc=0.8191 | val_loss=2.2734 acc=0.5578 | prec=0.9630 rec=0.1182 f1=0.2105 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3473 acc=0.8151 | val_loss=0.5251 acc=0.8141 | prec=0.9313 rec=0.6773 f1=0.7842 | time=13.0s\n",
            "Epoch 038 | train_loss=0.3273 acc=0.8321 | val_loss=0.8643 acc=0.5873 | prec=0.5477 rec=0.9909 f1=0.7055 | time=13.0s\n",
            "Epoch 039 | train_loss=0.3786 acc=0.8157 | val_loss=1.3710 acc=0.7732 | prec=0.9762 rec=0.5591 f1=0.7110 | time=13.0s\n",
            "Epoch 040 | train_loss=0.3931 acc=0.8009 | val_loss=0.4145 acc=0.8277 | prec=0.8077 rec=0.8591 f1=0.8326 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3477 acc=0.8179 | val_loss=0.6128 acc=0.8050 | prec=0.9467 rec=0.6455 f1=0.7676 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3458 acc=0.8389 | val_loss=0.5162 acc=0.7664 | prec=0.7349 rec=0.8318 f1=0.7804 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3369 acc=0.8253 | val_loss=0.4547 acc=0.8073 | prec=0.7647 rec=0.8864 f1=0.8211 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3279 acc=0.8406 | val_loss=0.5259 acc=0.8141 | prec=0.9059 rec=0.7000 f1=0.7897 | time=13.1s\n",
            "Epoch 045 | train_loss=0.3026 acc=0.8395 | val_loss=0.7318 acc=0.7370 | prec=0.9407 rec=0.5045 f1=0.6568 | time=12.9s\n",
            "Early stopping at epoch 45\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▆▅▄▆▁▆▂▇▇▇▇▆▇▇▃▇█▅▇▇█▇█▇███▇▇▆▇▇▇█▇▇█▆</td></tr><tr><td>precision</td><td>▄▁▄▅█▄▁▄█▇▇█▅▄█▇█▆▇███▆▅▆█▇▇▆█▆██▅█▇█▆▆█</td></tr><tr><td>recall</td><td>█▁█▄▃█▁█▁▅▅▅██▆▆▂▇▇▃▆▆▇██▅▇▇█▅█▅▆█▅▇▆▇▇▅</td></tr><tr><td>train_accuracy</td><td>▂▁▂▁▂▂▁▁▄▅▅▅▆▅▆▆▆▄▅▆▆▆▇▆▆▇▆▇▇▇▇████▇████</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▆▅▅▅▅▅▅▅▅▆▅▅▅▄▄▄▅▃▃▃▂▂▂▂▂▁▂▂▂▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▂▃▁▁▁▁▆▆▆▂▁▇▇▃▆▇▄▇▇▇▂▇▇█▇▇▆▆▆▇▃▆▇▇▆▇▅</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▃▃▃▂▂▂▄▄▁▁▅▂▂▂▁▁▂▅▁▁▁▁▁▂▁▁▂▄█▁▃▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.6568</td></tr><tr><td>precision</td><td>0.94068</td></tr><tr><td>recall</td><td>0.50455</td></tr><tr><td>train_accuracy</td><td>0.83948</td></tr><tr><td>train_loss</td><td>0.30262</td></tr><tr><td>validation_accuracy</td><td>0.73696</td></tr><tr><td>validation_loss</td><td>0.73182</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/w6xu1z9y' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/w6xu1z9y</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_000238-w6xu1z9y/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 00:12:23,627] Trial 5 finished with values: [0.7318184716360909, 0.7369614512471655] and parameters: {'lr': 0.008103692597410167, 'wd': 3.117711061434857e-05, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: lr=1.61e-03, wd=5.70e-02, filters = 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_001223-of3be0sc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/of3be0sc' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/of3be0sc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/of3be0sc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7966 acc=0.4776 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.4s\n",
            "Epoch 002 | train_loss=0.7369 acc=0.5190 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 003 | train_loss=0.7440 acc=0.5122 | val_loss=0.7031 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.2s\n",
            "Epoch 004 | train_loss=0.7425 acc=0.5048 | val_loss=0.6924 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.1s\n",
            "Epoch 005 | train_loss=0.7526 acc=0.5026 | val_loss=0.6863 acc=0.5102 | prec=1.0000 rec=0.0182 f1=0.0357 | time=18.1s\n",
            "Epoch 006 | train_loss=0.7067 acc=0.5570 | val_loss=0.5913 acc=0.7460 | prec=0.8971 rec=0.5545 f1=0.6854 | time=18.2s\n",
            "Epoch 007 | train_loss=0.6975 acc=0.5423 | val_loss=0.4801 acc=0.7982 | prec=0.7787 rec=0.8318 f1=0.8044 | time=18.1s\n",
            "Epoch 008 | train_loss=0.6126 acc=0.6750 | val_loss=0.4921 acc=0.8322 | prec=0.8411 rec=0.8182 f1=0.8295 | time=18.2s\n",
            "Epoch 009 | train_loss=0.5766 acc=0.6960 | val_loss=0.4090 acc=0.8322 | prec=0.8544 rec=0.8000 f1=0.8263 | time=18.1s\n",
            "Epoch 010 | train_loss=0.5493 acc=0.7181 | val_loss=0.4888 acc=0.8118 | prec=0.9477 rec=0.6591 f1=0.7775 | time=18.1s\n",
            "Epoch 011 | train_loss=0.5222 acc=0.7249 | val_loss=0.4756 acc=0.8050 | prec=0.9527 rec=0.6409 f1=0.7663 | time=18.1s\n",
            "Epoch 012 | train_loss=0.5028 acc=0.7476 | val_loss=0.5184 acc=0.6916 | prec=0.6243 rec=0.9591 f1=0.7563 | time=18.1s\n",
            "Epoch 013 | train_loss=0.4827 acc=0.7521 | val_loss=0.6094 acc=0.5488 | prec=0.5253 rec=0.9909 f1=0.6866 | time=18.2s\n",
            "Epoch 014 | train_loss=0.4856 acc=0.7504 | val_loss=0.3216 acc=0.8707 | prec=0.8826 rec=0.8545 f1=0.8684 | time=18.1s\n",
            "Epoch 015 | train_loss=0.4560 acc=0.7828 | val_loss=0.4518 acc=0.7914 | prec=0.7520 rec=0.8682 f1=0.8059 | time=18.3s\n",
            "Epoch 016 | train_loss=0.4555 acc=0.7862 | val_loss=0.8298 acc=0.6145 | prec=0.5644 rec=0.9955 f1=0.7204 | time=18.2s\n",
            "Epoch 017 | train_loss=0.4263 acc=0.7850 | val_loss=0.6108 acc=0.7279 | prec=0.6543 rec=0.9636 f1=0.7794 | time=18.3s\n",
            "Epoch 018 | train_loss=0.4502 acc=0.7862 | val_loss=0.4755 acc=0.8118 | prec=0.8586 rec=0.7455 f1=0.7981 | time=18.2s\n",
            "Epoch 019 | train_loss=0.4047 acc=0.8077 | val_loss=0.2991 acc=0.8844 | prec=0.9246 rec=0.8364 f1=0.8783 | time=18.2s\n",
            "Epoch 020 | train_loss=0.3862 acc=0.8191 | val_loss=0.4264 acc=0.8254 | prec=0.9799 rec=0.6636 f1=0.7913 | time=18.3s\n",
            "Epoch 021 | train_loss=0.3822 acc=0.8287 | val_loss=1.4269 acc=0.5193 | prec=0.5094 rec=0.9864 f1=0.6718 | time=18.2s\n",
            "Epoch 022 | train_loss=0.4456 acc=0.7623 | val_loss=0.3927 acc=0.8549 | prec=0.9483 rec=0.7500 f1=0.8376 | time=18.3s\n",
            "Epoch 023 | train_loss=0.4015 acc=0.7975 | val_loss=0.4971 acc=0.8299 | prec=0.9448 rec=0.7000 f1=0.8042 | time=18.1s\n",
            "Epoch 024 | train_loss=0.3805 acc=0.8253 | val_loss=0.3851 acc=0.8390 | prec=0.9357 rec=0.7273 f1=0.8184 | time=18.1s\n",
            "Epoch 025 | train_loss=0.3524 acc=0.8157 | val_loss=0.3833 acc=0.8413 | prec=0.8099 rec=0.8909 f1=0.8485 | time=18.2s\n",
            "Epoch 026 | train_loss=0.3206 acc=0.8457 | val_loss=0.3479 acc=0.8844 | prec=0.8967 rec=0.8682 f1=0.8822 | time=18.2s\n",
            "Epoch 027 | train_loss=0.3163 acc=0.8446 | val_loss=0.4363 acc=0.8209 | prec=0.9379 rec=0.6864 f1=0.7927 | time=18.3s\n",
            "Epoch 028 | train_loss=0.2966 acc=0.8520 | val_loss=0.5036 acc=0.8231 | prec=0.9034 rec=0.7227 f1=0.8030 | time=18.4s\n",
            "Epoch 029 | train_loss=0.2843 acc=0.8571 | val_loss=0.4342 acc=0.8322 | prec=0.7786 rec=0.9273 f1=0.8465 | time=18.4s\n",
            "Epoch 030 | train_loss=0.3381 acc=0.8412 | val_loss=0.4642 acc=0.7642 | prec=0.6895 rec=0.9591 f1=0.8023 | time=18.3s\n",
            "Epoch 031 | train_loss=0.3360 acc=0.8259 | val_loss=0.4627 acc=0.7959 | prec=0.9452 rec=0.6273 f1=0.7541 | time=18.4s\n",
            "Epoch 032 | train_loss=0.2908 acc=0.8469 | val_loss=0.5431 acc=0.7891 | prec=0.9847 rec=0.5864 f1=0.7350 | time=18.0s\n",
            "Epoch 033 | train_loss=0.2723 acc=0.8576 | val_loss=0.4695 acc=0.8073 | prec=0.8358 rec=0.7636 f1=0.7981 | time=18.2s\n",
            "Epoch 034 | train_loss=0.2796 acc=0.8644 | val_loss=0.4162 acc=0.8390 | prec=0.8860 rec=0.7773 f1=0.8281 | time=18.1s\n",
            "Early stopping at epoch 34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▆▆▁▆▇██▇▇▇▆█▇▇▇▇█▇▆█▇▇██▇▇█▇▇▇▇█</td></tr><tr><td>precision</td><td>▄▁▄▄█▇▆▇▇██▅▅▇▆▅▆▇▇█▅███▇▇█▇▆▆██▇▇</td></tr><tr><td>recall</td><td>█▁██▁▅▇▇▇▆▅██▇▇██▆▇▆█▆▆▆▇▇▆▆▇█▅▅▆▆</td></tr><tr><td>train_accuracy</td><td>▁▂▂▁▁▂▂▅▅▅▅▆▆▆▇▇▇▇▇▇▇▆▇▇▇█████▇███</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▃▃▂▂▂▂▁▁▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▅▆▇▇▇▇▅▂█▆▃▅▇█▇▁▇▇▇▇█▇▇▇▆▆▆▇▇</td></tr><tr><td>validation_loss</td><td>▃▃▄▃▃▃▂▂▂▂▂▂▃▁▂▄▃▂▁▂█▂▂▂▂▁▂▂▂▂▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82809</td></tr><tr><td>precision</td><td>0.88601</td></tr><tr><td>recall</td><td>0.77727</td></tr><tr><td>train_accuracy</td><td>0.86444</td></tr><tr><td>train_loss</td><td>0.27957</td></tr><tr><td>validation_accuracy</td><td>0.839</td></tr><tr><td>validation_loss</td><td>0.41624</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/of3be0sc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/of3be0sc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_001223-of3be0sc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 00:22:44,006] Trial 6 finished with values: [0.4162392701421465, 0.8390022675736961] and parameters: {'lr': 0.0016082997924311262, 'wd': 0.05704506462719705, 'num_filters': 150}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: lr=6.22e-03, wd=1.61e-04, filters = 180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_002244-e8ehygi8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/e8ehygi8' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/e8ehygi8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/e8ehygi8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7241 acc=0.5241 | val_loss=0.7037 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 002 | train_loss=0.7195 acc=0.4838 | val_loss=0.7092 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 003 | train_loss=0.7378 acc=0.5031 | val_loss=0.6989 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 004 | train_loss=0.7200 acc=0.4929 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 005 | train_loss=0.7148 acc=0.4872 | val_loss=0.6882 acc=0.5896 | prec=0.9756 rec=0.1818 f1=0.3065 | time=23.6s\n",
            "Epoch 006 | train_loss=0.6733 acc=0.5666 | val_loss=0.5967 acc=0.7823 | prec=0.8827 rec=0.6500 f1=0.7487 | time=23.5s\n",
            "Epoch 007 | train_loss=0.6255 acc=0.6483 | val_loss=0.5605 acc=0.8413 | prec=0.8571 rec=0.8182 f1=0.8372 | time=23.5s\n",
            "Epoch 008 | train_loss=0.5723 acc=0.7226 | val_loss=0.6611 acc=0.5283 | prec=0.5141 rec=0.9955 f1=0.6780 | time=23.6s\n",
            "Epoch 009 | train_loss=0.5552 acc=0.7175 | val_loss=0.6836 acc=0.5805 | prec=0.9487 rec=0.1682 f1=0.2857 | time=23.6s\n",
            "Epoch 010 | train_loss=0.5394 acc=0.7317 | val_loss=0.3787 acc=0.8390 | prec=0.8117 rec=0.8818 f1=0.8453 | time=23.9s\n",
            "Epoch 011 | train_loss=0.5370 acc=0.7448 | val_loss=0.5623 acc=0.6621 | prec=0.5967 rec=0.9955 f1=0.7462 | time=24.0s\n",
            "Epoch 012 | train_loss=0.5178 acc=0.7487 | val_loss=0.4566 acc=0.7959 | prec=0.9577 rec=0.6182 f1=0.7514 | time=23.8s\n",
            "Epoch 013 | train_loss=0.4887 acc=0.7612 | val_loss=0.3524 acc=0.8617 | prec=0.8841 rec=0.8318 f1=0.8571 | time=23.8s\n",
            "Epoch 014 | train_loss=0.5127 acc=0.7459 | val_loss=0.5624 acc=0.7891 | prec=0.9379 rec=0.6182 f1=0.7452 | time=23.9s\n",
            "Epoch 015 | train_loss=0.5227 acc=0.7317 | val_loss=0.5316 acc=0.8299 | prec=0.9290 rec=0.7136 f1=0.8072 | time=23.6s\n",
            "Epoch 016 | train_loss=0.5068 acc=0.7391 | val_loss=0.7776 acc=0.6803 | prec=0.6125 rec=0.9773 f1=0.7531 | time=23.8s\n",
            "Epoch 017 | train_loss=0.5057 acc=0.7431 | val_loss=1.4164 acc=0.5283 | prec=1.0000 rec=0.0545 f1=0.1034 | time=23.8s\n",
            "Epoch 018 | train_loss=0.4947 acc=0.7595 | val_loss=0.5153 acc=0.7755 | prec=0.7153 rec=0.9136 f1=0.8024 | time=23.8s\n",
            "Epoch 019 | train_loss=0.5051 acc=0.7470 | val_loss=0.6303 acc=0.6077 | prec=0.5604 rec=0.9909 f1=0.7159 | time=23.9s\n",
            "Epoch 020 | train_loss=0.5727 acc=0.6937 | val_loss=0.5426 acc=0.7483 | prec=0.9658 rec=0.5136 f1=0.6706 | time=23.8s\n",
            "Epoch 021 | train_loss=0.4595 acc=0.7629 | val_loss=0.7811 acc=0.6712 | prec=0.6062 rec=0.9727 f1=0.7469 | time=23.9s\n",
            "Epoch 022 | train_loss=0.4659 acc=0.7674 | val_loss=0.5126 acc=0.7528 | prec=0.9370 rec=0.5409 f1=0.6859 | time=23.8s\n",
            "Epoch 023 | train_loss=0.4506 acc=0.7612 | val_loss=0.4616 acc=0.8073 | prec=0.8902 rec=0.7000 f1=0.7837 | time=23.8s\n",
            "Epoch 024 | train_loss=0.4772 acc=0.7629 | val_loss=0.3804 acc=0.8435 | prec=0.8683 rec=0.8091 f1=0.8376 | time=23.7s\n",
            "Epoch 025 | train_loss=0.4664 acc=0.7635 | val_loss=0.4388 acc=0.8095 | prec=0.8434 rec=0.7591 f1=0.7990 | time=23.8s\n",
            "Epoch 026 | train_loss=0.4216 acc=0.8003 | val_loss=1.0356 acc=0.6145 | prec=0.5648 rec=0.9909 f1=0.7195 | time=23.7s\n",
            "Epoch 027 | train_loss=0.4415 acc=0.7754 | val_loss=0.8772 acc=0.5646 | prec=0.9667 rec=0.1318 f1=0.2320 | time=23.8s\n",
            "Epoch 028 | train_loss=0.5396 acc=0.7345 | val_loss=0.4918 acc=0.7687 | prec=0.9214 rec=0.5864 f1=0.7167 | time=23.9s\n",
            "Early stopping at epoch 28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▆▄▇█▇▃█▇▇█▇█▇▂█▇▆▇▇▇██▇▃▇</td></tr><tr><td>precision</td><td>▁▁▁▄█▇▇▅█▇▅█▇██▅█▆▅█▅█▇▇▇▅█▇</td></tr><tr><td>recall</td><td>▁▁▁█▂▆▇█▂▇█▅▇▅▆█▁▇█▅█▅▆▇▆█▂▅</td></tr><tr><td>train_accuracy</td><td>▂▁▁▁▁▃▅▆▆▆▇▇▇▇▆▇▇▇▇▆▇▇▇▇▇█▇▇</td></tr><tr><td>train_loss</td><td>████▇▇▆▄▄▄▄▃▂▃▃▃▃▃▃▄▂▂▂▂▂▁▁▄</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▃▆█▂▃█▄▇█▇▇▅▂▆▃▆▄▆▇█▇▃▂▆</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▂▃▃▁▂▂▁▂▂▄█▂▃▂▄▂▂▁▂▅▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.71667</td></tr><tr><td>precision</td><td>0.92143</td></tr><tr><td>recall</td><td>0.58636</td></tr><tr><td>train_accuracy</td><td>0.73454</td></tr><tr><td>train_loss</td><td>0.53962</td></tr><tr><td>validation_accuracy</td><td>0.76871</td></tr><tr><td>validation_loss</td><td>0.4918</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/e8ehygi8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/e8ehygi8</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_002244-e8ehygi8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 00:33:50,054] Trial 7 finished with values: [0.49180001446178984, 0.7687074829931972] and parameters: {'lr': 0.006217357874987124, 'wd': 0.0001607667053344263, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: lr=1.66e-04, wd=4.12e-03, filters = 180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_003350-nqyp7ugi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/nqyp7ugi' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/nqyp7ugi' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/nqyp7ugi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7584 acc=0.5201 | val_loss=0.6952 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 002 | train_loss=0.7620 acc=0.5184 | val_loss=0.6931 acc=0.4943 | prec=0.4961 rec=0.8636 f1=0.6302 | time=23.4s\n",
            "Epoch 003 | train_loss=0.7822 acc=0.4969 | val_loss=0.6996 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 004 | train_loss=0.7620 acc=0.4906 | val_loss=0.6961 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 005 | train_loss=0.7578 acc=0.5031 | val_loss=0.6995 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 006 | train_loss=0.7487 acc=0.4912 | val_loss=0.6990 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.4s\n",
            "Epoch 007 | train_loss=0.7323 acc=0.4974 | val_loss=0.6957 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 008 | train_loss=0.7358 acc=0.4906 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 009 | train_loss=0.7427 acc=0.4986 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 010 | train_loss=0.7460 acc=0.4855 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 011 | train_loss=0.7279 acc=0.5031 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 012 | train_loss=0.7234 acc=0.5031 | val_loss=0.6917 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 013 | train_loss=0.7325 acc=0.5037 | val_loss=0.6759 acc=0.6780 | prec=0.8679 rec=0.4182 f1=0.5644 | time=23.3s\n",
            "Epoch 014 | train_loss=0.6726 acc=0.5712 | val_loss=0.5445 acc=0.7392 | prec=0.6768 rec=0.9136 f1=0.7776 | time=23.5s\n",
            "Epoch 015 | train_loss=0.5987 acc=0.6670 | val_loss=0.4958 acc=0.8027 | prec=0.7830 rec=0.8364 f1=0.8088 | time=23.6s\n",
            "Epoch 016 | train_loss=0.5843 acc=0.6744 | val_loss=0.5047 acc=0.7619 | prec=0.9259 rec=0.5682 f1=0.7042 | time=23.6s\n",
            "Epoch 017 | train_loss=0.5385 acc=0.7164 | val_loss=0.5129 acc=0.8458 | prec=0.8654 rec=0.8182 f1=0.8411 | time=23.4s\n",
            "Epoch 018 | train_loss=0.5295 acc=0.7266 | val_loss=0.5271 acc=0.7188 | prec=0.6491 rec=0.9500 f1=0.7712 | time=23.5s\n",
            "Epoch 019 | train_loss=0.5315 acc=0.7209 | val_loss=0.4132 acc=0.8413 | prec=0.8074 rec=0.8955 f1=0.8491 | time=23.5s\n",
            "Epoch 020 | train_loss=0.5137 acc=0.7527 | val_loss=0.4187 acc=0.8776 | prec=0.9109 rec=0.8364 f1=0.8720 | time=23.5s\n",
            "Epoch 021 | train_loss=0.4864 acc=0.7572 | val_loss=0.3873 acc=0.8549 | prec=0.8421 rec=0.8727 f1=0.8571 | time=23.6s\n",
            "Epoch 022 | train_loss=0.4732 acc=0.7686 | val_loss=0.3898 acc=0.8594 | prec=0.8591 rec=0.8591 f1=0.8591 | time=23.6s\n",
            "Epoch 023 | train_loss=0.4315 acc=0.7964 | val_loss=0.3709 acc=0.8503 | prec=0.8319 rec=0.8773 f1=0.8540 | time=23.5s\n",
            "Epoch 024 | train_loss=0.4621 acc=0.7754 | val_loss=0.3870 acc=0.8594 | prec=0.9293 rec=0.7773 f1=0.8465 | time=23.6s\n",
            "Epoch 025 | train_loss=0.4276 acc=0.7822 | val_loss=0.3858 acc=0.8458 | prec=0.8585 rec=0.8273 f1=0.8426 | time=23.5s\n",
            "Epoch 026 | train_loss=0.4187 acc=0.8071 | val_loss=0.3486 acc=0.8571 | prec=0.8905 rec=0.8136 f1=0.8504 | time=23.5s\n",
            "Epoch 027 | train_loss=0.4410 acc=0.7771 | val_loss=0.4298 acc=0.8095 | prec=0.7500 rec=0.9273 f1=0.8293 | time=23.5s\n",
            "Epoch 028 | train_loss=0.4180 acc=0.7805 | val_loss=0.3620 acc=0.8458 | prec=0.8455 rec=0.8455 f1=0.8455 | time=23.5s\n",
            "Epoch 029 | train_loss=0.3796 acc=0.8293 | val_loss=0.5885 acc=0.6553 | prec=0.5950 rec=0.9682 f1=0.7370 | time=23.5s\n",
            "Epoch 030 | train_loss=0.3724 acc=0.8327 | val_loss=0.4437 acc=0.7823 | prec=0.7153 rec=0.9364 f1=0.8110 | time=23.6s\n",
            "Epoch 031 | train_loss=0.3682 acc=0.8208 | val_loss=0.5439 acc=0.7052 | prec=0.6339 rec=0.9682 f1=0.7662 | time=23.5s\n",
            "Epoch 032 | train_loss=0.3638 acc=0.8344 | val_loss=0.8385 acc=0.5964 | prec=0.5547 rec=0.9682 f1=0.7053 | time=23.5s\n",
            "Epoch 033 | train_loss=0.3750 acc=0.8117 | val_loss=0.3516 acc=0.8617 | prec=0.8768 rec=0.8409 f1=0.8585 | time=23.5s\n",
            "Epoch 034 | train_loss=0.3719 acc=0.8383 | val_loss=0.3962 acc=0.8209 | prec=0.7831 rec=0.8864 f1=0.8316 | time=23.5s\n",
            "Epoch 035 | train_loss=0.3436 acc=0.8366 | val_loss=0.4203 acc=0.8073 | prec=0.7528 rec=0.9136 f1=0.8255 | time=23.5s\n",
            "Epoch 036 | train_loss=0.3963 acc=0.8140 | val_loss=0.3724 acc=0.8435 | prec=0.9266 rec=0.7455 f1=0.8262 | time=23.5s\n",
            "Epoch 037 | train_loss=0.4039 acc=0.8083 | val_loss=0.3771 acc=0.8481 | prec=0.8696 rec=0.8182 f1=0.8431 | time=23.5s\n",
            "Epoch 038 | train_loss=0.3655 acc=0.8366 | val_loss=0.3659 acc=0.8549 | prec=0.8578 rec=0.8500 f1=0.8539 | time=23.6s\n",
            "Epoch 039 | train_loss=0.3566 acc=0.8304 | val_loss=0.4498 acc=0.7846 | prec=0.7289 rec=0.9045 f1=0.8073 | time=23.5s\n",
            "Epoch 040 | train_loss=0.3479 acc=0.8327 | val_loss=0.3731 acc=0.8549 | prec=0.8451 rec=0.8682 f1=0.8565 | time=23.4s\n",
            "Epoch 041 | train_loss=0.3230 acc=0.8417 | val_loss=0.3683 acc=0.8503 | prec=0.8263 rec=0.8864 f1=0.8553 | time=23.5s\n",
            "Early stopping at epoch 41\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▆▆▆▁▁▁▁▆▁▆▇▇▇█▇██████████▇█▇▇██████▇█</td></tr><tr><td>precision</td><td>▅▅▅▅▅▅▁▁▁▁▅▁█▆▇██▆▇█▇▇▇█▇█▇▇▅▆▆▅█▇▇██▇▆▇</td></tr><tr><td>recall</td><td>█▇████▁▁▁▁█▁▄▇▇▅▇█▇▇▇▇▇▆▇▇▇▇████▇▇▇▆▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▂▂▁▁▁▁▁▁▁▁▁▁▁▃▅▅▆▆▆▆▆▇▇▇▇▇▇▇████▇██▇▇███</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▇▇▇▆▅▅▄▄▄▄▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▂▂▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▅▇▆▇▅▇█████▇█▇▇▄▆▅▃█▇▇▇▇█▆█</td></tr><tr><td>validation_loss</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▄▃▃▃▄▂▂▂▂▁▂▂▁▂▁▄▂▄█▁▂▂▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85526</td></tr><tr><td>precision</td><td>0.82627</td></tr><tr><td>recall</td><td>0.88636</td></tr><tr><td>train_accuracy</td><td>0.84175</td></tr><tr><td>train_loss</td><td>0.323</td></tr><tr><td>validation_accuracy</td><td>0.85034</td></tr><tr><td>validation_loss</td><td>0.36829</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/nqyp7ugi' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/nqyp7ugi</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_003350-nqyp7ugi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 00:49:55,469] Trial 8 finished with values: [0.36828610407454626, 0.8503401360544217] and parameters: {'lr': 0.0001655741045417959, 'wd': 0.004116746760159083, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: lr=3.03e-03, wd=1.22e-03, filters = 120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_004955-x4werip7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/x4werip7' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/x4werip7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/x4werip7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7347 acc=0.5054 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7279 acc=0.5145 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 003 | train_loss=0.7392 acc=0.5048 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7317 acc=0.5037 | val_loss=0.6919 acc=0.5057 | prec=1.0000 rec=0.0091 f1=0.0180 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7256 acc=0.5173 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7143 acc=0.4963 | val_loss=0.6860 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7071 acc=0.5332 | val_loss=0.6784 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.6657 acc=0.5729 | val_loss=0.5437 acc=0.7687 | prec=0.7418 rec=0.8227 f1=0.7802 | time=12.7s\n",
            "Epoch 009 | train_loss=0.6055 acc=0.6682 | val_loss=0.4943 acc=0.7937 | prec=0.7817 rec=0.8136 f1=0.7973 | time=12.8s\n",
            "Epoch 010 | train_loss=0.5652 acc=0.7181 | val_loss=0.4738 acc=0.7823 | prec=0.7263 rec=0.9045 f1=0.8057 | time=12.7s\n",
            "Epoch 011 | train_loss=0.5265 acc=0.7323 | val_loss=0.5339 acc=0.6916 | prec=0.6228 rec=0.9682 f1=0.7580 | time=12.8s\n",
            "Epoch 012 | train_loss=0.5796 acc=0.7187 | val_loss=0.6528 acc=0.5147 | prec=0.5069 rec=1.0000 f1=0.6728 | time=12.9s\n",
            "Epoch 013 | train_loss=0.6276 acc=0.6211 | val_loss=0.4799 acc=0.8413 | prec=0.8713 rec=0.8000 f1=0.8341 | time=12.8s\n",
            "Epoch 014 | train_loss=0.5287 acc=0.7516 | val_loss=0.3736 acc=0.8707 | prec=0.8622 rec=0.8818 f1=0.8719 | time=12.7s\n",
            "Epoch 015 | train_loss=0.5152 acc=0.7521 | val_loss=0.3762 acc=0.8481 | prec=0.9274 rec=0.7545 f1=0.8321 | time=12.9s\n",
            "Epoch 016 | train_loss=0.4885 acc=0.7408 | val_loss=0.5502 acc=0.6440 | prec=0.5858 rec=0.9773 f1=0.7325 | time=12.8s\n",
            "Epoch 017 | train_loss=0.4564 acc=0.7822 | val_loss=0.7740 acc=0.5646 | prec=0.5343 rec=0.9909 f1=0.6943 | time=12.9s\n",
            "Epoch 018 | train_loss=0.4643 acc=0.7674 | val_loss=0.5688 acc=0.7370 | prec=0.9561 rec=0.4955 f1=0.6527 | time=12.8s\n",
            "Epoch 019 | train_loss=0.4360 acc=0.7862 | val_loss=0.6144 acc=0.6190 | prec=0.5677 rec=0.9909 f1=0.7219 | time=12.9s\n",
            "Epoch 020 | train_loss=0.4463 acc=0.7867 | val_loss=0.6267 acc=0.6961 | prec=0.6265 rec=0.9682 f1=0.7607 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4025 acc=0.7947 | val_loss=0.8375 acc=0.5556 | prec=0.5288 rec=1.0000 f1=0.6918 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4250 acc=0.7862 | val_loss=0.4757 acc=0.7800 | prec=0.7128 rec=0.9364 f1=0.8094 | time=13.0s\n",
            "Epoch 023 | train_loss=0.4161 acc=0.7998 | val_loss=0.4336 acc=0.8390 | prec=0.8282 rec=0.8545 f1=0.8412 | time=13.0s\n",
            "Epoch 024 | train_loss=0.3913 acc=0.8071 | val_loss=0.4540 acc=0.7891 | prec=0.7433 rec=0.8818 f1=0.8067 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3706 acc=0.8077 | val_loss=0.4133 acc=0.8390 | prec=0.7899 rec=0.9227 f1=0.8512 | time=12.9s\n",
            "Epoch 026 | train_loss=0.3507 acc=0.8128 | val_loss=0.4229 acc=0.7868 | prec=0.7128 rec=0.9591 f1=0.8178 | time=13.0s\n",
            "Epoch 027 | train_loss=0.3564 acc=0.8236 | val_loss=0.9459 acc=0.6100 | prec=0.5615 rec=0.9955 f1=0.7180 | time=12.9s\n",
            "Epoch 028 | train_loss=0.3226 acc=0.8486 | val_loss=0.4325 acc=0.8141 | prec=0.7537 rec=0.9318 f1=0.8333 | time=13.1s\n",
            "Epoch 029 | train_loss=0.3423 acc=0.8338 | val_loss=0.9918 acc=0.7732 | prec=0.9478 rec=0.5773 f1=0.7175 | time=13.0s\n",
            "Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▁▁▁▆▆▇▇▇▇▆███▇▇▆▇▇▇▇█▇██▇█▇</td></tr><tr><td>precision</td><td>▄▁▁█▁▄▄▆▆▆▅▅▇▇▇▅▅█▅▅▅▆▇▆▇▆▅▆█</td></tr><tr><td>recall</td><td>█▁▁▁▁██▇▇▇██▇▇▆██▄████▇▇▇███▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▄▅▆▅▃▆▆▆▇▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██████▇▇▆▅▄▅▆▄▄▄▃▃▃▃▂▃▃▂▂▁▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▆▇▆▅▁▇██▄▂▅▃▅▂▆▇▆▇▆▃▇▆</td></tr><tr><td>validation_loss</td><td>▅▅▅▅▅▅▄▃▂▂▃▄▂▁▁▃▆▃▄▄▆▂▂▂▁▂▇▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.71751</td></tr><tr><td>precision</td><td>0.94776</td></tr><tr><td>recall</td><td>0.57727</td></tr><tr><td>train_accuracy</td><td>0.83381</td></tr><tr><td>train_loss</td><td>0.34228</td></tr><tr><td>validation_accuracy</td><td>0.77324</td></tr><tr><td>validation_loss</td><td>0.99182</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/x4werip7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/x4werip7</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_004955-x4werip7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 00:56:09,967] Trial 9 finished with values: [0.9918230644294194, 0.7732426303854876] and parameters: {'lr': 0.0030273929768849963, 'wd': 0.0012217095723411132, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 10: lr=2.69e-03, wd=1.58e-05, filters = 180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_005609-rd8u46um</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/rd8u46um' target=\"_blank\">trial10</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/rd8u46um' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/rd8u46um</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7607 acc=0.5071 | val_loss=0.6967 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 002 | train_loss=0.7444 acc=0.5116 | val_loss=0.6969 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 003 | train_loss=0.7368 acc=0.5060 | val_loss=0.6903 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 004 | train_loss=0.6903 acc=0.5627 | val_loss=0.6003 acc=0.7166 | prec=0.9279 rec=0.4682 f1=0.6224 | time=23.6s\n",
            "Epoch 005 | train_loss=0.6130 acc=0.6466 | val_loss=0.5022 acc=0.8345 | prec=0.8657 rec=0.7909 f1=0.8266 | time=23.6s\n",
            "Epoch 006 | train_loss=0.5775 acc=0.6790 | val_loss=0.5561 acc=0.6984 | prec=0.6283 rec=0.9682 f1=0.7621 | time=23.5s\n",
            "Epoch 007 | train_loss=0.5673 acc=0.6841 | val_loss=0.4517 acc=0.8571 | prec=0.8520 rec=0.8636 f1=0.8578 | time=23.5s\n",
            "Epoch 008 | train_loss=0.5376 acc=0.7226 | val_loss=0.5169 acc=0.7370 | prec=0.9727 rec=0.4864 f1=0.6485 | time=23.4s\n",
            "Epoch 009 | train_loss=0.5504 acc=0.6886 | val_loss=0.5271 acc=0.7370 | prec=0.9727 rec=0.4864 f1=0.6485 | time=23.3s\n",
            "Epoch 010 | train_loss=0.5538 acc=0.7113 | val_loss=0.3998 acc=0.8458 | prec=0.8423 rec=0.8500 f1=0.8462 | time=23.4s\n",
            "Epoch 011 | train_loss=0.5701 acc=0.6892 | val_loss=0.4153 acc=0.8027 | prec=0.7454 rec=0.9182 f1=0.8228 | time=23.6s\n",
            "Epoch 012 | train_loss=0.5020 acc=0.7425 | val_loss=0.3385 acc=0.8776 | prec=0.8952 rec=0.8545 f1=0.8744 | time=23.6s\n",
            "Epoch 013 | train_loss=0.5066 acc=0.7465 | val_loss=0.5706 acc=0.6916 | prec=0.9884 rec=0.3864 f1=0.5556 | time=23.5s\n",
            "Epoch 014 | train_loss=0.5167 acc=0.7328 | val_loss=0.4636 acc=0.7710 | prec=0.9542 rec=0.5682 f1=0.7123 | time=23.5s\n",
            "Epoch 015 | train_loss=0.4628 acc=0.7788 | val_loss=0.5241 acc=0.6984 | prec=0.6268 rec=0.9773 f1=0.7638 | time=23.5s\n",
            "Epoch 016 | train_loss=0.4443 acc=0.7765 | val_loss=0.5353 acc=0.7370 | prec=0.6646 rec=0.9545 f1=0.7836 | time=23.7s\n",
            "Epoch 017 | train_loss=0.4503 acc=0.7771 | val_loss=0.5132 acc=0.7188 | prec=0.6463 rec=0.9636 f1=0.7737 | time=24.0s\n",
            "Epoch 018 | train_loss=0.4189 acc=0.7924 | val_loss=0.3320 acc=0.8639 | prec=0.9124 rec=0.8045 f1=0.8551 | time=24.1s\n",
            "Epoch 019 | train_loss=0.3957 acc=0.8066 | val_loss=1.1234 acc=0.5533 | prec=0.5276 rec=1.0000 f1=0.6907 | time=24.2s\n",
            "Epoch 020 | train_loss=0.3988 acc=0.8032 | val_loss=0.5271 acc=0.8027 | prec=0.9404 rec=0.6455 f1=0.7655 | time=24.3s\n",
            "Epoch 021 | train_loss=0.4451 acc=0.7816 | val_loss=0.3687 acc=0.8617 | prec=0.8533 rec=0.8727 f1=0.8629 | time=24.3s\n",
            "Epoch 022 | train_loss=0.4199 acc=0.7811 | val_loss=0.4964 acc=0.8118 | prec=0.9363 rec=0.6682 f1=0.7798 | time=24.2s\n",
            "Epoch 023 | train_loss=0.3969 acc=0.8123 | val_loss=0.6773 acc=0.7528 | prec=0.9744 rec=0.5182 f1=0.6766 | time=24.1s\n",
            "Epoch 024 | train_loss=0.4060 acc=0.7947 | val_loss=0.5922 acc=0.7029 | prec=0.6336 rec=0.9591 f1=0.7631 | time=24.0s\n",
            "Epoch 025 | train_loss=0.3954 acc=0.8049 | val_loss=0.3747 acc=0.8367 | prec=0.8458 rec=0.8227 f1=0.8341 | time=24.0s\n",
            "Epoch 026 | train_loss=0.4798 acc=0.7550 | val_loss=0.4317 acc=0.8345 | prec=0.8848 rec=0.7682 f1=0.8224 | time=24.0s\n",
            "Epoch 027 | train_loss=0.3649 acc=0.8225 | val_loss=0.5335 acc=0.7302 | prec=0.6624 rec=0.9364 f1=0.7759 | time=24.1s\n",
            "Epoch 028 | train_loss=0.3406 acc=0.8264 | val_loss=0.4082 acc=0.8413 | prec=0.8233 rec=0.8682 f1=0.8451 | time=23.9s\n",
            "Epoch 029 | train_loss=0.3765 acc=0.8310 | val_loss=0.5204 acc=0.7460 | prec=0.6765 rec=0.9409 f1=0.7871 | time=23.9s\n",
            "Epoch 030 | train_loss=0.3275 acc=0.8383 | val_loss=1.0226 acc=0.7914 | prec=0.9384 rec=0.6227 f1=0.7486 | time=24.1s\n",
            "Epoch 031 | train_loss=0.3414 acc=0.8349 | val_loss=0.5274 acc=0.8481 | prec=0.8732 rec=0.8136 f1=0.8424 | time=24.0s\n",
            "Epoch 032 | train_loss=0.3114 acc=0.8452 | val_loss=0.7305 acc=0.7052 | prec=0.6364 rec=0.9545 f1=0.7636 | time=24.1s\n",
            "Epoch 033 | train_loss=0.4136 acc=0.7884 | val_loss=0.4154 acc=0.8322 | prec=0.8318 rec=0.8318 f1=0.8318 | time=24.1s\n",
            "Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▁▆█▇█▆▆███▅▇▇▇▇█▇▇█▇▆▇██▇█▇▇█▇█</td></tr><tr><td>precision</td><td>▅▅▁█▇▅▇██▇▆▇██▅▆▆▇▅█▇██▅▇▇▆▇▆█▇▆▇</td></tr><tr><td>recall</td><td>██▁▄▇█▇▄▄▇▇▇▄▅███▇█▆▇▆▅█▇▆█▇█▅▇█▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▆██████▇</td></tr><tr><td>train_loss</td><td>███▇▆▅▅▅▅▅▅▄▄▄▃▃▃▃▂▂▃▃▂▂▂▄▂▁▂▁▁▁▃</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▅▇▅█▅▅▇▇█▅▆▅▅▅█▂▇█▇▆▅▇▇▅▇▆▆▇▅▇</td></tr><tr><td>validation_loss</td><td>▄▄▄▃▃▃▂▃▃▂▂▁▃▂▃▃▃▁█▃▁▂▄▃▁▂▃▂▃▇▃▅▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83182</td></tr><tr><td>precision</td><td>0.83182</td></tr><tr><td>recall</td><td>0.83182</td></tr><tr><td>train_accuracy</td><td>0.78843</td></tr><tr><td>train_loss</td><td>0.41359</td></tr><tr><td>validation_accuracy</td><td>0.8322</td></tr><tr><td>validation_loss</td><td>0.41539</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial10</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/rd8u46um' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/rd8u46um</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_005609-rd8u46um/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 01:09:16,923] Trial 10 finished with values: [0.41539155159677776, 0.8321995464852607] and parameters: {'lr': 0.0026939686155137664, 'wd': 1.5776755074863826e-05, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 11: lr=2.99e-04, wd=1.01e-03, filters = 150\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_010916-smj39l1z</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/smj39l1z' target=\"_blank\">trial11</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/smj39l1z' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/smj39l1z</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7571 acc=0.4952 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 002 | train_loss=0.7532 acc=0.4901 | val_loss=0.6958 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 003 | train_loss=0.7422 acc=0.5020 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.1s\n",
            "Epoch 004 | train_loss=0.7335 acc=0.5224 | val_loss=0.6929 acc=0.4966 | prec=0.4977 rec=0.9955 f1=0.6636 | time=18.0s\n",
            "Epoch 005 | train_loss=0.7436 acc=0.4969 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 006 | train_loss=0.7382 acc=0.5213 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.0s\n",
            "Epoch 007 | train_loss=0.7563 acc=0.5037 | val_loss=0.6922 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=18.1s\n",
            "Epoch 008 | train_loss=0.7292 acc=0.5116 | val_loss=0.6913 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=18.2s\n",
            "Epoch 009 | train_loss=0.7209 acc=0.5162 | val_loss=0.6279 acc=0.7528 | prec=0.7094 rec=0.8545 f1=0.7753 | time=18.2s\n",
            "Epoch 010 | train_loss=0.6514 acc=0.6018 | val_loss=0.5952 acc=0.7370 | prec=0.9407 rec=0.5045 f1=0.6568 | time=18.2s\n",
            "Epoch 011 | train_loss=0.6215 acc=0.6404 | val_loss=0.4838 acc=0.8322 | prec=0.8842 rec=0.7636 f1=0.8195 | time=18.2s\n",
            "Epoch 012 | train_loss=0.5631 acc=0.6863 | val_loss=0.4820 acc=0.8050 | prec=0.9295 rec=0.6591 f1=0.7713 | time=18.1s\n",
            "Epoch 013 | train_loss=0.5487 acc=0.7232 | val_loss=0.4266 acc=0.8617 | prec=0.9077 rec=0.8045 f1=0.8530 | time=18.0s\n",
            "Epoch 014 | train_loss=0.5356 acc=0.7340 | val_loss=0.4886 acc=0.7415 | prec=0.9492 rec=0.5091 f1=0.6627 | time=18.1s\n",
            "Epoch 015 | train_loss=0.5041 acc=0.7448 | val_loss=0.4230 acc=0.8594 | prec=0.8657 rec=0.8500 f1=0.8578 | time=18.2s\n",
            "Epoch 016 | train_loss=0.4898 acc=0.7691 | val_loss=0.3948 acc=0.8685 | prec=0.9355 rec=0.7909 f1=0.8571 | time=18.2s\n",
            "Epoch 017 | train_loss=0.4751 acc=0.7595 | val_loss=0.3992 acc=0.8435 | prec=0.8008 rec=0.9136 f1=0.8535 | time=18.1s\n",
            "Epoch 018 | train_loss=0.4560 acc=0.7782 | val_loss=0.3715 acc=0.8707 | prec=0.8826 rec=0.8545 f1=0.8684 | time=18.1s\n",
            "Epoch 019 | train_loss=0.4266 acc=0.7969 | val_loss=0.3776 acc=0.8707 | prec=0.8863 rec=0.8500 f1=0.8677 | time=18.1s\n",
            "Epoch 020 | train_loss=0.4172 acc=0.7958 | val_loss=0.3337 acc=0.8549 | prec=0.8451 rec=0.8682 f1=0.8565 | time=18.1s\n",
            "Epoch 021 | train_loss=0.4265 acc=0.7839 | val_loss=0.3716 acc=0.8458 | prec=0.8140 rec=0.8955 f1=0.8528 | time=18.1s\n",
            "Epoch 022 | train_loss=0.4130 acc=0.8037 | val_loss=0.3642 acc=0.8481 | prec=0.9227 rec=0.7591 f1=0.8329 | time=18.0s\n",
            "Epoch 023 | train_loss=0.3687 acc=0.8287 | val_loss=0.4180 acc=0.8186 | prec=0.7555 rec=0.9409 f1=0.8381 | time=18.2s\n",
            "Epoch 024 | train_loss=0.3657 acc=0.8259 | val_loss=0.3574 acc=0.8549 | prec=0.9194 rec=0.7773 f1=0.8424 | time=18.1s\n",
            "Epoch 025 | train_loss=0.3470 acc=0.8344 | val_loss=0.3676 acc=0.8549 | prec=0.9194 rec=0.7773 f1=0.8424 | time=18.0s\n",
            "Epoch 026 | train_loss=0.3705 acc=0.8111 | val_loss=0.3976 acc=0.8209 | prec=0.7975 rec=0.8591 f1=0.8271 | time=18.2s\n",
            "Epoch 027 | train_loss=0.3512 acc=0.8304 | val_loss=0.3551 acc=0.8345 | prec=0.8238 rec=0.8500 f1=0.8367 | time=18.0s\n",
            "Epoch 028 | train_loss=0.4273 acc=0.7725 | val_loss=0.3872 acc=0.8458 | prec=0.8519 rec=0.8364 f1=0.8440 | time=18.1s\n",
            "Epoch 029 | train_loss=0.3485 acc=0.8287 | val_loss=0.3887 acc=0.8209 | prec=0.9379 rec=0.6864 f1=0.7927 | time=18.0s\n",
            "Epoch 030 | train_loss=0.3408 acc=0.8236 | val_loss=0.3346 acc=0.8503 | prec=0.8889 rec=0.8000 f1=0.8421 | time=18.2s\n",
            "Epoch 031 | train_loss=0.3602 acc=0.8145 | val_loss=0.3936 acc=0.8231 | prec=0.9277 rec=0.7000 f1=0.7979 | time=18.1s\n",
            "Epoch 032 | train_loss=0.3139 acc=0.8440 | val_loss=0.3669 acc=0.8367 | prec=0.8190 rec=0.8636 f1=0.8407 | time=18.1s\n",
            "Epoch 033 | train_loss=0.3077 acc=0.8508 | val_loss=0.4977 acc=0.7506 | prec=0.6923 rec=0.9000 f1=0.7826 | time=18.1s\n",
            "Epoch 034 | train_loss=0.3163 acc=0.8361 | val_loss=0.3947 acc=0.8186 | prec=0.7991 rec=0.8500 f1=0.8238 | time=18.0s\n",
            "Epoch 035 | train_loss=0.2813 acc=0.8661 | val_loss=0.3775 acc=0.8254 | prec=0.8421 rec=0.8000 f1=0.8205 | time=18.1s\n",
            "Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▆▁▁▆▁▇▆█▇█▆██████████████▇█▇█▇██</td></tr><tr><td>precision</td><td>▁▁▁▅▁▁▅▁▆█████▇█▇██▇▇█▇██▇▇▇███▇▆▇▇</td></tr><tr><td>recall</td><td>▁▁▁█▁▁█▁▇▅▆▆▇▅▇▇▇▇▇▇▇▆█▆▆▇▇▇▆▇▆▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▁▂▁▁▁▃▄▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▆▇▇▇██▇█</td></tr><tr><td>train_loss</td><td>████████▇▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▃▂▂▂▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▆▅▇▇█▆██▇█████▇██▇▇█▇█▇▇▆▇▇</td></tr><tr><td>validation_loss</td><td>████████▇▆▄▄▃▄▃▂▂▂▂▁▂▂▃▁▂▂▁▂▂▁▂▂▄▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82051</td></tr><tr><td>precision</td><td>0.84211</td></tr><tr><td>recall</td><td>0.8</td></tr><tr><td>train_accuracy</td><td>0.86614</td></tr><tr><td>train_loss</td><td>0.28132</td></tr><tr><td>validation_accuracy</td><td>0.8254</td></tr><tr><td>validation_loss</td><td>0.37752</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial11</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/smj39l1z' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/smj39l1z</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_010916-smj39l1z/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 01:19:52,508] Trial 11 finished with values: [0.3775222088609423, 0.8253968253968254] and parameters: {'lr': 0.00029933632166593026, 'wd': 0.0010098366681389357, 'num_filters': 150}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 12: lr=1.25e-05, wd=5.51e-06, filters = 180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_011952-wia1m9cw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/wia1m9cw' target=\"_blank\">trial12</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/wia1m9cw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/wia1m9cw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7761 acc=0.5031 | val_loss=0.7036 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.7s\n",
            "Epoch 002 | train_loss=0.7635 acc=0.4980 | val_loss=0.6997 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 003 | train_loss=0.7727 acc=0.4759 | val_loss=0.6974 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 004 | train_loss=0.7590 acc=0.4872 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 005 | train_loss=0.7532 acc=0.4923 | val_loss=0.6930 acc=0.5034 | prec=1.0000 rec=0.0045 f1=0.0090 | time=23.4s\n",
            "Epoch 006 | train_loss=0.7501 acc=0.5026 | val_loss=0.6929 acc=0.4921 | prec=0.4167 rec=0.0455 f1=0.0820 | time=23.5s\n",
            "Epoch 007 | train_loss=0.7548 acc=0.4827 | val_loss=0.6929 acc=0.4989 | prec=0.4667 rec=0.0318 f1=0.0596 | time=23.4s\n",
            "Epoch 008 | train_loss=0.7428 acc=0.4946 | val_loss=0.6932 acc=0.5238 | prec=0.5806 rec=0.1636 f1=0.2553 | time=23.5s\n",
            "Epoch 009 | train_loss=0.7402 acc=0.4935 | val_loss=0.6930 acc=0.5011 | prec=0.5000 rec=0.0045 f1=0.0090 | time=23.6s\n",
            "Epoch 010 | train_loss=0.7365 acc=0.5150 | val_loss=0.6930 acc=0.4989 | prec=0.4667 rec=0.0318 f1=0.0596 | time=23.5s\n",
            "Epoch 011 | train_loss=0.7372 acc=0.4923 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 012 | train_loss=0.7392 acc=0.4912 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 013 | train_loss=0.7399 acc=0.4923 | val_loss=0.6954 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 014 | train_loss=0.7412 acc=0.4844 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 015 | train_loss=0.7219 acc=0.5133 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 016 | train_loss=0.7506 acc=0.4940 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.7s\n",
            "Epoch 017 | train_loss=0.7229 acc=0.4884 | val_loss=0.6954 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 018 | train_loss=0.7338 acc=0.4929 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 019 | train_loss=0.7329 acc=0.4861 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 020 | train_loss=0.7222 acc=0.5009 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.3s\n",
            "Epoch 021 | train_loss=0.7311 acc=0.5201 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Early stopping at epoch 21\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▃▃█▁▃▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▁▁▁█▄▄▅▅▄▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall</td><td>▁▁▁▁▁▃▂█▁▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▅▄▁▃▄▅▂▄▄▇▄▃▄▂▇▄▃▄▃▅█</td></tr><tr><td>train_loss</td><td>█▆█▆▅▅▅▄▃▃▃▃▃▃▁▅▁▃▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▁▂█▃▂▃▃▃▃▃▃▃▃▃▃▃</td></tr><tr><td>validation_loss</td><td>█▅▄▂▁▁▁▁▁▁▁▁▃▃▂▁▃▃▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.52014</td></tr><tr><td>train_loss</td><td>0.7311</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69397</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial12</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/wia1m9cw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/wia1m9cw</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_011952-wia1m9cw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 01:28:07,765] Trial 12 finished with values: [0.693972510950906, 0.5011337868480725] and parameters: {'lr': 1.2505015525340277e-05, 'wd': 5.50820428757708e-06, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 13: lr=1.33e-03, wd=3.66e-06, filters = 180\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_012807-zscel5vc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/zscel5vc' target=\"_blank\">trial13</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/zscel5vc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/zscel5vc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7503 acc=0.5065 | val_loss=0.6957 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 002 | train_loss=0.7342 acc=0.5082 | val_loss=0.6982 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 003 | train_loss=0.7387 acc=0.4850 | val_loss=0.6998 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 004 | train_loss=0.7317 acc=0.4901 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 005 | train_loss=0.7102 acc=0.5094 | val_loss=0.6927 acc=0.5011 | prec=0.5000 rec=0.0045 f1=0.0090 | time=23.5s\n",
            "Epoch 006 | train_loss=0.7157 acc=0.4889 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 007 | train_loss=0.7085 acc=0.5116 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 008 | train_loss=0.7504 acc=0.4929 | val_loss=0.6842 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.4s\n",
            "Epoch 009 | train_loss=0.7310 acc=0.5088 | val_loss=0.6919 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 010 | train_loss=0.6929 acc=0.5440 | val_loss=0.6684 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 011 | train_loss=0.6489 acc=0.6035 | val_loss=0.6334 acc=0.7846 | prec=0.7395 rec=0.8773 f1=0.8025 | time=23.6s\n",
            "Epoch 012 | train_loss=0.6138 acc=0.6585 | val_loss=0.5186 acc=0.7506 | prec=0.9365 rec=0.5364 f1=0.6821 | time=23.5s\n",
            "Epoch 013 | train_loss=0.5723 acc=0.7119 | val_loss=0.4660 acc=0.8345 | prec=0.8387 rec=0.8273 f1=0.8330 | time=23.6s\n",
            "Epoch 014 | train_loss=0.5432 acc=0.7107 | val_loss=0.5498 acc=0.8413 | prec=0.8099 rec=0.8909 f1=0.8485 | time=23.6s\n",
            "Epoch 015 | train_loss=0.5151 acc=0.7544 | val_loss=0.4310 acc=0.8005 | prec=0.7426 rec=0.9182 f1=0.8211 | time=23.6s\n",
            "Epoch 016 | train_loss=0.5016 acc=0.7544 | val_loss=0.5449 acc=0.6984 | prec=0.6246 rec=0.9909 f1=0.7663 | time=23.7s\n",
            "Epoch 017 | train_loss=0.4683 acc=0.7742 | val_loss=0.4062 acc=0.8005 | prec=0.7463 rec=0.9091 f1=0.8197 | time=23.6s\n",
            "Epoch 018 | train_loss=0.4900 acc=0.7510 | val_loss=0.6725 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=23.8s\n",
            "Epoch 019 | train_loss=0.5577 acc=0.7067 | val_loss=1.0336 acc=0.5079 | prec=1.0000 rec=0.0136 f1=0.0269 | time=23.8s\n",
            "Epoch 020 | train_loss=0.7145 acc=0.5740 | val_loss=0.5871 acc=0.7642 | prec=0.9462 rec=0.5591 f1=0.7029 | time=23.7s\n",
            "Epoch 021 | train_loss=0.5393 acc=0.7283 | val_loss=0.3891 acc=0.8685 | prec=0.8894 rec=0.8409 f1=0.8645 | time=23.7s\n",
            "Epoch 022 | train_loss=0.4913 acc=0.7629 | val_loss=0.4821 acc=0.7506 | prec=0.6858 rec=0.9227 f1=0.7868 | time=23.8s\n",
            "Epoch 023 | train_loss=0.4935 acc=0.7516 | val_loss=0.4012 acc=0.8277 | prec=0.7975 rec=0.8773 f1=0.8355 | time=23.8s\n",
            "Epoch 024 | train_loss=0.4574 acc=0.7850 | val_loss=0.4974 acc=0.6916 | prec=0.6235 rec=0.9636 f1=0.7571 | time=23.7s\n",
            "Epoch 025 | train_loss=0.4724 acc=0.7385 | val_loss=0.6156 acc=0.6077 | prec=0.5604 rec=0.9909 f1=0.7159 | time=23.9s\n",
            "Epoch 026 | train_loss=0.4893 acc=0.7635 | val_loss=1.0266 acc=0.5737 | prec=0.5392 rec=1.0000 f1=0.7006 | time=24.2s\n",
            "Epoch 027 | train_loss=0.5443 acc=0.6954 | val_loss=0.8924 acc=0.5283 | prec=0.5143 rec=0.9818 f1=0.6750 | time=24.2s\n",
            "Epoch 028 | train_loss=0.4548 acc=0.7811 | val_loss=0.3706 acc=0.8435 | prec=0.8268 rec=0.8682 f1=0.8470 | time=24.3s\n",
            "Epoch 029 | train_loss=0.4224 acc=0.7856 | val_loss=1.8761 acc=0.5442 | prec=0.5226 rec=1.0000 f1=0.6864 | time=24.3s\n",
            "Epoch 030 | train_loss=0.4195 acc=0.7816 | val_loss=0.8488 acc=0.7347 | prec=0.6614 rec=0.9591 f1=0.7829 | time=24.3s\n",
            "Epoch 031 | train_loss=0.4117 acc=0.7992 | val_loss=0.4045 acc=0.8141 | prec=0.7695 rec=0.8955 f1=0.8277 | time=24.2s\n",
            "Epoch 032 | train_loss=0.5301 acc=0.7442 | val_loss=0.4426 acc=0.8095 | prec=0.9595 rec=0.6455 f1=0.7717 | time=24.2s\n",
            "Epoch 033 | train_loss=0.4207 acc=0.7884 | val_loss=0.7562 acc=0.6735 | prec=0.6073 rec=0.9773 f1=0.7491 | time=24.2s\n",
            "Epoch 034 | train_loss=0.4098 acc=0.7896 | val_loss=0.5151 acc=0.7710 | prec=0.6964 rec=0.9591 f1=0.8069 | time=24.2s\n",
            "Epoch 035 | train_loss=0.4187 acc=0.7952 | val_loss=0.3575 acc=0.8367 | prec=0.7868 rec=0.9227 f1=0.8494 | time=24.3s\n",
            "Epoch 036 | train_loss=0.4151 acc=0.7782 | val_loss=0.3373 acc=0.8594 | prec=0.8559 rec=0.8636 f1=0.8597 | time=24.3s\n",
            "Epoch 037 | train_loss=0.3655 acc=0.8236 | val_loss=0.3386 acc=0.8571 | prec=0.8285 rec=0.9000 f1=0.8627 | time=24.1s\n",
            "Epoch 038 | train_loss=0.3687 acc=0.8134 | val_loss=0.7176 acc=0.7642 | prec=0.9833 rec=0.5364 f1=0.6941 | time=24.1s\n",
            "Epoch 039 | train_loss=0.3800 acc=0.8060 | val_loss=0.7122 acc=0.6485 | prec=0.5886 rec=0.9818 f1=0.7359 | time=24.2s\n",
            "Epoch 040 | train_loss=0.3708 acc=0.8077 | val_loss=0.3337 acc=0.8526 | prec=0.8163 rec=0.9091 f1=0.8602 | time=24.2s\n",
            "Epoch 041 | train_loss=0.3975 acc=0.7799 | val_loss=0.5399 acc=0.8413 | prec=0.9808 rec=0.6955 f1=0.8138 | time=24.3s\n",
            "Epoch 042 | train_loss=0.3813 acc=0.8026 | val_loss=0.6642 acc=0.6735 | prec=0.6080 rec=0.9727 f1=0.7483 | time=24.1s\n",
            "Epoch 043 | train_loss=0.3249 acc=0.8298 | val_loss=0.3334 acc=0.8730 | prec=0.9316 rec=0.8045 f1=0.8634 | time=24.2s\n",
            "Epoch 044 | train_loss=0.3280 acc=0.8242 | val_loss=0.3683 acc=0.8526 | prec=0.9641 rec=0.7318 f1=0.8320 | time=24.2s\n",
            "Epoch 045 | train_loss=0.3411 acc=0.8304 | val_loss=0.4204 acc=0.8345 | prec=0.7860 rec=0.9182 f1=0.8470 | time=24.2s\n",
            "Epoch 046 | train_loss=0.3168 acc=0.8304 | val_loss=0.3311 acc=0.8571 | prec=0.8945 rec=0.8091 f1=0.8496 | time=24.1s\n",
            "Epoch 047 | train_loss=0.3885 acc=0.8140 | val_loss=0.5135 acc=0.7120 | prec=0.6388 rec=0.9727 f1=0.7712 | time=24.1s\n",
            "Epoch 048 | train_loss=0.3777 acc=0.8191 | val_loss=0.4057 acc=0.8231 | prec=0.7689 rec=0.9227 f1=0.8388 | time=23.9s\n",
            "Epoch 049 | train_loss=0.3331 acc=0.8264 | val_loss=0.3526 acc=0.8639 | prec=0.8670 rec=0.8591 f1=0.8630 | time=23.9s\n",
            "Epoch 050 | train_loss=0.3206 acc=0.8361 | val_loss=0.3678 acc=0.8322 | prec=0.7920 rec=0.9000 f1=0.8426 | time=24.1s\n",
            "Epoch 051 | train_loss=0.3193 acc=0.8310 | val_loss=0.3792 acc=0.8526 | prec=0.9330 rec=0.7591 f1=0.8371 | time=24.1s\n",
            "Epoch 052 | train_loss=0.2922 acc=0.8474 | val_loss=0.3526 acc=0.8617 | prec=0.8565 rec=0.8682 f1=0.8623 | time=24.2s\n",
            "Epoch 053 | train_loss=0.3058 acc=0.8304 | val_loss=0.5270 acc=0.7710 | prec=0.9685 rec=0.5591 f1=0.7089 | time=24.2s\n",
            "Epoch 054 | train_loss=0.3025 acc=0.8434 | val_loss=0.4420 acc=0.8526 | prec=0.8818 rec=0.8136 f1=0.8463 | time=24.1s\n",
            "Epoch 055 | train_loss=0.2800 acc=0.8508 | val_loss=0.4113 acc=0.7937 | prec=0.7416 rec=0.9000 f1=0.8131 | time=24.1s\n",
            "Epoch 056 | train_loss=0.2830 acc=0.8383 | val_loss=0.4620 acc=0.7800 | prec=0.7269 rec=0.8955 f1=0.8024 | time=24.1s\n",
            "Epoch 057 | train_loss=0.2638 acc=0.8599 | val_loss=0.4878 acc=0.8141 | prec=0.9662 rec=0.6500 f1=0.7772 | time=23.9s\n",
            "Epoch 058 | train_loss=0.2474 acc=0.8588 | val_loss=0.4547 acc=0.7982 | prec=0.7529 rec=0.8864 f1=0.8142 | time=24.1s\n",
            "Epoch 059 | train_loss=0.2618 acc=0.8593 | val_loss=0.3772 acc=0.8367 | prec=0.8426 rec=0.8273 f1=0.8349 | time=24.2s\n",
            "Epoch 060 | train_loss=0.2794 acc=0.8474 | val_loss=0.4639 acc=0.8503 | prec=0.9278 rec=0.7591 f1=0.8350 | time=24.1s\n",
            "Epoch 061 | train_loss=0.2531 acc=0.8656 | val_loss=0.4849 acc=0.7800 | prec=0.7253 rec=0.9000 f1=0.8032 | time=24.2s\n",
            "Early stopping at epoch 61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▆▇▇██▇█▆█▇▇▇▇▆█▇█▇███▇▇████▇██▇██▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▅▁▅▅▅▆▇▇▆▆▅█▇▅▅▇▆▆█▅▆▇▇█▅▇██▇▆▆▆█▆▇▆</td></tr><tr><td>recall</td><td>▁▁▁▁▁▅▇▇██▇▇▇██▇█▇█▇▇▅█▇▆▇▆▇▇█▇▇▆▇▅▇▇▆▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▃▄▅▅▆▆▃▅▆▆▅▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████</td></tr><tr><td>train_loss</td><td>████▇▇██▇▆▄▄▅▇▅▄▄▄▄▅▃▅▃▃▃▃▃▃▂▂▂▃▃▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▆▆▇▇▁▆█▇▃▂▇▅▇▄▆▇█▆█▇███▇█▇██▆▇▇▇▆</td></tr><tr><td>validation_loss</td><td>▅▅▅▅▅▅▅▄▃▂▃▂▄▄▂▂▃▄█▇▂▂▅▁▁▅▁▄▁▁▁▃▂▁▁▂▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.80325</td></tr><tr><td>precision</td><td>0.72527</td></tr><tr><td>recall</td><td>0.9</td></tr><tr><td>train_accuracy</td><td>0.86557</td></tr><tr><td>train_loss</td><td>0.25308</td></tr><tr><td>validation_accuracy</td><td>0.78005</td></tr><tr><td>validation_loss</td><td>0.48493</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial13</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/zscel5vc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/zscel5vc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_012807-zscel5vc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 01:52:29,160] Trial 13 finished with values: [0.4849250039884022, 0.780045351473923] and parameters: {'lr': 0.0013285463239978575, 'wd': 3.6601105798717357e-06, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 14: lr=1.79e-03, wd=7.78e-06, filters = 120\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_015229-vm4mh6hq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/vm4mh6hq' target=\"_blank\">trial14</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/vm4mh6hq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/vm4mh6hq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7676 acc=0.4923 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7506 acc=0.4906 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7540 acc=0.4889 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7529 acc=0.4946 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7271 acc=0.5077 | val_loss=0.6924 acc=0.5488 | prec=0.5402 rec=0.6409 f1=0.5863 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7308 acc=0.5026 | val_loss=0.6933 acc=0.5011 | prec=0.5000 rec=0.0091 f1=0.0179 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7387 acc=0.4821 | val_loss=0.6912 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7241 acc=0.4878 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 009 | train_loss=0.7074 acc=0.5026 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7021 acc=0.4969 | val_loss=0.6819 acc=0.6961 | prec=0.9300 rec=0.4227 f1=0.5813 | time=12.8s\n",
            "Epoch 011 | train_loss=0.6817 acc=0.5553 | val_loss=0.5655 acc=0.7687 | prec=0.7379 rec=0.8318 f1=0.7821 | time=12.8s\n",
            "Epoch 012 | train_loss=0.6134 acc=0.6659 | val_loss=0.5802 acc=0.7052 | prec=0.9412 rec=0.4364 f1=0.5963 | time=12.9s\n",
            "Epoch 013 | train_loss=0.5546 acc=0.7067 | val_loss=0.7011 acc=0.5873 | prec=0.9524 rec=0.1818 f1=0.3053 | time=12.8s\n",
            "Epoch 014 | train_loss=0.5318 acc=0.7192 | val_loss=0.5044 acc=0.7959 | prec=0.9710 rec=0.6091 f1=0.7486 | time=12.8s\n",
            "Epoch 015 | train_loss=0.5114 acc=0.7283 | val_loss=0.4967 acc=0.7483 | prec=0.6719 rec=0.9682 f1=0.7933 | time=12.8s\n",
            "Epoch 016 | train_loss=0.4632 acc=0.7663 | val_loss=0.4850 acc=0.7302 | prec=0.6656 rec=0.9227 f1=0.7733 | time=12.8s\n",
            "Epoch 017 | train_loss=0.4443 acc=0.7879 | val_loss=0.4030 acc=0.8367 | prec=0.9302 rec=0.7273 f1=0.8163 | time=12.8s\n",
            "Epoch 018 | train_loss=0.4664 acc=0.7606 | val_loss=0.4617 acc=0.7438 | prec=0.6720 rec=0.9500 f1=0.7872 | time=12.8s\n",
            "Epoch 019 | train_loss=0.4361 acc=0.7896 | val_loss=0.3844 acc=0.8345 | prec=0.8585 rec=0.8000 f1=0.8282 | time=12.7s\n",
            "Epoch 020 | train_loss=0.4448 acc=0.8020 | val_loss=1.0091 acc=0.5646 | prec=0.9667 rec=0.1318 f1=0.2320 | time=12.8s\n",
            "Epoch 021 | train_loss=0.7593 acc=0.5201 | val_loss=0.6971 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7021 acc=0.5513 | val_loss=0.6987 acc=0.4989 | prec=0.4989 rec=1.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 023 | train_loss=0.6870 acc=0.5701 | val_loss=0.6888 acc=0.5011 | prec=0.0000 rec=0.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 024 | train_loss=0.6569 acc=0.6012 | val_loss=0.5842 acc=0.8005 | prec=0.8750 rec=0.7000 f1=0.7778 | time=12.8s\n",
            "Epoch 025 | train_loss=0.5374 acc=0.7062 | val_loss=0.4131 acc=0.8503 | prec=0.8812 rec=0.8091 f1=0.8436 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4808 acc=0.7538 | val_loss=0.6739 acc=0.7710 | prec=0.9542 rec=0.5682 f1=0.7123 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4319 acc=0.7879 | val_loss=0.6689 acc=0.7098 | prec=0.6394 rec=0.9591 f1=0.7673 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4979 acc=0.7459 | val_loss=0.4673 acc=0.8005 | prec=0.8143 rec=0.7773 f1=0.7953 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4441 acc=0.7873 | val_loss=0.4818 acc=0.8390 | prec=0.8901 rec=0.7727 f1=0.8273 | time=13.0s\n",
            "Epoch 030 | train_loss=0.4112 acc=0.7907 | val_loss=0.6688 acc=0.7868 | prec=0.7405 rec=0.8818 f1=0.8050 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3593 acc=0.8140 | val_loss=0.7905 acc=0.6712 | prec=0.6074 rec=0.9636 f1=0.7452 | time=13.0s\n",
            "Epoch 032 | train_loss=0.3534 acc=0.8395 | val_loss=0.7807 acc=0.7370 | prec=0.6667 rec=0.9455 f1=0.7820 | time=13.0s\n",
            "Epoch 033 | train_loss=0.3727 acc=0.8009 | val_loss=0.5566 acc=0.7664 | prec=0.7112 rec=0.8955 f1=0.7928 | time=13.0s\n",
            "Epoch 034 | train_loss=0.3755 acc=0.8412 | val_loss=0.7342 acc=0.8141 | prec=0.8108 rec=0.8182 f1=0.8145 | time=13.0s\n",
            "Early stopping at epoch 34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▇▆▁▁▁▁▆▇▆▄▇█▇███▃▇▇▁▇█▇▇███▇▇██</td></tr><tr><td>precision</td><td>▁▁▁▅▅▅▁▁▁█▆███▆▆█▆▇█▅▅▁▇▇█▆▇▇▆▅▆▆▇</td></tr><tr><td>recall</td><td>▁▁▁█▅▁▁▁▁▄▇▄▂▅█▇▆█▇▂██▁▆▇▅█▆▆▇██▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▅▅▆▆▇▇▆▇▇▂▂▃▃▅▆▇▆▇▇▇█▇█</td></tr><tr><td>train_loss</td><td>████▇▇█▇▇▇▇▅▄▄▄▃▃▃▂▃█▇▇▆▄▃▂▃▃▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▂▁▁▁▁▅▆▅▃▇▆▆█▆█▂▁▁▁▇█▆▅▇█▇▄▆▆▇</td></tr><tr><td>validation_loss</td><td>▄▄▄▄▄▄▄▄▄▄▃▃▅▂▂▂▁▂▁█▅▅▄▃▁▄▄▂▂▄▆▅▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81448</td></tr><tr><td>precision</td><td>0.81081</td></tr><tr><td>recall</td><td>0.81818</td></tr><tr><td>train_accuracy</td><td>0.84118</td></tr><tr><td>train_loss</td><td>0.37553</td></tr><tr><td>validation_accuracy</td><td>0.81406</td></tr><tr><td>validation_loss</td><td>0.73421</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial14</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/vm4mh6hq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4/runs/vm4mh6hq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_015229-vm4mh6hq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 01:59:47,121] Trial 14 finished with values: [0.7342137077025005, 0.8140589569160998] and parameters: {'lr': 0.0017901670261317257, 'wd': 7.775103911911879e-06, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pareto-optimal trials (val_loss, val_acc) and their params:\n",
            " Trial #8: values=[0.36828610407454626, 0.8503401360544217]\n",
            "              params={'lr': 0.0001655741045417959, 'wd': 0.004116746760159083, 'num_filters': 180}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous Results\n",
        "\n",
        "#### [Block = 1, Head = 3] 5 Fold Cross Validation (Results)\n",
        "1] Trial 0: lr=3.95e-05, wd=4.65e-06, pct_start=0.26\n",
        "* Epoch 084 | train_loss=0.4005 acc=0.8100 | val_loss=0.3896 acc=0.8277 | prec=0.8673 rec=0.7727 f1=0.8173 | time=12.8s\n",
        "Early stopping at epoch 84\n",
        "\n",
        "#### Block = 1, Head = 3 (Best Trial)\n",
        "1] Trial 7: lr=7.13e-05, wd=1.30e-05, pct_start=0.23\n",
        "- Epoch 088 | train_loss=0.2949 acc=0.8525 | val_loss=0.3569 acc=0.8458 | prec=0.8220 rec=0.8818 f1=0.8509 | time=12.9s\n",
        "Early stopping at epoch 88\n",
        "\n",
        "2] Trial 8: lr=3.44e-05, wd=3.04e-06, pct_start=0.14\n",
        "- Epoch 089 | train_loss=0.4120 acc=0.8100 | val_loss=0.4381 acc=0.8186 | prec=0.7734 rec=0.9000 f1=0.8319 | time=13.0s\n",
        "Early stopping at epoch 89\n",
        "\n",
        "\n",
        "#### Filter Change Result\n",
        "1] Trial 0: lr=1.99e-04, wd=1.25e-03, filters = 180\n",
        "* Epoch 060 | train_loss=0.2256 acc=0.8866 | val_loss=0.4457 acc=0.8299 | prec=0.8139 rec=0.8545 f1=0.8337 | time=23.7s\n",
        "Early stopping at epoch 60\n",
        "\n",
        "2] Trial 3: lr=7.29e-04, wd=1.88e-06, filters = 120\n",
        "* Epoch 036 | train_loss=0.2683 acc=0.8633 | val_loss=0.5991 acc=0.7506 | prec=0.6884 rec=0.9136 f1=0.7852 | time=12.8s\n",
        "Early stopping at epoch 36\n",
        "\n",
        "3] Trial 4: lr=3.90e-05, wd=1.17e-04, filters = 150\n",
        "* Epoch 079 | train_loss=0.3613 acc=0.8361 | val_loss=0.3856 acc=0.8390 | prec=0.9162 rec=0.7455 f1=0.8221 | time=18.1s\n",
        "Early stopping at epoch 79\n",
        "\n",
        "4] Trial 13: lr=1.33e-03, wd=3.66e-06, filters = 180\n",
        "* Epoch 061 | train_loss=0.2531 acc=0.8656 | val_loss=0.4849 acc=0.7800 | prec=0.7253 rec=0.9000 f1=0.8032 | time=24.2s\n",
        "Early stopping at epoch 61"
      ],
      "metadata": {
        "id": "yQVm_3rLYYtp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Final Optuna to decide how many filters to use\n",
        "- Filter Size = 120 or 180\n",
        "- Narrow down the search space\n",
        "- learning rate = [1e-5, 5e-4]\n",
        "- weight decay = [1e-6, 5e-4]\n"
      ],
      "metadata": {
        "id": "iV8ygiznj70o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 100\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr          = trial.suggest_float('lr', 1e-5, 5e-4, log=True)\n",
        "    wd          = trial.suggest_float('wd', 1e-6, 5e-4, log=True)\n",
        "    pct_start   = 0.23  # fixed\n",
        "    num_filters = trial.suggest_categorical('num_filters', [120, 180])\n",
        "\n",
        "    print(f\"Trial {trial.number}: filters={num_filters}, lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold-5',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr': lr, 'wd': wd, 'pct_start': pct_start, 'filters': num_filters},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len = dataset[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=num_filters,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Dynamic Weight Decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr / lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss  += loss.item()\n",
        "\n",
        "                preds   = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss   = vloss / len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds == val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "\n",
        "        # Compute specificity = TN / (TN + FP)\n",
        "        tn, fp, fn, tp = confusion_matrix(val_labels, val_preds).ravel()\n",
        "        specificity     = tn / (tn + fp)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={precision:.4f} rec={recall:.4f} spec={specificity:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize', 'maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ─── Print Pare토-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PmE99YKzj4si",
        "outputId": "95704677-38c7-4043-a877-27625d8b9b25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 04:06:00,799] A new study created in memory with name: no-name-8a8e5e07-4a09-409c-93bb-69da1073f0c0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: filters=180, lr=8.51e-05, wd=1.86e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_040600-kjwmy17m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/kjwmy17m' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/kjwmy17m' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/kjwmy17m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7415 acc=0.5258 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=24.5s\n",
            "Epoch 002 | train_loss=0.7438 acc=0.5065 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 003 | train_loss=0.7333 acc=0.5054 | val_loss=0.6931 acc=0.5034 | prec=0.5263 rec=0.0455 spec=0.9593 f1=0.0837 | time=23.7s\n",
            "Epoch 004 | train_loss=0.7365 acc=0.4827 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.7s\n",
            "Epoch 005 | train_loss=0.7298 acc=0.4844 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 006 | train_loss=0.7119 acc=0.5173 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 007 | train_loss=0.7202 acc=0.4929 | val_loss=0.6931 acc=0.4989 | prec=0.4982 rec=0.6227 spec=0.3756 f1=0.5535 | time=23.6s\n",
            "Epoch 008 | train_loss=0.7342 acc=0.4770 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 009 | train_loss=0.7142 acc=0.5037 | val_loss=0.6930 acc=0.5193 | prec=0.5290 rec=0.3318 spec=0.7059 f1=0.4078 | time=23.5s\n",
            "Epoch 010 | train_loss=0.7260 acc=0.4833 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 011 | train_loss=0.7115 acc=0.4923 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 012 | train_loss=0.7080 acc=0.5026 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 013 | train_loss=0.7093 acc=0.5037 | val_loss=0.6982 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.7s\n",
            "Epoch 014 | train_loss=0.7045 acc=0.5213 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.6s\n",
            "Epoch 015 | train_loss=0.7065 acc=0.5009 | val_loss=0.6820 acc=0.5601 | prec=0.9062 rec=0.1318 spec=0.9864 f1=0.2302 | time=23.5s\n",
            "Epoch 016 | train_loss=0.7013 acc=0.5213 | val_loss=0.6631 acc=0.6667 | prec=0.6123 rec=0.9045 spec=0.4299 f1=0.7303 | time=23.5s\n",
            "Epoch 017 | train_loss=0.6701 acc=0.5978 | val_loss=0.6098 acc=0.7302 | prec=0.6678 rec=0.9136 spec=0.5475 f1=0.7716 | time=23.6s\n",
            "Epoch 018 | train_loss=0.6111 acc=0.6546 | val_loss=0.5391 acc=0.8118 | prec=0.7729 rec=0.8818 spec=0.7421 f1=0.8238 | time=23.6s\n",
            "Epoch 019 | train_loss=0.5985 acc=0.6693 | val_loss=0.5086 acc=0.7959 | prec=0.7390 rec=0.9136 spec=0.6787 f1=0.8171 | time=23.5s\n",
            "Epoch 020 | train_loss=0.5961 acc=0.6869 | val_loss=0.4551 acc=0.8163 | prec=0.8717 rec=0.7409 spec=0.8914 f1=0.8010 | time=23.5s\n",
            "Epoch 021 | train_loss=0.5983 acc=0.6591 | val_loss=0.4981 acc=0.8095 | prec=0.7857 rec=0.8500 spec=0.7692 f1=0.8166 | time=23.6s\n",
            "Epoch 022 | train_loss=0.5480 acc=0.7085 | val_loss=0.4588 acc=0.8118 | prec=0.8624 rec=0.7409 spec=0.8824 f1=0.7971 | time=23.6s\n",
            "Epoch 023 | train_loss=0.5401 acc=0.7266 | val_loss=0.4297 acc=0.8458 | prec=0.8363 rec=0.8591 spec=0.8326 f1=0.8475 | time=23.6s\n",
            "Epoch 024 | train_loss=0.5137 acc=0.7311 | val_loss=0.4339 acc=0.8345 | prec=0.7882 rec=0.9136 spec=0.7557 f1=0.8463 | time=23.4s\n",
            "Epoch 025 | train_loss=0.5008 acc=0.7465 | val_loss=0.4241 acc=0.8458 | prec=0.8040 rec=0.9136 spec=0.7783 f1=0.8553 | time=23.6s\n",
            "Epoch 026 | train_loss=0.5056 acc=0.7431 | val_loss=0.4388 acc=0.8322 | prec=0.8883 rec=0.7591 spec=0.9050 f1=0.8186 | time=23.6s\n",
            "Epoch 027 | train_loss=0.4648 acc=0.7777 | val_loss=0.3794 acc=0.8730 | prec=0.8727 rec=0.8727 spec=0.8733 f1=0.8727 | time=23.6s\n",
            "Epoch 028 | train_loss=0.4882 acc=0.7725 | val_loss=0.4013 acc=0.8481 | prec=0.8923 rec=0.7909 spec=0.9050 f1=0.8386 | time=23.6s\n",
            "Epoch 029 | train_loss=0.4723 acc=0.7799 | val_loss=0.4005 acc=0.8458 | prec=0.9043 rec=0.7727 spec=0.9186 f1=0.8333 | time=23.5s\n",
            "Epoch 030 | train_loss=0.4591 acc=0.8020 | val_loss=0.4654 acc=0.7279 | prec=0.6534 rec=0.9682 spec=0.4887 f1=0.7802 | time=23.5s\n",
            "Epoch 031 | train_loss=0.4414 acc=0.7998 | val_loss=0.4282 acc=0.8367 | prec=0.9205 rec=0.7364 spec=0.9367 f1=0.8182 | time=23.5s\n",
            "Epoch 032 | train_loss=0.4355 acc=0.7930 | val_loss=0.3741 acc=0.8753 | prec=0.8733 rec=0.8773 spec=0.8733 f1=0.8753 | time=23.6s\n",
            "Epoch 033 | train_loss=0.4359 acc=0.7924 | val_loss=0.3919 acc=0.8458 | prec=0.9270 rec=0.7500 spec=0.9412 f1=0.8291 | time=23.7s\n",
            "Epoch 034 | train_loss=0.4137 acc=0.7975 | val_loss=0.4106 acc=0.8209 | prec=0.9273 rec=0.6955 spec=0.9457 f1=0.7948 | time=23.4s\n",
            "Epoch 035 | train_loss=0.3954 acc=0.8134 | val_loss=0.3615 acc=0.8866 | prec=0.8899 rec=0.8818 spec=0.8914 f1=0.8858 | time=23.6s\n",
            "Epoch 036 | train_loss=0.4137 acc=0.8140 | val_loss=0.3686 acc=0.8617 | prec=0.9077 rec=0.8045 spec=0.9186 f1=0.8530 | time=23.5s\n",
            "Epoch 037 | train_loss=0.4250 acc=0.8168 | val_loss=0.4296 acc=0.8005 | prec=0.7230 rec=0.9727 spec=0.6290 f1=0.8295 | time=23.5s\n",
            "Epoch 038 | train_loss=0.3866 acc=0.8106 | val_loss=0.3711 acc=0.8549 | prec=0.8305 rec=0.8909 spec=0.8190 f1=0.8596 | time=23.5s\n",
            "Epoch 039 | train_loss=0.3742 acc=0.8281 | val_loss=0.3698 acc=0.8571 | prec=0.8520 rec=0.8636 spec=0.8507 f1=0.8578 | time=23.6s\n",
            "Epoch 040 | train_loss=0.3647 acc=0.8298 | val_loss=0.3386 acc=0.8866 | prec=0.8728 rec=0.9045 spec=0.8688 f1=0.8884 | time=23.7s\n",
            "Epoch 041 | train_loss=0.3485 acc=0.8372 | val_loss=0.3676 acc=0.8503 | prec=0.8208 rec=0.8955 spec=0.8054 f1=0.8565 | time=23.5s\n",
            "Epoch 042 | train_loss=0.4159 acc=0.7998 | val_loss=0.3624 acc=0.8458 | prec=0.8220 rec=0.8818 spec=0.8100 f1=0.8509 | time=23.5s\n",
            "Epoch 043 | train_loss=0.3477 acc=0.8332 | val_loss=0.3450 acc=0.8481 | prec=0.8255 rec=0.8818 spec=0.8145 f1=0.8527 | time=23.4s\n",
            "Epoch 044 | train_loss=0.3215 acc=0.8480 | val_loss=0.3747 acc=0.8413 | prec=0.9213 rec=0.7455 spec=0.9367 f1=0.8241 | time=23.5s\n",
            "Epoch 045 | train_loss=0.3441 acc=0.8315 | val_loss=0.3416 acc=0.8662 | prec=0.8852 rec=0.8409 spec=0.8914 f1=0.8625 | time=23.5s\n",
            "Epoch 046 | train_loss=0.3409 acc=0.8355 | val_loss=0.3488 acc=0.8730 | prec=0.9059 rec=0.8318 spec=0.9140 f1=0.8673 | time=23.5s\n",
            "Epoch 047 | train_loss=0.3263 acc=0.8520 | val_loss=0.3561 acc=0.8639 | prec=0.8774 rec=0.8455 spec=0.8824 f1=0.8611 | time=23.6s\n",
            "Epoch 048 | train_loss=0.3331 acc=0.8338 | val_loss=0.3682 acc=0.8390 | prec=0.9071 rec=0.7545 spec=0.9231 f1=0.8238 | time=23.5s\n",
            "Epoch 049 | train_loss=0.3268 acc=0.8457 | val_loss=0.3576 acc=0.8345 | prec=0.8101 rec=0.8727 spec=0.7964 f1=0.8403 | time=23.6s\n",
            "Epoch 050 | train_loss=0.3983 acc=0.8208 | val_loss=0.4560 acc=0.7800 | prec=0.9301 rec=0.6045 spec=0.9548 f1=0.7328 | time=23.5s\n",
            "Epoch 051 | train_loss=0.3432 acc=0.8361 | val_loss=0.3709 acc=0.8322 | prec=0.8763 rec=0.7727 spec=0.8914 f1=0.8213 | time=23.5s\n",
            "Epoch 052 | train_loss=0.3405 acc=0.8452 | val_loss=0.3611 acc=0.8435 | prec=0.8326 rec=0.8591 spec=0.8281 f1=0.8456 | time=23.5s\n",
            "Epoch 053 | train_loss=0.3039 acc=0.8593 | val_loss=0.3525 acc=0.8503 | prec=0.8850 rec=0.8045 spec=0.8959 f1=0.8429 | time=23.4s\n",
            "Epoch 054 | train_loss=0.2985 acc=0.8610 | val_loss=0.3560 acc=0.8503 | prec=0.8775 rec=0.8136 spec=0.8869 f1=0.8443 | time=23.6s\n",
            "Epoch 055 | train_loss=0.3038 acc=0.8571 | val_loss=0.3549 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=23.6s\n",
            "Early stopping at epoch 55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▂▁▁▅▁▄▆▁▁▃▇█▇▇███▇██▇██████████████▇██</td></tr><tr><td>precision</td><td>▁▅▁▁▁▁▅▅▁▁▁█▆▆▇█▇█▇▇████▆███▆▇█▇▇▇███▇█▇</td></tr><tr><td>recall</td><td>█▁▁▁▁▁▁▁▂▇▇▇▆▇▆▇▆▇▇▆▆▇▆▇▇▇▇▇▇▇▆▇▇▇▆▅▆▇▇▇</td></tr><tr><td>specificity</td><td>▁█████▆▁████▄▅▆▇▆▇▆▇▇▇▄██▇▅▇▇▇▇█▇▇▇█▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▂▂▂▁▁▁▁▁▁▁▁▂▁▂▃▅▄▅▆▆▆▇▆▇▇▇▇▇▇▇█▇█▇██████</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▆▆▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▂▂▁▁▃▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▅▆▇▇▇▇▇█▇▅▇█▇██▆▇▇▇▇███▇▆▇▇▇</td></tr><tr><td>validation_loss</td><td>██████████▇▆▅▄▃▃▃▃▃▃▂▂▃▂▂▁▂▃▂▂▂▁▁▁▂▃▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83645</td></tr><tr><td>precision</td><td>0.86058</td></tr><tr><td>recall</td><td>0.81364</td></tr><tr><td>specificity</td><td>0.86878</td></tr><tr><td>train_accuracy</td><td>0.85706</td></tr><tr><td>train_loss</td><td>0.30381</td></tr><tr><td>validation_accuracy</td><td>0.84127</td></tr><tr><td>validation_loss</td><td>0.35488</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/kjwmy17m' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/kjwmy17m</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_040600-kjwmy17m/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 04:27:40,247] Trial 0 finished with values: [0.3548754038555281, 0.8412698412698413] and parameters: {'lr': 8.506993179590607e-05, 'wd': 1.8649812851561947e-06, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: filters=180, lr=2.85e-04, wd=3.40e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_042740-sp91qjsf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/sp91qjsf' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/sp91qjsf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/sp91qjsf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7993 acc=0.4923 | val_loss=0.7064 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 002 | train_loss=0.7991 acc=0.4952 | val_loss=0.7059 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 003 | train_loss=0.7709 acc=0.4952 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 004 | train_loss=0.7714 acc=0.5139 | val_loss=0.6931 acc=0.5079 | prec=0.5073 rec=0.4727 spec=0.5430 f1=0.4894 | time=23.6s\n",
            "Epoch 005 | train_loss=0.7652 acc=0.5190 | val_loss=0.6930 acc=0.4966 | prec=0.4977 rec=0.9773 spec=0.0181 f1=0.6595 | time=23.6s\n",
            "Epoch 006 | train_loss=0.7478 acc=0.5088 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 007 | train_loss=0.7543 acc=0.5054 | val_loss=0.6923 acc=0.5034 | prec=0.5556 rec=0.0227 spec=0.9819 f1=0.0437 | time=23.6s\n",
            "Epoch 008 | train_loss=0.7610 acc=0.4918 | val_loss=0.6988 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.7s\n",
            "Epoch 009 | train_loss=0.7540 acc=0.4963 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.4s\n",
            "Epoch 010 | train_loss=0.7303 acc=0.5247 | val_loss=0.6454 acc=0.6599 | prec=0.8431 rec=0.3909 spec=0.9276 f1=0.5342 | time=23.6s\n",
            "Epoch 011 | train_loss=0.6693 acc=0.5893 | val_loss=0.5938 acc=0.6689 | prec=0.6045 rec=0.9727 spec=0.3665 f1=0.7456 | time=23.5s\n",
            "Epoch 012 | train_loss=0.5899 acc=0.6602 | val_loss=0.5312 acc=0.7211 | prec=0.6492 rec=0.9591 spec=0.4842 f1=0.7743 | time=23.5s\n",
            "Epoch 013 | train_loss=0.5574 acc=0.7062 | val_loss=0.4611 acc=0.8141 | prec=0.8165 rec=0.8091 spec=0.8190 f1=0.8128 | time=23.5s\n",
            "Epoch 014 | train_loss=0.5668 acc=0.6886 | val_loss=0.4651 acc=0.8322 | prec=0.8318 rec=0.8318 spec=0.8326 f1=0.8318 | time=23.5s\n",
            "Epoch 015 | train_loss=0.5344 acc=0.7187 | val_loss=0.4736 acc=0.8163 | prec=0.7663 rec=0.9091 spec=0.7240 f1=0.8316 | time=23.6s\n",
            "Epoch 016 | train_loss=0.5692 acc=0.6909 | val_loss=0.4810 acc=0.8186 | prec=0.8571 rec=0.7636 spec=0.8733 f1=0.8077 | time=23.5s\n",
            "Epoch 017 | train_loss=0.5598 acc=0.7107 | val_loss=0.5031 acc=0.8050 | prec=0.8988 rec=0.6864 spec=0.9231 f1=0.7784 | time=23.6s\n",
            "Epoch 018 | train_loss=0.5024 acc=0.7374 | val_loss=0.4576 acc=0.8390 | prec=0.8282 rec=0.8545 spec=0.8235 f1=0.8412 | time=23.5s\n",
            "Epoch 019 | train_loss=0.4748 acc=0.7465 | val_loss=0.4427 acc=0.8186 | prec=0.7893 rec=0.8682 spec=0.7692 f1=0.8268 | time=23.5s\n",
            "Epoch 020 | train_loss=0.4711 acc=0.7652 | val_loss=0.4224 acc=0.8526 | prec=0.8571 rec=0.8455 spec=0.8597 f1=0.8513 | time=23.6s\n",
            "Epoch 021 | train_loss=0.4446 acc=0.7799 | val_loss=0.4568 acc=0.7664 | prec=0.9333 rec=0.5727 spec=0.9593 f1=0.7099 | time=23.5s\n",
            "Epoch 022 | train_loss=0.4380 acc=0.7839 | val_loss=0.4158 acc=0.8345 | prec=0.9249 rec=0.7273 spec=0.9412 f1=0.8142 | time=23.7s\n",
            "Epoch 023 | train_loss=0.4082 acc=0.8111 | val_loss=0.4327 acc=0.8005 | prec=0.7200 rec=0.9818 spec=0.6199 f1=0.8308 | time=23.4s\n",
            "Epoch 024 | train_loss=0.4246 acc=0.8049 | val_loss=0.3850 acc=0.8594 | prec=0.7992 rec=0.9591 spec=0.7602 f1=0.8719 | time=23.5s\n",
            "Epoch 025 | train_loss=0.4835 acc=0.7618 | val_loss=0.4549 acc=0.7914 | prec=0.9211 rec=0.6364 spec=0.9457 f1=0.7527 | time=23.4s\n",
            "Epoch 026 | train_loss=0.4240 acc=0.8049 | val_loss=0.3571 acc=0.8707 | prec=0.9137 rec=0.8182 spec=0.9231 f1=0.8633 | time=23.5s\n",
            "Epoch 027 | train_loss=0.3994 acc=0.8157 | val_loss=0.3885 acc=0.8753 | prec=0.8340 rec=0.9364 spec=0.8145 f1=0.8822 | time=23.5s\n",
            "Epoch 028 | train_loss=0.3762 acc=0.8117 | val_loss=0.3996 acc=0.8707 | prec=0.8826 rec=0.8545 spec=0.8869 f1=0.8684 | time=23.4s\n",
            "Epoch 029 | train_loss=0.3696 acc=0.8236 | val_loss=0.3467 acc=0.8481 | prec=0.8923 rec=0.7909 spec=0.9050 f1=0.8386 | time=23.6s\n",
            "Epoch 030 | train_loss=0.3524 acc=0.8185 | val_loss=0.3575 acc=0.8458 | prec=0.9270 rec=0.7500 spec=0.9412 f1=0.8291 | time=23.6s\n",
            "Epoch 031 | train_loss=0.3964 acc=0.8117 | val_loss=0.4621 acc=0.7732 | prec=0.6923 rec=0.9818 spec=0.5656 f1=0.8120 | time=23.5s\n",
            "Epoch 032 | train_loss=0.3792 acc=0.8140 | val_loss=0.4599 acc=0.7982 | prec=0.7191 rec=0.9773 spec=0.6199 f1=0.8285 | time=23.4s\n",
            "Epoch 033 | train_loss=0.3406 acc=0.8548 | val_loss=0.3682 acc=0.8685 | prec=0.8320 rec=0.9227 spec=0.8145 f1=0.8750 | time=23.4s\n",
            "Epoch 034 | train_loss=0.3547 acc=0.8219 | val_loss=0.3549 acc=0.8322 | prec=0.9148 rec=0.7318 spec=0.9321 f1=0.8131 | time=23.3s\n",
            "Epoch 035 | train_loss=0.3559 acc=0.8537 | val_loss=0.3280 acc=0.8685 | prec=0.8785 rec=0.8545 spec=0.8824 f1=0.8664 | time=23.4s\n",
            "Epoch 036 | train_loss=0.3314 acc=0.8474 | val_loss=0.3956 acc=0.8095 | prec=0.9304 rec=0.6682 spec=0.9502 f1=0.7778 | time=23.7s\n",
            "Epoch 037 | train_loss=0.3206 acc=0.8321 | val_loss=0.4038 acc=0.8413 | prec=0.7778 rec=0.9545 spec=0.7285 f1=0.8571 | time=23.6s\n",
            "Epoch 038 | train_loss=0.3289 acc=0.8293 | val_loss=0.3509 acc=0.8594 | prec=0.8211 rec=0.9182 spec=0.8009 f1=0.8670 | time=23.5s\n",
            "Epoch 039 | train_loss=0.2887 acc=0.8650 | val_loss=0.3421 acc=0.8594 | prec=0.8347 rec=0.8955 spec=0.8235 f1=0.8640 | time=23.4s\n",
            "Epoch 040 | train_loss=0.3134 acc=0.8627 | val_loss=0.4135 acc=0.8345 | prec=0.7692 rec=0.9545 spec=0.7149 f1=0.8519 | time=23.6s\n",
            "Epoch 041 | train_loss=0.4085 acc=0.8037 | val_loss=0.3410 acc=0.8776 | prec=0.9109 rec=0.8364 spec=0.9186 f1=0.8720 | time=23.4s\n",
            "Epoch 042 | train_loss=0.3300 acc=0.8253 | val_loss=0.4438 acc=0.8027 | prec=0.7301 rec=0.9591 spec=0.6471 f1=0.8291 | time=23.5s\n",
            "Epoch 043 | train_loss=0.2951 acc=0.8554 | val_loss=0.3186 acc=0.8662 | prec=0.8546 rec=0.8818 spec=0.8507 f1=0.8680 | time=23.5s\n",
            "Epoch 044 | train_loss=0.2705 acc=0.8599 | val_loss=0.3831 acc=0.8390 | prec=0.7854 rec=0.9318 spec=0.7466 f1=0.8524 | time=23.7s\n",
            "Epoch 045 | train_loss=0.2651 acc=0.8769 | val_loss=0.3905 acc=0.8435 | prec=0.7849 rec=0.9455 spec=0.7421 f1=0.8577 | time=23.5s\n",
            "Epoch 046 | train_loss=0.2608 acc=0.8780 | val_loss=0.3767 acc=0.8435 | prec=0.7961 rec=0.9227 spec=0.7647 f1=0.8547 | time=23.5s\n",
            "Epoch 047 | train_loss=0.2343 acc=0.8837 | val_loss=0.6112 acc=0.7732 | prec=0.6923 rec=0.9818 spec=0.5656 f1=0.8120 | time=23.4s\n",
            "Epoch 048 | train_loss=0.2460 acc=0.8701 | val_loss=0.4005 acc=0.8571 | prec=0.8078 rec=0.9364 spec=0.7783 f1=0.8674 | time=23.6s\n",
            "Epoch 049 | train_loss=0.2374 acc=0.8758 | val_loss=0.3599 acc=0.8458 | prec=0.8193 rec=0.8864 spec=0.8054 f1=0.8515 | time=23.4s\n",
            "Epoch 050 | train_loss=0.2431 acc=0.8769 | val_loss=0.5800 acc=0.7914 | prec=0.7162 rec=0.9636 spec=0.6199 f1=0.8217 | time=23.5s\n",
            "Epoch 051 | train_loss=0.2589 acc=0.8729 | val_loss=0.4570 acc=0.8322 | prec=0.7664 rec=0.9545 spec=0.7104 f1=0.8502 | time=23.7s\n",
            "Epoch 052 | train_loss=0.2306 acc=0.8741 | val_loss=0.3454 acc=0.8730 | prec=0.8565 rec=0.8955 spec=0.8507 f1=0.8756 | time=23.6s\n",
            "Epoch 053 | train_loss=0.2342 acc=0.8712 | val_loss=0.4384 acc=0.8390 | prec=0.7968 rec=0.9091 spec=0.7692 f1=0.8493 | time=23.6s\n",
            "Epoch 054 | train_loss=0.2304 acc=0.8798 | val_loss=0.4116 acc=0.8503 | prec=0.7939 rec=0.9455 spec=0.7557 f1=0.8631 | time=23.5s\n",
            "Epoch 055 | train_loss=0.2278 acc=0.8894 | val_loss=0.3828 acc=0.8571 | prec=0.8127 rec=0.9273 spec=0.7873 f1=0.8662 | time=23.4s\n",
            "Epoch 056 | train_loss=0.2132 acc=0.8939 | val_loss=0.3664 acc=0.8639 | prec=0.8571 rec=0.8727 spec=0.8552 f1=0.8649 | time=23.4s\n",
            "Epoch 057 | train_loss=0.2267 acc=0.8826 | val_loss=0.3553 acc=0.8730 | prec=0.8661 rec=0.8818 spec=0.8643 f1=0.8739 | time=23.6s\n",
            "Epoch 058 | train_loss=0.2221 acc=0.8758 | val_loss=0.3680 acc=0.8707 | prec=0.8688 rec=0.8727 spec=0.8688 f1=0.8707 | time=23.7s\n",
            "Early stopping at epoch 58\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▄▄▄▁▄▄▄▆▆▇▇▆▇▇█▇▇█▆██▇▇▇██▆██▇██████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▂▁▁▇▃▆▆▅█▆█▅▆██▇▇▄▅▆▇▆▇▅▅▆▆▆▄▆▅▅▆▆▇</td></tr><tr><td>recall</td><td>███▂███▁██▆▇▅▄▆▃▅█▄▆▆▆▅██▆▇▇▇▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>▁▁▁▅▁▁█▄▇▆█▇▇▇█▇██▇██▆▇██▇▇▆█▆▇▆▇▇▆▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▃▄▅▄▄▅▅▅▆▆▇▆▆▆▇▇▇▇▇▇▇▆▇███████████</td></tr><tr><td>train_loss</td><td>███▇▇▆▅▅▅▅▄▄▄▄▃▄▃▃▃▃▃▃▂▃▃▂▂▂▃▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▄▅▇▇▇▇▇█▆▇█▆████▇█▇▇█▇▇█▇▇▆█▇▆█▇███</td></tr><tr><td>validation_loss</td><td>███████▇▅▄▄▄▄▃▃▃▂▂▂▂▄▁▂▃▁▁▃▁▂▂▆▂▂▆▃▃▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.87075</td></tr><tr><td>precision</td><td>0.86878</td></tr><tr><td>recall</td><td>0.87273</td></tr><tr><td>specificity</td><td>0.86878</td></tr><tr><td>train_accuracy</td><td>0.87578</td></tr><tr><td>train_loss</td><td>0.22208</td></tr><tr><td>validation_accuracy</td><td>0.87075</td></tr><tr><td>validation_loss</td><td>0.36797</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/sp91qjsf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/sp91qjsf</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_042740-sp91qjsf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 04:50:26,033] Trial 1 finished with values: [0.3679687838469233, 0.8707482993197279] and parameters: {'lr': 0.0002851525315362799, 'wd': 3.3969696548624846e-06, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: filters=120, lr=4.35e-05, wd=8.53e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_045026-e2ftgfb4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/e2ftgfb4' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/e2ftgfb4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/e2ftgfb4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7842 acc=0.5020 | val_loss=0.7364 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7690 acc=0.4884 | val_loss=0.7232 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7500 acc=0.5065 | val_loss=0.7167 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7671 acc=0.5088 | val_loss=0.7099 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7432 acc=0.5082 | val_loss=0.7033 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7489 acc=0.4969 | val_loss=0.7022 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 007 | train_loss=0.7479 acc=0.4850 | val_loss=0.7055 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7484 acc=0.5009 | val_loss=0.7023 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7512 acc=0.5043 | val_loss=0.7020 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 010 | train_loss=0.7375 acc=0.4957 | val_loss=0.7010 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7329 acc=0.5145 | val_loss=0.6983 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.6s\n",
            "Epoch 012 | train_loss=0.7599 acc=0.4702 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7358 acc=0.5088 | val_loss=0.6951 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.4s\n",
            "Epoch 014 | train_loss=0.7226 acc=0.5309 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.6s\n",
            "Epoch 015 | train_loss=0.7330 acc=0.4974 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.6s\n",
            "Epoch 016 | train_loss=0.7314 acc=0.4867 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.5s\n",
            "Epoch 017 | train_loss=0.7417 acc=0.4799 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.5s\n",
            "Epoch 018 | train_loss=0.7310 acc=0.4787 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.6s\n",
            "Epoch 019 | train_loss=0.7210 acc=0.5037 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.7s\n",
            "Epoch 020 | train_loss=0.7317 acc=0.4946 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.4s\n",
            "Epoch 021 | train_loss=0.7307 acc=0.4929 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.6s\n",
            "Epoch 022 | train_loss=0.7160 acc=0.5179 | val_loss=0.6928 acc=0.5193 | prec=0.5952 rec=0.1136 spec=0.9231 f1=0.1908 | time=13.8s\n",
            "Epoch 023 | train_loss=0.7175 acc=0.4861 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.8s\n",
            "Epoch 024 | train_loss=0.7251 acc=0.4946 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=0.9909 spec=0.0090 f1=0.6636 | time=13.9s\n",
            "Epoch 025 | train_loss=0.7175 acc=0.4901 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 026 | train_loss=0.7146 acc=0.5026 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 027 | train_loss=0.7147 acc=0.5026 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 028 | train_loss=0.7194 acc=0.4952 | val_loss=0.6926 acc=0.5374 | prec=0.5235 rec=0.8091 spec=0.2670 f1=0.6357 | time=13.6s\n",
            "Epoch 029 | train_loss=0.7185 acc=0.4906 | val_loss=0.6926 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.3s\n",
            "Epoch 030 | train_loss=0.7144 acc=0.5150 | val_loss=0.6921 acc=0.5714 | prec=0.5779 rec=0.5227 spec=0.6199 f1=0.5489 | time=12.9s\n",
            "Epoch 031 | train_loss=0.7109 acc=0.4957 | val_loss=0.6919 acc=0.5828 | prec=0.6636 rec=0.3318 spec=0.8326 f1=0.4424 | time=12.8s\n",
            "Epoch 032 | train_loss=0.7114 acc=0.5043 | val_loss=0.6916 acc=0.5215 | prec=0.5106 rec=0.9864 spec=0.0588 f1=0.6729 | time=12.8s\n",
            "Epoch 033 | train_loss=0.7095 acc=0.5241 | val_loss=0.6912 acc=0.5760 | prec=0.5552 rec=0.7545 spec=0.3982 f1=0.6397 | time=12.9s\n",
            "Epoch 034 | train_loss=0.7147 acc=0.4918 | val_loss=0.6903 acc=0.5828 | prec=0.5559 rec=0.8136 spec=0.3529 f1=0.6605 | time=12.8s\n",
            "Epoch 035 | train_loss=0.7073 acc=0.4974 | val_loss=0.6889 acc=0.5805 | prec=0.5559 rec=0.7909 spec=0.3710 f1=0.6529 | time=12.8s\n",
            "Epoch 036 | train_loss=0.7028 acc=0.5224 | val_loss=0.6862 acc=0.6032 | prec=0.7586 rec=0.3000 spec=0.9050 f1=0.4300 | time=12.8s\n",
            "Epoch 037 | train_loss=0.7010 acc=0.5281 | val_loss=0.6778 acc=0.6667 | prec=0.7589 rec=0.4864 spec=0.8462 f1=0.5928 | time=12.8s\n",
            "Epoch 038 | train_loss=0.6945 acc=0.5474 | val_loss=0.6508 acc=0.6667 | prec=0.8544 rec=0.4000 spec=0.9321 f1=0.5449 | time=12.9s\n",
            "Epoch 039 | train_loss=0.6766 acc=0.5859 | val_loss=0.6113 acc=0.7551 | prec=0.7800 rec=0.7091 spec=0.8009 f1=0.7429 | time=12.9s\n",
            "Epoch 040 | train_loss=0.6463 acc=0.6251 | val_loss=0.5903 acc=0.7800 | prec=0.8324 rec=0.7000 spec=0.8597 f1=0.7605 | time=12.8s\n",
            "Epoch 041 | train_loss=0.6270 acc=0.6421 | val_loss=0.5636 acc=0.8027 | prec=0.7930 rec=0.8182 spec=0.7873 f1=0.8054 | time=12.8s\n",
            "Epoch 042 | train_loss=0.6251 acc=0.6642 | val_loss=0.5508 acc=0.8005 | prec=0.7946 rec=0.8091 spec=0.7919 f1=0.8018 | time=12.7s\n",
            "Epoch 043 | train_loss=0.5954 acc=0.6778 | val_loss=0.5514 acc=0.8073 | prec=0.7824 rec=0.8500 spec=0.7647 f1=0.8148 | time=13.0s\n",
            "Epoch 044 | train_loss=0.5892 acc=0.6926 | val_loss=0.5199 acc=0.8095 | prec=0.8542 rec=0.7455 spec=0.8733 f1=0.7961 | time=12.9s\n",
            "Epoch 045 | train_loss=0.5713 acc=0.7079 | val_loss=0.5119 acc=0.8118 | prec=0.8374 rec=0.7727 spec=0.8507 f1=0.8038 | time=12.9s\n",
            "Epoch 046 | train_loss=0.5724 acc=0.7022 | val_loss=0.5091 acc=0.8050 | prec=0.8384 rec=0.7545 spec=0.8552 f1=0.7943 | time=12.7s\n",
            "Epoch 047 | train_loss=0.5508 acc=0.7260 | val_loss=0.5099 acc=0.8209 | prec=0.8219 rec=0.8182 spec=0.8235 f1=0.8200 | time=12.8s\n",
            "Epoch 048 | train_loss=0.5609 acc=0.7119 | val_loss=0.5194 acc=0.8095 | prec=0.8148 rec=0.8000 spec=0.8190 f1=0.8073 | time=12.8s\n",
            "Epoch 049 | train_loss=0.5357 acc=0.7368 | val_loss=0.5115 acc=0.8050 | prec=0.8418 rec=0.7500 spec=0.8597 f1=0.7933 | time=12.8s\n",
            "Epoch 050 | train_loss=0.5407 acc=0.7408 | val_loss=0.4969 acc=0.8141 | prec=0.8710 rec=0.7364 spec=0.8914 f1=0.7980 | time=12.8s\n",
            "Epoch 051 | train_loss=0.5286 acc=0.7340 | val_loss=0.4911 acc=0.8254 | prec=0.8593 rec=0.7773 spec=0.8733 f1=0.8162 | time=12.9s\n",
            "Epoch 052 | train_loss=0.5347 acc=0.7277 | val_loss=0.4912 acc=0.8163 | prec=0.8145 rec=0.8182 spec=0.8145 f1=0.8163 | time=12.8s\n",
            "Epoch 053 | train_loss=0.5256 acc=0.7510 | val_loss=0.4748 acc=0.8322 | prec=0.8687 rec=0.7818 spec=0.8824 f1=0.8230 | time=12.9s\n",
            "Epoch 054 | train_loss=0.5198 acc=0.7396 | val_loss=0.4789 acc=0.8254 | prec=0.7849 rec=0.8955 spec=0.7557 f1=0.8365 | time=12.8s\n",
            "Epoch 055 | train_loss=0.5070 acc=0.7618 | val_loss=0.4738 acc=0.8299 | prec=0.8796 rec=0.7636 spec=0.8959 f1=0.8175 | time=12.9s\n",
            "Epoch 056 | train_loss=0.5154 acc=0.7516 | val_loss=0.4766 acc=0.8231 | prec=0.8480 rec=0.7864 spec=0.8597 f1=0.8160 | time=12.8s\n",
            "Epoch 057 | train_loss=0.5045 acc=0.7646 | val_loss=0.4674 acc=0.8299 | prec=0.8796 rec=0.7636 spec=0.8959 f1=0.8175 | time=12.8s\n",
            "Epoch 058 | train_loss=0.4946 acc=0.7686 | val_loss=0.4707 acc=0.8277 | prec=0.8636 rec=0.7773 spec=0.8778 f1=0.8182 | time=12.8s\n",
            "Epoch 059 | train_loss=0.5028 acc=0.7737 | val_loss=0.4615 acc=0.8277 | prec=0.8789 rec=0.7591 spec=0.8959 f1=0.8146 | time=13.0s\n",
            "Epoch 060 | train_loss=0.4896 acc=0.7635 | val_loss=0.4608 acc=0.8277 | prec=0.8600 rec=0.7818 spec=0.8733 f1=0.8190 | time=12.7s\n",
            "Epoch 061 | train_loss=0.4775 acc=0.7748 | val_loss=0.4573 acc=0.8299 | prec=0.8059 rec=0.8682 spec=0.7919 f1=0.8359 | time=12.7s\n",
            "Epoch 062 | train_loss=0.4770 acc=0.7765 | val_loss=0.4460 acc=0.8231 | prec=0.8586 rec=0.7727 spec=0.8733 f1=0.8134 | time=12.9s\n",
            "Epoch 063 | train_loss=0.4783 acc=0.7777 | val_loss=0.4505 acc=0.8413 | prec=0.8261 rec=0.8636 spec=0.8190 f1=0.8444 | time=12.8s\n",
            "Epoch 064 | train_loss=0.4708 acc=0.7714 | val_loss=0.4396 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=12.8s\n",
            "Epoch 065 | train_loss=0.4671 acc=0.7879 | val_loss=0.4615 acc=0.8299 | prec=0.8033 rec=0.8727 spec=0.7873 f1=0.8366 | time=12.7s\n",
            "Epoch 066 | train_loss=0.4754 acc=0.7720 | val_loss=0.4436 acc=0.8141 | prec=0.8710 rec=0.7364 spec=0.8914 f1=0.7980 | time=12.8s\n",
            "Epoch 067 | train_loss=0.4636 acc=0.7811 | val_loss=0.4629 acc=0.8231 | prec=0.7773 rec=0.9045 spec=0.7421 f1=0.8361 | time=12.8s\n",
            "Epoch 068 | train_loss=0.4697 acc=0.7782 | val_loss=0.4370 acc=0.8367 | prec=0.8364 rec=0.8364 spec=0.8371 f1=0.8364 | time=12.8s\n",
            "Epoch 069 | train_loss=0.4614 acc=0.7708 | val_loss=0.4369 acc=0.8322 | prec=0.8411 rec=0.8182 spec=0.8462 f1=0.8295 | time=12.9s\n",
            "Epoch 070 | train_loss=0.4681 acc=0.7828 | val_loss=0.4356 acc=0.8322 | prec=0.8230 rec=0.8455 spec=0.8190 f1=0.8341 | time=12.8s\n",
            "Epoch 071 | train_loss=0.4570 acc=0.7850 | val_loss=0.4471 acc=0.8367 | prec=0.8058 rec=0.8864 spec=0.7873 f1=0.8442 | time=12.8s\n",
            "Epoch 072 | train_loss=0.4571 acc=0.7794 | val_loss=0.4321 acc=0.8299 | prec=0.8756 rec=0.7682 spec=0.8914 f1=0.8184 | time=12.8s\n",
            "Epoch 073 | train_loss=0.4421 acc=0.7896 | val_loss=0.4439 acc=0.8413 | prec=0.8261 rec=0.8636 spec=0.8190 f1=0.8444 | time=12.8s\n",
            "Epoch 074 | train_loss=0.4635 acc=0.7935 | val_loss=0.4424 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=12.8s\n",
            "Epoch 075 | train_loss=0.4485 acc=0.8020 | val_loss=0.4300 acc=0.8413 | prec=0.8151 rec=0.8818 spec=0.8009 f1=0.8472 | time=12.8s\n",
            "Epoch 076 | train_loss=0.4451 acc=0.7986 | val_loss=0.4388 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=12.7s\n",
            "Epoch 077 | train_loss=0.4391 acc=0.7964 | val_loss=0.4393 acc=0.8299 | prec=0.8404 rec=0.8136 spec=0.8462 f1=0.8268 | time=12.8s\n",
            "Epoch 078 | train_loss=0.4348 acc=0.7992 | val_loss=0.4308 acc=0.8322 | prec=0.8202 rec=0.8500 spec=0.8145 f1=0.8348 | time=12.9s\n",
            "Epoch 079 | train_loss=0.4309 acc=0.7986 | val_loss=0.4271 acc=0.8322 | prec=0.8202 rec=0.8500 spec=0.8145 f1=0.8348 | time=12.8s\n",
            "Epoch 080 | train_loss=0.4373 acc=0.8111 | val_loss=0.4270 acc=0.8322 | prec=0.8259 rec=0.8409 spec=0.8235 f1=0.8333 | time=12.9s\n",
            "Epoch 081 | train_loss=0.4322 acc=0.8077 | val_loss=0.4355 acc=0.8345 | prec=0.8050 rec=0.8818 spec=0.7873 f1=0.8416 | time=12.7s\n",
            "Epoch 082 | train_loss=0.4276 acc=0.8066 | val_loss=0.4314 acc=0.8322 | prec=0.8230 rec=0.8455 spec=0.8190 f1=0.8341 | time=12.8s\n",
            "Epoch 083 | train_loss=0.4425 acc=0.8106 | val_loss=0.4348 acc=0.8299 | prec=0.8112 rec=0.8591 spec=0.8009 f1=0.8344 | time=12.8s\n",
            "Epoch 084 | train_loss=0.4373 acc=0.7969 | val_loss=0.4374 acc=0.8322 | prec=0.8380 rec=0.8227 spec=0.8416 f1=0.8303 | time=12.8s\n",
            "Epoch 085 | train_loss=0.4253 acc=0.8083 | val_loss=0.4333 acc=0.8322 | prec=0.8093 rec=0.8682 spec=0.7964 f1=0.8377 | time=12.9s\n",
            "Epoch 086 | train_loss=0.4355 acc=0.7992 | val_loss=0.4308 acc=0.8277 | prec=0.8130 rec=0.8500 spec=0.8054 f1=0.8311 | time=12.9s\n",
            "Epoch 087 | train_loss=0.4315 acc=0.7935 | val_loss=0.4280 acc=0.8367 | prec=0.8274 rec=0.8500 spec=0.8235 f1=0.8386 | time=12.9s\n",
            "Epoch 088 | train_loss=0.4481 acc=0.7958 | val_loss=0.4328 acc=0.8299 | prec=0.8085 rec=0.8636 spec=0.7964 f1=0.8352 | time=12.9s\n",
            "Epoch 089 | train_loss=0.4326 acc=0.8015 | val_loss=0.4313 acc=0.8254 | prec=0.8069 rec=0.8545 spec=0.7964 f1=0.8300 | time=12.8s\n",
            "Epoch 090 | train_loss=0.4365 acc=0.8037 | val_loss=0.4342 acc=0.8345 | prec=0.8296 rec=0.8409 spec=0.8281 f1=0.8352 | time=12.8s\n",
            "Epoch 091 | train_loss=0.4374 acc=0.8049 | val_loss=0.4342 acc=0.8367 | prec=0.8083 rec=0.8818 spec=0.7919 f1=0.8435 | time=12.9s\n",
            "Epoch 092 | train_loss=0.4305 acc=0.8009 | val_loss=0.4250 acc=0.8277 | prec=0.8130 rec=0.8500 spec=0.8054 f1=0.8311 | time=13.0s\n",
            "Epoch 093 | train_loss=0.4300 acc=0.7964 | val_loss=0.4268 acc=0.8231 | prec=0.8060 rec=0.8500 spec=0.7964 f1=0.8274 | time=12.7s\n",
            "Epoch 094 | train_loss=0.4282 acc=0.8140 | val_loss=0.4339 acc=0.8345 | prec=0.8075 rec=0.8773 spec=0.7919 f1=0.8410 | time=12.9s\n",
            "Epoch 095 | train_loss=0.4145 acc=0.8003 | val_loss=0.4329 acc=0.8299 | prec=0.8059 rec=0.8682 spec=0.7919 f1=0.8359 | time=12.9s\n",
            "Epoch 096 | train_loss=0.4299 acc=0.8145 | val_loss=0.4280 acc=0.8254 | prec=0.8095 rec=0.8500 spec=0.8009 f1=0.8293 | time=12.8s\n",
            "Epoch 097 | train_loss=0.4319 acc=0.8128 | val_loss=0.4326 acc=0.8367 | prec=0.8083 rec=0.8818 spec=0.7919 f1=0.8435 | time=12.8s\n",
            "Epoch 098 | train_loss=0.4440 acc=0.8066 | val_loss=0.4505 acc=0.8322 | prec=0.7874 rec=0.9091 spec=0.7557 f1=0.8439 | time=12.9s\n",
            "Epoch 099 | train_loss=0.4288 acc=0.8293 | val_loss=0.4384 acc=0.8231 | prec=0.8034 rec=0.8545 spec=0.7919 f1=0.8282 | time=12.8s\n",
            "Epoch 100 | train_loss=0.4184 acc=0.8191 | val_loss=0.4237 acc=0.8322 | prec=0.8349 rec=0.8273 spec=0.8371 f1=0.8311 | time=12.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▃▆▆▆▆▆▅▇██████████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▆▅▅▆▆▅▇▇████▇███▇██▇▇██▇▇▇█▇█▇▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁████▇█▆▇▄▄▆▇▇▆▆▆▇▆▆▇▆▇▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>██████▁▇▁▃▃▄▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▆▇▆▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▁▂▁▂▂▁▁▂▂▂▁▂▂▃▅▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>█▇███▇▇▇▇▇▇▇▇▇▇▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▃▃▃▆▇▇▇▇▇█▇██████▇██████████</td></tr><tr><td>validation_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▅▅▃▃▂▂▂▁▂▁▂▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83105</td></tr><tr><td>precision</td><td>0.83486</td></tr><tr><td>recall</td><td>0.82727</td></tr><tr><td>specificity</td><td>0.8371</td></tr><tr><td>train_accuracy</td><td>0.81906</td></tr><tr><td>train_loss</td><td>0.41839</td></tr><tr><td>validation_accuracy</td><td>0.8322</td></tr><tr><td>validation_loss</td><td>0.42373</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/e2ftgfb4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/e2ftgfb4</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_045026-e2ftgfb4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 05:12:04,943] Trial 2 finished with values: [0.42373415189129965, 0.8321995464852607] and parameters: {'lr': 4.351102843583648e-05, 'wd': 8.532378835254759e-06, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: filters=120, lr=1.36e-04, wd=6.44e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_051204-fmbiq5fq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/fmbiq5fq' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/fmbiq5fq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/fmbiq5fq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7903 acc=0.4770 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7628 acc=0.4918 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7608 acc=0.4991 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7490 acc=0.5150 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7336 acc=0.5184 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7408 acc=0.5026 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7450 acc=0.4957 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7268 acc=0.5213 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7473 acc=0.4929 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7407 acc=0.4895 | val_loss=0.6924 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 011 | train_loss=0.7329 acc=0.5031 | val_loss=0.6917 acc=0.6689 | prec=0.7256 rec=0.5409 spec=0.7964 f1=0.6198 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7383 acc=0.4997 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 013 | train_loss=0.7265 acc=0.5037 | val_loss=0.6785 acc=0.7075 | prec=0.6770 rec=0.7909 spec=0.6244 f1=0.7296 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7072 acc=0.5315 | val_loss=0.6629 acc=0.7256 | prec=0.6854 rec=0.8318 spec=0.6199 f1=0.7515 | time=12.8s\n",
            "Epoch 015 | train_loss=0.6597 acc=0.5842 | val_loss=0.5316 acc=0.7891 | prec=0.7981 rec=0.7727 spec=0.8054 f1=0.7852 | time=12.9s\n",
            "Epoch 016 | train_loss=0.5933 acc=0.6704 | val_loss=0.5211 acc=0.8163 | prec=0.8009 rec=0.8409 spec=0.7919 f1=0.8204 | time=12.8s\n",
            "Epoch 017 | train_loss=0.5824 acc=0.6801 | val_loss=0.4825 acc=0.8299 | prec=0.8166 rec=0.8500 spec=0.8100 f1=0.8330 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5615 acc=0.7033 | val_loss=0.4248 acc=0.8345 | prec=0.8419 rec=0.8227 spec=0.8462 f1=0.8322 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5270 acc=0.7243 | val_loss=0.5290 acc=0.7075 | prec=0.6342 rec=0.9773 spec=0.4389 f1=0.7692 | time=12.9s\n",
            "Epoch 020 | train_loss=0.5266 acc=0.7198 | val_loss=0.4787 acc=0.7868 | prec=0.7203 rec=0.9364 spec=0.6380 f1=0.8142 | time=13.0s\n",
            "Epoch 021 | train_loss=0.4952 acc=0.7533 | val_loss=0.4515 acc=0.8209 | prec=0.7621 rec=0.9318 spec=0.7104 f1=0.8384 | time=12.9s\n",
            "Epoch 022 | train_loss=0.4821 acc=0.7635 | val_loss=0.4533 acc=0.7755 | prec=0.7010 rec=0.9591 spec=0.5928 f1=0.8100 | time=12.8s\n",
            "Epoch 023 | train_loss=0.4879 acc=0.7493 | val_loss=0.4075 acc=0.8435 | prec=0.8240 rec=0.8727 spec=0.8145 f1=0.8477 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4688 acc=0.7663 | val_loss=0.4011 acc=0.8345 | prec=0.8419 rec=0.8227 spec=0.8462 f1=0.8322 | time=12.8s\n",
            "Epoch 025 | train_loss=0.4469 acc=0.8043 | val_loss=0.4217 acc=0.8254 | prec=0.8994 rec=0.7318 spec=0.9186 f1=0.8070 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4534 acc=0.7964 | val_loss=0.3970 acc=0.8390 | prec=0.8225 rec=0.8636 spec=0.8145 f1=0.8426 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4169 acc=0.7958 | val_loss=0.3955 acc=0.8299 | prec=0.8341 rec=0.8227 spec=0.8371 f1=0.8284 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4242 acc=0.7947 | val_loss=0.3953 acc=0.8186 | prec=0.8763 rec=0.7409 spec=0.8959 f1=0.8030 | time=12.7s\n",
            "Epoch 029 | train_loss=0.4343 acc=0.8094 | val_loss=0.3974 acc=0.8231 | prec=0.8901 rec=0.7364 spec=0.9095 f1=0.8060 | time=12.9s\n",
            "Epoch 030 | train_loss=0.4115 acc=0.8174 | val_loss=0.5112 acc=0.7574 | prec=0.9185 rec=0.5636 spec=0.9502 f1=0.6986 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4132 acc=0.8140 | val_loss=0.4301 acc=0.8050 | prec=0.8988 rec=0.6864 spec=0.9231 f1=0.7784 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4048 acc=0.8077 | val_loss=0.4072 acc=0.8209 | prec=0.9075 rec=0.7136 spec=0.9276 f1=0.7990 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4202 acc=0.8037 | val_loss=0.4007 acc=0.8209 | prec=0.9222 rec=0.7000 spec=0.9412 f1=0.7959 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3675 acc=0.8202 | val_loss=0.3563 acc=0.8526 | prec=0.8016 rec=0.9364 spec=0.7692 f1=0.8637 | time=13.1s\n",
            "Epoch 035 | train_loss=0.3562 acc=0.8423 | val_loss=0.3828 acc=0.8209 | prec=0.9029 rec=0.7182 spec=0.9231 f1=0.8000 | time=13.0s\n",
            "Epoch 036 | train_loss=0.3627 acc=0.8225 | val_loss=0.3728 acc=0.8390 | prec=0.9071 rec=0.7545 spec=0.9231 f1=0.8238 | time=13.0s\n",
            "Epoch 037 | train_loss=0.3666 acc=0.8213 | val_loss=0.4206 acc=0.8118 | prec=0.9363 rec=0.6682 spec=0.9548 f1=0.7798 | time=13.6s\n",
            "Epoch 038 | train_loss=0.3484 acc=0.8361 | val_loss=0.3593 acc=0.8458 | prec=0.8918 rec=0.7864 spec=0.9050 f1=0.8357 | time=13.2s\n",
            "Epoch 039 | train_loss=0.3591 acc=0.8247 | val_loss=0.5116 acc=0.7914 | prec=0.9324 rec=0.6273 spec=0.9548 f1=0.7500 | time=13.1s\n",
            "Epoch 040 | train_loss=0.3798 acc=0.8361 | val_loss=0.3907 acc=0.8163 | prec=0.9017 rec=0.7091 spec=0.9231 f1=0.7939 | time=12.8s\n",
            "Epoch 041 | train_loss=0.3705 acc=0.8259 | val_loss=0.4303 acc=0.8141 | prec=0.9157 rec=0.6909 spec=0.9367 f1=0.7876 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3480 acc=0.8400 | val_loss=0.4110 acc=0.8186 | prec=0.9023 rec=0.7136 spec=0.9231 f1=0.7970 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3389 acc=0.8332 | val_loss=0.3743 acc=0.8481 | prec=0.8806 rec=0.8045 spec=0.8914 f1=0.8409 | time=13.0s\n",
            "Epoch 044 | train_loss=0.3267 acc=0.8349 | val_loss=0.4170 acc=0.8118 | prec=0.9053 rec=0.6955 spec=0.9276 f1=0.7866 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2961 acc=0.8684 | val_loss=0.3763 acc=0.8435 | prec=0.8213 rec=0.8773 spec=0.8100 f1=0.8484 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3015 acc=0.8588 | val_loss=0.4224 acc=0.8209 | prec=0.8983 rec=0.7227 spec=0.9186 f1=0.8010 | time=13.0s\n",
            "Epoch 047 | train_loss=0.3256 acc=0.8406 | val_loss=0.4154 acc=0.8322 | prec=0.9011 rec=0.7455 spec=0.9186 f1=0.8159 | time=12.8s\n",
            "Epoch 048 | train_loss=0.2997 acc=0.8644 | val_loss=0.5660 acc=0.7778 | prec=0.9178 rec=0.6091 spec=0.9457 f1=0.7322 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2764 acc=0.8712 | val_loss=0.4127 acc=0.8050 | prec=0.8418 rec=0.7500 spec=0.8597 f1=0.7933 | time=12.8s\n",
            "Early stopping at epoch 49\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▆▁▆▁▆▇▇▇██▇█████████▇▇▇███▇▇▇▇▇█▇█▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▅▁▅▁▆▆▆▇▇▇▆▆▇▆▇█▇▇█████▇███████▇██▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁█▁█▁▅▇▇▆▇▇███▇▇▇▇▆▆▅▆▆█▆▆▇▅▆▆▆▆▇▆▆▆</td></tr><tr><td>specificity</td><td>█████▁█▁█▇▅▅▇▇▇▄▅▆▅▇▇▇▇▇▇▇▇█▆▇█▇█▇█▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▁▂▁▁▁▁▂▃▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▇▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▄▅▅▇▇█▅▇▇▆█▇██▇▇▇▇█▇██▇▇▇▇▇█▇█▇</td></tr><tr><td>validation_loss</td><td>██████████▇▅▄▄▂▄▃▃▂▂▂▂▂▂▄▂▂▁▂▁▁▄▂▃▂▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.79327</td></tr><tr><td>precision</td><td>0.84184</td></tr><tr><td>recall</td><td>0.75</td></tr><tr><td>specificity</td><td>0.85973</td></tr><tr><td>train_accuracy</td><td>0.87124</td></tr><tr><td>train_loss</td><td>0.27638</td></tr><tr><td>validation_accuracy</td><td>0.80499</td></tr><tr><td>validation_loss</td><td>0.41274</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/fmbiq5fq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/fmbiq5fq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_051204-fmbiq5fq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 05:22:38,718] Trial 3 finished with values: [0.4127419612237385, 0.8049886621315193] and parameters: {'lr': 0.0001356916145577529, 'wd': 6.436832522981879e-05, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: filters=180, lr=7.40e-05, wd=2.30e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_052238-q47qsmue</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/q47qsmue' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/q47qsmue' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/q47qsmue</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7935 acc=0.5043 | val_loss=0.7084 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 002 | train_loss=0.7564 acc=0.5060 | val_loss=0.7000 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 003 | train_loss=0.7738 acc=0.4833 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 004 | train_loss=0.7438 acc=0.5116 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.7s\n",
            "Epoch 005 | train_loss=0.7528 acc=0.4952 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 006 | train_loss=0.7431 acc=0.4844 | val_loss=0.6932 acc=0.4966 | prec=0.4977 rec=0.9727 spec=0.0226 f1=0.6585 | time=23.5s\n",
            "Epoch 007 | train_loss=0.7269 acc=0.4884 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 008 | train_loss=0.7210 acc=0.5048 | val_loss=0.6931 acc=0.5170 | prec=0.5125 rec=0.6500 spec=0.3846 f1=0.5731 | time=23.5s\n",
            "Epoch 009 | train_loss=0.7257 acc=0.5026 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.7s\n",
            "Epoch 010 | train_loss=0.7234 acc=0.5054 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 011 | train_loss=0.7286 acc=0.5082 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 012 | train_loss=0.7216 acc=0.5088 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 013 | train_loss=0.7248 acc=0.4963 | val_loss=0.6951 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 014 | train_loss=0.7183 acc=0.5116 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.4s\n",
            "Epoch 015 | train_loss=0.7354 acc=0.4878 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 016 | train_loss=0.7069 acc=0.5184 | val_loss=0.6892 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=23.5s\n",
            "Epoch 017 | train_loss=0.7160 acc=0.5139 | val_loss=0.6882 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=23.5s\n",
            "Epoch 018 | train_loss=0.7173 acc=0.5003 | val_loss=0.6729 acc=0.5465 | prec=0.9167 rec=0.1000 spec=0.9910 f1=0.1803 | time=23.7s\n",
            "Epoch 019 | train_loss=0.7012 acc=0.5423 | val_loss=0.6443 acc=0.7438 | prec=0.7801 rec=0.6773 spec=0.8100 f1=0.7251 | time=23.7s\n",
            "Epoch 020 | train_loss=0.6792 acc=0.5666 | val_loss=0.6063 acc=0.7755 | prec=0.7788 rec=0.7682 spec=0.7828 f1=0.7735 | time=23.6s\n",
            "Epoch 021 | train_loss=0.6228 acc=0.6194 | val_loss=0.5448 acc=0.7959 | prec=0.7481 rec=0.8909 spec=0.7014 f1=0.8133 | time=23.5s\n",
            "Epoch 022 | train_loss=0.6081 acc=0.6534 | val_loss=0.5353 acc=0.8027 | prec=0.8715 rec=0.7091 spec=0.8959 f1=0.7820 | time=23.5s\n",
            "Epoch 023 | train_loss=0.5769 acc=0.6875 | val_loss=0.5049 acc=0.8367 | prec=0.8364 rec=0.8364 spec=0.8371 f1=0.8364 | time=23.5s\n",
            "Epoch 024 | train_loss=0.5548 acc=0.7221 | val_loss=0.4669 acc=0.8390 | prec=0.8197 rec=0.8682 spec=0.8100 f1=0.8433 | time=23.4s\n",
            "Epoch 025 | train_loss=0.5383 acc=0.7362 | val_loss=0.4635 acc=0.8571 | prec=0.8792 rec=0.8273 spec=0.8869 f1=0.8525 | time=23.6s\n",
            "Epoch 026 | train_loss=0.5281 acc=0.7385 | val_loss=0.4698 acc=0.8571 | prec=0.8756 rec=0.8318 spec=0.8824 f1=0.8531 | time=23.7s\n",
            "Epoch 027 | train_loss=0.5239 acc=0.7436 | val_loss=0.4575 acc=0.8254 | prec=0.7871 rec=0.8909 spec=0.7602 f1=0.8358 | time=23.6s\n",
            "Epoch 028 | train_loss=0.5064 acc=0.7567 | val_loss=0.4590 acc=0.8367 | prec=0.8033 rec=0.8909 spec=0.7828 f1=0.8448 | time=23.5s\n",
            "Epoch 029 | train_loss=0.5066 acc=0.7725 | val_loss=0.4312 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=23.5s\n",
            "Epoch 030 | train_loss=0.5032 acc=0.7470 | val_loss=0.4589 acc=0.8027 | prec=0.7472 rec=0.9136 spec=0.6923 f1=0.8221 | time=23.5s\n",
            "Epoch 031 | train_loss=0.4950 acc=0.7510 | val_loss=0.4238 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=23.4s\n",
            "Epoch 032 | train_loss=0.4823 acc=0.7788 | val_loss=0.4337 acc=0.8413 | prec=0.8261 rec=0.8636 spec=0.8190 f1=0.8444 | time=23.7s\n",
            "Epoch 033 | train_loss=0.4741 acc=0.7901 | val_loss=0.4205 acc=0.8322 | prec=0.8147 rec=0.8591 spec=0.8054 f1=0.8363 | time=23.6s\n",
            "Epoch 034 | train_loss=0.4780 acc=0.7856 | val_loss=0.4439 acc=0.8299 | prec=0.8251 rec=0.8364 spec=0.8235 f1=0.8307 | time=23.5s\n",
            "Epoch 035 | train_loss=0.4379 acc=0.8162 | val_loss=0.4256 acc=0.8413 | prec=0.8074 rec=0.8955 spec=0.7873 f1=0.8491 | time=23.6s\n",
            "Epoch 036 | train_loss=0.4042 acc=0.8270 | val_loss=0.4008 acc=0.8367 | prec=0.8426 rec=0.8273 spec=0.8462 f1=0.8349 | time=23.6s\n",
            "Epoch 037 | train_loss=0.4190 acc=0.8100 | val_loss=0.4192 acc=0.8367 | prec=0.8058 rec=0.8864 spec=0.7873 f1=0.8442 | time=23.4s\n",
            "Epoch 038 | train_loss=0.4147 acc=0.8111 | val_loss=0.3994 acc=0.8413 | prec=0.8125 rec=0.8864 spec=0.7964 f1=0.8478 | time=23.5s\n",
            "Epoch 039 | train_loss=0.3977 acc=0.8355 | val_loss=0.4087 acc=0.8435 | prec=0.8133 rec=0.8909 spec=0.7964 f1=0.8503 | time=23.6s\n",
            "Epoch 040 | train_loss=0.3867 acc=0.8327 | val_loss=0.4024 acc=0.8458 | prec=0.8193 rec=0.8864 spec=0.8054 f1=0.8515 | time=23.6s\n",
            "Epoch 041 | train_loss=0.3821 acc=0.8247 | val_loss=0.3968 acc=0.8481 | prec=0.8148 rec=0.9000 spec=0.7964 f1=0.8553 | time=23.5s\n",
            "Epoch 042 | train_loss=0.3707 acc=0.8372 | val_loss=0.4300 acc=0.8231 | prec=0.7610 rec=0.9409 spec=0.7059 f1=0.8415 | time=23.6s\n",
            "Epoch 043 | train_loss=0.3851 acc=0.8191 | val_loss=0.3796 acc=0.8481 | prec=0.8806 rec=0.8045 spec=0.8914 f1=0.8409 | time=23.4s\n",
            "Epoch 044 | train_loss=0.3679 acc=0.8247 | val_loss=0.3651 acc=0.8639 | prec=0.8960 rec=0.8227 spec=0.9050 f1=0.8578 | time=23.5s\n",
            "Epoch 045 | train_loss=0.3637 acc=0.8344 | val_loss=0.3736 acc=0.8503 | prec=0.8738 rec=0.8182 spec=0.8824 f1=0.8451 | time=23.6s\n",
            "Epoch 046 | train_loss=0.3471 acc=0.8429 | val_loss=0.3546 acc=0.8571 | prec=0.8584 rec=0.8545 spec=0.8597 f1=0.8565 | time=23.5s\n",
            "Epoch 047 | train_loss=0.3515 acc=0.8622 | val_loss=0.3530 acc=0.8594 | prec=0.8591 rec=0.8591 spec=0.8597 f1=0.8591 | time=23.6s\n",
            "Epoch 048 | train_loss=0.3610 acc=0.8406 | val_loss=0.3578 acc=0.8594 | prec=0.8527 rec=0.8682 spec=0.8507 f1=0.8604 | time=23.5s\n",
            "Epoch 049 | train_loss=0.3316 acc=0.8571 | val_loss=0.3516 acc=0.8617 | prec=0.8565 rec=0.8682 spec=0.8552 f1=0.8623 | time=23.5s\n",
            "Epoch 050 | train_loss=0.3268 acc=0.8497 | val_loss=0.3561 acc=0.8639 | prec=0.8540 rec=0.8773 spec=0.8507 f1=0.8655 | time=23.4s\n",
            "Epoch 051 | train_loss=0.3228 acc=0.8582 | val_loss=0.3685 acc=0.8639 | prec=0.9082 rec=0.8091 spec=0.9186 f1=0.8558 | time=23.6s\n",
            "Epoch 052 | train_loss=0.3077 acc=0.8531 | val_loss=0.3610 acc=0.8526 | prec=0.8571 rec=0.8455 spec=0.8597 f1=0.8513 | time=23.6s\n",
            "Epoch 053 | train_loss=0.3016 acc=0.8508 | val_loss=0.3519 acc=0.8549 | prec=0.8645 rec=0.8409 spec=0.8688 f1=0.8525 | time=23.6s\n",
            "Epoch 054 | train_loss=0.3042 acc=0.8724 | val_loss=0.3691 acc=0.8435 | prec=0.8326 rec=0.8591 spec=0.8281 f1=0.8456 | time=23.6s\n",
            "Epoch 055 | train_loss=0.3186 acc=0.8457 | val_loss=0.3742 acc=0.8549 | prec=0.8824 rec=0.8182 spec=0.8914 f1=0.8491 | time=23.5s\n",
            "Epoch 056 | train_loss=0.2894 acc=0.8792 | val_loss=0.3770 acc=0.8413 | prec=0.8178 rec=0.8773 spec=0.8054 f1=0.8465 | time=23.5s\n",
            "Epoch 057 | train_loss=0.2888 acc=0.8729 | val_loss=0.4455 acc=0.7959 | prec=0.7338 rec=0.9273 spec=0.6652 f1=0.8193 | time=23.5s\n",
            "Epoch 058 | train_loss=0.3232 acc=0.8537 | val_loss=0.3721 acc=0.8435 | prec=0.8756 rec=0.8000 spec=0.8869 f1=0.8361 | time=23.4s\n",
            "Epoch 059 | train_loss=0.2934 acc=0.8593 | val_loss=0.3629 acc=0.8571 | prec=0.8489 rec=0.8682 spec=0.8462 f1=0.8584 | time=23.5s\n",
            "Epoch 060 | train_loss=0.2748 acc=0.8769 | val_loss=0.3644 acc=0.8503 | prec=0.8348 rec=0.8727 spec=0.8281 f1=0.8533 | time=23.4s\n",
            "Epoch 061 | train_loss=0.2850 acc=0.8798 | val_loss=0.3709 acc=0.8526 | prec=0.8243 rec=0.8955 spec=0.8100 f1=0.8584 | time=23.6s\n",
            "Epoch 062 | train_loss=0.2646 acc=0.8780 | val_loss=0.3746 acc=0.8662 | prec=0.8852 rec=0.8409 spec=0.8914 f1=0.8625 | time=23.6s\n",
            "Epoch 063 | train_loss=0.2713 acc=0.8837 | val_loss=0.3684 acc=0.8549 | prec=0.8391 rec=0.8773 spec=0.8326 f1=0.8578 | time=23.5s\n",
            "Epoch 064 | train_loss=0.2713 acc=0.8673 | val_loss=0.3856 acc=0.8458 | prec=0.8423 rec=0.8500 spec=0.8416 f1=0.8462 | time=23.5s\n",
            "Early stopping at epoch 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▆▆▆▁▆▁▁▁▂▇▇██████████████████████████</td></tr><tr><td>precision</td><td>▄▄▄▄▄▁▄▁▁▁█▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇</td></tr><tr><td>recall</td><td>██████▁▁█▁▁▁▂▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▄▁██████▆▆▇▇▇▆▆▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▆▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▁▁▁▁▁▁▁▂▂▁▂▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇████▇█████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▇▇▇▇▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▇▇▇█▇▇▇▇██▇▇█████████████████</td></tr><tr><td>validation_loss</td><td>███████████▇▇▆▅▄▃▃▃▃▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▃▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84615</td></tr><tr><td>precision</td><td>0.84234</td></tr><tr><td>recall</td><td>0.85</td></tr><tr><td>specificity</td><td>0.84163</td></tr><tr><td>train_accuracy</td><td>0.86727</td></tr><tr><td>train_loss</td><td>0.27131</td></tr><tr><td>validation_accuracy</td><td>0.8458</td></tr><tr><td>validation_loss</td><td>0.38562</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/q47qsmue' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/q47qsmue</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_052238-q47qsmue/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 05:47:46,947] Trial 4 finished with values: [0.3856206993971552, 0.8458049886621315] and parameters: {'lr': 7.402179014143407e-05, 'wd': 2.2979878234396192e-06, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: filters=180, lr=2.45e-04, wd=9.80e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_054746-9j6m8iwx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/9j6m8iwx' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/9j6m8iwx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/9j6m8iwx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7358 acc=0.5099 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 002 | train_loss=0.7193 acc=0.5235 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 003 | train_loss=0.7211 acc=0.5201 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 004 | train_loss=0.7176 acc=0.5020 | val_loss=0.6931 acc=0.4898 | prec=0.4934 rec=0.8545 spec=0.1267 f1=0.6256 | time=23.4s\n",
            "Epoch 005 | train_loss=0.7265 acc=0.5014 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 006 | train_loss=0.7239 acc=0.4895 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 007 | train_loss=0.7251 acc=0.4935 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 008 | train_loss=0.7159 acc=0.5020 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 009 | train_loss=0.7160 acc=0.5128 | val_loss=0.6921 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 010 | train_loss=0.6964 acc=0.5309 | val_loss=0.6927 acc=0.5102 | prec=1.0000 rec=0.0182 spec=1.0000 f1=0.0357 | time=23.5s\n",
            "Epoch 011 | train_loss=0.7101 acc=0.5372 | val_loss=0.6401 acc=0.7551 | prec=0.8146 rec=0.6591 spec=0.8507 f1=0.7286 | time=23.5s\n",
            "Epoch 012 | train_loss=0.6568 acc=0.5876 | val_loss=0.5897 acc=0.7937 | prec=0.7434 rec=0.8955 spec=0.6923 f1=0.8124 | time=23.5s\n",
            "Epoch 013 | train_loss=0.6014 acc=0.6631 | val_loss=0.5110 acc=0.7778 | prec=0.8547 rec=0.6682 spec=0.8869 f1=0.7500 | time=23.7s\n",
            "Epoch 014 | train_loss=0.5750 acc=0.7113 | val_loss=0.4728 acc=0.8209 | prec=0.8770 rec=0.7455 spec=0.8959 f1=0.8059 | time=23.6s\n",
            "Epoch 015 | train_loss=0.5605 acc=0.7102 | val_loss=0.4682 acc=0.8390 | prec=0.8706 rec=0.7955 spec=0.8824 f1=0.8314 | time=23.6s\n",
            "Epoch 016 | train_loss=0.5090 acc=0.7533 | val_loss=0.4895 acc=0.7778 | prec=0.7061 rec=0.9500 spec=0.6063 f1=0.8101 | time=23.4s\n",
            "Epoch 017 | train_loss=0.5006 acc=0.7618 | val_loss=0.4301 acc=0.8118 | prec=0.8870 rec=0.7136 spec=0.9095 f1=0.7909 | time=23.4s\n",
            "Epoch 018 | train_loss=0.4985 acc=0.7493 | val_loss=0.4233 acc=0.8186 | prec=0.9321 rec=0.6864 spec=0.9502 f1=0.7906 | time=23.4s\n",
            "Epoch 019 | train_loss=0.4718 acc=0.7646 | val_loss=0.4605 acc=0.7846 | prec=0.9433 rec=0.6045 spec=0.9638 f1=0.7368 | time=23.6s\n",
            "Epoch 020 | train_loss=0.4283 acc=0.7981 | val_loss=0.4073 acc=0.8617 | prec=0.8597 rec=0.8636 spec=0.8597 f1=0.8617 | time=23.7s\n",
            "Epoch 021 | train_loss=0.4153 acc=0.8054 | val_loss=0.3972 acc=0.8413 | prec=0.9121 rec=0.7545 spec=0.9276 f1=0.8259 | time=23.6s\n",
            "Epoch 022 | train_loss=0.3974 acc=0.8077 | val_loss=0.3629 acc=0.8526 | prec=0.9016 rec=0.7909 spec=0.9140 f1=0.8426 | time=23.6s\n",
            "Epoch 023 | train_loss=0.4251 acc=0.7958 | val_loss=0.4704 acc=0.7823 | prec=0.9429 rec=0.6000 spec=0.9638 f1=0.7333 | time=23.4s\n",
            "Epoch 024 | train_loss=0.4151 acc=0.8015 | val_loss=0.3566 acc=0.8571 | prec=0.8905 rec=0.8136 spec=0.9005 f1=0.8504 | time=23.7s\n",
            "Epoch 025 | train_loss=0.3856 acc=0.8208 | val_loss=0.3439 acc=0.8594 | prec=0.8624 rec=0.8545 spec=0.8643 f1=0.8584 | time=23.5s\n",
            "Epoch 026 | train_loss=0.3636 acc=0.8264 | val_loss=0.3492 acc=0.8481 | prec=0.8341 rec=0.8682 spec=0.8281 f1=0.8508 | time=23.6s\n",
            "Epoch 027 | train_loss=0.3442 acc=0.8412 | val_loss=0.3309 acc=0.8571 | prec=0.8685 rec=0.8409 spec=0.8733 f1=0.8545 | time=23.8s\n",
            "Epoch 028 | train_loss=0.3227 acc=0.8486 | val_loss=0.3919 acc=0.8390 | prec=0.9162 rec=0.7455 spec=0.9321 f1=0.8221 | time=23.5s\n",
            "Epoch 029 | train_loss=0.3238 acc=0.8457 | val_loss=0.3331 acc=0.8730 | prec=0.8661 rec=0.8818 spec=0.8643 f1=0.8739 | time=23.5s\n",
            "Epoch 030 | train_loss=0.3283 acc=0.8400 | val_loss=0.3870 acc=0.8345 | prec=0.7732 rec=0.9455 spec=0.7240 f1=0.8507 | time=23.5s\n",
            "Epoch 031 | train_loss=0.3295 acc=0.8690 | val_loss=0.3433 acc=0.8481 | prec=0.8883 rec=0.7955 spec=0.9005 f1=0.8393 | time=23.5s\n",
            "Epoch 032 | train_loss=0.3318 acc=0.8276 | val_loss=0.4343 acc=0.7891 | prec=0.9150 rec=0.6364 spec=0.9412 f1=0.7507 | time=23.5s\n",
            "Epoch 033 | train_loss=0.2983 acc=0.8480 | val_loss=0.3459 acc=0.8481 | prec=0.8696 rec=0.8182 spec=0.8778 f1=0.8431 | time=23.5s\n",
            "Epoch 034 | train_loss=0.3141 acc=0.8758 | val_loss=0.3418 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=23.6s\n",
            "Epoch 035 | train_loss=0.3886 acc=0.8128 | val_loss=0.3456 acc=0.8617 | prec=0.8383 rec=0.8955 spec=0.8281 f1=0.8659 | time=23.6s\n",
            "Epoch 036 | train_loss=0.3482 acc=0.8338 | val_loss=0.3528 acc=0.8503 | prec=0.8702 rec=0.8227 spec=0.8778 f1=0.8458 | time=23.4s\n",
            "Epoch 037 | train_loss=0.2875 acc=0.8656 | val_loss=0.4143 acc=0.8254 | prec=0.8950 rec=0.7364 spec=0.9140 f1=0.8080 | time=23.5s\n",
            "Epoch 038 | train_loss=0.2768 acc=0.8656 | val_loss=0.3470 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=23.5s\n",
            "Epoch 039 | train_loss=0.2635 acc=0.8605 | val_loss=0.4506 acc=0.8209 | prec=0.9029 rec=0.7182 spec=0.9231 f1=0.8000 | time=23.5s\n",
            "Epoch 040 | train_loss=0.2813 acc=0.8548 | val_loss=0.3468 acc=0.8526 | prec=0.8638 rec=0.8364 spec=0.8688 f1=0.8499 | time=23.5s\n",
            "Epoch 041 | train_loss=0.2545 acc=0.8712 | val_loss=0.3843 acc=0.8458 | prec=0.8725 rec=0.8091 spec=0.8824 f1=0.8396 | time=23.6s\n",
            "Epoch 042 | train_loss=0.2582 acc=0.8712 | val_loss=0.5200 acc=0.8163 | prec=0.9212 rec=0.6909 spec=0.9412 f1=0.7896 | time=23.8s\n",
            "Early stopping at epoch 42\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▆▆▆▆▆▆▁▇▇▇▇█▇▇▇▇██▇████████▇████▇█▇█▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁█▅▄▆▆▆▄▆▇▇▆▇▇▆▆▆▆▇▆▅▆▇▆▆▆▆▇▆▇▆▇</td></tr><tr><td>recall</td><td>███▇█████▁▆▇▆▆▇█▆▆▅▇▇▅▇▇▇▇▆▇█▇▅▇▇▇▇▆▇▆▇▆</td></tr><tr><td>specificity</td><td>▁▁▁▂▁▁▁▁▁█▇▆▇▇▇▅▇██▇▇█▇▇▇▇█▇▆▇█▇▇▇▇▇▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▂▂▁▁▁▁▁▁▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇█▇▇█▇▇█▇▇█████</td></tr><tr><td>train_loss</td><td>█████████▇█▇▆▆▅▅▅▅▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▃▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▇▆▇▇▆▇▇▆██▆████▇█▇█▆█▇██▇▇▇█▇</td></tr><tr><td>validation_loss</td><td>██████████▇▆▄▄▄▄▃▃▃▂▂▄▁▁▁▁▂▁▂▁▃▁▁▁▁▃▁▃▁▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78961</td></tr><tr><td>precision</td><td>0.92121</td></tr><tr><td>recall</td><td>0.69091</td></tr><tr><td>specificity</td><td>0.94118</td></tr><tr><td>train_accuracy</td><td>0.87124</td></tr><tr><td>train_loss</td><td>0.2582</td></tr><tr><td>validation_accuracy</td><td>0.81633</td></tr><tr><td>validation_loss</td><td>0.51995</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/9j6m8iwx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/9j6m8iwx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_054746-9j6m8iwx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 06:04:17,588] Trial 5 finished with values: [0.5199507794209889, 0.8163265306122449] and parameters: {'lr': 0.000245411312727054, 'wd': 9.8011662675323e-05, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: filters=120, lr=1.61e-05, wd=1.59e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_060417-zcpi14mn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/zcpi14mn' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/zcpi14mn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/zcpi14mn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7709 acc=0.5162 | val_loss=0.7164 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 002 | train_loss=0.7891 acc=0.4889 | val_loss=0.7173 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7787 acc=0.4912 | val_loss=0.7001 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7756 acc=0.5184 | val_loss=0.7094 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7830 acc=0.4963 | val_loss=0.7045 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7575 acc=0.5184 | val_loss=0.7050 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7654 acc=0.5065 | val_loss=0.7027 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7734 acc=0.4838 | val_loss=0.6978 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7554 acc=0.5020 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7670 acc=0.5009 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7404 acc=0.5162 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7556 acc=0.4799 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7558 acc=0.4923 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7460 acc=0.5037 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7458 acc=0.5020 | val_loss=0.6933 acc=0.5011 | prec=0.5000 rec=0.7273 spec=0.2760 f1=0.5926 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7673 acc=0.4782 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7526 acc=0.4997 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7513 acc=0.4991 | val_loss=0.6931 acc=0.5057 | prec=0.5023 rec=0.9909 spec=0.0226 f1=0.6667 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7474 acc=0.4776 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 020 | train_loss=0.7332 acc=0.5150 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7502 acc=0.4918 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7440 acc=0.5048 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7453 acc=0.4991 | val_loss=0.6966 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 024 | train_loss=0.7490 acc=0.5043 | val_loss=0.6957 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.6s\n",
            "Epoch 025 | train_loss=0.7359 acc=0.5014 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 026 | train_loss=0.7253 acc=0.5224 | val_loss=0.6983 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 027 | train_loss=0.7384 acc=0.4952 | val_loss=0.6992 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 028 | train_loss=0.7407 acc=0.5133 | val_loss=0.7035 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 029 | train_loss=0.7401 acc=0.4906 | val_loss=0.6970 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 030 | train_loss=0.7269 acc=0.5116 | val_loss=0.7004 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 031 | train_loss=0.7450 acc=0.4918 | val_loss=0.7053 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 032 | train_loss=0.7339 acc=0.4997 | val_loss=0.7020 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 033 | train_loss=0.7294 acc=0.5088 | val_loss=0.7001 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁██▇█▁█▁██████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁████▁█▁██████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁██▆█▁█▁██████████████</td></tr><tr><td>specificity</td><td>████████████▁▁▃▁█▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▇▃▃▇▄▇▆▂▅▅▇▁▃▅▅▁▄▄▁▇▃▅▄▅▅█▄▇▃▆▃▄▆</td></tr><tr><td>train_loss</td><td>▆█▇▇▇▅▅▆▄▆▃▄▄▃▃▆▄▄▃▂▄▃▃▄▂▁▂▃▃▁▃▂▁</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▁▁▃▁▃█▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>██▃▆▄▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▃▃▄▂▃▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.66566</td></tr><tr><td>precision</td><td>0.49887</td></tr><tr><td>recall</td><td>1</td></tr><tr><td>specificity</td><td>0</td></tr><tr><td>train_accuracy</td><td>0.50879</td></tr><tr><td>train_loss</td><td>0.72936</td></tr><tr><td>validation_accuracy</td><td>0.49887</td></tr><tr><td>validation_loss</td><td>0.70006</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/zcpi14mn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/zcpi14mn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_060417-zcpi14mn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 06:11:22,951] Trial 6 finished with values: [0.70005516069276, 0.4988662131519274] and parameters: {'lr': 1.6068856346415636e-05, 'wd': 1.588080060455655e-05, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: filters=120, lr=1.24e-04, wd=1.13e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_061122-etlevb2q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/etlevb2q' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/etlevb2q' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/etlevb2q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7429 acc=0.4748 | val_loss=0.6930 acc=0.5011 | prec=0.5000 rec=0.0727 spec=0.9276 f1=0.1270 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7360 acc=0.4782 | val_loss=0.6928 acc=0.5465 | prec=0.5333 rec=0.7273 spec=0.3665 f1=0.6154 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7278 acc=0.4816 | val_loss=0.6926 acc=0.5215 | prec=0.5276 rec=0.3909 spec=0.6516 f1=0.4491 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7226 acc=0.5031 | val_loss=0.6929 acc=0.5034 | prec=0.5714 rec=0.0182 spec=0.9864 f1=0.0352 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7192 acc=0.5020 | val_loss=0.6925 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7157 acc=0.5088 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7065 acc=0.5037 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7197 acc=0.5077 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7174 acc=0.5173 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7107 acc=0.4935 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7211 acc=0.4782 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7257 acc=0.4804 | val_loss=0.6929 acc=0.5034 | prec=0.5012 rec=0.9864 spec=0.0226 f1=0.6646 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7112 acc=0.5145 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7167 acc=0.4838 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7142 acc=0.4986 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 016 | train_loss=0.7067 acc=0.4991 | val_loss=0.6901 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 017 | train_loss=0.7096 acc=0.5031 | val_loss=0.6756 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 018 | train_loss=0.6832 acc=0.5423 | val_loss=0.6629 acc=0.6372 | prec=0.8750 rec=0.3182 spec=0.9548 f1=0.4667 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6604 acc=0.5825 | val_loss=0.6067 acc=0.7914 | prec=0.8265 rec=0.7364 spec=0.8462 f1=0.7788 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6184 acc=0.6387 | val_loss=0.5500 acc=0.8050 | prec=0.8454 rec=0.7455 spec=0.8643 f1=0.7923 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5893 acc=0.6795 | val_loss=0.5423 acc=0.8095 | prec=0.8696 rec=0.7273 spec=0.8914 f1=0.7921 | time=12.8s\n",
            "Epoch 022 | train_loss=0.5656 acc=0.7153 | val_loss=0.5005 acc=0.8186 | prec=0.7846 rec=0.8773 spec=0.7602 f1=0.8283 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5673 acc=0.7249 | val_loss=0.4539 acc=0.8458 | prec=0.8619 rec=0.8227 spec=0.8688 f1=0.8419 | time=12.8s\n",
            "Epoch 024 | train_loss=0.5311 acc=0.7323 | val_loss=0.4660 acc=0.8435 | prec=0.8416 rec=0.8455 spec=0.8416 f1=0.8435 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5141 acc=0.7465 | val_loss=0.4749 acc=0.7642 | prec=0.9203 rec=0.5773 spec=0.9502 f1=0.7095 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5967 acc=0.6568 | val_loss=0.5108 acc=0.7370 | prec=0.9062 rec=0.5273 spec=0.9457 f1=0.6667 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5237 acc=0.7391 | val_loss=0.4898 acc=0.7460 | prec=0.9286 rec=0.5318 spec=0.9593 f1=0.6763 | time=13.1s\n",
            "Epoch 028 | train_loss=0.5128 acc=0.7306 | val_loss=0.4395 acc=0.8390 | prec=0.8942 rec=0.7682 spec=0.9095 f1=0.8264 | time=13.2s\n",
            "Epoch 029 | train_loss=0.4809 acc=0.7737 | val_loss=0.3947 acc=0.8639 | prec=0.8774 rec=0.8455 spec=0.8824 f1=0.8611 | time=13.4s\n",
            "Epoch 030 | train_loss=0.4780 acc=0.7572 | val_loss=0.4457 acc=0.7868 | prec=0.9257 rec=0.6227 spec=0.9502 f1=0.7446 | time=13.1s\n",
            "Epoch 031 | train_loss=0.4687 acc=0.7805 | val_loss=0.4278 acc=0.8186 | prec=0.9268 rec=0.6909 spec=0.9457 f1=0.7917 | time=13.8s\n",
            "Epoch 032 | train_loss=0.4467 acc=0.7816 | val_loss=0.4014 acc=0.8186 | prec=0.9268 rec=0.6909 spec=0.9457 f1=0.7917 | time=13.7s\n",
            "Epoch 033 | train_loss=0.4573 acc=0.8003 | val_loss=0.3910 acc=0.8277 | prec=0.9235 rec=0.7136 spec=0.9412 f1=0.8051 | time=13.5s\n",
            "Epoch 034 | train_loss=0.4302 acc=0.7856 | val_loss=0.3775 acc=0.8571 | prec=0.9153 rec=0.7864 spec=0.9276 f1=0.8460 | time=12.9s\n",
            "Epoch 035 | train_loss=0.4079 acc=0.8100 | val_loss=0.3941 acc=0.8503 | prec=0.9185 rec=0.7682 spec=0.9321 f1=0.8366 | time=13.0s\n",
            "Epoch 036 | train_loss=0.4058 acc=0.8196 | val_loss=0.3792 acc=0.8549 | prec=0.9062 rec=0.7909 spec=0.9186 f1=0.8447 | time=13.1s\n",
            "Epoch 037 | train_loss=0.4212 acc=0.8009 | val_loss=0.3987 acc=0.8231 | prec=0.9226 rec=0.7045 spec=0.9412 f1=0.7990 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3948 acc=0.8043 | val_loss=0.3605 acc=0.8526 | prec=0.8974 rec=0.7955 spec=0.9095 f1=0.8434 | time=13.1s\n",
            "Epoch 039 | train_loss=0.3833 acc=0.8315 | val_loss=0.3715 acc=0.8413 | prec=0.9261 rec=0.7409 spec=0.9412 f1=0.8232 | time=13.0s\n",
            "Epoch 040 | train_loss=0.3904 acc=0.8174 | val_loss=0.3645 acc=0.8662 | prec=0.9215 rec=0.8000 spec=0.9321 f1=0.8564 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3841 acc=0.8196 | val_loss=0.3662 acc=0.8549 | prec=0.9194 rec=0.7773 spec=0.9321 f1=0.8424 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3457 acc=0.8315 | val_loss=0.3559 acc=0.8662 | prec=0.9305 rec=0.7909 spec=0.9412 f1=0.8550 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3738 acc=0.8247 | val_loss=0.3526 acc=0.8549 | prec=0.8305 rec=0.8909 spec=0.8190 f1=0.8596 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3779 acc=0.8145 | val_loss=0.3923 acc=0.8209 | prec=0.9379 rec=0.6864 spec=0.9548 f1=0.7927 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3662 acc=0.8287 | val_loss=0.3328 acc=0.8639 | prec=0.8448 rec=0.8909 spec=0.8371 f1=0.8673 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3480 acc=0.8446 | val_loss=0.3325 acc=0.8639 | prec=0.8670 rec=0.8591 spec=0.8688 f1=0.8630 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3538 acc=0.8372 | val_loss=0.3938 acc=0.8390 | prec=0.9306 rec=0.7318 spec=0.9457 f1=0.8193 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3622 acc=0.8202 | val_loss=0.3534 acc=0.8526 | prec=0.9058 rec=0.7864 spec=0.9186 f1=0.8418 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3479 acc=0.8361 | val_loss=0.3971 acc=0.8254 | prec=0.9181 rec=0.7136 spec=0.9367 f1=0.8031 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3619 acc=0.8366 | val_loss=0.4973 acc=0.7551 | prec=0.9179 rec=0.5591 spec=0.9502 f1=0.6949 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3467 acc=0.8417 | val_loss=0.3801 acc=0.8345 | prec=0.9153 rec=0.7364 spec=0.9321 f1=0.8161 | time=12.9s\n",
            "Epoch 052 | train_loss=0.3184 acc=0.8559 | val_loss=0.3918 acc=0.8413 | prec=0.9310 rec=0.7364 spec=0.9457 f1=0.8223 | time=12.8s\n",
            "Epoch 053 | train_loss=0.3396 acc=0.8383 | val_loss=0.4003 acc=0.8345 | prec=0.9349 rec=0.7182 spec=0.9502 f1=0.8123 | time=12.8s\n",
            "Epoch 054 | train_loss=0.3455 acc=0.8327 | val_loss=0.3980 acc=0.8367 | prec=0.9253 rec=0.7318 spec=0.9412 f1=0.8173 | time=12.8s\n",
            "Epoch 055 | train_loss=0.3168 acc=0.8417 | val_loss=0.4966 acc=0.7687 | prec=0.9275 rec=0.5818 spec=0.9548 f1=0.7151 | time=12.8s\n",
            "Epoch 056 | train_loss=0.3030 acc=0.8701 | val_loss=0.3470 acc=0.8594 | prec=0.9158 rec=0.7909 spec=0.9276 f1=0.8488 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3190 acc=0.8565 | val_loss=0.3830 acc=0.8390 | prec=0.9306 rec=0.7318 spec=0.9457 f1=0.8193 | time=12.9s\n",
            "Epoch 058 | train_loss=0.3103 acc=0.8576 | val_loss=0.3897 acc=0.8209 | prec=0.9273 rec=0.6955 spec=0.9457 f1=0.7948 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2972 acc=0.8656 | val_loss=0.4221 acc=0.8254 | prec=0.9281 rec=0.7045 spec=0.9457 f1=0.8010 | time=12.8s\n",
            "Epoch 060 | train_loss=0.3249 acc=0.8554 | val_loss=0.3647 acc=0.8413 | prec=0.9167 rec=0.7500 spec=0.9321 f1=0.8250 | time=12.8s\n",
            "Epoch 061 | train_loss=0.2986 acc=0.8667 | val_loss=0.3796 acc=0.8435 | prec=0.9218 rec=0.7500 spec=0.9367 f1=0.8271 | time=12.8s\n",
            "Early stopping at epoch 61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▂▆▅▁▆▁▁▆▆▁▆▆▅▇▇▆▆█▇▇███▇█████▇█████▇█▇▇█</td></tr><tr><td>precision</td><td>▅▅▅▅▅▁▁▅▅▅▅▅▅█▇▇▇▇██████████▇███████████</td></tr><tr><td>recall</td><td>▆▄▁█▁▁███▁█▃▆▆▆▇▅▅▅▆▆▆▇▆▇▇▆▇▇▇▇▇▇▆▆▆▅▇▆▆</td></tr><tr><td>specificity</td><td>▇█▁██▁▁█▁▁█▇▇▆▇█▇▇███▇█▇████▇█▇█▇███▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▁▁▁▂▁▄▅▅▅▆▄▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>train_loss</td><td>████████████▇▆▆▅▅▆▅▄▄▃▄▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▁▁▁▁▁▁▁▁▄▇▇▇█▆▆▆▇▆▇██▇████▇▇█▇▆▇▆█▇▇█</td></tr><tr><td>validation_loss</td><td>█████████████▆▅▄▃▄▄▄▂▃▃▂▂▂▂▂▂▁▁▂▁▂▂▂▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82707</td></tr><tr><td>precision</td><td>0.92179</td></tr><tr><td>recall</td><td>0.75</td></tr><tr><td>specificity</td><td>0.93665</td></tr><tr><td>train_accuracy</td><td>0.8667</td></tr><tr><td>train_loss</td><td>0.29865</td></tr><tr><td>validation_accuracy</td><td>0.84354</td></tr><tr><td>validation_loss</td><td>0.3796</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/etlevb2q' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/etlevb2q</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_061122-etlevb2q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 06:24:32,435] Trial 7 finished with values: [0.37959521796022144, 0.8435374149659864] and parameters: {'lr': 0.00012377993225897986, 'wd': 1.128890528748028e-05, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: filters=180, lr=1.07e-04, wd=4.01e-04, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_062432-lw3jep48</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/lw3jep48' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/lw3jep48' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/lw3jep48</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7888 acc=0.4867 | val_loss=0.7010 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 002 | train_loss=0.7504 acc=0.5031 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 003 | train_loss=0.7409 acc=0.5077 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 004 | train_loss=0.7345 acc=0.5150 | val_loss=0.6972 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.7s\n",
            "Epoch 005 | train_loss=0.7424 acc=0.5048 | val_loss=0.7002 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 006 | train_loss=0.7281 acc=0.5179 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 007 | train_loss=0.7365 acc=0.5156 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.4s\n",
            "Epoch 008 | train_loss=0.7441 acc=0.4912 | val_loss=0.6979 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.4s\n",
            "Epoch 009 | train_loss=0.7428 acc=0.4901 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.4s\n",
            "Epoch 010 | train_loss=0.7281 acc=0.5020 | val_loss=0.6972 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 011 | train_loss=0.7416 acc=0.4793 | val_loss=0.7004 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 012 | train_loss=0.7302 acc=0.5003 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 013 | train_loss=0.7370 acc=0.4912 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 014 | train_loss=0.7325 acc=0.5145 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 015 | train_loss=0.7326 acc=0.5037 | val_loss=0.6922 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.5s\n",
            "Epoch 016 | train_loss=0.7324 acc=0.4895 | val_loss=0.6868 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=23.6s\n",
            "Epoch 017 | train_loss=0.7086 acc=0.5298 | val_loss=0.6649 acc=0.7256 | prec=0.8194 rec=0.5773 spec=0.8733 f1=0.6773 | time=23.7s\n",
            "Epoch 018 | train_loss=0.6697 acc=0.5961 | val_loss=0.5772 acc=0.7551 | prec=0.8333 rec=0.6364 spec=0.8733 f1=0.7216 | time=23.7s\n",
            "Epoch 019 | train_loss=0.6350 acc=0.6551 | val_loss=0.5244 acc=0.7982 | prec=0.8164 rec=0.7682 spec=0.8281 f1=0.7916 | time=23.6s\n",
            "Epoch 020 | train_loss=0.5978 acc=0.6585 | val_loss=0.5177 acc=0.8209 | prec=0.8052 rec=0.8455 spec=0.7964 f1=0.8248 | time=23.7s\n",
            "Epoch 021 | train_loss=0.5721 acc=0.6869 | val_loss=0.4925 acc=0.8322 | prec=0.8067 rec=0.8727 spec=0.7919 f1=0.8384 | time=23.6s\n",
            "Epoch 022 | train_loss=0.5628 acc=0.7136 | val_loss=0.4559 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=23.5s\n",
            "Epoch 023 | train_loss=0.5348 acc=0.7249 | val_loss=0.4876 acc=0.7959 | prec=0.8963 rec=0.6682 spec=0.9231 f1=0.7656 | time=23.6s\n",
            "Epoch 024 | train_loss=0.5179 acc=0.7311 | val_loss=0.4580 acc=0.8413 | prec=0.8000 rec=0.9091 spec=0.7738 f1=0.8511 | time=23.5s\n",
            "Epoch 025 | train_loss=0.5235 acc=0.7345 | val_loss=0.4345 acc=0.8526 | prec=0.8414 rec=0.8682 spec=0.8371 f1=0.8546 | time=23.6s\n",
            "Epoch 026 | train_loss=0.5134 acc=0.7396 | val_loss=0.4514 acc=0.8231 | prec=0.7500 rec=0.9682 spec=0.6787 f1=0.8452 | time=23.6s\n",
            "Epoch 027 | train_loss=0.4972 acc=0.7550 | val_loss=0.4233 acc=0.8231 | prec=0.9277 rec=0.7000 spec=0.9457 f1=0.7979 | time=23.6s\n",
            "Epoch 028 | train_loss=0.5168 acc=0.7362 | val_loss=0.4431 acc=0.8141 | prec=0.7537 rec=0.9318 spec=0.6968 f1=0.8333 | time=23.6s\n",
            "Epoch 029 | train_loss=0.4976 acc=0.7482 | val_loss=0.4580 acc=0.7710 | prec=0.6990 rec=0.9500 spec=0.5928 f1=0.8054 | time=23.4s\n",
            "Epoch 030 | train_loss=0.4518 acc=0.7856 | val_loss=0.3949 acc=0.8503 | prec=0.8291 rec=0.8818 spec=0.8190 f1=0.8546 | time=23.5s\n",
            "Epoch 031 | train_loss=0.4397 acc=0.7901 | val_loss=0.4134 acc=0.8526 | prec=0.8063 rec=0.9273 spec=0.7783 f1=0.8626 | time=23.4s\n",
            "Epoch 032 | train_loss=0.4403 acc=0.7816 | val_loss=0.4078 acc=0.8299 | prec=0.7656 rec=0.9500 spec=0.7104 f1=0.8479 | time=23.5s\n",
            "Epoch 033 | train_loss=0.4229 acc=0.7964 | val_loss=0.3586 acc=0.8662 | prec=0.8744 rec=0.8545 spec=0.8778 f1=0.8644 | time=23.6s\n",
            "Epoch 034 | train_loss=0.4027 acc=0.8128 | val_loss=0.3732 acc=0.8594 | prec=0.8873 rec=0.8227 spec=0.8959 f1=0.8538 | time=23.6s\n",
            "Epoch 035 | train_loss=0.4151 acc=0.8088 | val_loss=0.3570 acc=0.8549 | prec=0.8223 rec=0.9045 spec=0.8054 f1=0.8615 | time=23.6s\n",
            "Epoch 036 | train_loss=0.3837 acc=0.8259 | val_loss=0.3597 acc=0.8571 | prec=0.8340 rec=0.8909 spec=0.8235 f1=0.8615 | time=23.5s\n",
            "Epoch 037 | train_loss=0.4108 acc=0.8071 | val_loss=0.4118 acc=0.8209 | prec=0.7527 rec=0.9545 spec=0.6878 f1=0.8417 | time=23.5s\n",
            "Epoch 038 | train_loss=0.4059 acc=0.7981 | val_loss=0.4823 acc=0.7596 | prec=0.6815 rec=0.9727 spec=0.5475 f1=0.8015 | time=23.5s\n",
            "Epoch 039 | train_loss=0.3649 acc=0.8270 | val_loss=0.4024 acc=0.8231 | prec=0.7572 rec=0.9500 spec=0.6968 f1=0.8427 | time=23.6s\n",
            "Epoch 040 | train_loss=0.3532 acc=0.8366 | val_loss=0.3560 acc=0.8526 | prec=0.8216 rec=0.9000 spec=0.8054 f1=0.8590 | time=23.6s\n",
            "Epoch 041 | train_loss=0.3510 acc=0.8417 | val_loss=0.5038 acc=0.7687 | prec=0.6916 rec=0.9682 spec=0.5701 f1=0.8068 | time=23.6s\n",
            "Epoch 042 | train_loss=0.3591 acc=0.8287 | val_loss=0.3530 acc=0.8639 | prec=0.8252 rec=0.9227 spec=0.8054 f1=0.8712 | time=23.5s\n",
            "Epoch 043 | train_loss=0.3288 acc=0.8565 | val_loss=0.3469 acc=0.8662 | prec=0.9086 rec=0.8136 spec=0.9186 f1=0.8585 | time=23.5s\n",
            "Epoch 044 | train_loss=0.3188 acc=0.8508 | val_loss=0.3309 acc=0.8617 | prec=0.8565 rec=0.8682 spec=0.8552 f1=0.8623 | time=23.5s\n",
            "Epoch 045 | train_loss=0.3314 acc=0.8389 | val_loss=0.3840 acc=0.8458 | prec=0.7901 rec=0.9409 spec=0.7511 f1=0.8589 | time=23.5s\n",
            "Epoch 046 | train_loss=0.3071 acc=0.8605 | val_loss=0.3457 acc=0.8685 | prec=0.8432 rec=0.9045 spec=0.8326 f1=0.8728 | time=23.6s\n",
            "Epoch 047 | train_loss=0.3226 acc=0.8440 | val_loss=0.3582 acc=0.8571 | prec=0.8945 rec=0.8091 spec=0.9050 f1=0.8496 | time=23.6s\n",
            "Epoch 048 | train_loss=0.3098 acc=0.8599 | val_loss=0.3363 acc=0.8639 | prec=0.8509 rec=0.8818 spec=0.8462 f1=0.8661 | time=23.6s\n",
            "Epoch 049 | train_loss=0.2909 acc=0.8576 | val_loss=0.3568 acc=0.8526 | prec=0.8138 rec=0.9136 spec=0.7919 f1=0.8608 | time=23.5s\n",
            "Epoch 050 | train_loss=0.2924 acc=0.8622 | val_loss=0.3320 acc=0.8639 | prec=0.8448 rec=0.8909 spec=0.8371 f1=0.8673 | time=23.7s\n",
            "Epoch 051 | train_loss=0.3088 acc=0.8656 | val_loss=0.4863 acc=0.8050 | prec=0.7359 rec=0.9500 spec=0.6606 f1=0.8294 | time=23.5s\n",
            "Epoch 052 | train_loss=0.3400 acc=0.8332 | val_loss=0.3856 acc=0.8435 | prec=0.7938 rec=0.9273 spec=0.7602 f1=0.8553 | time=23.5s\n",
            "Epoch 053 | train_loss=0.2943 acc=0.8548 | val_loss=0.4087 acc=0.8322 | prec=0.7724 rec=0.9409 spec=0.7240 f1=0.8484 | time=23.4s\n",
            "Epoch 054 | train_loss=0.2765 acc=0.8661 | val_loss=0.3546 acc=0.8549 | prec=0.8362 rec=0.8818 spec=0.8281 f1=0.8584 | time=23.6s\n",
            "Epoch 055 | train_loss=0.2790 acc=0.8741 | val_loss=0.3996 acc=0.8345 | prec=0.7952 rec=0.9000 spec=0.7692 f1=0.8443 | time=23.6s\n",
            "Epoch 056 | train_loss=0.2644 acc=0.8735 | val_loss=0.3774 acc=0.8367 | prec=0.8663 rec=0.7955 spec=0.8778 f1=0.8294 | time=23.5s\n",
            "Epoch 057 | train_loss=0.2860 acc=0.8599 | val_loss=0.3728 acc=0.8594 | prec=0.8435 rec=0.8818 spec=0.8371 f1=0.8622 | time=23.5s\n",
            "Epoch 058 | train_loss=0.2897 acc=0.8576 | val_loss=0.3677 acc=0.8617 | prec=0.8245 rec=0.9182 spec=0.8054 f1=0.8688 | time=23.4s\n",
            "Epoch 059 | train_loss=0.2492 acc=0.8746 | val_loss=0.3780 acc=0.8594 | prec=0.8185 rec=0.9227 spec=0.7964 f1=0.8675 | time=23.3s\n",
            "Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▇▆▄▇▇▆▇███▇▆▇█▆██▇█▇█▇▇██</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▆▆▆▆▇▆▇▅█▅▆▅▇▇▆▅▅▆▄▆▇▆▇▇▅▅▇▆▇▆</td></tr><tr><td>recall</td><td>██████████▁▂▄▆▅▆▆▇▃▇▆▇▇▆▆▇▆▇▇▅▇▆▅▆▇▆▆▅▆▇</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁▁▇▇▇█▇█▆▅▇▆▇█▇▆▆▇▅▇█▇▇▇▇▆▆▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▁▁▁▁▁▃▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███████▇█████</td></tr><tr><td>train_loss</td><td>██████████▇▆▆▅▅▄▄▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▆▇▇▇▇▇█▇▆██▇███▆▇██████▇█▇▇█</td></tr><tr><td>validation_loss</td><td>██████████▇▆▅▅▄▄▃▃▃▂▂▂▂▂▃▂▄▁▁▂▁▁▁▁▄▂▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.86752</td></tr><tr><td>precision</td><td>0.81855</td></tr><tr><td>recall</td><td>0.92273</td></tr><tr><td>specificity</td><td>0.79638</td></tr><tr><td>train_accuracy</td><td>0.87465</td></tr><tr><td>train_loss</td><td>0.24922</td></tr><tr><td>validation_accuracy</td><td>0.85941</td></tr><tr><td>validation_loss</td><td>0.37802</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/lw3jep48' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/lw3jep48</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_062432-lw3jep48/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 06:47:43,435] Trial 8 finished with values: [0.37802345944302423, 0.8594104308390023] and parameters: {'lr': 0.00010729101235675608, 'wd': 0.00040065253291381023, 'num_filters': 180}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: filters=120, lr=9.18e-05, wd=4.24e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250505_064743-2xvq45j1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/2xvq45j1' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/2xvq45j1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/2xvq45j1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8049 acc=0.4946 | val_loss=0.7319 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7874 acc=0.4861 | val_loss=0.7255 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.8022 acc=0.4906 | val_loss=0.7058 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7706 acc=0.5037 | val_loss=0.7023 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7472 acc=0.5048 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7471 acc=0.5105 | val_loss=0.7035 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7463 acc=0.4889 | val_loss=0.7037 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7386 acc=0.5094 | val_loss=0.7059 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 009 | train_loss=0.7400 acc=0.5145 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7226 acc=0.5014 | val_loss=0.7026 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7264 acc=0.5094 | val_loss=0.6997 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7274 acc=0.5048 | val_loss=0.6998 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 013 | train_loss=0.7143 acc=0.5224 | val_loss=0.6996 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7338 acc=0.4980 | val_loss=0.6982 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7193 acc=0.5156 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7246 acc=0.4969 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7165 acc=0.5094 | val_loss=0.6912 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7046 acc=0.5286 | val_loss=0.6885 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6995 acc=0.5581 | val_loss=0.6620 acc=0.5556 | prec=0.9615 rec=0.1136 spec=0.9955 f1=0.2033 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6631 acc=0.5814 | val_loss=0.5997 acc=0.7370 | prec=0.8768 rec=0.5500 spec=0.9231 f1=0.6760 | time=12.8s\n",
            "Epoch 021 | train_loss=0.6370 acc=0.6427 | val_loss=0.5795 acc=0.7800 | prec=0.8596 rec=0.6682 spec=0.8914 f1=0.7519 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5969 acc=0.6778 | val_loss=0.5295 acc=0.7891 | prec=0.8671 rec=0.6818 spec=0.8959 f1=0.7634 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5882 acc=0.6807 | val_loss=0.5161 acc=0.7959 | prec=0.8916 rec=0.6727 spec=0.9186 f1=0.7668 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5680 acc=0.6982 | val_loss=0.4794 acc=0.8299 | prec=0.8571 rec=0.7909 spec=0.8688 f1=0.8227 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5770 acc=0.7164 | val_loss=0.4581 acc=0.8481 | prec=0.8732 rec=0.8136 spec=0.8824 f1=0.8424 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5396 acc=0.7311 | val_loss=0.4557 acc=0.8571 | prec=0.8584 rec=0.8545 spec=0.8597 f1=0.8565 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5347 acc=0.7538 | val_loss=0.4657 acc=0.8231 | prec=0.9034 rec=0.7227 spec=0.9231 f1=0.8030 | time=12.8s\n",
            "Epoch 028 | train_loss=0.5041 acc=0.7533 | val_loss=0.4635 acc=0.8254 | prec=0.9086 rec=0.7227 spec=0.9276 f1=0.8051 | time=12.9s\n",
            "Epoch 029 | train_loss=0.4921 acc=0.7669 | val_loss=0.4348 acc=0.8571 | prec=0.8428 rec=0.8773 spec=0.8371 f1=0.8597 | time=12.9s\n",
            "Epoch 030 | train_loss=0.5142 acc=0.7561 | val_loss=0.4220 acc=0.8549 | prec=0.8900 rec=0.8091 spec=0.9005 f1=0.8476 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4822 acc=0.7601 | val_loss=0.4209 acc=0.8186 | prec=0.9023 rec=0.7136 spec=0.9231 f1=0.7970 | time=13.0s\n",
            "Epoch 032 | train_loss=0.4583 acc=0.7907 | val_loss=0.4339 acc=0.8163 | prec=0.7584 rec=0.9273 spec=0.7059 f1=0.8344 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4762 acc=0.7760 | val_loss=0.3998 acc=0.8390 | prec=0.8282 rec=0.8545 spec=0.8235 f1=0.8412 | time=12.8s\n",
            "Epoch 034 | train_loss=0.4494 acc=0.7952 | val_loss=0.3973 acc=0.8526 | prec=0.8414 rec=0.8682 spec=0.8371 f1=0.8546 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4443 acc=0.7935 | val_loss=0.3946 acc=0.8367 | prec=0.8776 rec=0.7818 spec=0.8914 f1=0.8269 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4298 acc=0.7952 | val_loss=0.3866 acc=0.8435 | prec=0.8082 rec=0.9000 spec=0.7873 f1=0.8516 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3943 acc=0.8168 | val_loss=0.4003 acc=0.8322 | prec=0.8842 rec=0.7636 spec=0.9005 f1=0.8195 | time=12.8s\n",
            "Epoch 038 | train_loss=0.4035 acc=0.7992 | val_loss=0.4450 acc=0.8050 | prec=0.9351 rec=0.6545 spec=0.9548 f1=0.7701 | time=12.8s\n",
            "Epoch 039 | train_loss=0.4044 acc=0.8009 | val_loss=0.3610 acc=0.8481 | prec=0.8696 rec=0.8182 spec=0.8778 f1=0.8431 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3857 acc=0.8134 | val_loss=0.3848 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.7s\n",
            "Epoch 041 | train_loss=0.4104 acc=0.8179 | val_loss=0.3985 acc=0.8209 | prec=0.8852 rec=0.7364 spec=0.9050 f1=0.8040 | time=13.0s\n",
            "Epoch 042 | train_loss=0.3845 acc=0.8298 | val_loss=0.3654 acc=0.8458 | prec=0.8762 rec=0.8045 spec=0.8869 f1=0.8389 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3834 acc=0.8128 | val_loss=0.4031 acc=0.8186 | prec=0.8933 rec=0.7227 spec=0.9140 f1=0.7990 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3705 acc=0.8242 | val_loss=0.3973 acc=0.8209 | prec=0.8939 rec=0.7273 spec=0.9140 f1=0.8020 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3653 acc=0.8372 | val_loss=0.3609 acc=0.8526 | prec=0.8605 rec=0.8409 spec=0.8643 f1=0.8506 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3576 acc=0.8327 | val_loss=0.3695 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3538 acc=0.8349 | val_loss=0.4007 acc=0.8277 | prec=0.8789 rec=0.7591 spec=0.8959 f1=0.8146 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3417 acc=0.8344 | val_loss=0.3726 acc=0.8526 | prec=0.8673 rec=0.8318 spec=0.8733 f1=0.8492 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3277 acc=0.8548 | val_loss=0.4445 acc=0.8118 | prec=0.8870 rec=0.7136 spec=0.9095 f1=0.7909 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3535 acc=0.8259 | val_loss=0.3907 acc=0.8231 | prec=0.8447 rec=0.7909 spec=0.8552 f1=0.8169 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3250 acc=0.8423 | val_loss=0.4501 acc=0.8163 | prec=0.8840 rec=0.7273 spec=0.9050 f1=0.7980 | time=12.9s\n",
            "Epoch 052 | train_loss=0.3286 acc=0.8389 | val_loss=0.3990 acc=0.8231 | prec=0.8447 rec=0.7909 spec=0.8552 f1=0.8169 | time=12.8s\n",
            "Epoch 053 | train_loss=0.3108 acc=0.8554 | val_loss=0.4091 acc=0.8231 | prec=0.8622 rec=0.7682 spec=0.8778 f1=0.8125 | time=12.8s\n",
            "Epoch 054 | train_loss=0.3031 acc=0.8616 | val_loss=0.4060 acc=0.8163 | prec=0.8564 rec=0.7591 spec=0.8733 f1=0.8048 | time=12.9s\n",
            "Epoch 055 | train_loss=0.3467 acc=0.8389 | val_loss=0.3972 acc=0.8299 | prec=0.8571 rec=0.7909 spec=0.8688 f1=0.8227 | time=12.9s\n",
            "Epoch 056 | train_loss=0.3102 acc=0.8576 | val_loss=0.3987 acc=0.8254 | prec=0.8488 rec=0.7909 spec=0.8597 f1=0.8188 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2994 acc=0.8701 | val_loss=0.4030 acc=0.8163 | prec=0.8424 rec=0.7773 spec=0.8552 f1=0.8085 | time=12.8s\n",
            "Epoch 058 | train_loss=0.3045 acc=0.8525 | val_loss=0.4160 acc=0.8277 | prec=0.8750 rec=0.7636 spec=0.8914 f1=0.8155 | time=12.8s\n",
            "Epoch 059 | train_loss=0.3154 acc=0.8644 | val_loss=0.4173 acc=0.8299 | prec=0.8607 rec=0.7864 spec=0.8733 f1=0.8219 | time=12.8s\n",
            "Epoch 060 | train_loss=0.3007 acc=0.8503 | val_loss=0.4189 acc=0.8231 | prec=0.8515 rec=0.7818 spec=0.8643 f1=0.8152 | time=12.8s\n",
            "Early stopping at epoch 60\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▇██████████████████▇██████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁█▇▇▇▇▇▇██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▆▇▇▇█▇██▇█▆▇▇▇▇█▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>█████████████▆▅▆▅▅▅▆▄▆▁▄▄▃▇▅▅▆▆▅▆▆▅▅▅▅▅▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▁▂▁▂▂▃▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇███</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▆▅▅▅▅▄▄▃▃▃▂▂▂▃▂▂▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▆▇▇██▇██▇▇██▇█▇▇██▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>███▇▇▇█▇▇▇▇▇▇▇▆▄▄▃▃▃▃▂▂▂▂▁▁▂▁▂▁▂▁▃▃▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81517</td></tr><tr><td>precision</td><td>0.85149</td></tr><tr><td>recall</td><td>0.78182</td></tr><tr><td>specificity</td><td>0.86425</td></tr><tr><td>train_accuracy</td><td>0.85026</td></tr><tr><td>train_loss</td><td>0.30073</td></tr><tr><td>validation_accuracy</td><td>0.82313</td></tr><tr><td>validation_loss</td><td>0.41894</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/2xvq45j1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5/runs/2xvq45j1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250505_064743-2xvq45j1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-05 07:00:35,693] Trial 9 finished with values: [0.41894412466457914, 0.8231292517006803] and parameters: {'lr': 9.178813016933477e-05, 'wd': 4.241884759163697e-06, 'num_filters': 120}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pareto-optimal trials (val_loss, val_acc) and their params:\n",
            " Trial #0: values=[0.3548754038555281, 0.8412698412698413]\n",
            "              params={'lr': 8.506993179590607e-05, 'wd': 1.8649812851561947e-06, 'num_filters': 180}\n",
            " Trial #1: values=[0.3679687838469233, 0.8707482993197279]\n",
            "              params={'lr': 0.0002851525315362799, 'wd': 3.3969696548624846e-06, 'num_filters': 180}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Overall Results Wrap Up\n",
        "\n",
        "#### [Block = 1, Head = 3, Filter = 120] 5-Fold Cross Validation (Best Trial Result)\n",
        "1] Trial 0: lr=3.95e-05, wd=4.65e-06, pct_start=0.26\n",
        "* Epoch 084 | train_loss=0.4005 acc=0.8100 | val_loss=0.3896 acc=0.8277 | prec=0.8673 rec=0.7727 f1=0.8173 | time=12.8s\n",
        "Early stopping at epoch 84\n",
        "\n",
        "#### [Block = 1, Head = 3, Filter = 120] (Best Trial)\n",
        "1] Trial 7: lr=7.13e-05, wd=1.30e-05, pct_start=0.23\n",
        "- Epoch 088 | train_loss=0.2949 acc=0.8525 | val_loss=0.3569 acc=0.8458 | prec=0.8220 rec=0.8818 f1=0.8509 | time=12.9s\n",
        "Early stopping at epoch 88\n",
        "\n",
        "2] Trial 8: lr=3.44e-05, wd=3.04e-06, pct_start=0.14\n",
        "- Epoch 089 | train_loss=0.4120 acc=0.8100 | val_loss=0.4381 acc=0.8186 | prec=0.7734 rec=0.9000 f1=0.8319 | time=13.0s\n",
        "Early stopping at epoch 89\n",
        "\n",
        "\n",
        "#### Filter Change Result 120 vs 150 vs 180 (Best Trial)\n",
        "1] Trial 0: lr=1.99e-04, wd=1.25e-03, filters = 180\n",
        "* Epoch 060 | train_loss=0.2256 acc=0.8866 | val_loss=0.4457 acc=0.8299 | prec=0.8139 rec=0.8545 f1=0.8337 | time=23.7s\n",
        "Early stopping at epoch 60\n",
        "\n",
        "2] Trial 3: lr=7.29e-04, wd=1.88e-06, filters = 120\n",
        "* Epoch 036 | train_loss=0.2683 acc=0.8633 | val_loss=0.5991 acc=0.7506 | prec=0.6884 rec=0.9136 f1=0.7852 | time=12.8s\n",
        "Early stopping at epoch 36\n",
        "\n",
        "3] Trial 4: lr=3.90e-05, wd=1.17e-04, filters = 150\n",
        "* Epoch 079 | train_loss=0.3613 acc=0.8361 | val_loss=0.3856 acc=0.8390 | prec=0.9162 rec=0.7455 f1=0.8221 | time=18.1s\n",
        "Early stopping at epoch 79\n",
        "\n",
        "4] Trial 13: lr=1.33e-03, wd=3.66e-06, filters = 180\n",
        "* Epoch 061 | train_loss=0.2531 acc=0.8656 | val_loss=0.4849 acc=0.7800 | prec=0.7253 rec=0.9000 f1=0.8032 | time=24.2s\n",
        "Early stopping at epoch 61\n",
        "\n",
        "#### Filter Comparison 120 vs 180 (Best Trial)\n",
        "1] Trial 0: filters=180, lr=8.51e-05, wd=1.86e-06, pct_start=0.23\n",
        "* Epoch 055 | train_loss=0.3038 acc=0.8571 | val_loss=0.3549 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=23.6s\n",
        "Early stopping at epoch 55\n",
        "\n",
        "2] Trial 1: filters=180, lr=2.85e-04, wd=3.40e-06, pct_start=0.23\n",
        "* Epoch 058 | train_loss=0.2221 acc=0.8758 | val_loss=0.3680 acc=0.8707 | prec=0.8688 rec=0.8727 spec=0.8688 f1=0.8707 | time=23.7s\n",
        "Early stopping at epoch 58\n",
        "\n",
        "3]Trial 2: filters=120, lr=4.35e-05, wd=8.53e-06, pct_start=0.23\n",
        "* Epoch 100 | train_loss=0.4184 acc=0.8191 | val_loss=0.4237 acc=0.8322 | prec=0.8349 rec=0.8273 spec=0.8371 f1=0.8311 | time=12.9s\n",
        "\n",
        "4] Trial 4: filters=180, lr=7.40e-05, wd=2.30e-06, pct_start=0.23\n",
        "* Epoch 064 | train_loss=0.2713 acc=0.8673 | val_loss=0.3856 acc=0.8458 | prec=0.8423 rec=0.8500 spec=0.8416 f1=0.8462 | time=23.5s\n",
        "Early stopping at epoch 64\n",
        "\n",
        "5] Trial 7: filters=120, lr=1.24e-04, wd=1.13e-05, pct_start=0.23\n",
        "* Epoch 061 | train_loss=0.2986 acc=0.8667 | val_loss=0.3796 acc=0.8435 | prec=0.9218 rec=0.7500 spec=0.9367 f1=0.8271 | time=12.8s\n",
        "Early stopping at epoch 61\n",
        "\n",
        "6] Trial 8: filters=180, lr=1.07e-04, wd=4.01e-04, pct_start=0.23\n",
        "* Epoch 059 | train_loss=0.2492 acc=0.8746 | val_loss=0.3780 acc=0.8594 | prec=0.8185 rec=0.9227 spec=0.7964 f1=0.8675 | time=23.3s\n",
        "Early stopping at epoch 59\n",
        "\n",
        "7] Trial 9: filters=120, lr=9.18e-05, wd=4.24e-06, pct_start=0.23\n",
        "* Epoch 060 | train_loss=0.3007 acc=0.8503 | val_loss=0.4189 acc=0.8231 | prec=0.8515 rec=0.7818 spec=0.8643 f1=0.8152 | time=12.8s\n",
        "Early stopping at epoch 60"
      ],
      "metadata": {
        "id": "5Zewt54vM9au"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease the dropout rate and alleviate regularization\n",
        "* Decrease the dropout rate = 0.2 -> 0.1 and update the EEGformer model to improve the train_loss\n",
        "* Decrease the dropout rate = 0.5 -> 0.3 in CNN Decoder\n"
      ],
      "metadata": {
        "id": "45-zIGQER7lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 200\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr          = trial.suggest_float('lr', 1e-5, 5e-4, log=True)\n",
        "    wd          = trial.suggest_float('wd', 1e-6, 5e-4, log=True)\n",
        "    pct_start   = 0.23  # fixed\n",
        "    num_filters = 120\n",
        "\n",
        "    print(f\"Trial {trial.number}: filters={num_filters}, lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold-6',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr': lr, 'wd': wd, 'pct_start': pct_start, 'filters': num_filters},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len = dataset[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=num_filters,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Dynamic Weight Decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr / lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss  += loss.item()\n",
        "\n",
        "                preds   = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss   = vloss / len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds == val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "\n",
        "        # Compute specificity = TN / (TN + FP)\n",
        "        tn, fp, fn, tp = confusion_matrix(val_labels, val_preds).ravel()\n",
        "        specificity     = tn / (tn + fp)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={precision:.4f} rec={recall:.4f} spec={specificity:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize', 'maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ─── Print Pare토-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hqoFWNQHSDKg",
        "outputId": "a6bba051-e038-47f7-d6c7-f93324d4ddff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 02:22:00,633] A new study created in memory with name: no-name-5fa48b97-14e3-47ee-868d-0dbd6a0ae8ca\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: filters=120, lr=3.43e-04, wd=6.98e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_022200-a1mio5oa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/a1mio5oa' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/a1mio5oa' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/a1mio5oa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7333 acc=0.5122 | val_loss=0.6980 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=186.3s\n",
            "Epoch 002 | train_loss=0.7380 acc=0.4884 | val_loss=0.6960 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7203 acc=0.5122 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7288 acc=0.4838 | val_loss=0.6987 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7228 acc=0.4969 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7125 acc=0.5150 | val_loss=0.6901 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7134 acc=0.5150 | val_loss=0.6869 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.6899 acc=0.5462 | val_loss=0.6677 acc=0.6984 | prec=0.6548 rec=0.8364 spec=0.5611 f1=0.7345 | time=13.0s\n",
            "Epoch 009 | train_loss=0.6406 acc=0.6222 | val_loss=0.5919 acc=0.8027 | prec=0.8093 rec=0.7909 spec=0.8145 f1=0.8000 | time=12.7s\n",
            "Epoch 010 | train_loss=0.6143 acc=0.6478 | val_loss=0.5458 acc=0.8050 | prec=0.8722 rec=0.7136 spec=0.8959 f1=0.7850 | time=12.7s\n",
            "Epoch 011 | train_loss=0.5648 acc=0.7039 | val_loss=0.5123 acc=0.7959 | prec=0.9114 rec=0.6545 spec=0.9367 f1=0.7619 | time=12.8s\n",
            "Epoch 012 | train_loss=0.5329 acc=0.7499 | val_loss=0.4866 acc=0.8118 | prec=0.8072 rec=0.8182 spec=0.8054 f1=0.8126 | time=12.8s\n",
            "Epoch 013 | train_loss=0.5074 acc=0.7635 | val_loss=0.4800 acc=0.7937 | prec=0.7471 rec=0.8864 spec=0.7014 f1=0.8108 | time=12.8s\n",
            "Epoch 014 | train_loss=0.5104 acc=0.7572 | val_loss=0.4369 acc=0.8390 | prec=0.8465 rec=0.8273 spec=0.8507 f1=0.8368 | time=12.9s\n",
            "Epoch 015 | train_loss=0.4819 acc=0.7856 | val_loss=0.4081 acc=0.8481 | prec=0.8626 rec=0.8273 spec=0.8688 f1=0.8445 | time=12.8s\n",
            "Epoch 016 | train_loss=0.4704 acc=0.7884 | val_loss=0.4347 acc=0.8345 | prec=0.9016 rec=0.7500 spec=0.9186 f1=0.8189 | time=12.7s\n",
            "Epoch 017 | train_loss=0.4302 acc=0.8179 | val_loss=0.3854 acc=0.8571 | prec=0.8428 rec=0.8773 spec=0.8371 f1=0.8597 | time=12.7s\n",
            "Epoch 018 | train_loss=0.4025 acc=0.8247 | val_loss=0.3781 acc=0.8413 | prec=0.8866 rec=0.7818 spec=0.9005 f1=0.8309 | time=12.7s\n",
            "Epoch 019 | train_loss=0.3909 acc=0.8310 | val_loss=0.5834 acc=0.6803 | prec=0.6125 rec=0.9773 spec=0.3846 f1=0.7531 | time=12.9s\n",
            "Epoch 020 | train_loss=0.4164 acc=0.8230 | val_loss=0.3704 acc=0.8503 | prec=0.8031 rec=0.9273 spec=0.7738 f1=0.8608 | time=12.9s\n",
            "Epoch 021 | train_loss=0.3484 acc=0.8616 | val_loss=0.4005 acc=0.8299 | prec=0.9341 rec=0.7091 spec=0.9502 f1=0.8062 | time=12.8s\n",
            "Epoch 022 | train_loss=0.3346 acc=0.8656 | val_loss=0.7709 acc=0.6327 | prec=0.5759 rec=1.0000 spec=0.2670 f1=0.7309 | time=12.7s\n",
            "Epoch 023 | train_loss=0.3443 acc=0.8338 | val_loss=0.3828 acc=0.8458 | prec=0.9000 rec=0.7773 spec=0.9140 f1=0.8341 | time=12.7s\n",
            "Epoch 024 | train_loss=0.3089 acc=0.8724 | val_loss=0.3563 acc=0.8435 | prec=0.8356 rec=0.8545 spec=0.8326 f1=0.8449 | time=12.9s\n",
            "Epoch 025 | train_loss=0.2784 acc=0.8798 | val_loss=0.3970 acc=0.8390 | prec=0.7899 rec=0.9227 spec=0.7557 f1=0.8512 | time=12.7s\n",
            "Epoch 026 | train_loss=0.2918 acc=0.8735 | val_loss=0.3975 acc=0.8277 | prec=0.7769 rec=0.9182 spec=0.7376 f1=0.8417 | time=12.8s\n",
            "Epoch 027 | train_loss=0.2712 acc=0.8894 | val_loss=0.3537 acc=0.8503 | prec=0.8565 rec=0.8409 spec=0.8597 f1=0.8486 | time=12.8s\n",
            "Epoch 028 | train_loss=0.2748 acc=0.8871 | val_loss=0.3845 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.8s\n",
            "Epoch 029 | train_loss=0.2458 acc=0.8996 | val_loss=0.4934 acc=0.7914 | prec=0.7302 rec=0.9227 spec=0.6606 f1=0.8153 | time=12.8s\n",
            "Epoch 030 | train_loss=0.2357 acc=0.9036 | val_loss=0.7217 acc=0.7188 | prec=0.6472 rec=0.9591 spec=0.4796 f1=0.7729 | time=12.6s\n",
            "Epoch 031 | train_loss=0.2416 acc=0.9041 | val_loss=0.3599 acc=0.8594 | prec=0.8798 rec=0.8318 spec=0.8869 f1=0.8551 | time=12.8s\n",
            "Epoch 032 | train_loss=0.1880 acc=0.9144 | val_loss=0.6654 acc=0.7347 | prec=0.6700 rec=0.9227 spec=0.5475 f1=0.7763 | time=12.8s\n",
            "Epoch 033 | train_loss=0.1972 acc=0.9104 | val_loss=0.4552 acc=0.8345 | prec=0.7816 rec=0.9273 spec=0.7421 f1=0.8482 | time=12.8s\n",
            "Epoch 034 | train_loss=0.1764 acc=0.9166 | val_loss=0.4019 acc=0.8458 | prec=0.8304 rec=0.8682 spec=0.8235 f1=0.8489 | time=12.7s\n",
            "Epoch 035 | train_loss=0.1736 acc=0.9183 | val_loss=0.4965 acc=0.8322 | prec=0.8724 rec=0.7773 spec=0.8869 f1=0.8221 | time=12.9s\n",
            "Epoch 036 | train_loss=0.1780 acc=0.9251 | val_loss=0.4579 acc=0.8299 | prec=0.8112 rec=0.8591 spec=0.8009 f1=0.8344 | time=12.7s\n",
            "Epoch 037 | train_loss=0.1566 acc=0.9251 | val_loss=0.4456 acc=0.8481 | prec=0.8806 rec=0.8045 spec=0.8914 f1=0.8409 | time=12.8s\n",
            "Epoch 038 | train_loss=0.1556 acc=0.9291 | val_loss=0.6352 acc=0.7256 | prec=0.6592 rec=0.9318 spec=0.5204 f1=0.7721 | time=12.7s\n",
            "Epoch 039 | train_loss=0.1891 acc=0.9064 | val_loss=0.4847 acc=0.8367 | prec=0.8304 rec=0.8455 spec=0.8281 f1=0.8378 | time=12.9s\n",
            "Epoch 040 | train_loss=0.2149 acc=0.9109 | val_loss=0.5645 acc=0.8390 | prec=0.8821 rec=0.7818 spec=0.8959 f1=0.8289 | time=12.7s\n",
            "Epoch 041 | train_loss=0.1534 acc=0.9314 | val_loss=0.5684 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=12.8s\n",
            "Epoch 042 | train_loss=0.1551 acc=0.9178 | val_loss=0.4600 acc=0.8299 | prec=0.8341 rec=0.8227 spec=0.8371 f1=0.8284 | time=12.8s\n",
            "Early stopping at epoch 42\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▃▆▅▄▆▆▇▇▆█▇▄█▃▇▇█▇█▇▆▅█▅██▇▇▇▅▇▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▄▆▇█▆▅▇▇█▇█▃▆▂█▇▆▆▇▇▅▄▇▄▆▇▇▆▇▄▇█▇</td></tr><tr><td>recall</td><td>███████▅▄▂▁▄▆▅▅▃▆▄█▇█▃▅▆▆▅▄▆▇▅▆▇▅▃▅▄▇▅▄▄</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▅▇██▇▆▇▇█▇█▄▇▃█▇▇▇▇▇▆▅█▅▇▇█▇█▅▇█▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▃▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>███████▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▂▃▂▂▂▂▂▁▂▁▁▁▁▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▅▇▇▇▇▇█████▅█▄███▇█▇▇▅█▆██▇▇█▅██▇</td></tr><tr><td>validation_loss</td><td>▇▇▇▇▇▇▇▆▅▄▄▃▃▂▂▂▂▁▅▁█▁▁▂▂▁▂▃▇▁▆▃▂▃▃▃▆▃▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82838</td></tr><tr><td>precision</td><td>0.8341</td></tr><tr><td>recall</td><td>0.82273</td></tr><tr><td>specificity</td><td>0.8371</td></tr><tr><td>train_accuracy</td><td>0.91775</td></tr><tr><td>train_loss</td><td>0.15512</td></tr><tr><td>validation_accuracy</td><td>0.82993</td></tr><tr><td>validation_loss</td><td>0.45999</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/a1mio5oa' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/a1mio5oa</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_022200-a1mio5oa/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 02:34:10,983] Trial 0 finished with values: [0.45999425117458614, 0.8299319727891157] and parameters: {'lr': 0.0003426778612510669, 'wd': 6.976378901037059e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: filters=120, lr=1.25e-04, wd=1.19e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_023410-s14l1yt3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/s14l1yt3' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/s14l1yt3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/s14l1yt3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7244 acc=0.5156 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7241 acc=0.5003 | val_loss=0.6960 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7141 acc=0.5094 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7244 acc=0.4929 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7235 acc=0.4963 | val_loss=0.6932 acc=0.5034 | prec=0.5333 rec=0.0364 spec=0.9683 f1=0.0681 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7141 acc=0.5179 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7219 acc=0.4855 | val_loss=0.6915 acc=0.5420 | prec=0.5310 rec=0.7000 spec=0.3846 f1=0.6039 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7064 acc=0.5145 | val_loss=0.6915 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7100 acc=0.5082 | val_loss=0.6969 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7014 acc=0.5269 | val_loss=0.6818 acc=0.6259 | prec=0.7132 rec=0.4182 spec=0.8326 f1=0.5272 | time=13.3s\n",
            "Epoch 011 | train_loss=0.7114 acc=0.5122 | val_loss=0.6682 acc=0.6916 | prec=0.6511 rec=0.8227 spec=0.5611 f1=0.7269 | time=13.5s\n",
            "Epoch 012 | train_loss=0.6781 acc=0.5780 | val_loss=0.6461 acc=0.7029 | prec=0.7432 rec=0.6182 spec=0.7873 f1=0.6749 | time=13.4s\n",
            "Epoch 013 | train_loss=0.6747 acc=0.5718 | val_loss=0.6138 acc=0.7279 | prec=0.8521 rec=0.5500 spec=0.9050 f1=0.6685 | time=13.5s\n",
            "Epoch 014 | train_loss=0.6179 acc=0.6529 | val_loss=0.5273 acc=0.7619 | prec=0.7778 rec=0.7318 spec=0.7919 f1=0.7541 | time=13.5s\n",
            "Epoch 015 | train_loss=0.5605 acc=0.7102 | val_loss=0.5527 acc=0.7256 | prec=0.9541 rec=0.4727 spec=0.9774 f1=0.6322 | time=13.6s\n",
            "Epoch 016 | train_loss=0.5389 acc=0.7362 | val_loss=0.5194 acc=0.7528 | prec=0.7048 rec=0.8682 spec=0.6380 f1=0.7780 | time=13.6s\n",
            "Epoch 017 | train_loss=0.5099 acc=0.7544 | val_loss=0.5040 acc=0.7687 | prec=0.7218 rec=0.8727 spec=0.6652 f1=0.7901 | time=13.5s\n",
            "Epoch 018 | train_loss=0.5082 acc=0.7561 | val_loss=0.4544 acc=0.7914 | prec=0.7759 rec=0.8182 spec=0.7647 f1=0.7965 | time=13.5s\n",
            "Epoch 019 | train_loss=0.4731 acc=0.7924 | val_loss=0.4607 acc=0.8027 | prec=0.7806 rec=0.8409 spec=0.7647 f1=0.8096 | time=13.7s\n",
            "Epoch 020 | train_loss=0.4848 acc=0.7839 | val_loss=0.4607 acc=0.7914 | prec=0.9324 rec=0.6273 spec=0.9548 f1=0.7500 | time=13.4s\n",
            "Epoch 021 | train_loss=0.4573 acc=0.7958 | val_loss=0.4610 acc=0.8073 | prec=0.7455 rec=0.9318 spec=0.6833 f1=0.8283 | time=13.6s\n",
            "Epoch 022 | train_loss=0.4489 acc=0.7992 | val_loss=0.3966 acc=0.8435 | prec=0.8612 rec=0.8182 spec=0.8688 f1=0.8392 | time=13.7s\n",
            "Epoch 023 | train_loss=0.4173 acc=0.8168 | val_loss=0.4137 acc=0.8435 | prec=0.9081 rec=0.7636 spec=0.9231 f1=0.8296 | time=13.7s\n",
            "Epoch 024 | train_loss=0.3971 acc=0.8389 | val_loss=0.3948 acc=0.8413 | prec=0.9213 rec=0.7455 spec=0.9367 f1=0.8241 | time=13.9s\n",
            "Epoch 025 | train_loss=0.3889 acc=0.8327 | val_loss=0.3552 acc=0.8503 | prec=0.8775 rec=0.8136 spec=0.8869 f1=0.8443 | time=13.8s\n",
            "Epoch 026 | train_loss=0.3699 acc=0.8423 | val_loss=0.3707 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=13.7s\n",
            "Epoch 027 | train_loss=0.3705 acc=0.8372 | val_loss=0.3527 acc=0.8503 | prec=0.9010 rec=0.7864 spec=0.9140 f1=0.8398 | time=13.7s\n",
            "Epoch 028 | train_loss=0.3614 acc=0.8503 | val_loss=0.3870 acc=0.8413 | prec=0.8906 rec=0.7773 spec=0.9050 f1=0.8301 | time=13.6s\n",
            "Epoch 029 | train_loss=0.3389 acc=0.8559 | val_loss=0.4753 acc=0.8005 | prec=0.9459 rec=0.6364 spec=0.9638 f1=0.7609 | time=13.3s\n",
            "Epoch 030 | train_loss=0.3200 acc=0.8661 | val_loss=0.3654 acc=0.8345 | prec=0.7860 rec=0.9182 spec=0.7511 f1=0.8470 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3090 acc=0.8639 | val_loss=0.4000 acc=0.8345 | prec=0.9249 rec=0.7273 spec=0.9412 f1=0.8142 | time=12.8s\n",
            "Epoch 032 | train_loss=0.2826 acc=0.8888 | val_loss=0.3845 acc=0.8277 | prec=0.7880 rec=0.8955 spec=0.7602 f1=0.8383 | time=12.8s\n",
            "Epoch 033 | train_loss=0.2641 acc=0.8917 | val_loss=0.3505 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.9s\n",
            "Epoch 034 | train_loss=0.2948 acc=0.8780 | val_loss=0.5710 acc=0.7438 | prec=0.6667 rec=0.9727 spec=0.5158 f1=0.7911 | time=12.8s\n",
            "Epoch 035 | train_loss=0.2678 acc=0.9002 | val_loss=0.4464 acc=0.8118 | prec=0.9053 rec=0.6955 spec=0.9276 f1=0.7866 | time=12.7s\n",
            "Epoch 036 | train_loss=0.2740 acc=0.8894 | val_loss=0.3851 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2647 acc=0.8883 | val_loss=0.3919 acc=0.8254 | prec=0.8295 rec=0.8182 spec=0.8326 f1=0.8238 | time=12.7s\n",
            "Epoch 038 | train_loss=0.2265 acc=0.9075 | val_loss=0.4286 acc=0.8141 | prec=0.7556 rec=0.9273 spec=0.7014 f1=0.8327 | time=12.8s\n",
            "Epoch 039 | train_loss=0.2265 acc=0.9121 | val_loss=0.3868 acc=0.8526 | prec=0.9235 rec=0.7682 spec=0.9367 f1=0.8387 | time=12.8s\n",
            "Epoch 040 | train_loss=0.2208 acc=0.8968 | val_loss=0.5290 acc=0.8027 | prec=0.9650 rec=0.6273 spec=0.9774 f1=0.7603 | time=12.7s\n",
            "Epoch 041 | train_loss=0.2166 acc=0.9104 | val_loss=0.4327 acc=0.8163 | prec=0.8089 rec=0.8273 spec=0.8054 f1=0.8180 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2261 acc=0.9087 | val_loss=0.3717 acc=0.8481 | prec=0.8844 rec=0.8000 spec=0.8959 f1=0.8401 | time=12.9s\n",
            "Epoch 043 | train_loss=0.2322 acc=0.8968 | val_loss=0.4690 acc=0.8231 | prec=0.9277 rec=0.7000 spec=0.9457 f1=0.7979 | time=12.9s\n",
            "Epoch 044 | train_loss=0.2742 acc=0.8724 | val_loss=0.3940 acc=0.8186 | prec=0.7823 rec=0.8818 spec=0.7557 f1=0.8291 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2052 acc=0.9149 | val_loss=0.3668 acc=0.8209 | prec=0.8133 rec=0.8318 spec=0.8100 f1=0.8225 | time=12.8s\n",
            "Epoch 046 | train_loss=0.1964 acc=0.9166 | val_loss=0.4145 acc=0.8367 | prec=0.7891 rec=0.9182 spec=0.7557 f1=0.8487 | time=12.9s\n",
            "Epoch 047 | train_loss=0.1660 acc=0.9336 | val_loss=0.4593 acc=0.8186 | prec=0.7734 rec=0.9000 spec=0.7376 f1=0.8319 | time=12.7s\n",
            "Epoch 048 | train_loss=0.1802 acc=0.9263 | val_loss=0.4213 acc=0.8299 | prec=0.8877 rec=0.7545 spec=0.9050 f1=0.8157 | time=12.8s\n",
            "Early stopping at epoch 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▂▆▁▁▅▇▇▇▆▇██▇███████▇████▇███▇██████</td></tr><tr><td>precision</td><td>▁▁▁▁▅▅█▁▆▆▇▆█▆▆▆█▆▇▇▇▇▇▇█▇▇▇▆▇▇▆▇█▇▇▆▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▆▁▁▄▇▅▆▄▇▇▇▆█▇▆▇▇▇▇▆▆▇▇█▆▇█▇▆▇▆▇▇█▆</td></tr><tr><td>specificity</td><td>█████▁██▆▃▇▆█▄▄▅▇▄▇▇▇▆▇▇█▇▅▆▂▇▆▅▇█▆▇▅▆▅▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▂▁▂▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇█████▇███</td></tr><tr><td>train_loss</td><td>██████████▇▇▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▂▁▁▃▅▆▆▅▆▆▇▇▇██████▇███▆▇▇▇█▇▇▇▇▇██</td></tr><tr><td>validation_loss</td><td>█████████▇▆▅▅▄▄▃▃▃▂▂▁▁▁▂▄▂▂▁▅▃▂▃▂▅▃▃▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81572</td></tr><tr><td>precision</td><td>0.8877</td></tr><tr><td>recall</td><td>0.75455</td></tr><tr><td>specificity</td><td>0.90498</td></tr><tr><td>train_accuracy</td><td>0.92626</td></tr><tr><td>train_loss</td><td>0.18021</td></tr><tr><td>validation_accuracy</td><td>0.82993</td></tr><tr><td>validation_loss</td><td>0.42133</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/s14l1yt3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/s14l1yt3</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_023410-s14l1yt3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 02:44:42,628] Trial 1 finished with values: [0.42132928328854696, 0.8299319727891157] and parameters: {'lr': 0.00012527308320585515, 'wd': 1.1851002627136725e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: filters=120, lr=6.70e-05, wd=1.10e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_024442-neyuvuwn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/neyuvuwn' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/neyuvuwn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/neyuvuwn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7375 acc=0.5207 | val_loss=0.6988 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7521 acc=0.5071 | val_loss=0.7080 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7400 acc=0.5048 | val_loss=0.6976 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7276 acc=0.5167 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7222 acc=0.5009 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 006 | train_loss=0.7295 acc=0.4912 | val_loss=0.6958 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7247 acc=0.5009 | val_loss=0.6992 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7232 acc=0.5031 | val_loss=0.7004 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7310 acc=0.5060 | val_loss=0.6968 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7208 acc=0.5071 | val_loss=0.6971 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7151 acc=0.5099 | val_loss=0.6923 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7281 acc=0.5054 | val_loss=0.7012 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 013 | train_loss=0.7222 acc=0.5014 | val_loss=0.6963 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7052 acc=0.5162 | val_loss=0.6832 acc=0.5397 | prec=0.8400 rec=0.0955 spec=0.9819 f1=0.1714 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7053 acc=0.5298 | val_loss=0.6804 acc=0.5283 | prec=1.0000 rec=0.0545 spec=1.0000 f1=0.1034 | time=12.9s\n",
            "Epoch 016 | train_loss=0.6869 acc=0.5530 | val_loss=0.6460 acc=0.6667 | prec=0.8174 rec=0.4273 spec=0.9050 f1=0.5612 | time=12.9s\n",
            "Epoch 017 | train_loss=0.6651 acc=0.5854 | val_loss=0.6170 acc=0.7256 | prec=0.8075 rec=0.5909 spec=0.8597 f1=0.6824 | time=12.7s\n",
            "Epoch 018 | train_loss=0.6293 acc=0.6500 | val_loss=0.5605 acc=0.7687 | prec=0.8105 rec=0.7000 spec=0.8371 f1=0.7512 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5922 acc=0.7022 | val_loss=0.5519 acc=0.7256 | prec=0.6581 rec=0.9364 spec=0.5158 f1=0.7730 | time=12.8s\n",
            "Epoch 020 | train_loss=0.5800 acc=0.7107 | val_loss=0.5226 acc=0.7528 | prec=0.9237 rec=0.5500 spec=0.9548 f1=0.6895 | time=12.7s\n",
            "Epoch 021 | train_loss=0.5291 acc=0.7453 | val_loss=0.4604 acc=0.8118 | prec=0.8374 rec=0.7727 spec=0.8507 f1=0.8038 | time=12.8s\n",
            "Epoch 022 | train_loss=0.5354 acc=0.7368 | val_loss=0.4590 acc=0.7800 | prec=0.7450 rec=0.8500 spec=0.7104 f1=0.7941 | time=12.8s\n",
            "Epoch 023 | train_loss=0.4989 acc=0.7703 | val_loss=0.4349 acc=0.8277 | prec=0.8711 rec=0.7682 spec=0.8869 f1=0.8164 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4812 acc=0.7907 | val_loss=0.4526 acc=0.7868 | prec=0.7333 rec=0.9000 spec=0.6742 f1=0.8082 | time=12.7s\n",
            "Epoch 025 | train_loss=0.4883 acc=0.7754 | val_loss=0.4274 acc=0.8254 | prec=0.8705 rec=0.7636 spec=0.8869 f1=0.8136 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4544 acc=0.7969 | val_loss=0.4055 acc=0.8367 | prec=0.8426 rec=0.8273 spec=0.8462 f1=0.8349 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4368 acc=0.7958 | val_loss=0.3816 acc=0.8526 | prec=0.8638 rec=0.8364 spec=0.8688 f1=0.8499 | time=12.7s\n",
            "Epoch 028 | train_loss=0.4325 acc=0.8043 | val_loss=0.4238 acc=0.8390 | prec=0.7922 rec=0.9182 spec=0.7602 f1=0.8505 | time=12.7s\n",
            "Epoch 029 | train_loss=0.4472 acc=0.7969 | val_loss=0.4194 acc=0.8209 | prec=0.7681 rec=0.9182 spec=0.7240 f1=0.8364 | time=12.7s\n",
            "Epoch 030 | train_loss=0.4008 acc=0.8259 | val_loss=0.4366 acc=0.8050 | prec=0.7428 rec=0.9318 spec=0.6787 f1=0.8266 | time=12.8s\n",
            "Epoch 031 | train_loss=0.3952 acc=0.8276 | val_loss=0.3810 acc=0.8367 | prec=0.8895 rec=0.7682 spec=0.9050 f1=0.8244 | time=12.8s\n",
            "Epoch 032 | train_loss=0.3877 acc=0.8361 | val_loss=0.4196 acc=0.8322 | prec=0.8067 rec=0.8727 spec=0.7919 f1=0.8384 | time=12.7s\n",
            "Epoch 033 | train_loss=0.3737 acc=0.8395 | val_loss=0.3904 acc=0.8345 | prec=0.7860 rec=0.9182 spec=0.7511 f1=0.8470 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3575 acc=0.8531 | val_loss=0.3746 acc=0.8367 | prec=0.7891 rec=0.9182 spec=0.7557 f1=0.8487 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3322 acc=0.8633 | val_loss=0.3745 acc=0.8413 | prec=0.9213 rec=0.7455 spec=0.9367 f1=0.8241 | time=12.7s\n",
            "Epoch 036 | train_loss=0.3518 acc=0.8542 | val_loss=0.4066 acc=0.8163 | prec=0.7546 rec=0.9364 spec=0.6968 f1=0.8357 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3232 acc=0.8627 | val_loss=0.3636 acc=0.8526 | prec=0.8298 rec=0.8864 spec=0.8190 f1=0.8571 | time=12.7s\n",
            "Epoch 038 | train_loss=0.3178 acc=0.8605 | val_loss=0.3434 acc=0.8503 | prec=0.8156 rec=0.9045 spec=0.7964 f1=0.8578 | time=12.8s\n",
            "Epoch 039 | train_loss=0.3173 acc=0.8718 | val_loss=0.3839 acc=0.8345 | prec=0.9349 rec=0.7182 spec=0.9502 f1=0.8123 | time=12.7s\n",
            "Epoch 040 | train_loss=0.4277 acc=0.8054 | val_loss=0.3867 acc=0.8367 | prec=0.9458 rec=0.7136 spec=0.9593 f1=0.8135 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3442 acc=0.8633 | val_loss=0.3522 acc=0.8435 | prec=0.8756 rec=0.8000 spec=0.8869 f1=0.8361 | time=12.7s\n",
            "Epoch 042 | train_loss=0.3209 acc=0.8593 | val_loss=0.4220 acc=0.8095 | prec=0.7446 rec=0.9409 spec=0.6787 f1=0.8313 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3157 acc=0.8815 | val_loss=0.3516 acc=0.8345 | prec=0.8128 rec=0.8682 spec=0.8009 f1=0.8396 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2981 acc=0.8786 | val_loss=0.3697 acc=0.8571 | prec=0.9243 rec=0.7773 spec=0.9367 f1=0.8444 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3080 acc=0.8763 | val_loss=0.3667 acc=0.8322 | prec=0.7967 rec=0.8909 spec=0.7738 f1=0.8412 | time=12.8s\n",
            "Epoch 046 | train_loss=0.2548 acc=0.8951 | val_loss=0.3952 acc=0.8277 | prec=0.7748 rec=0.9227 spec=0.7330 f1=0.8423 | time=12.7s\n",
            "Epoch 047 | train_loss=0.2487 acc=0.8973 | val_loss=0.3359 acc=0.8481 | prec=0.8493 rec=0.8455 spec=0.8507 f1=0.8474 | time=12.8s\n",
            "Epoch 048 | train_loss=0.2621 acc=0.8985 | val_loss=0.3697 acc=0.8413 | prec=0.7885 rec=0.9318 spec=0.7511 f1=0.8542 | time=12.7s\n",
            "Epoch 049 | train_loss=0.2745 acc=0.8786 | val_loss=0.3382 acc=0.8549 | prec=0.8482 rec=0.8636 spec=0.8462 f1=0.8559 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2246 acc=0.9075 | val_loss=0.4351 acc=0.8277 | prec=0.7628 rec=0.9500 spec=0.7059 f1=0.8462 | time=12.7s\n",
            "Epoch 051 | train_loss=0.2236 acc=0.9104 | val_loss=0.3940 acc=0.8390 | prec=0.7833 rec=0.9364 spec=0.7421 f1=0.8530 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2277 acc=0.9195 | val_loss=0.3513 acc=0.8571 | prec=0.8720 rec=0.8364 spec=0.8778 f1=0.8538 | time=12.8s\n",
            "Epoch 053 | train_loss=0.2195 acc=0.9195 | val_loss=0.4821 acc=0.8118 | prec=0.7455 rec=0.9455 spec=0.6787 f1=0.8337 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2196 acc=0.9002 | val_loss=0.3679 acc=0.8458 | prec=0.8958 rec=0.7818 spec=0.9095 f1=0.8350 | time=12.8s\n",
            "Epoch 055 | train_loss=0.1818 acc=0.9325 | val_loss=0.3928 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=12.8s\n",
            "Epoch 056 | train_loss=0.2021 acc=0.9104 | val_loss=0.4050 acc=0.8458 | prec=0.7923 rec=0.9364 spec=0.7557 f1=0.8583 | time=12.7s\n",
            "Epoch 057 | train_loss=0.2070 acc=0.9030 | val_loss=0.3646 acc=0.8503 | prec=0.8319 rec=0.8773 spec=0.8235 f1=0.8540 | time=12.7s\n",
            "Epoch 058 | train_loss=0.1749 acc=0.9246 | val_loss=0.4000 acc=0.8481 | prec=0.8048 rec=0.9182 spec=0.7783 f1=0.8577 | time=12.9s\n",
            "Epoch 059 | train_loss=0.1786 acc=0.9217 | val_loss=0.3897 acc=0.8367 | prec=0.8058 rec=0.8864 spec=0.7873 f1=0.8442 | time=12.7s\n",
            "Epoch 060 | train_loss=0.1667 acc=0.9308 | val_loss=0.5349 acc=0.8186 | prec=0.7482 rec=0.9591 spec=0.6787 f1=0.8406 | time=12.8s\n",
            "Epoch 061 | train_loss=0.1727 acc=0.9240 | val_loss=0.4255 acc=0.8458 | prec=0.7923 rec=0.9364 spec=0.7557 f1=0.8583 | time=12.7s\n",
            "Epoch 062 | train_loss=0.1612 acc=0.9234 | val_loss=0.4336 acc=0.8390 | prec=0.7922 rec=0.9182 spec=0.7602 f1=0.8505 | time=12.9s\n",
            "Early stopping at epoch 62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▂▆▇▇▇▇█████████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁█▇▇▇▇▆▇▇▇▆▇▇▇██▇▇▆▇▇▆▆▆▇▇▇▇▇▆▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▂▁▄█▇▇▇█▇▇█▇▇█▆█▇██▇██▇▇█▇█▇█▇███</td></tr><tr><td>specificity</td><td>██████████▆▆▁▇▆▆▆▆▄▃▅▄▇▄▅▇▇▆▃▅▅▄▄▃▇▄▅▅▃▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██▇███████</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▆▅▅▄▄▄▄▄▄▄▃▃▃▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▄▅▅▇▇███▇████████▇██████████</td></tr><tr><td>validation_loss</td><td>██████████▇▆▅▅▃▃▂▂▃▃▃▂▂▂▂▁▂▂▁▁▂▁▄▂▂▂▂▂▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85053</td></tr><tr><td>precision</td><td>0.79216</td></tr><tr><td>recall</td><td>0.91818</td></tr><tr><td>specificity</td><td>0.76018</td></tr><tr><td>train_accuracy</td><td>0.92343</td></tr><tr><td>train_loss</td><td>0.16123</td></tr><tr><td>validation_accuracy</td><td>0.839</td></tr><tr><td>validation_loss</td><td>0.43357</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/neyuvuwn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/neyuvuwn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_024442-neyuvuwn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 02:57:57,537] Trial 2 finished with values: [0.4335672802158764, 0.8390022675736961] and parameters: {'lr': 6.704038535844278e-05, 'wd': 1.1003236848381844e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: filters=120, lr=3.64e-05, wd=9.05e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_025757-nvkiwufc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/nvkiwufc' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/nvkiwufc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/nvkiwufc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7351 acc=0.5179 | val_loss=0.6919 acc=0.5034 | prec=0.5017 rec=0.6818 spec=0.3258 f1=0.5780 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7593 acc=0.4940 | val_loss=0.6921 acc=0.5238 | prec=0.6190 rec=0.1182 spec=0.9276 f1=0.1985 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7236 acc=0.5241 | val_loss=0.6925 acc=0.5079 | prec=0.5042 rec=0.8091 spec=0.2081 f1=0.6213 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7261 acc=0.5077 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=0.9909 spec=0.0090 f1=0.6636 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7237 acc=0.5150 | val_loss=0.6935 acc=0.4989 | prec=0.4988 rec=0.9591 spec=0.0407 f1=0.6563 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7264 acc=0.5082 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7224 acc=0.5133 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7247 acc=0.4986 | val_loss=0.6992 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7197 acc=0.5077 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7163 acc=0.5167 | val_loss=0.6974 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 011 | train_loss=0.7173 acc=0.4946 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7228 acc=0.4935 | val_loss=0.6922 acc=0.5125 | prec=0.8571 rec=0.0273 spec=0.9955 f1=0.0529 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7110 acc=0.5196 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7187 acc=0.4918 | val_loss=0.6994 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7079 acc=0.5309 | val_loss=0.6966 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7102 acc=0.5094 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▇▃███▁▁▁▁▁▁▂▁▁▁▁</td></tr><tr><td>precision</td><td>▅▆▅▅▅▁▁▁▁▁▁█▁▁▁▁</td></tr><tr><td>recall</td><td>▆▂▇██▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>specificity</td><td>▃▇▂▁▁███████████</td></tr><tr><td>train_accuracy</td><td>▆▁▇▄▅▄▅▂▄▅▂▁▆▁█▄</td></tr><tr><td>train_loss</td><td>▅█▃▃▃▄▃▃▃▂▂▃▁▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▂█▄▁▁▂▂▂▂▂▂▅▂▂▂▂</td></tr><tr><td>validation_loss</td><td>▁▁▂▃▃▄▄█▅▆▂▁▄█▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>specificity</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.50936</td></tr><tr><td>train_loss</td><td>0.71021</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69615</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/nvkiwufc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/nvkiwufc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_025757-nvkiwufc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:01:23,675] Trial 3 finished with values: [0.696151601416724, 0.5011337868480725] and parameters: {'lr': 3.642707821024785e-05, 'wd': 9.052672563747598e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: filters=120, lr=2.59e-04, wd=1.68e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_030123-21ko10k3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/21ko10k3' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/21ko10k3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/21ko10k3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7150 acc=0.5020 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=0.9955 spec=0.0045 f1=0.6646 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7178 acc=0.4793 | val_loss=0.6929 acc=0.5147 | prec=0.5417 rec=0.1773 spec=0.8507 f1=0.2671 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7080 acc=0.4980 | val_loss=0.6928 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7129 acc=0.4946 | val_loss=0.6912 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7044 acc=0.5065 | val_loss=0.6873 acc=0.5850 | prec=0.7681 rec=0.2409 spec=0.9276 f1=0.3668 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7034 acc=0.5145 | val_loss=0.6729 acc=0.7211 | prec=0.7175 rec=0.7273 spec=0.7149 f1=0.7223 | time=12.6s\n",
            "Epoch 007 | train_loss=0.6643 acc=0.5910 | val_loss=0.5995 acc=0.7029 | prec=0.8560 rec=0.4864 spec=0.9186 f1=0.6203 | time=12.7s\n",
            "Epoch 008 | train_loss=0.6164 acc=0.6648 | val_loss=0.5650 acc=0.7574 | prec=0.7216 rec=0.8364 spec=0.6787 f1=0.7747 | time=12.9s\n",
            "Epoch 009 | train_loss=0.5662 acc=0.7073 | val_loss=0.5410 acc=0.7755 | prec=0.7373 rec=0.8545 spec=0.6968 f1=0.7916 | time=12.7s\n",
            "Epoch 010 | train_loss=0.5611 acc=0.7062 | val_loss=0.5094 acc=0.8073 | prec=0.8426 rec=0.7545 spec=0.8597 f1=0.7962 | time=12.8s\n",
            "Epoch 011 | train_loss=0.5355 acc=0.7425 | val_loss=0.5108 acc=0.7914 | prec=0.7857 rec=0.8000 spec=0.7828 f1=0.7928 | time=12.8s\n",
            "Epoch 012 | train_loss=0.5173 acc=0.7674 | val_loss=0.4810 acc=0.8163 | prec=0.9017 rec=0.7091 spec=0.9231 f1=0.7939 | time=12.8s\n",
            "Epoch 013 | train_loss=0.5235 acc=0.7618 | val_loss=0.5011 acc=0.7982 | prec=0.7472 rec=0.9000 spec=0.6968 f1=0.8165 | time=12.7s\n",
            "Epoch 014 | train_loss=0.4925 acc=0.7952 | val_loss=0.4545 acc=0.8118 | prec=0.8018 rec=0.8273 spec=0.7964 f1=0.8143 | time=12.7s\n",
            "Epoch 015 | train_loss=0.5106 acc=0.7828 | val_loss=0.4698 acc=0.8073 | prec=0.7733 rec=0.8682 spec=0.7466 f1=0.8180 | time=12.8s\n",
            "Epoch 016 | train_loss=0.4617 acc=0.7918 | val_loss=0.4826 acc=0.7823 | prec=0.7183 rec=0.9273 spec=0.6380 f1=0.8095 | time=12.7s\n",
            "Epoch 017 | train_loss=0.4468 acc=0.8083 | val_loss=0.4313 acc=0.8390 | prec=0.7992 rec=0.9045 spec=0.7738 f1=0.8486 | time=12.9s\n",
            "Epoch 018 | train_loss=0.4203 acc=0.8315 | val_loss=0.4703 acc=0.7982 | prec=0.7298 rec=0.9455 spec=0.6516 f1=0.8238 | time=12.7s\n",
            "Epoch 019 | train_loss=0.4029 acc=0.8344 | val_loss=0.4207 acc=0.8345 | prec=0.9153 rec=0.7364 spec=0.9321 f1=0.8161 | time=12.9s\n",
            "Epoch 020 | train_loss=0.3937 acc=0.8344 | val_loss=0.4222 acc=0.8231 | prec=0.7689 rec=0.9227 spec=0.7240 f1=0.8388 | time=12.8s\n",
            "Epoch 021 | train_loss=0.3537 acc=0.8588 | val_loss=0.3791 acc=0.8481 | prec=0.8430 rec=0.8545 spec=0.8416 f1=0.8488 | time=12.9s\n",
            "Epoch 022 | train_loss=0.3409 acc=0.8588 | val_loss=0.3605 acc=0.8390 | prec=0.8984 rec=0.7636 spec=0.9140 f1=0.8256 | time=12.9s\n",
            "Epoch 023 | train_loss=0.3368 acc=0.8520 | val_loss=0.3948 acc=0.8231 | prec=0.8989 rec=0.7273 spec=0.9186 f1=0.8040 | time=12.7s\n",
            "Epoch 024 | train_loss=0.3121 acc=0.8684 | val_loss=0.4443 acc=0.8095 | prec=0.9474 rec=0.6545 spec=0.9638 f1=0.7742 | time=12.8s\n",
            "Epoch 025 | train_loss=0.3341 acc=0.8616 | val_loss=0.4626 acc=0.7937 | prec=0.7186 rec=0.9636 spec=0.6244 f1=0.8233 | time=12.8s\n",
            "Epoch 026 | train_loss=0.3081 acc=0.8849 | val_loss=0.4361 acc=0.8254 | prec=0.7992 rec=0.8682 spec=0.7828 f1=0.8322 | time=12.8s\n",
            "Epoch 027 | train_loss=0.2922 acc=0.8826 | val_loss=0.4163 acc=0.8118 | prec=0.7605 rec=0.9091 spec=0.7149 f1=0.8282 | time=12.9s\n",
            "Epoch 028 | train_loss=0.2524 acc=0.8985 | val_loss=0.4098 acc=0.8141 | prec=0.7654 rec=0.9045 spec=0.7240 f1=0.8292 | time=12.8s\n",
            "Epoch 029 | train_loss=0.2566 acc=0.8883 | val_loss=0.3873 acc=0.8186 | prec=0.7893 rec=0.8682 spec=0.7692 f1=0.8268 | time=12.8s\n",
            "Epoch 030 | train_loss=0.2405 acc=0.8934 | val_loss=0.3698 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=12.8s\n",
            "Epoch 031 | train_loss=0.2091 acc=0.9087 | val_loss=0.4723 acc=0.7732 | prec=0.7174 rec=0.9000 spec=0.6471 f1=0.7984 | time=12.8s\n",
            "Epoch 032 | train_loss=0.2346 acc=0.8996 | val_loss=0.4592 acc=0.7891 | prec=0.7309 rec=0.9136 spec=0.6652 f1=0.8121 | time=12.7s\n",
            "Epoch 033 | train_loss=0.2689 acc=0.8758 | val_loss=0.4365 acc=0.7937 | prec=0.7416 rec=0.9000 spec=0.6878 f1=0.8131 | time=12.7s\n",
            "Epoch 034 | train_loss=0.2357 acc=0.8843 | val_loss=0.4428 acc=0.8118 | prec=0.8044 rec=0.8227 spec=0.8009 f1=0.8135 | time=12.9s\n",
            "Epoch 035 | train_loss=0.1936 acc=0.9115 | val_loss=0.4381 acc=0.8118 | prec=0.7866 rec=0.8545 spec=0.7692 f1=0.8192 | time=12.7s\n",
            "Epoch 036 | train_loss=0.1890 acc=0.9155 | val_loss=0.5115 acc=0.8050 | prec=0.7538 rec=0.9045 spec=0.7059 f1=0.8223 | time=12.9s\n",
            "Epoch 037 | train_loss=0.1990 acc=0.9109 | val_loss=0.4401 acc=0.8118 | prec=0.7645 rec=0.9000 spec=0.7240 f1=0.8267 | time=12.8s\n",
            "Early stopping at epoch 37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▆▆▂▆▅▇▇▇▇▇██████████▇▇██████▇██████</td></tr><tr><td>precision</td><td>▁▂▁▁▅▄▇▄▅▆▅▇▅▆▅▄▆▅▇▅▆▇▇█▄▆▅▅▆▇▄▅▅▆▅▅▅</td></tr><tr><td>recall</td><td>█▁██▂▆▄▇▇▆▆▆▇▇▇▇▇█▆▇▇▆▆▅█▇▇▇▇▆▇▇▇▆▇▇▇</td></tr><tr><td>specificity</td><td>▁▇▁▁█▆█▆▆▇▇█▆▇▆▆▇▆█▆▇███▆▇▆▆▇▇▆▆▆▇▇▆▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▂▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█████▇▇███</td></tr><tr><td>train_loss</td><td>██████▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▃▅▅▆▇▇▇▇▇▇▇▇█▇█▇██▇▇▇█▇▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>██████▆▅▅▄▄▄▄▃▃▄▂▃▂▂▁▁▂▃▃▃▂▂▂▁▃▃▃▃▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82672</td></tr><tr><td>precision</td><td>0.76448</td></tr><tr><td>recall</td><td>0.9</td></tr><tr><td>specificity</td><td>0.72398</td></tr><tr><td>train_accuracy</td><td>0.91095</td></tr><tr><td>train_loss</td><td>0.19903</td></tr><tr><td>validation_accuracy</td><td>0.81179</td></tr><tr><td>validation_loss</td><td>0.44013</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/21ko10k3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/21ko10k3</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_030123-21ko10k3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:09:18,689] Trial 4 finished with values: [0.4401332608291081, 0.8117913832199547] and parameters: {'lr': 0.0002589870436544787, 'wd': 1.6816523172574557e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: filters=120, lr=1.32e-05, wd=1.36e-04, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_030918-ulv0dqtg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/ulv0dqtg' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/ulv0dqtg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/ulv0dqtg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7370 acc=0.5065 | val_loss=0.6936 acc=0.4762 | prec=0.4844 rec=0.7773 spec=0.1765 f1=0.5969 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7371 acc=0.4923 | val_loss=0.6937 acc=0.4671 | prec=0.4793 rec=0.7909 spec=0.1448 f1=0.5969 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7300 acc=0.5247 | val_loss=0.6937 acc=0.5011 | prec=0.5000 rec=0.0909 spec=0.9095 f1=0.1538 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7248 acc=0.5230 | val_loss=0.6936 acc=0.4989 | prec=0.4947 rec=0.2136 spec=0.7828 f1=0.2984 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7351 acc=0.5031 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7349 acc=0.4986 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7544 acc=0.4714 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7410 acc=0.4878 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7430 acc=0.4816 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7395 acc=0.4725 | val_loss=0.6983 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7259 acc=0.4963 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7543 acc=0.4589 | val_loss=0.6991 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7191 acc=0.5167 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7412 acc=0.4923 | val_loss=0.6974 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7391 acc=0.4946 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7260 acc=0.5122 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>██▃▄▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>████▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall</td><td>██▂▃▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>specificity</td><td>▁▁▇▆████████████</td></tr><tr><td>train_accuracy</td><td>▆▅██▆▅▂▄▃▂▅▁▇▅▅▇</td></tr><tr><td>train_loss</td><td>▅▅▃▂▄▄█▅▆▅▂█▁▅▅▂</td></tr><tr><td>validation_accuracy</td><td>▃▁██████████████</td></tr><tr><td>validation_loss</td><td>▁▁▁▁▂▄▄▄▂▇▅█▄▆▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>specificity</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.5122</td></tr><tr><td>train_loss</td><td>0.72597</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69472</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/ulv0dqtg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/ulv0dqtg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_030918-ulv0dqtg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:12:44,958] Trial 5 finished with values: [0.6947199617113385, 0.5011337868480725] and parameters: {'lr': 1.3154074881979716e-05, 'wd': 0.00013626547831827284}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: filters=120, lr=1.26e-05, wd=5.65e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_031244-vqdxom7s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/vqdxom7s' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/vqdxom7s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/vqdxom7s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7889 acc=0.4810 | val_loss=0.7005 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7749 acc=0.5054 | val_loss=0.6996 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7598 acc=0.4974 | val_loss=0.7003 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7766 acc=0.4935 | val_loss=0.6977 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7711 acc=0.4974 | val_loss=0.6996 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7495 acc=0.5150 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7734 acc=0.4884 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7628 acc=0.5077 | val_loss=0.6971 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7476 acc=0.5071 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7521 acc=0.5037 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7442 acc=0.5145 | val_loss=0.6977 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7431 acc=0.4974 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7522 acc=0.4838 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7540 acc=0.4963 | val_loss=0.6958 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7373 acc=0.5162 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7320 acc=0.5179 | val_loss=0.6969 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7396 acc=0.5099 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7398 acc=0.5077 | val_loss=0.6986 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 019 | train_loss=0.7483 acc=0.5048 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 020 | train_loss=0.7424 acc=0.5094 | val_loss=0.7019 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 021 | train_loss=0.7327 acc=0.5128 | val_loss=0.7029 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7354 acc=0.5122 | val_loss=0.7055 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7434 acc=0.4952 | val_loss=0.6981 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 024 | train_loss=0.7399 acc=0.5088 | val_loss=0.6998 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7310 acc=0.5026 | val_loss=0.6997 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_accuracy</td><td>▁▆▄▃▄▇▂▆▆▅▇▄▂▄██▆▆▆▆▇▇▄▆▅</td></tr><tr><td>train_loss</td><td>█▆▄▇▆▃▆▅▃▄▃▂▄▄▂▁▂▂▃▂▁▂▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▅▄▅▃▄▃▂▃▂▁▃▂▂▂▂▂▂▄▃▆▆█▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>specificity</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.50255</td></tr><tr><td>train_loss</td><td>0.73096</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69966</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/vqdxom7s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/vqdxom7s</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_031244-vqdxom7s/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:18:06,575] Trial 6 finished with values: [0.6996580234595707, 0.5011337868480725] and parameters: {'lr': 1.2563073624148532e-05, 'wd': 5.645037001313774e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: filters=120, lr=4.69e-04, wd=7.98e-06, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_031806-msby8teb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/msby8teb' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/msby8teb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/msby8teb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7458 acc=0.4997 | val_loss=0.7008 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7142 acc=0.5116 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7213 acc=0.4997 | val_loss=0.6926 acc=0.5624 | prec=0.5405 rec=0.8182 spec=0.3077 f1=0.6510 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7149 acc=0.4895 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7171 acc=0.4884 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7106 acc=0.4997 | val_loss=0.6919 acc=0.5578 | prec=0.5332 rec=0.9136 spec=0.2036 f1=0.6734 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7057 acc=0.5060 | val_loss=0.6897 acc=0.6417 | prec=0.6422 rec=0.6364 spec=0.6471 f1=0.6393 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7100 acc=0.5026 | val_loss=0.6615 acc=0.7120 | prec=0.6996 rec=0.7409 spec=0.6833 f1=0.7196 | time=12.9s\n",
            "Epoch 009 | train_loss=0.6502 acc=0.6143 | val_loss=0.5221 acc=0.7596 | prec=0.8132 rec=0.6727 spec=0.8462 f1=0.7363 | time=12.8s\n",
            "Epoch 010 | train_loss=0.5684 acc=0.7209 | val_loss=0.5071 acc=0.7959 | prec=0.8155 rec=0.7636 spec=0.8281 f1=0.7887 | time=12.8s\n",
            "Epoch 011 | train_loss=0.5312 acc=0.7396 | val_loss=0.4645 acc=0.8027 | prec=0.8065 rec=0.7955 spec=0.8100 f1=0.8009 | time=12.7s\n",
            "Epoch 012 | train_loss=0.4953 acc=0.7794 | val_loss=0.4588 acc=0.8095 | prec=0.9198 rec=0.6773 spec=0.9412 f1=0.7801 | time=12.8s\n",
            "Epoch 013 | train_loss=0.4712 acc=0.8071 | val_loss=0.4283 acc=0.8186 | prec=0.8125 rec=0.8273 spec=0.8100 f1=0.8198 | time=12.7s\n",
            "Epoch 014 | train_loss=0.4322 acc=0.8060 | val_loss=0.4181 acc=0.8390 | prec=0.8860 rec=0.7773 spec=0.9005 f1=0.8281 | time=12.8s\n",
            "Epoch 015 | train_loss=0.4083 acc=0.8259 | val_loss=0.4108 acc=0.8231 | prec=0.8060 rec=0.8500 spec=0.7964 f1=0.8274 | time=12.7s\n",
            "Epoch 016 | train_loss=0.3712 acc=0.8486 | val_loss=0.4785 acc=0.7846 | prec=0.9433 rec=0.6045 spec=0.9638 f1=0.7368 | time=12.9s\n",
            "Epoch 017 | train_loss=0.3339 acc=0.8661 | val_loss=0.3983 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.7s\n",
            "Epoch 018 | train_loss=0.3442 acc=0.8673 | val_loss=0.3843 acc=0.8118 | prec=0.7729 rec=0.8818 spec=0.7421 f1=0.8238 | time=12.8s\n",
            "Epoch 019 | train_loss=0.3322 acc=0.8673 | val_loss=0.4207 acc=0.8141 | prec=0.8450 rec=0.7682 spec=0.8597 f1=0.8048 | time=12.8s\n",
            "Epoch 020 | train_loss=0.3242 acc=0.8593 | val_loss=0.4862 acc=0.7778 | prec=0.7089 rec=0.9409 spec=0.6154 f1=0.8086 | time=12.9s\n",
            "Epoch 021 | train_loss=0.2576 acc=0.9007 | val_loss=0.4275 acc=0.8141 | prec=0.8416 rec=0.7727 spec=0.8552 f1=0.8057 | time=12.8s\n",
            "Epoch 022 | train_loss=0.2645 acc=0.8877 | val_loss=0.4636 acc=0.8027 | prec=0.7529 rec=0.9000 spec=0.7059 f1=0.8199 | time=12.7s\n",
            "Epoch 023 | train_loss=0.2678 acc=0.8866 | val_loss=0.5032 acc=0.7574 | prec=0.6877 rec=0.9409 spec=0.5747 f1=0.7946 | time=12.8s\n",
            "Epoch 024 | train_loss=0.2643 acc=0.8877 | val_loss=0.3990 acc=0.8163 | prec=0.8294 rec=0.7955 spec=0.8371 f1=0.8121 | time=12.8s\n",
            "Epoch 025 | train_loss=0.2518 acc=0.8939 | val_loss=0.5262 acc=0.7710 | prec=0.9161 rec=0.5955 spec=0.9457 f1=0.7218 | time=12.8s\n",
            "Epoch 026 | train_loss=0.2527 acc=0.8911 | val_loss=0.5169 acc=0.7528 | prec=0.6796 rec=0.9545 spec=0.5520 f1=0.7940 | time=12.7s\n",
            "Epoch 027 | train_loss=0.2180 acc=0.9013 | val_loss=0.4074 acc=0.8277 | prec=0.8636 rec=0.7773 spec=0.8778 f1=0.8182 | time=12.8s\n",
            "Epoch 028 | train_loss=0.2002 acc=0.9121 | val_loss=0.4193 acc=0.8209 | prec=0.8615 rec=0.7636 spec=0.8778 f1=0.8096 | time=12.8s\n",
            "Epoch 029 | train_loss=0.2284 acc=0.8951 | val_loss=0.4783 acc=0.8027 | prec=0.7472 rec=0.9136 spec=0.6923 f1=0.8221 | time=12.8s\n",
            "Epoch 030 | train_loss=0.3083 acc=0.8622 | val_loss=0.4287 acc=0.8367 | prec=0.8162 rec=0.8682 spec=0.8054 f1=0.8414 | time=12.6s\n",
            "Epoch 031 | train_loss=0.2707 acc=0.9030 | val_loss=0.4392 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=12.7s\n",
            "Epoch 032 | train_loss=0.2048 acc=0.9087 | val_loss=0.4690 acc=0.8163 | prec=0.7814 rec=0.8773 spec=0.7557 f1=0.8266 | time=12.7s\n",
            "Epoch 033 | train_loss=0.1895 acc=0.9144 | val_loss=0.4624 acc=0.8163 | prec=0.8009 rec=0.8409 spec=0.7919 f1=0.8204 | time=12.7s\n",
            "Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▆▇▇▇▆▇▇██▇███▇████████▇████████</td></tr><tr><td>precision</td><td>▁▁▅▅▅▅▆▆▇▇▇█▇█▇█▇▇▇▆▇▇▆▇█▆▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▇██▇▅▆▆▆▇▆▇▆▇▅▇▇▆█▆▇█▇▅█▆▆▇▇▇▇▇</td></tr><tr><td>specificity</td><td>██▃▁▁▂▆▆▇▇▇█▇▇▇█▇▆▇▅▇▆▅▇█▅▇▇▆▇▇▆▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▃▅▅▆▆▆▇▇▇▇▇▇█████████▇███</td></tr><tr><td>train_loss</td><td>██████▇█▇▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▂▁▁▂▄▅▆▇▇▇███▇█▇▇▇▇▇▆▇▇▆██▇██▇▇</td></tr><tr><td>validation_loss</td><td>███████▇▄▄▃▃▂▂▂▃▁▁▂▃▂▃▄▁▄▄▂▂▃▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.8204</td></tr><tr><td>precision</td><td>0.80087</td></tr><tr><td>recall</td><td>0.84091</td></tr><tr><td>specificity</td><td>0.79186</td></tr><tr><td>train_accuracy</td><td>0.91435</td></tr><tr><td>train_loss</td><td>0.18948</td></tr><tr><td>validation_accuracy</td><td>0.81633</td></tr><tr><td>validation_loss</td><td>0.46239</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/msby8teb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/msby8teb</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_031806-msby8teb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:25:10,145] Trial 7 finished with values: [0.46238577153001514, 0.8163265306122449] and parameters: {'lr': 0.00046905486683096334, 'wd': 7.976802247969639e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: filters=120, lr=2.59e-05, wd=2.74e-04, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_032510-tp6il17b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/tp6il17b' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/tp6il17b' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/tp6il17b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7494 acc=0.5162 | val_loss=0.6982 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 002 | train_loss=0.7603 acc=0.5060 | val_loss=0.6930 acc=0.5079 | prec=0.5036 rec=0.9591 spec=0.0588 f1=0.6604 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7399 acc=0.5218 | val_loss=0.6931 acc=0.5125 | prec=0.6923 rec=0.0409 spec=0.9819 f1=0.0773 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7604 acc=0.4895 | val_loss=0.6927 acc=0.5034 | prec=0.5034 rec=0.3318 spec=0.6742 f1=0.4000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7574 acc=0.4957 | val_loss=0.6976 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7350 acc=0.4997 | val_loss=0.6966 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7367 acc=0.5071 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7431 acc=0.4912 | val_loss=0.7006 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7457 acc=0.4991 | val_loss=0.6975 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7571 acc=0.4918 | val_loss=0.6978 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7395 acc=0.4963 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7362 acc=0.4957 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7507 acc=0.4889 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7480 acc=0.4793 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7263 acc=0.5196 | val_loss=0.6998 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7326 acc=0.5173 | val_loss=0.6963 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7265 acc=0.5156 | val_loss=0.6972 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 018 | train_loss=0.7342 acc=0.5037 | val_loss=0.6983 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7253 acc=0.5105 | val_loss=0.6957 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>██▂▅▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▆▆█▆▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>recall</td><td>██▁▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>specificity</td><td>▁▁█▆███████████████</td></tr><tr><td>train_accuracy</td><td>▇▅█▃▄▄▆▃▄▃▄▄▃▁█▇▇▅▆</td></tr><tr><td>train_loss</td><td>▆█▄█▇▃▃▅▅▇▄▃▆▆▁▂▁▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▆█▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>validation_loss</td><td>▆▁▁▁▅▄▃█▅▅▂▂▄▃▇▄▅▆▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>specificity</td><td>1</td></tr><tr><td>train_accuracy</td><td>0.51049</td></tr><tr><td>train_loss</td><td>0.72535</td></tr><tr><td>validation_accuracy</td><td>0.50113</td></tr><tr><td>validation_loss</td><td>0.69569</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/tp6il17b' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/tp6il17b</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_032510-tp6il17b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:29:14,647] Trial 8 finished with values: [0.6956918665340969, 0.5011337868480725] and parameters: {'lr': 2.5940525771073308e-05, 'wd': 0.0002742079399582553}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: filters=120, lr=2.22e-05, wd=1.91e-05, pct_start=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_032914-zvzz6q4j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/zvzz6q4j' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/zvzz6q4j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/zvzz6q4j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7443 acc=0.5213 | val_loss=0.7111 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7519 acc=0.4838 | val_loss=0.7098 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 003 | train_loss=0.7445 acc=0.4929 | val_loss=0.7014 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7377 acc=0.4957 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7345 acc=0.4940 | val_loss=0.6987 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7316 acc=0.5060 | val_loss=0.7009 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7307 acc=0.5088 | val_loss=0.6992 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7218 acc=0.5105 | val_loss=0.7000 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7244 acc=0.4878 | val_loss=0.7008 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 010 | train_loss=0.7108 acc=0.5145 | val_loss=0.7013 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7084 acc=0.5043 | val_loss=0.6992 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7110 acc=0.5060 | val_loss=0.6983 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7234 acc=0.4787 | val_loss=0.6983 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7004 acc=0.5286 | val_loss=0.6979 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7069 acc=0.4929 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7149 acc=0.4617 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7103 acc=0.4991 | val_loss=0.6961 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7087 acc=0.4889 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 019 | train_loss=0.7163 acc=0.4855 | val_loss=0.6995 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7103 acc=0.4980 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7113 acc=0.4889 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 022 | train_loss=0.6952 acc=0.5014 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7019 acc=0.5037 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7087 acc=0.5009 | val_loss=0.6963 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7064 acc=0.4923 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 026 | train_loss=0.7037 acc=0.4991 | val_loss=0.6923 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.7s\n",
            "Epoch 027 | train_loss=0.7019 acc=0.4991 | val_loss=0.6918 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 028 | train_loss=0.7020 acc=0.5060 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 029 | train_loss=0.6974 acc=0.5190 | val_loss=0.6884 acc=0.5125 | prec=1.0000 rec=0.0227 spec=1.0000 f1=0.0444 | time=12.6s\n",
            "Epoch 030 | train_loss=0.6981 acc=0.5303 | val_loss=0.6924 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 031 | train_loss=0.6836 acc=0.5542 | val_loss=0.6652 acc=0.6916 | prec=0.7958 rec=0.5136 spec=0.8688 f1=0.6243 | time=12.9s\n",
            "Epoch 032 | train_loss=0.6661 acc=0.5882 | val_loss=0.6471 acc=0.6236 | prec=0.8649 rec=0.2909 spec=0.9548 f1=0.4354 | time=12.8s\n",
            "Epoch 033 | train_loss=0.6504 acc=0.6427 | val_loss=0.6556 acc=0.5714 | prec=0.9429 rec=0.1500 spec=0.9910 f1=0.2588 | time=12.7s\n",
            "Epoch 034 | train_loss=0.6123 acc=0.6863 | val_loss=0.5933 acc=0.7528 | prec=0.7846 rec=0.6955 spec=0.8100 f1=0.7373 | time=12.8s\n",
            "Epoch 035 | train_loss=0.6107 acc=0.6875 | val_loss=0.5743 acc=0.7392 | prec=0.9200 rec=0.5227 spec=0.9548 f1=0.6667 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5676 acc=0.7005 | val_loss=0.5390 acc=0.7551 | prec=0.9058 rec=0.5682 spec=0.9412 f1=0.6983 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5686 acc=0.7204 | val_loss=0.5000 acc=0.7937 | prec=0.8177 rec=0.7545 spec=0.8326 f1=0.7849 | time=12.8s\n",
            "Epoch 038 | train_loss=0.5556 acc=0.7226 | val_loss=0.5099 acc=0.7868 | prec=0.8580 rec=0.6864 spec=0.8869 f1=0.7626 | time=12.9s\n",
            "Epoch 039 | train_loss=0.5255 acc=0.7499 | val_loss=0.4945 acc=0.8005 | prec=0.8626 rec=0.7136 spec=0.8869 f1=0.7811 | time=12.8s\n",
            "Epoch 040 | train_loss=0.5260 acc=0.7499 | val_loss=0.4911 acc=0.8073 | prec=0.8462 rec=0.7500 spec=0.8643 f1=0.7952 | time=12.8s\n",
            "Epoch 041 | train_loss=0.5168 acc=0.7527 | val_loss=0.4695 acc=0.8118 | prec=0.8513 rec=0.7545 spec=0.8688 f1=0.8000 | time=12.8s\n",
            "Epoch 042 | train_loss=0.5001 acc=0.7754 | val_loss=0.4721 acc=0.8141 | prec=0.8594 rec=0.7500 spec=0.8778 f1=0.8010 | time=12.7s\n",
            "Epoch 043 | train_loss=0.4921 acc=0.7816 | val_loss=0.4901 acc=0.8027 | prec=0.7806 rec=0.8409 spec=0.7647 f1=0.8096 | time=12.8s\n",
            "Epoch 044 | train_loss=0.4941 acc=0.7799 | val_loss=0.4630 acc=0.8186 | prec=0.8646 rec=0.7545 spec=0.8824 f1=0.8058 | time=12.9s\n",
            "Epoch 045 | train_loss=0.4787 acc=0.7975 | val_loss=0.4630 acc=0.8231 | prec=0.8622 rec=0.7682 spec=0.8778 f1=0.8125 | time=12.7s\n",
            "Epoch 046 | train_loss=0.4845 acc=0.7924 | val_loss=0.4555 acc=0.8254 | prec=0.8295 rec=0.8182 spec=0.8326 f1=0.8238 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4686 acc=0.8088 | val_loss=0.4766 acc=0.8209 | prec=0.7925 rec=0.8682 spec=0.7738 f1=0.8286 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4559 acc=0.8077 | val_loss=0.4427 acc=0.8299 | prec=0.8404 rec=0.8136 spec=0.8462 f1=0.8268 | time=12.7s\n",
            "Epoch 049 | train_loss=0.4568 acc=0.7941 | val_loss=0.4584 acc=0.8345 | prec=0.7976 rec=0.8955 spec=0.7738 f1=0.8437 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4510 acc=0.8083 | val_loss=0.4437 acc=0.8277 | prec=0.8186 rec=0.8409 spec=0.8145 f1=0.8296 | time=12.7s\n",
            "Epoch 051 | train_loss=0.4487 acc=0.8162 | val_loss=0.4611 acc=0.8345 | prec=0.7952 rec=0.9000 spec=0.7692 f1=0.8443 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4465 acc=0.8242 | val_loss=0.4287 acc=0.8481 | prec=0.8964 rec=0.7864 spec=0.9095 f1=0.8378 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4379 acc=0.8230 | val_loss=0.4262 acc=0.8526 | prec=0.8571 rec=0.8455 spec=0.8597 f1=0.8513 | time=12.7s\n",
            "Epoch 054 | train_loss=0.4205 acc=0.8242 | val_loss=0.4225 acc=0.8481 | prec=0.8228 rec=0.8864 spec=0.8100 f1=0.8534 | time=12.7s\n",
            "Epoch 055 | train_loss=0.4082 acc=0.8304 | val_loss=0.4147 acc=0.8549 | prec=0.8679 rec=0.8364 spec=0.8733 f1=0.8519 | time=12.9s\n",
            "Epoch 056 | train_loss=0.4109 acc=0.8406 | val_loss=0.3937 acc=0.8526 | prec=0.8673 rec=0.8318 spec=0.8733 f1=0.8492 | time=12.7s\n",
            "Epoch 057 | train_loss=0.3964 acc=0.8423 | val_loss=0.4055 acc=0.8526 | prec=0.8744 rec=0.8227 spec=0.8824 f1=0.8478 | time=12.8s\n",
            "Epoch 058 | train_loss=0.4011 acc=0.8406 | val_loss=0.3932 acc=0.8549 | prec=0.8714 rec=0.8318 spec=0.8778 f1=0.8512 | time=12.8s\n",
            "Epoch 059 | train_loss=0.3856 acc=0.8446 | val_loss=0.3852 acc=0.8503 | prec=0.8598 rec=0.8364 spec=0.8643 f1=0.8479 | time=12.8s\n",
            "Epoch 060 | train_loss=0.3778 acc=0.8457 | val_loss=0.3912 acc=0.8549 | prec=0.8786 rec=0.8227 spec=0.8869 f1=0.8498 | time=12.7s\n",
            "Epoch 061 | train_loss=0.3683 acc=0.8571 | val_loss=0.3845 acc=0.8594 | prec=0.8762 rec=0.8364 spec=0.8824 f1=0.8558 | time=12.7s\n",
            "Epoch 062 | train_loss=0.3557 acc=0.8724 | val_loss=0.3901 acc=0.8617 | prec=0.9036 rec=0.8091 spec=0.9140 f1=0.8537 | time=12.7s\n",
            "Epoch 063 | train_loss=0.3828 acc=0.8503 | val_loss=0.4068 acc=0.8345 | prec=0.7860 rec=0.9182 spec=0.7511 f1=0.8470 | time=12.8s\n",
            "Epoch 064 | train_loss=0.3501 acc=0.8769 | val_loss=0.3982 acc=0.8435 | prec=0.9171 rec=0.7545 spec=0.9321 f1=0.8279 | time=12.8s\n",
            "Epoch 065 | train_loss=0.3410 acc=0.8769 | val_loss=0.3744 acc=0.8549 | prec=0.8514 rec=0.8591 spec=0.8507 f1=0.8552 | time=12.7s\n",
            "Epoch 066 | train_loss=0.3414 acc=0.8633 | val_loss=0.3843 acc=0.8617 | prec=0.8597 rec=0.8636 spec=0.8597 f1=0.8617 | time=12.7s\n",
            "Epoch 067 | train_loss=0.3336 acc=0.8803 | val_loss=0.3753 acc=0.8503 | prec=0.8377 rec=0.8682 spec=0.8326 f1=0.8527 | time=12.7s\n",
            "Epoch 068 | train_loss=0.3360 acc=0.8684 | val_loss=0.4106 acc=0.8299 | prec=0.7715 rec=0.9364 spec=0.7240 f1=0.8460 | time=12.7s\n",
            "Epoch 069 | train_loss=0.3333 acc=0.8809 | val_loss=0.3664 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=12.7s\n",
            "Epoch 070 | train_loss=0.3337 acc=0.8860 | val_loss=0.3718 acc=0.8526 | prec=0.8934 rec=0.8000 spec=0.9050 f1=0.8441 | time=12.7s\n",
            "Epoch 071 | train_loss=0.3172 acc=0.8803 | val_loss=0.3728 acc=0.8481 | prec=0.8806 rec=0.8045 spec=0.8914 f1=0.8409 | time=12.8s\n",
            "Epoch 072 | train_loss=0.3130 acc=0.8900 | val_loss=0.3718 acc=0.8458 | prec=0.8115 rec=0.9000 spec=0.7919 f1=0.8534 | time=12.8s\n",
            "Epoch 073 | train_loss=0.2979 acc=0.8973 | val_loss=0.3772 acc=0.8435 | prec=0.8186 rec=0.8818 spec=0.8054 f1=0.8490 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3100 acc=0.8820 | val_loss=0.3687 acc=0.8481 | prec=0.8255 rec=0.8818 spec=0.8145 f1=0.8527 | time=13.0s\n",
            "Epoch 075 | train_loss=0.3099 acc=0.8917 | val_loss=0.3963 acc=0.8322 | prec=0.7852 rec=0.9136 spec=0.7511 f1=0.8445 | time=12.8s\n",
            "Epoch 076 | train_loss=0.2889 acc=0.9024 | val_loss=0.3690 acc=0.8458 | prec=0.8725 rec=0.8091 spec=0.8824 f1=0.8396 | time=12.7s\n",
            "Epoch 077 | train_loss=0.2736 acc=0.9087 | val_loss=0.3856 acc=0.8322 | prec=0.7944 rec=0.8955 spec=0.7692 f1=0.8419 | time=12.8s\n",
            "Epoch 078 | train_loss=0.2780 acc=0.8945 | val_loss=0.3788 acc=0.8481 | prec=0.8732 rec=0.8136 spec=0.8824 f1=0.8424 | time=12.8s\n",
            "Epoch 079 | train_loss=0.2837 acc=0.9121 | val_loss=0.3738 acc=0.8390 | prec=0.8091 rec=0.8864 spec=0.7919 f1=0.8460 | time=12.7s\n",
            "Epoch 080 | train_loss=0.2775 acc=0.9013 | val_loss=0.4274 acc=0.7982 | prec=0.7365 rec=0.9273 spec=0.6697 f1=0.8209 | time=12.7s\n",
            "Epoch 081 | train_loss=0.2786 acc=0.8962 | val_loss=0.3695 acc=0.8458 | prec=0.8619 rec=0.8227 spec=0.8688 f1=0.8419 | time=12.8s\n",
            "Epoch 082 | train_loss=0.2668 acc=0.9024 | val_loss=0.3871 acc=0.8503 | prec=0.9010 rec=0.7864 spec=0.9140 f1=0.8398 | time=12.7s\n",
            "Epoch 083 | train_loss=0.2839 acc=0.9053 | val_loss=0.3746 acc=0.8526 | prec=0.8856 rec=0.8091 spec=0.8959 f1=0.8456 | time=12.8s\n",
            "Epoch 084 | train_loss=0.2691 acc=0.9007 | val_loss=0.3847 acc=0.8299 | prec=0.7935 rec=0.8909 spec=0.7692 f1=0.8394 | time=12.8s\n",
            "Early stopping at epoch 84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▇▇▇██████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁██▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▆▇▆▇▇█▇█▇▇█▇▇█▇█▇▇███▇█</td></tr><tr><td>specificity</td><td>██████████▅▇█▄▇▆▅▅▃▃▄▃▅▅▅▅▅▆▇▅▄▂▅▆▄▆▄▁▆▃</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>train_loss</td><td>██████████▇▇▇▇▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▃▂▆▇▇▇▇▇▇▇▇▇█████████▇█████</td></tr><tr><td>validation_loss</td><td>████████████████▇▇▅▄▄▃▃▃▃▃▃▂▃▂▁▁▁▂▂▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.8394</td></tr><tr><td>precision</td><td>0.79352</td></tr><tr><td>recall</td><td>0.89091</td></tr><tr><td>specificity</td><td>0.76923</td></tr><tr><td>train_accuracy</td><td>0.90074</td></tr><tr><td>train_loss</td><td>0.26914</td></tr><tr><td>validation_accuracy</td><td>0.82993</td></tr><tr><td>validation_loss</td><td>0.38471</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/zvzz6q4j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6/runs/zvzz6q4j</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_032914-zvzz6q4j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 03:47:10,402] Trial 9 finished with values: [0.38471304305962156, 0.8299319727891157] and parameters: {'lr': 2.2239463334903503e-05, 'wd': 1.9124782834348135e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pareto-optimal trials (val_loss, val_acc) and their params:\n",
            " Trial #2: values=[0.4335672802158764, 0.8390022675736961]\n",
            "              params={'lr': 6.704038535844278e-05, 'wd': 1.1003236848381844e-05}\n",
            " Trial #9: values=[0.38471304305962156, 0.8299319727891157]\n",
            "              params={'lr': 2.2239463334903503e-05, 'wd': 1.9124782834348135e-05}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### After decreasing the dropout rate and alleviate regularization (Best Result)\n",
        "\n",
        "1] Trial 2: filters=120, lr=6.70e-05, wd=1.10e-05, pct_start=0.23\n",
        "* Epoch 062 | train_loss=0.1612 acc=0.9234 | val_loss=0.4336 acc=0.8390 | prec=0.7922 rec=0.9182 spec=0.7602 f1=0.8505 | time=12.9s\n",
        "Early stopping at epoch 62\n",
        "\n",
        "2] Trial 9: filters=120, lr=2.22e-05, wd=1.91e-05, pct_start=0.23\n",
        "* Epoch 084 | train_loss=0.2691 acc=0.9007 | val_loss=0.3847 acc=0.8299 | prec=0.7935 rec=0.8909 spec=0.7692 f1=0.8394 | time=12.8s\n",
        "Early stopping at epoch 84"
      ],
      "metadata": {
        "id": "4xyWAl7O6XFA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Decrease the search space and try again\n",
        "- Change the MAX EPOCH to 150\n",
        "- Decrease dropout rate from 0.3 to 0.2 in CNNDecoder"
      ],
      "metadata": {
        "id": "93dQnWyyEJ3Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_7 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr          = trial.suggest_float('lr', 1e-5, 2e-4, log=True)\n",
        "    wd          = trial.suggest_float('wd', 1e-6, 1e-4, log=True)\n",
        "    pct_start   = 0.2  # fixed\n",
        "    num_filters = 120\n",
        "\n",
        "    print(f\"Trial {trial.number}: filters={num_filters}, lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold-7',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr': lr, 'wd': wd, 'pct_start': pct_start, 'filters': num_filters},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len = dataset[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=num_filters,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Dynamic Weight Decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr / lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss  += loss.item()\n",
        "\n",
        "                preds   = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss   = vloss / len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds == val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "\n",
        "        # Compute specificity = TN / (TN + FP)\n",
        "        tn, fp, fn, tp = confusion_matrix(val_labels, val_preds).ravel()\n",
        "        specificity     = tn / (tn + fp)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={precision:.4f} rec={recall:.4f} spec={specificity:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize', 'maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ─── Print Pareto-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ribSXlsE6XMd",
        "outputId": "ce909dde-f305-409d-faef-a06adb216594"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 05:48:13,942] A new study created in memory with name: no-name-32fbebaf-7b0f-4b2e-ba90-1dd2cf065fb4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: filters=120, lr=1.26e-05, wd=1.65e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_054813-4i481zxg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/4i481zxg' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/4i481zxg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/4i481zxg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7557 acc=0.4912 | val_loss=0.7003 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=31.3s\n",
            "Epoch 002 | train_loss=0.7505 acc=0.4940 | val_loss=0.6980 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7416 acc=0.5122 | val_loss=0.6940 acc=0.4966 | prec=0.4970 rec=0.7545 spec=0.2398 f1=0.5993 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7259 acc=0.5133 | val_loss=0.6941 acc=0.5102 | prec=0.5062 rec=0.7409 spec=0.2805 f1=0.6015 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7220 acc=0.5082 | val_loss=0.6945 acc=0.5011 | prec=0.5000 rec=0.9773 spec=0.0271 f1=0.6615 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7226 acc=0.5065 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7196 acc=0.5133 | val_loss=0.6938 acc=0.5034 | prec=0.5011 rec=0.9909 spec=0.0181 f1=0.6656 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7061 acc=0.5224 | val_loss=0.6936 acc=0.4966 | prec=0.4975 rec=0.9091 spec=0.0860 f1=0.6431 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7138 acc=0.5048 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7141 acc=0.5133 | val_loss=0.6935 acc=0.5034 | prec=0.5012 rec=0.9318 spec=0.0769 f1=0.6518 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7213 acc=0.4901 | val_loss=0.6933 acc=0.5170 | prec=0.5185 rec=0.4455 spec=0.5882 f1=0.4792 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7186 acc=0.4912 | val_loss=0.6935 acc=0.4943 | prec=0.4966 rec=0.9864 spec=0.0045 f1=0.6606 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7113 acc=0.5077 | val_loss=0.6934 acc=0.4853 | prec=0.4901 rec=0.7864 spec=0.1855 f1=0.6038 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7123 acc=0.5128 | val_loss=0.6932 acc=0.5034 | prec=0.5012 rec=0.9318 spec=0.0769 f1=0.6518 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7091 acc=0.5133 | val_loss=0.6931 acc=0.5034 | prec=0.5011 rec=0.9909 spec=0.0181 f1=0.6656 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7189 acc=0.4901 | val_loss=0.6929 acc=0.5147 | prec=0.5078 rec=0.8909 spec=0.1403 f1=0.6469 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7171 acc=0.4946 | val_loss=0.6930 acc=0.5079 | prec=0.5039 rec=0.8864 spec=0.1312 f1=0.6425 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7041 acc=0.5145 | val_loss=0.6928 acc=0.5034 | prec=0.5022 rec=0.5091 spec=0.4977 f1=0.5056 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7154 acc=0.4918 | val_loss=0.6930 acc=0.5125 | prec=0.5058 rec=0.9909 spec=0.0362 f1=0.6697 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7085 acc=0.5020 | val_loss=0.6925 acc=0.5215 | prec=0.5153 rec=0.6909 spec=0.3529 f1=0.5903 | time=12.8s\n",
            "Epoch 021 | train_loss=0.7080 acc=0.5139 | val_loss=0.6920 acc=0.4989 | prec=0.4980 rec=0.5682 spec=0.4299 f1=0.5308 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7090 acc=0.5133 | val_loss=0.6916 acc=0.5215 | prec=0.5584 rec=0.1955 spec=0.8462 f1=0.2896 | time=12.9s\n",
            "Epoch 023 | train_loss=0.7119 acc=0.5054 | val_loss=0.6896 acc=0.5601 | prec=0.5357 rec=0.8864 spec=0.2353 f1=0.6678 | time=12.9s\n",
            "Epoch 024 | train_loss=0.7102 acc=0.5292 | val_loss=0.6856 acc=0.5850 | prec=0.5649 rec=0.7318 spec=0.4389 f1=0.6376 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7006 acc=0.5320 | val_loss=0.6794 acc=0.5805 | prec=0.5450 rec=0.9636 spec=0.1991 f1=0.6962 | time=12.8s\n",
            "Epoch 026 | train_loss=0.6901 acc=0.5400 | val_loss=0.6762 acc=0.6032 | prec=0.8571 rec=0.2455 spec=0.9593 f1=0.3816 | time=12.8s\n",
            "Epoch 027 | train_loss=0.6623 acc=0.5956 | val_loss=0.6436 acc=0.6916 | prec=0.7121 rec=0.6409 spec=0.7421 f1=0.6746 | time=12.8s\n",
            "Epoch 028 | train_loss=0.6610 acc=0.5854 | val_loss=0.6235 acc=0.7438 | prec=0.8452 rec=0.5955 spec=0.8914 f1=0.6987 | time=12.9s\n",
            "Epoch 029 | train_loss=0.6582 acc=0.6143 | val_loss=0.6132 acc=0.7574 | prec=0.7756 rec=0.7227 spec=0.7919 f1=0.7482 | time=12.8s\n",
            "Epoch 030 | train_loss=0.6191 acc=0.6653 | val_loss=0.5974 acc=0.7778 | prec=0.8280 rec=0.7000 spec=0.8552 f1=0.7586 | time=12.8s\n",
            "Epoch 031 | train_loss=0.6161 acc=0.6580 | val_loss=0.5766 acc=0.7800 | prec=0.8361 rec=0.6955 spec=0.8643 f1=0.7593 | time=12.9s\n",
            "Epoch 032 | train_loss=0.5936 acc=0.6846 | val_loss=0.5684 acc=0.8005 | prec=0.8367 rec=0.7455 spec=0.8552 f1=0.7885 | time=12.9s\n",
            "Epoch 033 | train_loss=0.5941 acc=0.6954 | val_loss=0.5557 acc=0.7868 | prec=0.8580 rec=0.6864 spec=0.8869 f1=0.7626 | time=12.9s\n",
            "Epoch 034 | train_loss=0.5887 acc=0.6965 | val_loss=0.5433 acc=0.7937 | prec=0.8728 rec=0.6864 spec=0.9005 f1=0.7684 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5897 acc=0.7033 | val_loss=0.5269 acc=0.7891 | prec=0.8470 rec=0.7045 spec=0.8733 f1=0.7692 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5760 acc=0.7050 | val_loss=0.5366 acc=0.8005 | prec=0.8548 rec=0.7227 spec=0.8778 f1=0.7833 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5693 acc=0.7153 | val_loss=0.5381 acc=0.7959 | prec=0.8186 rec=0.7591 spec=0.8326 f1=0.7877 | time=12.8s\n",
            "Epoch 038 | train_loss=0.5744 acc=0.6999 | val_loss=0.5272 acc=0.7959 | prec=0.8066 rec=0.7773 spec=0.8145 f1=0.7917 | time=12.9s\n",
            "Epoch 039 | train_loss=0.5517 acc=0.7323 | val_loss=0.5297 acc=0.8073 | prec=0.8497 rec=0.7455 spec=0.8688 f1=0.7942 | time=12.9s\n",
            "Epoch 040 | train_loss=0.5458 acc=0.7516 | val_loss=0.5192 acc=0.7914 | prec=0.8596 rec=0.6955 spec=0.8869 f1=0.7688 | time=12.8s\n",
            "Epoch 041 | train_loss=0.5384 acc=0.7567 | val_loss=0.5110 acc=0.7959 | prec=0.8125 rec=0.7682 spec=0.8235 f1=0.7897 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5422 acc=0.7538 | val_loss=0.4997 acc=0.7982 | prec=0.8018 rec=0.7909 spec=0.8054 f1=0.7963 | time=12.8s\n",
            "Epoch 043 | train_loss=0.5183 acc=0.7618 | val_loss=0.4920 acc=0.8073 | prec=0.8534 rec=0.7409 spec=0.8733 f1=0.7932 | time=12.8s\n",
            "Epoch 044 | train_loss=0.5219 acc=0.7635 | val_loss=0.4825 acc=0.8027 | prec=0.8009 rec=0.8045 spec=0.8009 f1=0.8027 | time=12.9s\n",
            "Epoch 045 | train_loss=0.5065 acc=0.7680 | val_loss=0.4865 acc=0.8050 | prec=0.8131 rec=0.7909 spec=0.8190 f1=0.8018 | time=12.8s\n",
            "Epoch 046 | train_loss=0.4934 acc=0.7845 | val_loss=0.4758 acc=0.8118 | prec=0.8186 rec=0.8000 spec=0.8235 f1=0.8092 | time=12.8s\n",
            "Epoch 047 | train_loss=0.5096 acc=0.7811 | val_loss=0.4695 acc=0.8163 | prec=0.8325 rec=0.7909 spec=0.8416 f1=0.8112 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4896 acc=0.7760 | val_loss=0.4557 acc=0.8073 | prec=0.8000 rec=0.8182 spec=0.7964 f1=0.8090 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4806 acc=0.7714 | val_loss=0.4653 acc=0.8118 | prec=0.8785 rec=0.7227 spec=0.9005 f1=0.7930 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4969 acc=0.7737 | val_loss=0.4513 acc=0.8186 | prec=0.8608 rec=0.7591 spec=0.8778 f1=0.8068 | time=12.8s\n",
            "Epoch 051 | train_loss=0.4637 acc=0.7930 | val_loss=0.4426 acc=0.8209 | prec=0.8691 rec=0.7545 spec=0.8869 f1=0.8078 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4688 acc=0.7969 | val_loss=0.4480 acc=0.8163 | prec=0.8174 rec=0.8136 spec=0.8190 f1=0.8155 | time=12.7s\n",
            "Epoch 053 | train_loss=0.4592 acc=0.7975 | val_loss=0.4574 acc=0.8095 | prec=0.9000 rec=0.6955 spec=0.9231 f1=0.7846 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4660 acc=0.8054 | val_loss=0.4226 acc=0.8209 | prec=0.8579 rec=0.7682 spec=0.8733 f1=0.8106 | time=12.7s\n",
            "Epoch 055 | train_loss=0.4580 acc=0.8015 | val_loss=0.4419 acc=0.8209 | prec=0.8895 rec=0.7318 spec=0.9095 f1=0.8030 | time=12.8s\n",
            "Epoch 056 | train_loss=0.4625 acc=0.8071 | val_loss=0.4338 acc=0.8299 | prec=0.8502 rec=0.8000 spec=0.8597 f1=0.8244 | time=12.8s\n",
            "Epoch 057 | train_loss=0.4675 acc=0.8100 | val_loss=0.4266 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=12.9s\n",
            "Epoch 058 | train_loss=0.4517 acc=0.8145 | val_loss=0.4335 acc=0.8231 | prec=0.8817 rec=0.7455 spec=0.9005 f1=0.8079 | time=12.8s\n",
            "Epoch 059 | train_loss=0.4595 acc=0.8111 | val_loss=0.4153 acc=0.8322 | prec=0.8724 rec=0.7773 spec=0.8869 f1=0.8221 | time=12.9s\n",
            "Epoch 060 | train_loss=0.4472 acc=0.8054 | val_loss=0.4187 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=12.8s\n",
            "Epoch 061 | train_loss=0.4284 acc=0.8270 | val_loss=0.4242 acc=0.8186 | prec=0.8933 rec=0.7227 spec=0.9140 f1=0.7990 | time=12.7s\n",
            "Epoch 062 | train_loss=0.4356 acc=0.8230 | val_loss=0.4172 acc=0.8481 | prec=0.8462 rec=0.8500 spec=0.8462 f1=0.8481 | time=12.9s\n",
            "Epoch 063 | train_loss=0.4233 acc=0.8111 | val_loss=0.4147 acc=0.8481 | prec=0.8430 rec=0.8545 spec=0.8416 f1=0.8488 | time=12.8s\n",
            "Epoch 064 | train_loss=0.4121 acc=0.8202 | val_loss=0.4064 acc=0.8345 | prec=0.8808 rec=0.7727 spec=0.8959 f1=0.8232 | time=12.9s\n",
            "Epoch 065 | train_loss=0.4200 acc=0.8287 | val_loss=0.4108 acc=0.8231 | prec=0.8060 rec=0.8500 spec=0.7964 f1=0.8274 | time=12.9s\n",
            "Epoch 066 | train_loss=0.4150 acc=0.8293 | val_loss=0.3996 acc=0.8571 | prec=0.8792 rec=0.8273 spec=0.8869 f1=0.8525 | time=12.8s\n",
            "Epoch 067 | train_loss=0.4059 acc=0.8298 | val_loss=0.3939 acc=0.8549 | prec=0.8714 rec=0.8318 spec=0.8778 f1=0.8512 | time=12.8s\n",
            "Epoch 068 | train_loss=0.4162 acc=0.8157 | val_loss=0.4093 acc=0.8186 | prec=0.9023 rec=0.7136 spec=0.9231 f1=0.7970 | time=12.7s\n",
            "Epoch 069 | train_loss=0.4163 acc=0.8219 | val_loss=0.3882 acc=0.8526 | prec=0.8818 rec=0.8136 spec=0.8914 f1=0.8463 | time=12.8s\n",
            "Epoch 070 | train_loss=0.3944 acc=0.8395 | val_loss=0.3903 acc=0.8435 | prec=0.8912 rec=0.7818 spec=0.9050 f1=0.8329 | time=12.8s\n",
            "Epoch 071 | train_loss=0.4015 acc=0.8383 | val_loss=0.4012 acc=0.8254 | prec=0.9040 rec=0.7273 spec=0.9231 f1=0.8060 | time=12.8s\n",
            "Epoch 072 | train_loss=0.3970 acc=0.8230 | val_loss=0.3921 acc=0.8481 | prec=0.8696 rec=0.8182 spec=0.8778 f1=0.8431 | time=12.8s\n",
            "Epoch 073 | train_loss=0.4081 acc=0.8383 | val_loss=0.3861 acc=0.8458 | prec=0.8958 rec=0.7818 spec=0.9095 f1=0.8350 | time=12.8s\n",
            "Epoch 074 | train_loss=0.4024 acc=0.8372 | val_loss=0.3797 acc=0.8526 | prec=0.8934 rec=0.8000 spec=0.9050 f1=0.8441 | time=12.9s\n",
            "Epoch 075 | train_loss=0.3945 acc=0.8474 | val_loss=0.3744 acc=0.8571 | prec=0.8756 rec=0.8318 spec=0.8824 f1=0.8531 | time=12.8s\n",
            "Epoch 076 | train_loss=0.3878 acc=0.8406 | val_loss=0.3728 acc=0.8458 | prec=0.8800 rec=0.8000 spec=0.8914 f1=0.8381 | time=12.9s\n",
            "Epoch 077 | train_loss=0.3932 acc=0.8327 | val_loss=0.3829 acc=0.8277 | prec=0.9000 rec=0.7364 spec=0.9186 f1=0.8100 | time=12.8s\n",
            "Epoch 078 | train_loss=0.3983 acc=0.8372 | val_loss=0.3757 acc=0.8571 | prec=0.8945 rec=0.8091 spec=0.9050 f1=0.8496 | time=12.8s\n",
            "Epoch 079 | train_loss=0.3871 acc=0.8395 | val_loss=0.3705 acc=0.8526 | prec=0.8894 rec=0.8045 spec=0.9005 f1=0.8449 | time=12.9s\n",
            "Epoch 080 | train_loss=0.3685 acc=0.8554 | val_loss=0.3727 acc=0.8481 | prec=0.8964 rec=0.7864 spec=0.9095 f1=0.8378 | time=12.8s\n",
            "Epoch 081 | train_loss=0.3684 acc=0.8525 | val_loss=0.3708 acc=0.8481 | prec=0.8883 rec=0.7955 spec=0.9005 f1=0.8393 | time=12.9s\n",
            "Epoch 082 | train_loss=0.3821 acc=0.8491 | val_loss=0.3770 acc=0.8458 | prec=0.9043 rec=0.7727 spec=0.9186 f1=0.8333 | time=12.8s\n",
            "Epoch 083 | train_loss=0.3665 acc=0.8690 | val_loss=0.3758 acc=0.8367 | prec=0.8936 rec=0.7636 spec=0.9095 f1=0.8235 | time=12.8s\n",
            "Epoch 084 | train_loss=0.3579 acc=0.8554 | val_loss=0.3703 acc=0.8345 | prec=0.9061 rec=0.7455 spec=0.9231 f1=0.8180 | time=12.9s\n",
            "Epoch 085 | train_loss=0.3631 acc=0.8593 | val_loss=0.3634 acc=0.8390 | prec=0.8901 rec=0.7727 spec=0.9050 f1=0.8273 | time=12.9s\n",
            "Epoch 086 | train_loss=0.3618 acc=0.8605 | val_loss=0.3701 acc=0.8322 | prec=0.9011 rec=0.7455 spec=0.9186 f1=0.8159 | time=12.8s\n",
            "Epoch 087 | train_loss=0.3713 acc=0.8446 | val_loss=0.3830 acc=0.8345 | prec=0.9298 rec=0.7227 spec=0.9457 f1=0.8133 | time=12.9s\n",
            "Epoch 088 | train_loss=0.3585 acc=0.8565 | val_loss=0.4054 acc=0.8163 | prec=0.9317 rec=0.6818 spec=0.9502 f1=0.7874 | time=12.9s\n",
            "Epoch 089 | train_loss=0.3647 acc=0.8593 | val_loss=0.3546 acc=0.8526 | prec=0.8708 rec=0.8273 spec=0.8778 f1=0.8485 | time=12.9s\n",
            "Epoch 090 | train_loss=0.3654 acc=0.8503 | val_loss=0.3720 acc=0.8345 | prec=0.9106 rec=0.7409 spec=0.9276 f1=0.8170 | time=12.8s\n",
            "Epoch 091 | train_loss=0.3590 acc=0.8491 | val_loss=0.3564 acc=0.8571 | prec=0.8829 rec=0.8227 spec=0.8914 f1=0.8518 | time=12.9s\n",
            "Epoch 092 | train_loss=0.3643 acc=0.8582 | val_loss=0.3744 acc=0.8299 | prec=0.9143 rec=0.7273 spec=0.9321 f1=0.8101 | time=13.0s\n",
            "Epoch 093 | train_loss=0.3378 acc=0.8627 | val_loss=0.3960 acc=0.8277 | prec=0.9337 rec=0.7045 spec=0.9502 f1=0.8031 | time=12.9s\n",
            "Epoch 094 | train_loss=0.3629 acc=0.8531 | val_loss=0.3683 acc=0.8299 | prec=0.9006 rec=0.7409 spec=0.9186 f1=0.8130 | time=13.0s\n",
            "Epoch 095 | train_loss=0.3457 acc=0.8633 | val_loss=0.3514 acc=0.8730 | prec=0.8761 rec=0.8682 spec=0.8778 f1=0.8721 | time=12.8s\n",
            "Epoch 096 | train_loss=0.3579 acc=0.8503 | val_loss=0.3479 acc=0.8549 | prec=0.8824 rec=0.8182 spec=0.8914 f1=0.8491 | time=12.9s\n",
            "Epoch 097 | train_loss=0.3396 acc=0.8673 | val_loss=0.3711 acc=0.8322 | prec=0.9101 rec=0.7364 spec=0.9276 f1=0.8141 | time=12.7s\n",
            "Epoch 098 | train_loss=0.3410 acc=0.8633 | val_loss=0.3620 acc=0.8322 | prec=0.8925 rec=0.7545 spec=0.9095 f1=0.8177 | time=12.9s\n",
            "Epoch 099 | train_loss=0.3468 acc=0.8633 | val_loss=0.3590 acc=0.8390 | prec=0.8984 rec=0.7636 spec=0.9140 f1=0.8256 | time=12.9s\n",
            "Epoch 100 | train_loss=0.3417 acc=0.8571 | val_loss=0.3821 acc=0.8345 | prec=0.9298 rec=0.7227 spec=0.9457 f1=0.8133 | time=12.7s\n",
            "Epoch 101 | train_loss=0.3510 acc=0.8724 | val_loss=0.3515 acc=0.8413 | prec=0.8947 rec=0.7727 spec=0.9095 f1=0.8293 | time=12.8s\n",
            "Epoch 102 | train_loss=0.3320 acc=0.8650 | val_loss=0.3614 acc=0.8435 | prec=0.9081 rec=0.7636 spec=0.9231 f1=0.8296 | time=12.8s\n",
            "Epoch 103 | train_loss=0.3377 acc=0.8701 | val_loss=0.3623 acc=0.8345 | prec=0.9153 rec=0.7364 spec=0.9321 f1=0.8161 | time=12.9s\n",
            "Epoch 104 | train_loss=0.3548 acc=0.8576 | val_loss=0.3863 acc=0.8322 | prec=0.9345 rec=0.7136 spec=0.9502 f1=0.8093 | time=12.9s\n",
            "Epoch 105 | train_loss=0.3247 acc=0.8894 | val_loss=0.3596 acc=0.8277 | prec=0.9000 rec=0.7364 spec=0.9186 f1=0.8100 | time=12.9s\n",
            "Epoch 106 | train_loss=0.3329 acc=0.8582 | val_loss=0.3476 acc=0.8549 | prec=0.8824 rec=0.8182 spec=0.8914 f1=0.8491 | time=12.9s\n",
            "Epoch 107 | train_loss=0.3484 acc=0.8735 | val_loss=0.3645 acc=0.8299 | prec=0.9050 rec=0.7364 spec=0.9231 f1=0.8120 | time=12.9s\n",
            "Epoch 108 | train_loss=0.3194 acc=0.8780 | val_loss=0.3647 acc=0.8299 | prec=0.9050 rec=0.7364 spec=0.9231 f1=0.8120 | time=12.8s\n",
            "Epoch 109 | train_loss=0.3231 acc=0.8718 | val_loss=0.3896 acc=0.8322 | prec=0.9398 rec=0.7091 spec=0.9548 f1=0.8083 | time=12.8s\n",
            "Epoch 110 | train_loss=0.3359 acc=0.8735 | val_loss=0.3824 acc=0.8299 | prec=0.9290 rec=0.7136 spec=0.9457 f1=0.8072 | time=12.8s\n",
            "Epoch 111 | train_loss=0.3385 acc=0.8644 | val_loss=0.3846 acc=0.8277 | prec=0.9286 rec=0.7091 spec=0.9457 f1=0.8041 | time=12.9s\n",
            "Epoch 112 | train_loss=0.3267 acc=0.8775 | val_loss=0.3709 acc=0.8345 | prec=0.9200 rec=0.7318 spec=0.9367 f1=0.8152 | time=12.8s\n",
            "Epoch 113 | train_loss=0.3465 acc=0.8622 | val_loss=0.3673 acc=0.8345 | prec=0.9249 rec=0.7273 spec=0.9412 f1=0.8142 | time=12.8s\n",
            "Epoch 114 | train_loss=0.3394 acc=0.8661 | val_loss=0.3808 acc=0.8277 | prec=0.9286 rec=0.7091 spec=0.9457 f1=0.8041 | time=12.8s\n",
            "Epoch 115 | train_loss=0.3323 acc=0.8707 | val_loss=0.3619 acc=0.8390 | prec=0.9209 rec=0.7409 spec=0.9367 f1=0.8212 | time=12.8s\n",
            "Epoch 116 | train_loss=0.3240 acc=0.8775 | val_loss=0.3690 acc=0.8299 | prec=0.9191 rec=0.7227 spec=0.9367 f1=0.8092 | time=12.9s\n",
            "Epoch 117 | train_loss=0.3038 acc=0.8894 | val_loss=0.3701 acc=0.8367 | prec=0.9253 rec=0.7318 spec=0.9412 f1=0.8173 | time=12.9s\n",
            "Epoch 118 | train_loss=0.3333 acc=0.8695 | val_loss=0.3709 acc=0.8322 | prec=0.9244 rec=0.7227 spec=0.9412 f1=0.8112 | time=12.8s\n",
            "Epoch 119 | train_loss=0.3146 acc=0.8735 | val_loss=0.3827 acc=0.8277 | prec=0.9286 rec=0.7091 spec=0.9457 f1=0.8041 | time=12.8s\n",
            "Epoch 120 | train_loss=0.3178 acc=0.8792 | val_loss=0.3759 acc=0.8322 | prec=0.9195 rec=0.7273 spec=0.9367 f1=0.8122 | time=12.9s\n",
            "Epoch 121 | train_loss=0.3188 acc=0.8667 | val_loss=0.3768 acc=0.8345 | prec=0.9249 rec=0.7273 spec=0.9412 f1=0.8142 | time=12.8s\n",
            "Early stopping at epoch 121\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▅▆▅▁▆▇▇▇▇▇▇▇█▇███████████▇█▇█████▇▇███</td></tr><tr><td>precision</td><td>▁▁▁▁▁▂▅▆▇▇▆▇▆▆▆▇▆█▇▇▇▇▇▇█▇█▇██▇█▇█▇█████</td></tr><tr><td>recall</td><td>█▆██▃█▇▁▇▅▆▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▆▆▆▆▆▆▆▆▆▆▆▆▅▆</td></tr><tr><td>specificity</td><td>▁▃▁▁▁▂▇▃▄▆▇█▇▇█▇▇█▇▇███▇████████████████</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▁▁▁▁▁▁▂▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▃▃▅▇▇▇▇▇▇▇▇████████▇███▇█████▇███</td></tr><tr><td>validation_loss</td><td>████████▇▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▁▂▁▂▁▁▂▁▂▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81425</td></tr><tr><td>precision</td><td>0.92486</td></tr><tr><td>recall</td><td>0.72727</td></tr><tr><td>specificity</td><td>0.94118</td></tr><tr><td>train_accuracy</td><td>0.8667</td></tr><tr><td>train_loss</td><td>0.31878</td></tr><tr><td>validation_accuracy</td><td>0.83447</td></tr><tr><td>validation_loss</td><td>0.37677</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/4i481zxg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/4i481zxg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_054813-4i481zxg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 06:14:29,128] Trial 0 finished with values: [0.37676977259772165, 0.8344671201814059] and parameters: {'lr': 1.2570221387936817e-05, 'wd': 1.6489807154594614e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: filters=120, lr=2.42e-05, wd=8.26e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_061429-g1ufrfuk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/g1ufrfuk' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/g1ufrfuk' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/g1ufrfuk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7525 acc=0.4997 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7343 acc=0.4969 | val_loss=0.6945 acc=0.4762 | prec=0.4304 rec=0.1545 spec=0.7964 f1=0.2274 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7219 acc=0.4952 | val_loss=0.6934 acc=0.4943 | prec=0.4964 rec=0.9273 spec=0.0633 f1=0.6466 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7271 acc=0.4884 | val_loss=0.6933 acc=0.4853 | prec=0.4903 rec=0.8000 spec=0.1719 f1=0.6079 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7178 acc=0.5088 | val_loss=0.6938 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7160 acc=0.4929 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7116 acc=0.4974 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7144 acc=0.4844 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 009 | train_loss=0.7136 acc=0.4906 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.3s\n",
            "Epoch 010 | train_loss=0.7083 acc=0.5020 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.2s\n",
            "Epoch 011 | train_loss=0.7122 acc=0.4918 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.6s\n",
            "Epoch 012 | train_loss=0.7058 acc=0.5060 | val_loss=0.6924 acc=0.5306 | prec=0.5178 rec=0.8591 spec=0.2036 f1=0.6462 | time=13.7s\n",
            "Epoch 013 | train_loss=0.7084 acc=0.4901 | val_loss=0.6919 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.6s\n",
            "Epoch 014 | train_loss=0.7136 acc=0.5003 | val_loss=0.6893 acc=0.5850 | prec=0.8246 rec=0.2136 spec=0.9548 f1=0.3394 | time=13.6s\n",
            "Epoch 015 | train_loss=0.7045 acc=0.5088 | val_loss=0.6881 acc=0.5147 | prec=0.5069 rec=1.0000 spec=0.0317 f1=0.6728 | time=13.7s\n",
            "Epoch 016 | train_loss=0.6935 acc=0.5337 | val_loss=0.6746 acc=0.5850 | prec=0.5478 rec=0.9636 spec=0.2081 f1=0.6985 | time=13.6s\n",
            "Epoch 017 | train_loss=0.6808 acc=0.5638 | val_loss=0.6230 acc=0.7188 | prec=0.8200 rec=0.5591 spec=0.8778 f1=0.6649 | time=13.8s\n",
            "Epoch 018 | train_loss=0.6635 acc=0.6018 | val_loss=0.6330 acc=0.7075 | prec=0.6492 rec=0.9000 spec=0.5158 f1=0.7543 | time=13.9s\n",
            "Epoch 019 | train_loss=0.6239 acc=0.6597 | val_loss=0.5729 acc=0.7596 | prec=0.8750 rec=0.6045 spec=0.9140 f1=0.7151 | time=13.9s\n",
            "Epoch 020 | train_loss=0.6064 acc=0.6841 | val_loss=0.5820 acc=0.7574 | prec=0.7251 rec=0.8273 spec=0.6878 f1=0.7728 | time=13.8s\n",
            "Epoch 021 | train_loss=0.6086 acc=0.6716 | val_loss=0.5613 acc=0.7732 | prec=0.7970 rec=0.7318 spec=0.8145 f1=0.7630 | time=13.6s\n",
            "Epoch 022 | train_loss=0.5909 acc=0.7022 | val_loss=0.5839 acc=0.7642 | prec=0.7148 rec=0.8773 spec=0.6516 f1=0.7878 | time=13.6s\n",
            "Epoch 023 | train_loss=0.5826 acc=0.7067 | val_loss=0.5434 acc=0.7800 | prec=0.8868 rec=0.6409 spec=0.9186 f1=0.7441 | time=13.6s\n",
            "Epoch 024 | train_loss=0.5773 acc=0.7221 | val_loss=0.5528 acc=0.7732 | prec=0.7256 rec=0.8773 spec=0.6697 f1=0.7942 | time=13.4s\n",
            "Epoch 025 | train_loss=0.5489 acc=0.7499 | val_loss=0.5178 acc=0.7914 | prec=0.8902 rec=0.6636 spec=0.9186 f1=0.7604 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5423 acc=0.7266 | val_loss=0.4976 acc=0.7959 | prec=0.8421 rec=0.7273 spec=0.8643 f1=0.7805 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4940 acc=0.7595 | val_loss=0.4866 acc=0.7959 | prec=0.8571 rec=0.7091 spec=0.8824 f1=0.7761 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5055 acc=0.7765 | val_loss=0.4622 acc=0.8141 | prec=0.7974 rec=0.8409 spec=0.7873 f1=0.8186 | time=12.9s\n",
            "Epoch 029 | train_loss=0.5009 acc=0.7708 | val_loss=0.4615 acc=0.8186 | prec=0.8571 rec=0.7636 spec=0.8733 f1=0.8077 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4759 acc=0.7941 | val_loss=0.4432 acc=0.8322 | prec=0.8288 rec=0.8364 spec=0.8281 f1=0.8326 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4488 acc=0.8157 | val_loss=0.4302 acc=0.8435 | prec=0.8447 rec=0.8409 spec=0.8462 f1=0.8428 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4371 acc=0.8128 | val_loss=0.4279 acc=0.8413 | prec=0.8099 rec=0.8909 spec=0.7919 f1=0.8485 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4385 acc=0.8077 | val_loss=0.4174 acc=0.8413 | prec=0.8178 rec=0.8773 spec=0.8054 f1=0.8465 | time=12.8s\n",
            "Epoch 034 | train_loss=0.4246 acc=0.8162 | val_loss=0.4071 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4107 acc=0.8355 | val_loss=0.4140 acc=0.8503 | prec=0.8348 rec=0.8727 spec=0.8281 f1=0.8533 | time=12.7s\n",
            "Epoch 036 | train_loss=0.4020 acc=0.8389 | val_loss=0.4112 acc=0.8367 | prec=0.8426 rec=0.8273 spec=0.8462 f1=0.8349 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3828 acc=0.8508 | val_loss=0.4040 acc=0.8345 | prec=0.8693 rec=0.7864 spec=0.8824 f1=0.8258 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3763 acc=0.8525 | val_loss=0.3946 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.8s\n",
            "Epoch 039 | train_loss=0.3993 acc=0.8463 | val_loss=0.4454 acc=0.7937 | prec=0.9108 rec=0.6500 spec=0.9367 f1=0.7586 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3480 acc=0.8650 | val_loss=0.3924 acc=0.8367 | prec=0.8426 rec=0.8273 spec=0.8462 f1=0.8349 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3602 acc=0.8656 | val_loss=0.4228 acc=0.8231 | prec=0.8944 rec=0.7318 spec=0.9140 f1=0.8050 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3509 acc=0.8673 | val_loss=0.3917 acc=0.8390 | prec=0.8253 rec=0.8591 spec=0.8190 f1=0.8419 | time=13.0s\n",
            "Epoch 043 | train_loss=0.3418 acc=0.8639 | val_loss=0.3965 acc=0.8277 | prec=0.8158 rec=0.8455 spec=0.8100 f1=0.8304 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3258 acc=0.8798 | val_loss=0.4089 acc=0.8481 | prec=0.9005 rec=0.7818 spec=0.9140 f1=0.8370 | time=12.7s\n",
            "Epoch 045 | train_loss=0.3160 acc=0.8939 | val_loss=0.4020 acc=0.8390 | prec=0.8565 rec=0.8136 spec=0.8643 f1=0.8345 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3173 acc=0.8775 | val_loss=0.4018 acc=0.8571 | prec=0.9110 rec=0.7909 spec=0.9231 f1=0.8467 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3238 acc=0.8763 | val_loss=0.3951 acc=0.8435 | prec=0.8612 rec=0.8182 spec=0.8688 f1=0.8392 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3262 acc=0.9030 | val_loss=0.4126 acc=0.8231 | prec=0.8944 rec=0.7318 spec=0.9140 f1=0.8050 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3205 acc=0.8939 | val_loss=0.3960 acc=0.8526 | prec=0.8934 rec=0.8000 spec=0.9050 f1=0.8441 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3114 acc=0.8792 | val_loss=0.3927 acc=0.8367 | prec=0.8304 rec=0.8455 spec=0.8281 f1=0.8378 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2931 acc=0.8888 | val_loss=0.3908 acc=0.8277 | prec=0.8077 rec=0.8591 spec=0.7964 f1=0.8326 | time=12.9s\n",
            "Epoch 052 | train_loss=0.3104 acc=0.8945 | val_loss=0.3886 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2957 acc=0.8911 | val_loss=0.3985 acc=0.8435 | prec=0.8545 rec=0.8273 spec=0.8597 f1=0.8406 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2736 acc=0.8962 | val_loss=0.3923 acc=0.8435 | prec=0.8240 rec=0.8727 spec=0.8145 f1=0.8477 | time=12.7s\n",
            "Epoch 055 | train_loss=0.2918 acc=0.8956 | val_loss=0.3890 acc=0.8367 | prec=0.8274 rec=0.8500 spec=0.8235 f1=0.8386 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2822 acc=0.8945 | val_loss=0.4197 acc=0.8209 | prec=0.9123 rec=0.7091 spec=0.9321 f1=0.7980 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2829 acc=0.8968 | val_loss=0.4592 acc=0.8095 | prec=0.9146 rec=0.6818 spec=0.9367 f1=0.7812 | time=12.9s\n",
            "Epoch 058 | train_loss=0.2946 acc=0.8826 | val_loss=0.4049 acc=0.8322 | prec=0.8578 rec=0.7955 spec=0.8688 f1=0.8255 | time=12.7s\n",
            "Epoch 059 | train_loss=0.2672 acc=0.9064 | val_loss=0.3898 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.8s\n",
            "Epoch 060 | train_loss=0.2681 acc=0.8928 | val_loss=0.4185 acc=0.8413 | prec=0.8989 rec=0.7682 spec=0.9140 f1=0.8284 | time=12.8s\n",
            "Epoch 061 | train_loss=0.2625 acc=0.9115 | val_loss=0.4159 acc=0.8390 | prec=0.9116 rec=0.7500 spec=0.9276 f1=0.8229 | time=12.9s\n",
            "Epoch 062 | train_loss=0.2676 acc=0.9075 | val_loss=0.4075 acc=0.8345 | prec=0.8693 rec=0.7864 spec=0.8824 f1=0.8258 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2357 acc=0.9121 | val_loss=0.3986 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=12.9s\n",
            "Epoch 064 | train_loss=0.2291 acc=0.9166 | val_loss=0.3998 acc=0.8390 | prec=0.8531 rec=0.8182 spec=0.8597 f1=0.8353 | time=12.7s\n",
            "Epoch 065 | train_loss=0.2657 acc=0.9087 | val_loss=0.3991 acc=0.8413 | prec=0.8289 rec=0.8591 spec=0.8235 f1=0.8438 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2619 acc=0.8922 | val_loss=0.3930 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.7s\n",
            "Epoch 067 | train_loss=0.2372 acc=0.9092 | val_loss=0.3967 acc=0.8526 | prec=0.8444 rec=0.8636 spec=0.8416 f1=0.8539 | time=12.8s\n",
            "Early stopping at epoch 67\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▆▆▆▆▆▆▆▆▄▆▇▇▇▇█▇▇██████▇███████████████</td></tr><tr><td>precision</td><td>▁▅▅▅▅▅▇▅▇▆▇▇█▇█▇█▇▇█▇█▇█▇███▇█▇████████▇</td></tr><tr><td>recall</td><td>▁▂▇▇████▇▂█▅▇▅▇▅▇▆▆▆▇▇▇▇▇▆▇▆▇▇▆▇▇▇▇▇▆▇▇▇</td></tr><tr><td>specificity</td><td>█▁▂▁▁▁▁▂▁█▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▃▄▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇█▇█▇█████████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▂▃▂▆▆▆▇▇▇█████▇█▇███▇█████▇▇█████</td></tr><tr><td>validation_loss</td><td>████████████▇▇▅▅▅▅▃▃▂▂▂▁▁▂▁▁▁▁▂▁▁▁▁▂▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85393</td></tr><tr><td>precision</td><td>0.84444</td></tr><tr><td>recall</td><td>0.86364</td></tr><tr><td>specificity</td><td>0.84163</td></tr><tr><td>train_accuracy</td><td>0.90925</td></tr><tr><td>train_loss</td><td>0.23721</td></tr><tr><td>validation_accuracy</td><td>0.85261</td></tr><tr><td>validation_loss</td><td>0.39671</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/g1ufrfuk' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/g1ufrfuk</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_061429-g1ufrfuk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 06:29:03,888] Trial 1 finished with values: [0.39670870133808683, 0.8526077097505669] and parameters: {'lr': 2.4166877561293106e-05, 'wd': 8.262372253115548e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: filters=120, lr=1.57e-04, wd=4.50e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_062903-3x3afhx2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/3x3afhx2' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/3x3afhx2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/3x3afhx2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7430 acc=0.5048 | val_loss=0.6944 acc=0.4875 | prec=0.4931 rec=0.9682 spec=0.0090 f1=0.6534 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7258 acc=0.5099 | val_loss=0.6945 acc=0.4966 | prec=0.3333 rec=0.0091 spec=0.9819 f1=0.0177 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7165 acc=0.5122 | val_loss=0.6945 acc=0.4739 | prec=0.4855 rec=0.9136 spec=0.0362 f1=0.6341 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7236 acc=0.4878 | val_loss=0.6941 acc=0.4603 | prec=0.4740 rec=0.7455 spec=0.1765 f1=0.5795 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7266 acc=0.5031 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=0.9955 spec=0.0045 f1=0.6646 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7102 acc=0.5139 | val_loss=0.7029 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 007 | train_loss=0.7111 acc=0.5043 | val_loss=0.7018 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7118 acc=0.5031 | val_loss=0.6933 acc=0.5057 | prec=0.5023 rec=1.0000 spec=0.0136 f1=0.6687 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7136 acc=0.5173 | val_loss=0.6986 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7054 acc=0.5247 | val_loss=0.6866 acc=0.6100 | prec=0.8243 rec=0.2773 spec=0.9412 f1=0.4150 | time=13.0s\n",
            "Epoch 011 | train_loss=0.6914 acc=0.5423 | val_loss=0.6719 acc=0.5578 | prec=0.8788 rec=0.1318 spec=0.9819 f1=0.2292 | time=12.8s\n",
            "Epoch 012 | train_loss=0.6769 acc=0.5944 | val_loss=0.6128 acc=0.7075 | prec=0.8583 rec=0.4955 spec=0.9186 f1=0.6282 | time=13.0s\n",
            "Epoch 013 | train_loss=0.6138 acc=0.6790 | val_loss=0.6001 acc=0.7279 | prec=0.6799 rec=0.8591 spec=0.5973 f1=0.7590 | time=12.8s\n",
            "Epoch 014 | train_loss=0.5704 acc=0.7272 | val_loss=0.5131 acc=0.7846 | prec=0.7637 rec=0.8227 spec=0.7466 f1=0.7921 | time=12.7s\n",
            "Epoch 015 | train_loss=0.5197 acc=0.7629 | val_loss=0.4909 acc=0.7755 | prec=0.7665 rec=0.7909 spec=0.7602 f1=0.7785 | time=12.8s\n",
            "Epoch 016 | train_loss=0.4859 acc=0.7873 | val_loss=0.4633 acc=0.8141 | prec=0.8450 rec=0.7682 spec=0.8597 f1=0.8048 | time=12.9s\n",
            "Epoch 017 | train_loss=0.4731 acc=0.7964 | val_loss=0.4456 acc=0.7982 | prec=0.8701 rec=0.7000 spec=0.8959 f1=0.7758 | time=12.8s\n",
            "Epoch 018 | train_loss=0.4361 acc=0.8123 | val_loss=0.4198 acc=0.8186 | prec=0.8398 rec=0.7864 spec=0.8507 f1=0.8122 | time=12.9s\n",
            "Epoch 019 | train_loss=0.4259 acc=0.8236 | val_loss=0.4138 acc=0.8209 | prec=0.8106 rec=0.8364 spec=0.8054 f1=0.8233 | time=12.8s\n",
            "Epoch 020 | train_loss=0.3941 acc=0.8321 | val_loss=0.4601 acc=0.7914 | prec=0.7238 rec=0.9409 spec=0.6425 f1=0.8182 | time=12.8s\n",
            "Epoch 021 | train_loss=0.4022 acc=0.8264 | val_loss=0.4131 acc=0.8141 | prec=0.7575 rec=0.9227 spec=0.7059 f1=0.8320 | time=12.8s\n",
            "Epoch 022 | train_loss=0.3731 acc=0.8531 | val_loss=0.3726 acc=0.8367 | prec=0.8592 rec=0.8045 spec=0.8688 f1=0.8310 | time=12.8s\n",
            "Epoch 023 | train_loss=0.3437 acc=0.8599 | val_loss=0.3715 acc=0.8345 | prec=0.8326 rec=0.8364 spec=0.8326 f1=0.8345 | time=12.8s\n",
            "Epoch 024 | train_loss=0.3247 acc=0.8741 | val_loss=0.3418 acc=0.8571 | prec=0.8552 rec=0.8591 spec=0.8552 f1=0.8571 | time=12.8s\n",
            "Epoch 025 | train_loss=0.3028 acc=0.8735 | val_loss=0.4993 acc=0.7778 | prec=0.9067 rec=0.6182 spec=0.9367 f1=0.7351 | time=12.9s\n",
            "Epoch 026 | train_loss=0.2965 acc=0.8883 | val_loss=0.3843 acc=0.8231 | prec=0.8034 rec=0.8545 spec=0.7919 f1=0.8282 | time=12.9s\n",
            "Epoch 027 | train_loss=0.3128 acc=0.8809 | val_loss=0.4664 acc=0.7959 | prec=0.9114 rec=0.6545 spec=0.9367 f1=0.7619 | time=12.8s\n",
            "Epoch 028 | train_loss=0.3078 acc=0.8593 | val_loss=0.5200 acc=0.7891 | prec=0.9504 rec=0.6091 spec=0.9683 f1=0.7424 | time=12.9s\n",
            "Epoch 029 | train_loss=0.2567 acc=0.9030 | val_loss=0.3612 acc=0.8345 | prec=0.8551 rec=0.8045 spec=0.8643 f1=0.8290 | time=12.9s\n",
            "Epoch 030 | train_loss=0.2577 acc=0.9070 | val_loss=0.4074 acc=0.8254 | prec=0.7871 rec=0.8909 spec=0.7602 f1=0.8358 | time=12.9s\n",
            "Epoch 031 | train_loss=0.2486 acc=0.8939 | val_loss=0.3796 acc=0.8277 | prec=0.8830 rec=0.7545 spec=0.9005 f1=0.8137 | time=12.9s\n",
            "Epoch 032 | train_loss=0.2153 acc=0.9109 | val_loss=0.4413 acc=0.8027 | prec=0.7548 rec=0.8955 spec=0.7104 f1=0.8191 | time=12.9s\n",
            "Epoch 033 | train_loss=0.2044 acc=0.9183 | val_loss=0.3975 acc=0.8345 | prec=0.8296 rec=0.8409 spec=0.8281 f1=0.8352 | time=12.9s\n",
            "Epoch 034 | train_loss=0.1883 acc=0.9280 | val_loss=0.4017 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=12.7s\n",
            "Epoch 035 | train_loss=0.1876 acc=0.9200 | val_loss=0.4242 acc=0.8118 | prec=0.8703 rec=0.7318 spec=0.8914 f1=0.7951 | time=12.8s\n",
            "Epoch 036 | train_loss=0.1670 acc=0.9336 | val_loss=0.3913 acc=0.8435 | prec=0.8794 rec=0.7955 spec=0.8914 f1=0.8353 | time=12.8s\n",
            "Epoch 037 | train_loss=0.1466 acc=0.9433 | val_loss=0.4179 acc=0.8390 | prec=0.8670 rec=0.8000 spec=0.8778 f1=0.8322 | time=12.9s\n",
            "Epoch 038 | train_loss=0.1518 acc=0.9331 | val_loss=0.4438 acc=0.8277 | prec=0.8303 rec=0.8227 spec=0.8326 f1=0.8265 | time=12.8s\n",
            "Epoch 039 | train_loss=0.1632 acc=0.9291 | val_loss=0.4829 acc=0.8050 | prec=0.7792 rec=0.8500 spec=0.7602 f1=0.8130 | time=12.9s\n",
            "Early stopping at epoch 39\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▁▆▆▆▆▆▆▆▄▃▆▇▇▇█▇███████▇█▇▇██████▇████</td></tr><tr><td>precision</td><td>▃▁▃▃▃▃▃▃▃▇▇▇▅▆▆▇▇▇▆▅▆▇▇▇█▆██▇▆▇▆▇▇▇▇▇▇▆</td></tr><tr><td>recall</td><td>█▁▇▆█████▃▂▄▇▇▇▆▆▆▇█▇▇▇▇▅▇▆▅▇▇▆▇▇▇▆▇▇▇▇</td></tr><tr><td>specificity</td><td>▁█▁▂▁▁▁▁▁███▅▆▆▇▇▇▇▆▆▇▇▇█▇██▇▆▇▆▇▇▇▇▇▇▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▂▃▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>██████████▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▁▂▂▂▂▂▄▃▅▆▇▇▇▇▇▇▇▇███▇▇▇▇█▇▇▇██▇██▇▇</td></tr><tr><td>validation_loss</td><td>██████████▇▆▆▄▄▃▃▃▂▃▂▂▂▁▄▂▃▄▁▂▂▃▂▂▃▂▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81304</td></tr><tr><td>precision</td><td>0.77917</td></tr><tr><td>recall</td><td>0.85</td></tr><tr><td>specificity</td><td>0.76018</td></tr><tr><td>train_accuracy</td><td>0.9291</td></tr><tr><td>train_loss</td><td>0.1632</td></tr><tr><td>validation_accuracy</td><td>0.80499</td></tr><tr><td>validation_loss</td><td>0.48292</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/3x3afhx2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/3x3afhx2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_062903-3x3afhx2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 06:37:26,365] Trial 2 finished with values: [0.4829153333391462, 0.8049886621315193] and parameters: {'lr': 0.0001567752377902421, 'wd': 4.4997456886668276e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: filters=120, lr=2.21e-05, wd=4.30e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_063726-wedbgzxv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/wedbgzxv' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/wedbgzxv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/wedbgzxv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7769 acc=0.4991 | val_loss=0.7126 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7545 acc=0.5043 | val_loss=0.7148 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7364 acc=0.4912 | val_loss=0.7054 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7327 acc=0.5014 | val_loss=0.6981 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7339 acc=0.5088 | val_loss=0.7022 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7418 acc=0.4889 | val_loss=0.7006 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7162 acc=0.5111 | val_loss=0.6987 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7192 acc=0.5105 | val_loss=0.6969 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7232 acc=0.4878 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7257 acc=0.4810 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7221 acc=0.4997 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7169 acc=0.5009 | val_loss=0.6941 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7151 acc=0.5065 | val_loss=0.6931 acc=0.5057 | prec=0.5046 rec=0.4955 spec=0.5158 f1=0.5000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7226 acc=0.4889 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 015 | train_loss=0.7067 acc=0.5116 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7053 acc=0.5111 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7050 acc=0.5139 | val_loss=0.6920 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7077 acc=0.5014 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 019 | train_loss=0.7003 acc=0.5196 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7015 acc=0.5128 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7035 acc=0.5201 | val_loss=0.6879 acc=0.5193 | prec=0.9000 rec=0.0409 spec=0.9955 f1=0.0783 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7023 acc=0.5235 | val_loss=0.6839 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 023 | train_loss=0.7011 acc=0.5235 | val_loss=0.6782 acc=0.7098 | prec=0.7875 rec=0.5727 spec=0.8462 f1=0.6632 | time=12.8s\n",
            "Epoch 024 | train_loss=0.6887 acc=0.5474 | val_loss=0.6662 acc=0.7256 | prec=0.7368 rec=0.7000 spec=0.7511 f1=0.7179 | time=12.9s\n",
            "Epoch 025 | train_loss=0.6643 acc=0.5837 | val_loss=0.6395 acc=0.7347 | prec=0.8084 rec=0.6136 spec=0.8552 f1=0.6977 | time=12.8s\n",
            "Epoch 026 | train_loss=0.6385 acc=0.6296 | val_loss=0.6193 acc=0.7166 | prec=0.8926 rec=0.4909 spec=0.9412 f1=0.6334 | time=12.8s\n",
            "Epoch 027 | train_loss=0.6148 acc=0.6648 | val_loss=0.5758 acc=0.7846 | prec=0.8743 rec=0.6636 spec=0.9050 f1=0.7545 | time=12.8s\n",
            "Epoch 028 | train_loss=0.5932 acc=0.6982 | val_loss=0.5552 acc=0.7891 | prec=0.8360 rec=0.7182 spec=0.8597 f1=0.7726 | time=12.8s\n",
            "Epoch 029 | train_loss=0.5730 acc=0.7158 | val_loss=0.5422 acc=0.7868 | prec=0.8663 rec=0.6773 spec=0.8959 f1=0.7602 | time=12.9s\n",
            "Epoch 030 | train_loss=0.5693 acc=0.7187 | val_loss=0.5393 acc=0.7868 | prec=0.8706 rec=0.6727 spec=0.9005 f1=0.7590 | time=12.9s\n",
            "Epoch 031 | train_loss=0.5496 acc=0.7249 | val_loss=0.5229 acc=0.7891 | prec=0.8848 rec=0.6636 spec=0.9140 f1=0.7584 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5337 acc=0.7567 | val_loss=0.5098 acc=0.7937 | prec=0.8686 rec=0.6909 spec=0.8959 f1=0.7696 | time=12.7s\n",
            "Epoch 033 | train_loss=0.5357 acc=0.7521 | val_loss=0.5064 acc=0.7959 | prec=0.8186 rec=0.7591 spec=0.8326 f1=0.7877 | time=13.0s\n",
            "Epoch 034 | train_loss=0.5194 acc=0.7657 | val_loss=0.5067 acc=0.8005 | prec=0.8793 rec=0.6955 spec=0.9050 f1=0.7766 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5166 acc=0.7623 | val_loss=0.5046 acc=0.7914 | prec=0.8951 rec=0.6591 spec=0.9231 f1=0.7592 | time=12.9s\n",
            "Epoch 036 | train_loss=0.5012 acc=0.7799 | val_loss=0.4894 acc=0.8027 | prec=0.8519 rec=0.7318 spec=0.8733 f1=0.7873 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4909 acc=0.7952 | val_loss=0.5007 acc=0.7891 | prec=0.8848 rec=0.6636 spec=0.9140 f1=0.7584 | time=12.9s\n",
            "Epoch 038 | train_loss=0.4888 acc=0.7856 | val_loss=0.4923 acc=0.8050 | prec=0.8454 rec=0.7455 spec=0.8643 f1=0.7923 | time=12.9s\n",
            "Epoch 039 | train_loss=0.4838 acc=0.7913 | val_loss=0.4871 acc=0.8027 | prec=0.8889 rec=0.6909 spec=0.9140 f1=0.7775 | time=12.8s\n",
            "Epoch 040 | train_loss=0.4749 acc=0.7913 | val_loss=0.4994 acc=0.7732 | prec=0.9167 rec=0.6000 spec=0.9457 f1=0.7253 | time=12.8s\n",
            "Epoch 041 | train_loss=0.4591 acc=0.8117 | val_loss=0.4842 acc=0.7982 | prec=0.9018 rec=0.6682 spec=0.9276 f1=0.7676 | time=12.8s\n",
            "Epoch 042 | train_loss=0.4536 acc=0.8196 | val_loss=0.4569 acc=0.8118 | prec=0.8785 rec=0.7227 spec=0.9005 f1=0.7930 | time=12.8s\n",
            "Epoch 043 | train_loss=0.4512 acc=0.8106 | val_loss=0.4691 acc=0.7914 | prec=0.9051 rec=0.6500 spec=0.9321 f1=0.7566 | time=12.8s\n",
            "Epoch 044 | train_loss=0.4470 acc=0.8236 | val_loss=0.4787 acc=0.7732 | prec=0.9348 rec=0.5864 spec=0.9593 f1=0.7207 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4412 acc=0.8140 | val_loss=0.4559 acc=0.7982 | prec=0.9119 rec=0.6591 spec=0.9367 f1=0.7652 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4231 acc=0.8259 | val_loss=0.4420 acc=0.8073 | prec=0.8261 rec=0.7773 spec=0.8371 f1=0.8009 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4183 acc=0.8304 | val_loss=0.4460 acc=0.8050 | prec=0.9241 rec=0.6636 spec=0.9457 f1=0.7725 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3958 acc=0.8440 | val_loss=0.4428 acc=0.8141 | prec=0.7949 rec=0.8455 spec=0.7828 f1=0.8194 | time=12.9s\n",
            "Epoch 049 | train_loss=0.4017 acc=0.8372 | val_loss=0.4555 acc=0.7823 | prec=0.9306 rec=0.6091 spec=0.9548 f1=0.7363 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3919 acc=0.8338 | val_loss=0.4196 acc=0.8141 | prec=0.8670 rec=0.7409 spec=0.8869 f1=0.7990 | time=12.9s\n",
            "Epoch 051 | train_loss=0.3760 acc=0.8497 | val_loss=0.4334 acc=0.8027 | prec=0.8800 rec=0.7000 spec=0.9050 f1=0.7797 | time=12.9s\n",
            "Epoch 052 | train_loss=0.3670 acc=0.8582 | val_loss=0.4237 acc=0.8186 | prec=0.8804 rec=0.7364 spec=0.9005 f1=0.8020 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3648 acc=0.8576 | val_loss=0.4409 acc=0.7959 | prec=0.9167 rec=0.6500 spec=0.9412 f1=0.7606 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3777 acc=0.8480 | val_loss=0.4269 acc=0.7982 | prec=0.8970 rec=0.6727 spec=0.9231 f1=0.7688 | time=12.8s\n",
            "Epoch 055 | train_loss=0.3611 acc=0.8537 | val_loss=0.4100 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.8s\n",
            "Epoch 056 | train_loss=0.3453 acc=0.8701 | val_loss=0.4064 acc=0.8299 | prec=0.8571 rec=0.7909 spec=0.8688 f1=0.8227 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3374 acc=0.8718 | val_loss=0.4206 acc=0.8050 | prec=0.9036 rec=0.6818 spec=0.9276 f1=0.7772 | time=13.0s\n",
            "Epoch 058 | train_loss=0.3347 acc=0.8576 | val_loss=0.4042 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=13.0s\n",
            "Epoch 059 | train_loss=0.3251 acc=0.8707 | val_loss=0.4114 acc=0.8322 | prec=0.8967 rec=0.7500 spec=0.9140 f1=0.8168 | time=12.9s\n",
            "Epoch 060 | train_loss=0.3255 acc=0.8701 | val_loss=0.4234 acc=0.8027 | prec=0.9030 rec=0.6773 spec=0.9276 f1=0.7740 | time=12.9s\n",
            "Epoch 061 | train_loss=0.3198 acc=0.8809 | val_loss=0.4356 acc=0.7937 | prec=0.9216 rec=0.6409 spec=0.9457 f1=0.7560 | time=12.9s\n",
            "Epoch 062 | train_loss=0.3196 acc=0.8695 | val_loss=0.4120 acc=0.8095 | prec=0.9096 rec=0.6864 spec=0.9321 f1=0.7824 | time=13.0s\n",
            "Epoch 063 | train_loss=0.3090 acc=0.8826 | val_loss=0.3909 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=13.0s\n",
            "Epoch 064 | train_loss=0.3086 acc=0.8803 | val_loss=0.3913 acc=0.8299 | prec=0.8718 rec=0.7727 spec=0.8869 f1=0.8193 | time=12.8s\n",
            "Epoch 065 | train_loss=0.3190 acc=0.8888 | val_loss=0.3900 acc=0.8277 | prec=0.8871 rec=0.7500 spec=0.9050 f1=0.8128 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2984 acc=0.8939 | val_loss=0.3958 acc=0.8163 | prec=0.8840 rec=0.7273 spec=0.9050 f1=0.7980 | time=12.9s\n",
            "Epoch 067 | train_loss=0.2958 acc=0.8883 | val_loss=0.4145 acc=0.8073 | prec=0.9091 rec=0.6818 spec=0.9321 f1=0.7792 | time=12.9s\n",
            "Epoch 068 | train_loss=0.2889 acc=0.8951 | val_loss=0.4072 acc=0.8118 | prec=0.8960 rec=0.7045 spec=0.9186 f1=0.7888 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2857 acc=0.8843 | val_loss=0.3827 acc=0.8481 | prec=0.8732 rec=0.8136 spec=0.8824 f1=0.8424 | time=12.8s\n",
            "Epoch 070 | train_loss=0.2945 acc=0.8798 | val_loss=0.4070 acc=0.8163 | prec=0.9162 rec=0.6955 spec=0.9367 f1=0.7907 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2728 acc=0.8996 | val_loss=0.4268 acc=0.8027 | prec=0.9130 rec=0.6682 spec=0.9367 f1=0.7717 | time=12.8s\n",
            "Epoch 072 | train_loss=0.2650 acc=0.8951 | val_loss=0.3923 acc=0.8367 | prec=0.8978 rec=0.7591 spec=0.9140 f1=0.8227 | time=12.8s\n",
            "Epoch 073 | train_loss=0.2701 acc=0.8996 | val_loss=0.4386 acc=0.7982 | prec=0.9119 rec=0.6591 spec=0.9367 f1=0.7652 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2885 acc=0.8911 | val_loss=0.3993 acc=0.8209 | prec=0.9029 rec=0.7182 spec=0.9231 f1=0.8000 | time=12.9s\n",
            "Epoch 075 | train_loss=0.2676 acc=0.8894 | val_loss=0.4099 acc=0.8186 | prec=0.9070 rec=0.7091 spec=0.9276 f1=0.7959 | time=12.9s\n",
            "Epoch 076 | train_loss=0.2518 acc=0.9013 | val_loss=0.3832 acc=0.8413 | prec=0.8641 rec=0.8091 spec=0.8733 f1=0.8357 | time=12.9s\n",
            "Epoch 077 | train_loss=0.2793 acc=0.9081 | val_loss=0.3875 acc=0.8231 | prec=0.8777 rec=0.7500 spec=0.8959 f1=0.8088 | time=12.8s\n",
            "Epoch 078 | train_loss=0.2612 acc=0.8956 | val_loss=0.4046 acc=0.8254 | prec=0.9086 rec=0.7227 spec=0.9276 f1=0.8051 | time=12.9s\n",
            "Epoch 079 | train_loss=0.2584 acc=0.9058 | val_loss=0.4084 acc=0.8186 | prec=0.9023 rec=0.7136 spec=0.9231 f1=0.7970 | time=12.9s\n",
            "Epoch 080 | train_loss=0.2731 acc=0.9013 | val_loss=0.3817 acc=0.8367 | prec=0.8737 rec=0.7864 spec=0.8869 f1=0.8278 | time=12.8s\n",
            "Epoch 081 | train_loss=0.2536 acc=0.8962 | val_loss=0.4110 acc=0.8186 | prec=0.9070 rec=0.7091 spec=0.9276 f1=0.7959 | time=12.9s\n",
            "Epoch 082 | train_loss=0.2426 acc=0.9109 | val_loss=0.3881 acc=0.8345 | prec=0.8693 rec=0.7864 spec=0.8824 f1=0.8258 | time=12.9s\n",
            "Epoch 083 | train_loss=0.2302 acc=0.9081 | val_loss=0.4197 acc=0.8095 | prec=0.8953 rec=0.7000 spec=0.9186 f1=0.7857 | time=12.8s\n",
            "Epoch 084 | train_loss=0.2399 acc=0.9115 | val_loss=0.4081 acc=0.8163 | prec=0.8927 rec=0.7182 spec=0.9140 f1=0.7960 | time=12.9s\n",
            "Epoch 085 | train_loss=0.2481 acc=0.8985 | val_loss=0.4010 acc=0.8231 | prec=0.8737 rec=0.7545 spec=0.8914 f1=0.8098 | time=12.8s\n",
            "Epoch 086 | train_loss=0.2262 acc=0.9144 | val_loss=0.4213 acc=0.8073 | prec=0.8947 rec=0.6955 spec=0.9186 f1=0.7826 | time=12.9s\n",
            "Epoch 087 | train_loss=0.2245 acc=0.9183 | val_loss=0.4020 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=12.8s\n",
            "Epoch 088 | train_loss=0.2262 acc=0.9070 | val_loss=0.4415 acc=0.8005 | prec=0.9074 rec=0.6682 spec=0.9321 f1=0.7696 | time=12.8s\n",
            "Epoch 089 | train_loss=0.2307 acc=0.9109 | val_loss=0.4034 acc=0.8209 | prec=0.8770 rec=0.7455 spec=0.8959 f1=0.8059 | time=12.9s\n",
            "Epoch 090 | train_loss=0.2166 acc=0.9172 | val_loss=0.4275 acc=0.8118 | prec=0.9053 rec=0.6955 spec=0.9276 f1=0.7866 | time=12.8s\n",
            "Epoch 091 | train_loss=0.2200 acc=0.9229 | val_loss=0.4283 acc=0.8073 | prec=0.8994 rec=0.6909 spec=0.9231 f1=0.7815 | time=12.9s\n",
            "Epoch 092 | train_loss=0.2146 acc=0.9223 | val_loss=0.4174 acc=0.8027 | prec=0.8757 rec=0.7045 spec=0.9005 f1=0.7809 | time=12.8s\n",
            "Epoch 093 | train_loss=0.2168 acc=0.9161 | val_loss=0.4086 acc=0.8163 | prec=0.8971 rec=0.7136 spec=0.9186 f1=0.7949 | time=12.9s\n",
            "Epoch 094 | train_loss=0.2119 acc=0.9234 | val_loss=0.4383 acc=0.8141 | prec=0.9107 rec=0.6955 spec=0.9321 f1=0.7887 | time=12.9s\n",
            "Epoch 095 | train_loss=0.2182 acc=0.9183 | val_loss=0.4281 acc=0.8050 | prec=0.8851 rec=0.7000 spec=0.9095 f1=0.7817 | time=12.9s\n",
            "Early stopping at epoch 95\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▂▂▂▂▂▂▂▂▃▁▅▆▆▅▆▆▄▇▇▆█▅▇▇▇█▅█▇▆▆▇▆▆▇▇▇▆▆▆</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁█▁▆▆▇▇▆▇▇███▇▇▇▇█▇▇▇▇█▇██▇█▇▇▇█▇█▇</td></tr><tr><td>recall</td><td>██████▁▅▅▆▆▆▆▆▅▅▅▆▆▆▆▇▆▆▆▆▆▆▇▆▆▇▆▆▆▆▆▆▆▆</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁██▇███▇█▇███▇███▇▇██████▇█████</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇██▇████████</td></tr><tr><td>train_loss</td><td>████████████▇▇▆▆▆▆▅▅▄▄▄▄▄▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▇▇▇▇▇▇▇▇▇▇▇▇▇████▇▇███▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>██████████▇▇▇▇▄▃▃▃▃▃▃▂▂▂▂▂▁▁▂▂▂▁▁▂▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78173</td></tr><tr><td>precision</td><td>0.88506</td></tr><tr><td>recall</td><td>0.7</td></tr><tr><td>specificity</td><td>0.9095</td></tr><tr><td>train_accuracy</td><td>0.91832</td></tr><tr><td>train_loss</td><td>0.21817</td></tr><tr><td>validation_accuracy</td><td>0.80499</td></tr><tr><td>validation_loss</td><td>0.42809</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/wedbgzxv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/wedbgzxv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_063726-wedbgzxv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 06:57:49,618] Trial 3 finished with values: [0.4280863936458315, 0.8049886621315193] and parameters: {'lr': 2.2126147980575945e-05, 'wd': 4.3038205729757115e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: filters=120, lr=2.66e-05, wd=3.64e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_065749-epysiq9t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/epysiq9t' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/epysiq9t' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/epysiq9t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7342 acc=0.5128 | val_loss=0.7087 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7340 acc=0.4940 | val_loss=0.7071 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7121 acc=0.5060 | val_loss=0.6996 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7275 acc=0.5026 | val_loss=0.6990 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7234 acc=0.5088 | val_loss=0.6987 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7239 acc=0.5009 | val_loss=0.6969 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7197 acc=0.5037 | val_loss=0.6962 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7200 acc=0.5077 | val_loss=0.6932 acc=0.5147 | prec=0.5087 rec=0.7955 spec=0.2353 f1=0.6206 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7144 acc=0.5105 | val_loss=0.6993 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7108 acc=0.5156 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7251 acc=0.4912 | val_loss=0.6992 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7143 acc=0.5026 | val_loss=0.6963 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7217 acc=0.4861 | val_loss=0.6992 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7117 acc=0.5235 | val_loss=0.6920 acc=0.5125 | prec=0.5061 rec=0.9409 spec=0.0860 f1=0.6582 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7211 acc=0.4918 | val_loss=0.6906 acc=0.5760 | prec=0.6107 rec=0.4136 spec=0.7376 f1=0.4932 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7238 acc=0.4821 | val_loss=0.6921 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.6992 acc=0.5355 | val_loss=0.6866 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7053 acc=0.5196 | val_loss=0.6708 acc=0.5329 | prec=0.5167 rec=0.9818 spec=0.0860 f1=0.6771 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6903 acc=0.5564 | val_loss=0.6504 acc=0.6145 | prec=0.5694 rec=0.9318 spec=0.2986 f1=0.7069 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6536 acc=0.6041 | val_loss=0.5901 acc=0.7370 | prec=0.7955 rec=0.6364 spec=0.8371 f1=0.7071 | time=12.9s\n",
            "Epoch 021 | train_loss=0.6083 acc=0.6642 | val_loss=0.5798 acc=0.7324 | prec=0.6932 rec=0.8318 spec=0.6335 f1=0.7562 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5837 acc=0.6999 | val_loss=0.5412 acc=0.7506 | prec=0.8571 rec=0.6000 spec=0.9005 f1=0.7059 | time=12.8s\n",
            "Epoch 023 | train_loss=0.5588 acc=0.7181 | val_loss=0.5226 acc=0.7664 | prec=0.7468 rec=0.8045 spec=0.7285 f1=0.7746 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5373 acc=0.7204 | val_loss=0.5170 acc=0.7891 | prec=0.8068 rec=0.7591 spec=0.8190 f1=0.7822 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5357 acc=0.7516 | val_loss=0.5078 acc=0.7778 | prec=0.7364 rec=0.8636 spec=0.6923 f1=0.7950 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5294 acc=0.7385 | val_loss=0.4998 acc=0.7732 | prec=0.7290 rec=0.8682 spec=0.6787 f1=0.7925 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5133 acc=0.7646 | val_loss=0.4723 acc=0.8095 | prec=0.8333 rec=0.7727 spec=0.8462 f1=0.8019 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4997 acc=0.7782 | val_loss=0.4753 acc=0.7823 | prec=0.7441 rec=0.8591 spec=0.7059 f1=0.7975 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4850 acc=0.7714 | val_loss=0.4561 acc=0.8073 | prec=0.8392 rec=0.7591 spec=0.8552 f1=0.7971 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4589 acc=0.7941 | val_loss=0.4695 acc=0.7755 | prec=0.7266 rec=0.8818 spec=0.6697 f1=0.7967 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4594 acc=0.8066 | val_loss=0.4466 acc=0.8095 | prec=0.8542 rec=0.7455 spec=0.8733 f1=0.7961 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4612 acc=0.8032 | val_loss=0.4410 acc=0.8027 | prec=0.7759 rec=0.8500 spec=0.7557 f1=0.8113 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4592 acc=0.8043 | val_loss=0.4540 acc=0.7800 | prec=0.7338 rec=0.8773 spec=0.6833 f1=0.7992 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4479 acc=0.8100 | val_loss=0.4520 acc=0.7868 | prec=0.7405 rec=0.8818 spec=0.6923 f1=0.8050 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4182 acc=0.8242 | val_loss=0.4609 acc=0.7732 | prec=0.7206 rec=0.8909 spec=0.6561 f1=0.7967 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4223 acc=0.8276 | val_loss=0.4620 acc=0.7755 | prec=0.7153 rec=0.9136 spec=0.6380 f1=0.8024 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3994 acc=0.8327 | val_loss=0.4189 acc=0.8027 | prec=0.7830 rec=0.8364 spec=0.7692 f1=0.8088 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3916 acc=0.8355 | val_loss=0.4189 acc=0.8163 | prec=0.8174 rec=0.8136 spec=0.8190 f1=0.8155 | time=12.8s\n",
            "Epoch 039 | train_loss=0.4014 acc=0.8236 | val_loss=0.4042 acc=0.8186 | prec=0.8097 rec=0.8318 spec=0.8054 f1=0.8206 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3769 acc=0.8497 | val_loss=0.4020 acc=0.8254 | prec=0.8265 rec=0.8227 spec=0.8281 f1=0.8246 | time=12.8s\n",
            "Epoch 041 | train_loss=0.3631 acc=0.8514 | val_loss=0.4174 acc=0.8027 | prec=0.7588 rec=0.8864 spec=0.7195 f1=0.8176 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3579 acc=0.8486 | val_loss=0.4028 acc=0.8186 | prec=0.7941 rec=0.8591 spec=0.7783 f1=0.8253 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3715 acc=0.8417 | val_loss=0.4048 acc=0.8050 | prec=0.7557 rec=0.9000 spec=0.7104 f1=0.8216 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3638 acc=0.8497 | val_loss=0.3872 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3513 acc=0.8644 | val_loss=0.3828 acc=0.8254 | prec=0.8043 rec=0.8591 spec=0.7919 f1=0.8308 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3479 acc=0.8514 | val_loss=0.3875 acc=0.8254 | prec=0.8069 rec=0.8545 spec=0.7964 f1=0.8300 | time=12.9s\n",
            "Epoch 047 | train_loss=0.3431 acc=0.8571 | val_loss=0.4089 acc=0.8050 | prec=0.7577 rec=0.8955 spec=0.7149 f1=0.8208 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3410 acc=0.8554 | val_loss=0.4074 acc=0.8118 | prec=0.7585 rec=0.9136 spec=0.7104 f1=0.8289 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3225 acc=0.8832 | val_loss=0.3846 acc=0.8322 | prec=0.8510 rec=0.8045 spec=0.8597 f1=0.8271 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3224 acc=0.8792 | val_loss=0.3812 acc=0.8322 | prec=0.8230 rec=0.8455 spec=0.8190 f1=0.8341 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3057 acc=0.8843 | val_loss=0.3811 acc=0.8277 | prec=0.8077 rec=0.8591 spec=0.7964 f1=0.8326 | time=12.9s\n",
            "Epoch 052 | train_loss=0.2940 acc=0.8917 | val_loss=0.3733 acc=0.8481 | prec=0.8696 rec=0.8182 spec=0.8778 f1=0.8431 | time=12.8s\n",
            "Epoch 053 | train_loss=0.2904 acc=0.8951 | val_loss=0.3791 acc=0.8277 | prec=0.8103 rec=0.8545 spec=0.8009 f1=0.8319 | time=12.8s\n",
            "Epoch 054 | train_loss=0.3085 acc=0.8741 | val_loss=0.4211 acc=0.7982 | prec=0.7382 rec=0.9227 spec=0.6742 f1=0.8202 | time=12.8s\n",
            "Epoch 055 | train_loss=0.3023 acc=0.8843 | val_loss=0.3739 acc=0.8458 | prec=0.8619 rec=0.8227 spec=0.8688 f1=0.8419 | time=12.8s\n",
            "Epoch 056 | train_loss=0.2768 acc=0.8826 | val_loss=0.3926 acc=0.8209 | prec=0.8219 rec=0.8182 spec=0.8235 f1=0.8200 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2967 acc=0.8820 | val_loss=0.3886 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2826 acc=0.8900 | val_loss=0.3757 acc=0.8481 | prec=0.8660 rec=0.8227 spec=0.8733 f1=0.8438 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2710 acc=0.8945 | val_loss=0.4007 acc=0.8141 | prec=0.7760 rec=0.8818 spec=0.7466 f1=0.8255 | time=12.8s\n",
            "Epoch 060 | train_loss=0.2760 acc=0.8883 | val_loss=0.3735 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2766 acc=0.8911 | val_loss=0.3919 acc=0.8277 | prec=0.8000 rec=0.8727 spec=0.7828 f1=0.8348 | time=12.9s\n",
            "Epoch 062 | train_loss=0.2561 acc=0.8951 | val_loss=0.3854 acc=0.8390 | prec=0.8371 rec=0.8409 spec=0.8371 f1=0.8390 | time=12.9s\n",
            "Epoch 063 | train_loss=0.2570 acc=0.9024 | val_loss=0.3919 acc=0.8231 | prec=0.7910 rec=0.8773 spec=0.7692 f1=0.8319 | time=12.8s\n",
            "Epoch 064 | train_loss=0.2608 acc=0.9053 | val_loss=0.3970 acc=0.8231 | prec=0.7958 rec=0.8682 spec=0.7783 f1=0.8304 | time=13.0s\n",
            "Epoch 065 | train_loss=0.2607 acc=0.9019 | val_loss=0.3866 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2615 acc=0.8905 | val_loss=0.3961 acc=0.8367 | prec=0.8246 rec=0.8545 spec=0.8190 f1=0.8393 | time=12.7s\n",
            "Epoch 067 | train_loss=0.2467 acc=0.9047 | val_loss=0.3990 acc=0.8526 | prec=0.8818 rec=0.8136 spec=0.8914 f1=0.8463 | time=12.8s\n",
            "Early stopping at epoch 67\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▄▄▄▄▄▄▄▄▄▁▅▆▅▇▇▇▇▇▇▇▇▇▇▇▇███████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▃▁▁▆▅█▆▇▅▇▅▇▅▅▅▅▆▇▆▆▇▇▆█▇▅█▇█▆▇▇█</td></tr><tr><td>recall</td><td>█████████▁██▄▆▃▆▆▅▇▆▇▇▇▆▆▆▆▆▇▇▆▆▆▆▇▆▇▆▇▆</td></tr><tr><td>specificity</td><td>▁▁▃▁▁▁▁▂█▆▇▇▆▆██▆█▆▆▇▇█▇▇█▇▇▇▇▇▇██▇█▇▇██</td></tr><tr><td>train_accuracy</td><td>▂▁▁▁▁▁▁▂▁▁▂▂▃▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███▇███████</td></tr><tr><td>train_loss</td><td>██████████▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▃▁▂▃▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇██▇██████</td></tr><tr><td>validation_loss</td><td>██████████████▇▅▄▄▃▃▃▃▂▃▂▂▂▁▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84634</td></tr><tr><td>precision</td><td>0.88177</td></tr><tr><td>recall</td><td>0.81364</td></tr><tr><td>specificity</td><td>0.8914</td></tr><tr><td>train_accuracy</td><td>0.90471</td></tr><tr><td>train_loss</td><td>0.24674</td></tr><tr><td>validation_accuracy</td><td>0.85261</td></tr><tr><td>validation_loss</td><td>0.399</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/epysiq9t' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/epysiq9t</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_065749-epysiq9t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 07:12:12,093] Trial 4 finished with values: [0.39899651386908125, 0.8526077097505669] and parameters: {'lr': 2.658940271236436e-05, 'wd': 3.6374970242645205e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: filters=120, lr=4.56e-05, wd=1.60e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_071212-qzxgrewn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qzxgrewn' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qzxgrewn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qzxgrewn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7317 acc=0.5031 | val_loss=0.7290 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7409 acc=0.4912 | val_loss=0.7164 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7231 acc=0.5031 | val_loss=0.6969 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7120 acc=0.5071 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7138 acc=0.5060 | val_loss=0.6967 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7071 acc=0.5009 | val_loss=0.6952 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7157 acc=0.4810 | val_loss=0.6946 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7202 acc=0.5020 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7092 acc=0.5111 | val_loss=0.6930 acc=0.4966 | prec=0.4977 rec=0.9864 spec=0.0090 f1=0.6616 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7090 acc=0.5088 | val_loss=0.6931 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7106 acc=0.5037 | val_loss=0.6930 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7034 acc=0.4980 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7074 acc=0.5099 | val_loss=0.6900 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7004 acc=0.5269 | val_loss=0.6839 acc=0.6531 | prec=0.8317 rec=0.3818 spec=0.9231 f1=0.5234 | time=12.8s\n",
            "Epoch 015 | train_loss=0.6939 acc=0.5292 | val_loss=0.6693 acc=0.7234 | prec=0.7094 rec=0.7545 spec=0.6923 f1=0.7313 | time=12.8s\n",
            "Epoch 016 | train_loss=0.6611 acc=0.6052 | val_loss=0.6289 acc=0.7052 | prec=0.8082 rec=0.5364 spec=0.8733 f1=0.6448 | time=13.0s\n",
            "Epoch 017 | train_loss=0.6124 acc=0.6784 | val_loss=0.5321 acc=0.7551 | prec=0.8256 rec=0.6455 spec=0.8643 f1=0.7245 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5668 acc=0.7232 | val_loss=0.5805 acc=0.7029 | prec=0.9495 rec=0.4273 spec=0.9774 f1=0.5893 | time=13.0s\n",
            "Epoch 019 | train_loss=0.5474 acc=0.7300 | val_loss=0.5078 acc=0.7823 | prec=0.8163 rec=0.7273 spec=0.8371 f1=0.7692 | time=12.9s\n",
            "Epoch 020 | train_loss=0.5254 acc=0.7436 | val_loss=0.5322 acc=0.7347 | prec=0.6794 rec=0.8864 spec=0.5837 f1=0.7692 | time=12.8s\n",
            "Epoch 021 | train_loss=0.4962 acc=0.7714 | val_loss=0.5219 acc=0.7506 | prec=0.6978 rec=0.8818 spec=0.6199 f1=0.7791 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4934 acc=0.7788 | val_loss=0.4704 acc=0.8118 | prec=0.8586 rec=0.7455 spec=0.8778 f1=0.7981 | time=13.0s\n",
            "Epoch 023 | train_loss=0.4639 acc=0.7981 | val_loss=0.4660 acc=0.8095 | prec=0.8400 rec=0.7636 spec=0.8552 f1=0.8000 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4585 acc=0.8037 | val_loss=0.4683 acc=0.7937 | prec=0.7611 rec=0.8545 spec=0.7330 f1=0.8051 | time=13.0s\n",
            "Epoch 025 | train_loss=0.4440 acc=0.8106 | val_loss=0.4552 acc=0.8027 | prec=0.8674 rec=0.7136 spec=0.8914 f1=0.7830 | time=12.9s\n",
            "Epoch 026 | train_loss=0.4405 acc=0.8043 | val_loss=0.4476 acc=0.8186 | prec=0.8723 rec=0.7455 spec=0.8914 f1=0.8039 | time=12.8s\n",
            "Epoch 027 | train_loss=0.4106 acc=0.8276 | val_loss=0.4470 acc=0.8095 | prec=0.8778 rec=0.7182 spec=0.9005 f1=0.7900 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4042 acc=0.8361 | val_loss=0.4299 acc=0.8209 | prec=0.8000 rec=0.8545 spec=0.7873 f1=0.8264 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3733 acc=0.8599 | val_loss=0.4174 acc=0.8277 | prec=0.8462 rec=0.8000 spec=0.8552 f1=0.8224 | time=12.9s\n",
            "Epoch 030 | train_loss=0.3746 acc=0.8588 | val_loss=0.4150 acc=0.8277 | prec=0.8186 rec=0.8409 spec=0.8145 f1=0.8296 | time=13.0s\n",
            "Epoch 031 | train_loss=0.3417 acc=0.8769 | val_loss=0.4275 acc=0.8141 | prec=0.7949 rec=0.8455 spec=0.7828 f1=0.8194 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3382 acc=0.8650 | val_loss=0.4140 acc=0.8254 | prec=0.8206 rec=0.8318 spec=0.8190 f1=0.8262 | time=12.9s\n",
            "Epoch 033 | train_loss=0.3267 acc=0.8684 | val_loss=0.5080 acc=0.7732 | prec=0.9225 rec=0.5955 spec=0.9502 f1=0.7238 | time=13.0s\n",
            "Epoch 034 | train_loss=0.3201 acc=0.8832 | val_loss=0.4146 acc=0.8186 | prec=0.7991 rec=0.8500 spec=0.7873 f1=0.8238 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3162 acc=0.8900 | val_loss=0.4354 acc=0.8209 | prec=0.8543 rec=0.7727 spec=0.8688 f1=0.8115 | time=12.7s\n",
            "Epoch 036 | train_loss=0.3441 acc=0.8554 | val_loss=0.4145 acc=0.8231 | prec=0.8381 rec=0.8000 spec=0.8462 f1=0.8186 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3001 acc=0.9019 | val_loss=0.4157 acc=0.8299 | prec=0.8281 rec=0.8318 spec=0.8281 f1=0.8299 | time=12.8s\n",
            "Epoch 038 | train_loss=0.2877 acc=0.8900 | val_loss=0.4331 acc=0.8027 | prec=0.7692 rec=0.8636 spec=0.7421 f1=0.8137 | time=12.8s\n",
            "Epoch 039 | train_loss=0.2740 acc=0.8951 | val_loss=0.4196 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.8s\n",
            "Epoch 040 | train_loss=0.2621 acc=0.9007 | val_loss=0.4427 acc=0.8118 | prec=0.7890 rec=0.8500 spec=0.7738 f1=0.8184 | time=12.8s\n",
            "Epoch 041 | train_loss=0.2689 acc=0.9019 | val_loss=0.4834 acc=0.7868 | prec=0.7423 rec=0.8773 spec=0.6968 f1=0.8042 | time=12.8s\n",
            "Epoch 042 | train_loss=0.2673 acc=0.9075 | val_loss=0.4286 acc=0.8231 | prec=0.8447 rec=0.7909 spec=0.8552 f1=0.8169 | time=13.0s\n",
            "Epoch 043 | train_loss=0.2537 acc=0.9087 | val_loss=0.4374 acc=0.8209 | prec=0.8615 rec=0.7636 spec=0.8778 f1=0.8096 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2435 acc=0.9092 | val_loss=0.4333 acc=0.8277 | prec=0.8871 rec=0.7500 spec=0.9050 f1=0.8128 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2221 acc=0.9246 | val_loss=0.4430 acc=0.8322 | prec=0.8724 rec=0.7773 spec=0.8869 f1=0.8221 | time=13.0s\n",
            "Epoch 046 | train_loss=0.2257 acc=0.9217 | val_loss=0.4741 acc=0.8005 | prec=0.8929 rec=0.6818 spec=0.9186 f1=0.7732 | time=12.9s\n",
            "Epoch 047 | train_loss=0.2210 acc=0.9121 | val_loss=0.4670 acc=0.8073 | prec=0.8902 rec=0.7000 spec=0.9140 f1=0.7837 | time=13.0s\n",
            "Early stopping at epoch 47\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▅▃▅▁▆▇▇▇▇▇▇██████▇████▇█▇██▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▆▆█▆▄▇▆▅▇▇▆▆▆▆▆▆▇▆▆▅▆▅▆▇▇▇▇</td></tr><tr><td>recall</td><td>████████████▅▂▄▁▅▇▅▅▆▄▅▆▆▆▆▆▆▅▆▆▆▆▆▅▅▅▅▄</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▆▇▇█▇▅▇▇▆▇▇▇▇▇▇▇▇▇▇▇▆▇▆▇▇▇▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▃▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█▇███████</td></tr><tr><td>train_loss</td><td>██████████▇█▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▆▅▆▅▇▆██▇▇██████████▇█▇████▇</td></tr><tr><td>validation_loss</td><td>██▇▇▇▇▇▇▇▇▇▇▇▆▄▅▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▃▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78372</td></tr><tr><td>precision</td><td>0.89017</td></tr><tr><td>recall</td><td>0.7</td></tr><tr><td>specificity</td><td>0.91403</td></tr><tr><td>train_accuracy</td><td>0.91208</td></tr><tr><td>train_loss</td><td>0.22097</td></tr><tr><td>validation_accuracy</td><td>0.80726</td></tr><tr><td>validation_loss</td><td>0.46705</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qzxgrewn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qzxgrewn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_071212-qzxgrewn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 07:22:18,868] Trial 5 finished with values: [0.4670457105551447, 0.8072562358276644] and parameters: {'lr': 4.5618779571518155e-05, 'wd': 1.5998714594832107e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: filters=120, lr=2.17e-05, wd=6.58e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_072218-9pem0iq1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/9pem0iq1' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/9pem0iq1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/9pem0iq1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7844 acc=0.4969 | val_loss=0.7257 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7741 acc=0.5071 | val_loss=0.7096 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7477 acc=0.5031 | val_loss=0.6968 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7356 acc=0.4901 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7315 acc=0.5031 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7336 acc=0.4861 | val_loss=0.6939 acc=0.4943 | prec=0.4965 rec=0.9545 spec=0.0362 f1=0.6532 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7263 acc=0.4912 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7155 acc=0.5116 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7131 acc=0.5088 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7058 acc=0.5105 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7138 acc=0.4957 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7194 acc=0.4855 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7036 acc=0.5060 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7103 acc=0.5026 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7027 acc=0.5054 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7115 acc=0.4969 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7137 acc=0.5139 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7103 acc=0.5122 | val_loss=0.6908 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 019 | train_loss=0.6974 acc=0.5292 | val_loss=0.6971 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 020 | train_loss=0.6883 acc=0.5547 | val_loss=0.6596 acc=0.7302 | prec=0.7440 rec=0.7000 spec=0.7602 f1=0.7213 | time=12.8s\n",
            "Epoch 021 | train_loss=0.6655 acc=0.5876 | val_loss=0.6864 acc=0.5215 | prec=1.0000 rec=0.0409 spec=1.0000 f1=0.0786 | time=12.8s\n",
            "Epoch 022 | train_loss=0.6216 acc=0.6500 | val_loss=0.6184 acc=0.6712 | prec=0.9121 rec=0.3773 spec=0.9638 f1=0.5338 | time=13.1s\n",
            "Epoch 023 | train_loss=0.5835 acc=0.6994 | val_loss=0.5713 acc=0.7438 | prec=0.9023 rec=0.5455 spec=0.9412 f1=0.6799 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5683 acc=0.7096 | val_loss=0.5399 acc=0.7687 | prec=0.8554 rec=0.6455 spec=0.8914 f1=0.7358 | time=12.8s\n",
            "Epoch 025 | train_loss=0.5533 acc=0.7362 | val_loss=0.5218 acc=0.7868 | prec=0.8387 rec=0.7091 spec=0.8643 f1=0.7685 | time=12.8s\n",
            "Epoch 026 | train_loss=0.5459 acc=0.7453 | val_loss=0.5411 acc=0.7846 | prec=0.8882 rec=0.6500 spec=0.9186 f1=0.7507 | time=13.0s\n",
            "Epoch 027 | train_loss=0.5280 acc=0.7521 | val_loss=0.5247 acc=0.7868 | prec=0.9038 rec=0.6409 spec=0.9321 f1=0.7500 | time=13.0s\n",
            "Epoch 028 | train_loss=0.5262 acc=0.7697 | val_loss=0.5200 acc=0.7982 | prec=0.8701 rec=0.7000 spec=0.8959 f1=0.7758 | time=12.8s\n",
            "Epoch 029 | train_loss=0.5068 acc=0.7618 | val_loss=0.5050 acc=0.7959 | prec=0.7902 rec=0.8045 spec=0.7873 f1=0.7973 | time=12.8s\n",
            "Epoch 030 | train_loss=0.5128 acc=0.7680 | val_loss=0.5000 acc=0.8095 | prec=0.8505 rec=0.7500 spec=0.8688 f1=0.7971 | time=12.8s\n",
            "Epoch 031 | train_loss=0.5051 acc=0.7828 | val_loss=0.4920 acc=0.7891 | prec=0.8944 rec=0.6545 spec=0.9231 f1=0.7559 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4968 acc=0.7964 | val_loss=0.4785 acc=0.7982 | prec=0.8466 rec=0.7273 spec=0.8688 f1=0.7824 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4984 acc=0.7805 | val_loss=0.4897 acc=0.7914 | prec=0.8902 rec=0.6636 spec=0.9186 f1=0.7604 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4549 acc=0.7998 | val_loss=0.4869 acc=0.7982 | prec=0.8786 rec=0.6909 spec=0.9050 f1=0.7735 | time=12.9s\n",
            "Epoch 035 | train_loss=0.4588 acc=0.8015 | val_loss=0.4756 acc=0.7937 | prec=0.9108 rec=0.6500 spec=0.9367 f1=0.7586 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4583 acc=0.8009 | val_loss=0.5046 acc=0.7642 | prec=0.9754 rec=0.5409 spec=0.9864 f1=0.6959 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4373 acc=0.8140 | val_loss=0.4555 acc=0.8231 | prec=0.8737 rec=0.7545 spec=0.8914 f1=0.8098 | time=12.8s\n",
            "Epoch 038 | train_loss=0.4257 acc=0.8242 | val_loss=0.4810 acc=0.8050 | prec=0.9351 rec=0.6545 spec=0.9548 f1=0.7701 | time=12.8s\n",
            "Epoch 039 | train_loss=0.4278 acc=0.8106 | val_loss=0.4419 acc=0.8209 | prec=0.8895 rec=0.7318 spec=0.9095 f1=0.8030 | time=12.8s\n",
            "Epoch 040 | train_loss=0.4425 acc=0.8162 | val_loss=0.4345 acc=0.8322 | prec=0.8883 rec=0.7591 spec=0.9050 f1=0.8186 | time=12.8s\n",
            "Epoch 041 | train_loss=0.4194 acc=0.8225 | val_loss=0.4720 acc=0.7982 | prec=0.9456 rec=0.6318 spec=0.9638 f1=0.7575 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3968 acc=0.8338 | val_loss=0.4811 acc=0.8141 | prec=0.9481 rec=0.6636 spec=0.9638 f1=0.7807 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3801 acc=0.8452 | val_loss=0.4836 acc=0.7778 | prec=0.9621 rec=0.5773 spec=0.9774 f1=0.7216 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3875 acc=0.8508 | val_loss=0.5012 acc=0.7551 | prec=0.9746 rec=0.5227 spec=0.9864 f1=0.6805 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3822 acc=0.8582 | val_loss=0.4472 acc=0.8390 | prec=0.9027 rec=0.7591 spec=0.9186 f1=0.8247 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3739 acc=0.8469 | val_loss=0.4630 acc=0.7937 | prec=0.9448 rec=0.6227 spec=0.9638 f1=0.7507 | time=12.7s\n",
            "Epoch 047 | train_loss=0.3643 acc=0.8497 | val_loss=0.4430 acc=0.8277 | prec=0.9186 rec=0.7182 spec=0.9367 f1=0.8061 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3585 acc=0.8616 | val_loss=0.4650 acc=0.7937 | prec=0.9388 rec=0.6273 spec=0.9593 f1=0.7520 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3668 acc=0.8667 | val_loss=0.4542 acc=0.7914 | prec=0.9267 rec=0.6318 spec=0.9502 f1=0.7514 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3555 acc=0.8554 | val_loss=0.4541 acc=0.7959 | prec=0.9276 rec=0.6409 spec=0.9502 f1=0.7581 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3463 acc=0.8724 | val_loss=0.5009 acc=0.7574 | prec=0.9748 rec=0.5273 spec=0.9864 f1=0.6844 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3364 acc=0.8656 | val_loss=0.4780 acc=0.7710 | prec=0.9542 rec=0.5682 spec=0.9729 f1=0.7123 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3300 acc=0.8798 | val_loss=0.4700 acc=0.7664 | prec=0.9535 rec=0.5591 spec=0.9729 f1=0.7049 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3194 acc=0.8866 | val_loss=0.5141 acc=0.7483 | prec=0.9580 rec=0.5182 spec=0.9774 f1=0.6726 | time=12.9s\n",
            "Epoch 055 | train_loss=0.3221 acc=0.8746 | val_loss=0.4756 acc=0.7891 | prec=0.9320 rec=0.6227 spec=0.9548 f1=0.7466 | time=12.8s\n",
            "Early stopping at epoch 55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▇▇▇▇▇▁▁▁▁▁▁▁▁▁▇▆▇▇█▇███▇██▇████▇█▇█▇▇▇▇▇</td></tr><tr><td>precision</td><td>▅▅▅▅▅▅▁▁▁▁▁▁▁▁▆█▇▇▇▇▇▇▇▇▇███▇▇███▇██████</td></tr><tr><td>recall</td><td>█████▁▁▁▁▁▁▁▁▁▁▆▁▅▆▆▅▆▇▆▆▆▆▆▆▆▅▆▅▅▆▆▅▅▅▅</td></tr><tr><td>specificity</td><td>▁▁▁▁▁█████████████▇▇▇▇▇▇▇██▇█▇███▇██████</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▄▅▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇▇█████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▇▇▇▇▇▇▅▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▆▇▇▇▇▇▇▇▇▆█▇██▇▇▆█▇▇▆▇▇▇</td></tr><tr><td>validation_loss</td><td>██████████████▇▆▄▄▃▃▃▃▂▂▂▂▃▁▂▁▂▃▁▂▁▁▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.74659</td></tr><tr><td>precision</td><td>0.93197</td></tr><tr><td>recall</td><td>0.62273</td></tr><tr><td>specificity</td><td>0.95475</td></tr><tr><td>train_accuracy</td><td>0.87465</td></tr><tr><td>train_loss</td><td>0.32207</td></tr><tr><td>validation_accuracy</td><td>0.78912</td></tr><tr><td>validation_loss</td><td>0.47559</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/9pem0iq1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/9pem0iq1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_072218-9pem0iq1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 07:34:08,160] Trial 6 finished with values: [0.4755913040467671, 0.7891156462585034] and parameters: {'lr': 2.1717529935431917e-05, 'wd': 6.5839520976037974e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: filters=120, lr=2.31e-05, wd=7.20e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_073408-qedcuuzv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qedcuuzv' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qedcuuzv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qedcuuzv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7524 acc=0.4969 | val_loss=0.7017 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7168 acc=0.5026 | val_loss=0.7036 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7205 acc=0.5082 | val_loss=0.7004 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7075 acc=0.5162 | val_loss=0.7020 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7134 acc=0.5065 | val_loss=0.6992 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7137 acc=0.5009 | val_loss=0.6989 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7106 acc=0.4844 | val_loss=0.6957 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7090 acc=0.4991 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7068 acc=0.4980 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7053 acc=0.5026 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.6997 acc=0.5366 | val_loss=0.6930 acc=0.5011 | prec=0.5000 rec=0.9864 spec=0.0181 f1=0.6636 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7030 acc=0.5065 | val_loss=0.6918 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7016 acc=0.5355 | val_loss=0.6919 acc=0.5034 | prec=0.6667 rec=0.0091 spec=0.9955 f1=0.0179 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7123 acc=0.5116 | val_loss=0.6907 acc=0.5556 | prec=0.5331 rec=0.8773 spec=0.2353 f1=0.6632 | time=12.8s\n",
            "Epoch 015 | train_loss=0.6995 acc=0.5071 | val_loss=0.6920 acc=0.5374 | prec=0.5211 rec=0.9000 spec=0.1765 f1=0.6600 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7057 acc=0.5020 | val_loss=0.6915 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7074 acc=0.4957 | val_loss=0.6903 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 018 | train_loss=0.6962 acc=0.5224 | val_loss=0.6829 acc=0.6780 | prec=0.7097 rec=0.6000 spec=0.7557 f1=0.6502 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6901 acc=0.5428 | val_loss=0.6724 acc=0.5737 | prec=0.9211 rec=0.1591 spec=0.9864 f1=0.2713 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6778 acc=0.5627 | val_loss=0.6528 acc=0.6327 | prec=0.8816 rec=0.3045 spec=0.9593 f1=0.4527 | time=12.9s\n",
            "Epoch 021 | train_loss=0.6570 acc=0.6018 | val_loss=0.6240 acc=0.6576 | prec=0.9157 rec=0.3455 spec=0.9683 f1=0.5017 | time=12.9s\n",
            "Epoch 022 | train_loss=0.6306 acc=0.6563 | val_loss=0.5824 acc=0.7619 | prec=0.8662 rec=0.6182 spec=0.9050 f1=0.7215 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5991 acc=0.6948 | val_loss=0.5564 acc=0.7642 | prec=0.8494 rec=0.6409 spec=0.8869 f1=0.7306 | time=12.8s\n",
            "Epoch 024 | train_loss=0.5871 acc=0.7050 | val_loss=0.6004 acc=0.7256 | prec=0.6656 rec=0.9045 spec=0.5475 f1=0.7669 | time=12.8s\n",
            "Epoch 025 | train_loss=0.5686 acc=0.7249 | val_loss=0.5721 acc=0.7664 | prec=0.7259 rec=0.8545 spec=0.6787 f1=0.7850 | time=12.8s\n",
            "Epoch 026 | train_loss=0.5686 acc=0.7158 | val_loss=0.5536 acc=0.7778 | prec=0.7563 rec=0.8182 spec=0.7376 f1=0.7860 | time=13.0s\n",
            "Epoch 027 | train_loss=0.5605 acc=0.7436 | val_loss=0.5377 acc=0.7778 | prec=0.7773 rec=0.7773 spec=0.7783 f1=0.7773 | time=13.0s\n",
            "Epoch 028 | train_loss=0.5430 acc=0.7419 | val_loss=0.5260 acc=0.7868 | prec=0.8387 rec=0.7091 spec=0.8643 f1=0.7685 | time=12.8s\n",
            "Epoch 029 | train_loss=0.5314 acc=0.7482 | val_loss=0.5203 acc=0.7846 | prec=0.8613 rec=0.6773 spec=0.8914 f1=0.7583 | time=12.9s\n",
            "Epoch 030 | train_loss=0.5297 acc=0.7561 | val_loss=0.5389 acc=0.7982 | prec=0.7885 rec=0.8136 spec=0.7828 f1=0.8009 | time=12.9s\n",
            "Epoch 031 | train_loss=0.5154 acc=0.7697 | val_loss=0.4885 acc=0.8027 | prec=0.8276 rec=0.7636 spec=0.8416 f1=0.7943 | time=13.0s\n",
            "Epoch 032 | train_loss=0.5125 acc=0.7714 | val_loss=0.4944 acc=0.8118 | prec=0.8216 rec=0.7955 spec=0.8281 f1=0.8083 | time=13.0s\n",
            "Epoch 033 | train_loss=0.4857 acc=0.7890 | val_loss=0.4906 acc=0.7823 | prec=0.8690 rec=0.6636 spec=0.9005 f1=0.7526 | time=12.8s\n",
            "Epoch 034 | train_loss=0.4683 acc=0.8015 | val_loss=0.4766 acc=0.8186 | prec=0.8125 rec=0.8273 spec=0.8100 f1=0.8198 | time=12.9s\n",
            "Epoch 035 | train_loss=0.4764 acc=0.8026 | val_loss=0.4815 acc=0.7937 | prec=0.8862 rec=0.6727 spec=0.9140 f1=0.7649 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4755 acc=0.7850 | val_loss=0.4751 acc=0.7937 | prec=0.8862 rec=0.6727 spec=0.9140 f1=0.7649 | time=13.0s\n",
            "Epoch 037 | train_loss=0.4541 acc=0.8032 | val_loss=0.4730 acc=0.8005 | prec=0.8929 rec=0.6818 spec=0.9186 f1=0.7732 | time=12.9s\n",
            "Epoch 038 | train_loss=0.4580 acc=0.8088 | val_loss=0.4457 acc=0.8163 | prec=0.8357 rec=0.7864 spec=0.8462 f1=0.8103 | time=12.9s\n",
            "Epoch 039 | train_loss=0.4448 acc=0.8196 | val_loss=0.4498 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=12.9s\n",
            "Epoch 040 | train_loss=0.4499 acc=0.8196 | val_loss=0.4498 acc=0.8254 | prec=0.8593 rec=0.7773 spec=0.8733 f1=0.8162 | time=12.9s\n",
            "Epoch 041 | train_loss=0.4246 acc=0.8366 | val_loss=0.4386 acc=0.8231 | prec=0.8318 rec=0.8091 spec=0.8371 f1=0.8203 | time=12.9s\n",
            "Epoch 042 | train_loss=0.4303 acc=0.8202 | val_loss=0.4257 acc=0.8322 | prec=0.8842 rec=0.7636 spec=0.9005 f1=0.8195 | time=12.9s\n",
            "Epoch 043 | train_loss=0.4156 acc=0.8542 | val_loss=0.4360 acc=0.8277 | prec=0.8303 rec=0.8227 spec=0.8326 f1=0.8265 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3926 acc=0.8434 | val_loss=0.4403 acc=0.8027 | prec=0.8844 rec=0.6955 spec=0.9095 f1=0.7786 | time=12.9s\n",
            "Epoch 045 | train_loss=0.4162 acc=0.8276 | val_loss=0.4212 acc=0.8299 | prec=0.7959 rec=0.8864 spec=0.7738 f1=0.8387 | time=13.0s\n",
            "Epoch 046 | train_loss=0.3885 acc=0.8565 | val_loss=0.4209 acc=0.8186 | prec=0.8763 rec=0.7409 spec=0.8959 f1=0.8030 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3960 acc=0.8452 | val_loss=0.4098 acc=0.8299 | prec=0.8718 rec=0.7727 spec=0.8869 f1=0.8193 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3751 acc=0.8593 | val_loss=0.4443 acc=0.7959 | prec=0.9062 rec=0.6591 spec=0.9321 f1=0.7632 | time=13.0s\n",
            "Epoch 049 | train_loss=0.3746 acc=0.8497 | val_loss=0.4160 acc=0.8141 | prec=0.8876 rec=0.7182 spec=0.9095 f1=0.7940 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3643 acc=0.8559 | val_loss=0.4016 acc=0.8277 | prec=0.8913 rec=0.7455 spec=0.9095 f1=0.8119 | time=12.9s\n",
            "Epoch 051 | train_loss=0.3511 acc=0.8729 | val_loss=0.4168 acc=0.8231 | prec=0.9034 rec=0.7227 spec=0.9231 f1=0.8030 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3560 acc=0.8741 | val_loss=0.3921 acc=0.8549 | prec=0.8578 rec=0.8500 spec=0.8597 f1=0.8539 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3600 acc=0.8678 | val_loss=0.4421 acc=0.7914 | prec=0.9156 rec=0.6409 spec=0.9412 f1=0.7540 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3511 acc=0.8667 | val_loss=0.4279 acc=0.8095 | prec=0.9096 rec=0.6864 spec=0.9321 f1=0.7824 | time=12.8s\n",
            "Epoch 055 | train_loss=0.3303 acc=0.8843 | val_loss=0.4119 acc=0.8118 | prec=0.8785 rec=0.7227 spec=0.9005 f1=0.7930 | time=12.8s\n",
            "Epoch 056 | train_loss=0.3275 acc=0.8763 | val_loss=0.3908 acc=0.8549 | prec=0.8786 rec=0.8227 spec=0.8869 f1=0.8498 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3315 acc=0.8752 | val_loss=0.3779 acc=0.8503 | prec=0.8565 rec=0.8409 spec=0.8597 f1=0.8486 | time=12.8s\n",
            "Epoch 058 | train_loss=0.3124 acc=0.8866 | val_loss=0.4533 acc=0.7891 | prec=0.9504 rec=0.6091 spec=0.9683 f1=0.7424 | time=12.9s\n",
            "Epoch 059 | train_loss=0.3229 acc=0.8843 | val_loss=0.3752 acc=0.8571 | prec=0.8685 rec=0.8409 spec=0.8733 f1=0.8545 | time=12.9s\n",
            "Epoch 060 | train_loss=0.3378 acc=0.8622 | val_loss=0.4353 acc=0.8027 | prec=0.9290 rec=0.6545 spec=0.9502 f1=0.7680 | time=13.0s\n",
            "Epoch 061 | train_loss=0.3154 acc=0.8917 | val_loss=0.4251 acc=0.8095 | prec=0.9304 rec=0.6682 spec=0.9502 f1=0.7778 | time=12.9s\n",
            "Epoch 062 | train_loss=0.3130 acc=0.8678 | val_loss=0.3776 acc=0.8435 | prec=0.8832 rec=0.7909 spec=0.8959 f1=0.8345 | time=12.8s\n",
            "Epoch 063 | train_loss=0.3083 acc=0.8928 | val_loss=0.3656 acc=0.8571 | prec=0.8651 rec=0.8455 spec=0.8688 f1=0.8552 | time=12.8s\n",
            "Epoch 064 | train_loss=0.3311 acc=0.8656 | val_loss=0.4170 acc=0.8141 | prec=0.9059 rec=0.7000 spec=0.9276 f1=0.7897 | time=12.9s\n",
            "Epoch 065 | train_loss=0.3090 acc=0.8809 | val_loss=0.4333 acc=0.8095 | prec=0.9198 rec=0.6773 spec=0.9412 f1=0.7801 | time=12.7s\n",
            "Epoch 066 | train_loss=0.2889 acc=0.9007 | val_loss=0.4000 acc=0.8254 | prec=0.8994 rec=0.7318 spec=0.9186 f1=0.8070 | time=12.8s\n",
            "Epoch 067 | train_loss=0.2977 acc=0.8871 | val_loss=0.4036 acc=0.8254 | prec=0.8950 rec=0.7364 spec=0.9140 f1=0.8080 | time=12.8s\n",
            "Epoch 068 | train_loss=0.2884 acc=0.8900 | val_loss=0.4354 acc=0.8141 | prec=0.9367 rec=0.6727 spec=0.9548 f1=0.7831 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2863 acc=0.8866 | val_loss=0.3663 acc=0.8571 | prec=0.8369 rec=0.8864 spec=0.8281 f1=0.8609 | time=13.0s\n",
            "Epoch 070 | train_loss=0.2627 acc=0.9087 | val_loss=0.3815 acc=0.8390 | prec=0.8901 rec=0.7727 spec=0.9050 f1=0.8273 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2646 acc=0.9087 | val_loss=0.3977 acc=0.8322 | prec=0.8925 rec=0.7545 spec=0.9095 f1=0.8177 | time=12.9s\n",
            "Epoch 072 | train_loss=0.2775 acc=0.8973 | val_loss=0.3842 acc=0.8458 | prec=0.9086 rec=0.7682 spec=0.9231 f1=0.8325 | time=12.9s\n",
            "Epoch 073 | train_loss=0.2743 acc=0.9087 | val_loss=0.3661 acc=0.8481 | prec=0.8660 rec=0.8227 spec=0.8733 f1=0.8438 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2709 acc=0.9058 | val_loss=0.3864 acc=0.8390 | prec=0.8984 rec=0.7636 spec=0.9140 f1=0.8256 | time=12.8s\n",
            "Epoch 075 | train_loss=0.2711 acc=0.9030 | val_loss=0.4416 acc=0.8254 | prec=0.9554 rec=0.6818 spec=0.9683 f1=0.7958 | time=12.8s\n",
            "Epoch 076 | train_loss=0.2628 acc=0.9070 | val_loss=0.4307 acc=0.8231 | prec=0.9226 rec=0.7045 spec=0.9412 f1=0.7990 | time=12.9s\n",
            "Epoch 077 | train_loss=0.2689 acc=0.9081 | val_loss=0.4052 acc=0.8254 | prec=0.8994 rec=0.7318 spec=0.9186 f1=0.8070 | time=12.9s\n",
            "Epoch 078 | train_loss=0.2725 acc=0.8911 | val_loss=0.3691 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=13.0s\n",
            "Early stopping at epoch 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▆▆▆▅▇▇███▇▇███▇▇██▇█▇█▇▇█▇█████</td></tr><tr><td>precision</td><td>▁▁▁▅█▄▄▇▇▇▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁█▁▇█▂▅▅▇▆▇▆▇▆▆▆▆▆▇▆▆▆▆▇▇▆▆▇▆▆▇▆▆▇</td></tr><tr><td>specificity</td><td>███████▁██▂▁▁▆█▇▅▆▆▇▆▇▇▇▇▇▇▇▇█▇▇█▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▁▁▁▂▃▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇████████</td></tr><tr><td>train_loss</td><td>███████████▇▇▆▆▅▅▅▅▄▄▄▃▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▂▁▁▂▆▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█▇▇▇███▇</td></tr><tr><td>validation_loss</td><td>████████▆▅▅▅▄▄▅▄▄▃▃▃▃▂▃▂▂▂▃▂▂▁▂▂▂▂▂▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83294</td></tr><tr><td>precision</td><td>0.86341</td></tr><tr><td>recall</td><td>0.80455</td></tr><tr><td>specificity</td><td>0.8733</td></tr><tr><td>train_accuracy</td><td>0.89109</td></tr><tr><td>train_loss</td><td>0.27245</td></tr><tr><td>validation_accuracy</td><td>0.839</td></tr><tr><td>validation_loss</td><td>0.36906</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qedcuuzv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/qedcuuzv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_073408-qedcuuzv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 07:50:53,979] Trial 7 finished with values: [0.3690581832613264, 0.8390022675736961] and parameters: {'lr': 2.3089123428359085e-05, 'wd': 7.204776343575214e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: filters=120, lr=4.53e-05, wd=2.78e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_075053-vsxuuq08</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/vsxuuq08' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/vsxuuq08' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/vsxuuq08</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7813 acc=0.4867 | val_loss=0.7196 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7666 acc=0.4867 | val_loss=0.7192 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7572 acc=0.4816 | val_loss=0.7114 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7584 acc=0.4827 | val_loss=0.7044 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7281 acc=0.5009 | val_loss=0.7020 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7275 acc=0.5230 | val_loss=0.6979 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7276 acc=0.5060 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7369 acc=0.5031 | val_loss=0.7007 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7325 acc=0.5116 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7221 acc=0.5111 | val_loss=0.6907 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7021 acc=0.5372 | val_loss=0.6859 acc=0.5601 | prec=0.8095 rec=0.1545 spec=0.9638 f1=0.2595 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7145 acc=0.5235 | val_loss=0.6884 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7082 acc=0.5383 | val_loss=0.6911 acc=0.5147 | prec=1.0000 rec=0.0273 spec=1.0000 f1=0.0531 | time=13.0s\n",
            "Epoch 014 | train_loss=0.6684 acc=0.5893 | val_loss=0.6690 acc=0.5805 | prec=0.9730 rec=0.1636 spec=0.9955 f1=0.2802 | time=12.9s\n",
            "Epoch 015 | train_loss=0.6100 acc=0.6625 | val_loss=0.5486 acc=0.7347 | prec=0.6943 rec=0.8364 spec=0.6335 f1=0.7588 | time=12.8s\n",
            "Epoch 016 | train_loss=0.5630 acc=0.7136 | val_loss=0.5205 acc=0.7664 | prec=0.9270 rec=0.5773 spec=0.9548 f1=0.7115 | time=12.9s\n",
            "Epoch 017 | train_loss=0.5399 acc=0.7419 | val_loss=0.5071 acc=0.7438 | prec=0.6877 rec=0.8909 spec=0.5973 f1=0.7762 | time=13.0s\n",
            "Epoch 018 | train_loss=0.5265 acc=0.7362 | val_loss=0.4407 acc=0.7959 | prec=0.8283 rec=0.7455 spec=0.8462 f1=0.7847 | time=12.8s\n",
            "Epoch 019 | train_loss=0.4877 acc=0.7601 | val_loss=0.4390 acc=0.8141 | prec=0.8416 rec=0.7727 spec=0.8552 f1=0.8057 | time=12.9s\n",
            "Epoch 020 | train_loss=0.4741 acc=0.8003 | val_loss=0.5035 acc=0.7596 | prec=0.9254 rec=0.5636 spec=0.9548 f1=0.7006 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4629 acc=0.7890 | val_loss=0.4367 acc=0.8073 | prec=0.8857 rec=0.7045 spec=0.9095 f1=0.7848 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4417 acc=0.7992 | val_loss=0.4229 acc=0.8141 | prec=0.8791 rec=0.7273 spec=0.9005 f1=0.7960 | time=12.9s\n",
            "Epoch 023 | train_loss=0.4164 acc=0.8145 | val_loss=0.4793 acc=0.7800 | prec=0.9073 rec=0.6227 spec=0.9367 f1=0.7385 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4188 acc=0.8009 | val_loss=0.4118 acc=0.8163 | prec=0.8883 rec=0.7227 spec=0.9095 f1=0.7970 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3895 acc=0.8270 | val_loss=0.5543 acc=0.7256 | prec=0.9459 rec=0.4773 spec=0.9729 f1=0.6344 | time=12.7s\n",
            "Epoch 026 | train_loss=0.3902 acc=0.8315 | val_loss=0.5372 acc=0.7347 | prec=0.9558 rec=0.4909 spec=0.9774 f1=0.6486 | time=13.0s\n",
            "Epoch 027 | train_loss=0.3749 acc=0.8321 | val_loss=0.5024 acc=0.7528 | prec=0.9237 rec=0.5500 spec=0.9548 f1=0.6895 | time=12.7s\n",
            "Epoch 028 | train_loss=0.3766 acc=0.8338 | val_loss=0.3978 acc=0.8254 | prec=0.8743 rec=0.7591 spec=0.8914 f1=0.8127 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3293 acc=0.8582 | val_loss=0.5098 acc=0.7574 | prec=0.9380 rec=0.5500 spec=0.9638 f1=0.6934 | time=13.0s\n",
            "Epoch 030 | train_loss=0.3381 acc=0.8667 | val_loss=0.5002 acc=0.7528 | prec=0.9051 rec=0.5636 spec=0.9412 f1=0.6947 | time=12.8s\n",
            "Epoch 031 | train_loss=0.3249 acc=0.8741 | val_loss=0.3695 acc=0.8435 | prec=0.8647 rec=0.8136 spec=0.8733 f1=0.8384 | time=13.0s\n",
            "Epoch 032 | train_loss=0.3094 acc=0.8605 | val_loss=0.3946 acc=0.8186 | prec=0.8646 rec=0.7545 spec=0.8824 f1=0.8058 | time=12.9s\n",
            "Epoch 033 | train_loss=0.3051 acc=0.8695 | val_loss=0.5657 acc=0.7392 | prec=0.9268 rec=0.5182 spec=0.9593 f1=0.6647 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3002 acc=0.8792 | val_loss=0.3810 acc=0.8322 | prec=0.8067 rec=0.8727 spec=0.7919 f1=0.8384 | time=13.0s\n",
            "Epoch 035 | train_loss=0.2762 acc=0.9002 | val_loss=0.4853 acc=0.7868 | prec=0.9315 rec=0.6182 spec=0.9548 f1=0.7432 | time=12.8s\n",
            "Epoch 036 | train_loss=0.2629 acc=0.8996 | val_loss=0.3754 acc=0.8277 | prec=0.8364 rec=0.8136 spec=0.8416 f1=0.8249 | time=12.8s\n",
            "Epoch 037 | train_loss=0.2513 acc=0.8951 | val_loss=0.3952 acc=0.8209 | prec=0.8106 rec=0.8364 spec=0.8054 f1=0.8233 | time=12.9s\n",
            "Epoch 038 | train_loss=0.2453 acc=0.9070 | val_loss=0.4822 acc=0.7959 | prec=0.9167 rec=0.6500 spec=0.9412 f1=0.7606 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2570 acc=0.8985 | val_loss=0.6014 acc=0.7415 | prec=0.9492 rec=0.5091 spec=0.9729 f1=0.6627 | time=12.9s\n",
            "Epoch 040 | train_loss=0.2748 acc=0.8871 | val_loss=0.5625 acc=0.7596 | prec=0.9453 rec=0.5500 spec=0.9683 f1=0.6954 | time=12.8s\n",
            "Epoch 041 | train_loss=0.2511 acc=0.8922 | val_loss=0.5512 acc=0.7732 | prec=0.9478 rec=0.5773 spec=0.9683 f1=0.7175 | time=12.8s\n",
            "Epoch 042 | train_loss=0.2291 acc=0.9104 | val_loss=0.5411 acc=0.7868 | prec=0.9257 rec=0.6227 spec=0.9502 f1=0.7446 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2216 acc=0.9149 | val_loss=0.5796 acc=0.7664 | prec=0.9466 rec=0.5636 spec=0.9683 f1=0.7066 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2477 acc=0.9019 | val_loss=0.4948 acc=0.7914 | prec=0.9103 rec=0.6455 spec=0.9367 f1=0.7553 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2051 acc=0.9155 | val_loss=0.4663 acc=0.8231 | prec=0.8989 rec=0.7273 spec=0.9186 f1=0.8040 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2016 acc=0.9200 | val_loss=0.4553 acc=0.8186 | prec=0.8933 rec=0.7227 spec=0.9140 f1=0.7990 | time=12.9s\n",
            "Early stopping at epoch 46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▃▁▁▃▇▇██▇███▆▆▇█▇▇█▇█▇██▇▇▇▇▇▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁█▇▁██▆▆▇▇▇▇▇▇██▇▇█▇▇▇▇█▇▇███▇█▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂██▇▇▅▇▇▇▅▅▅▇▅▅▇▅█▆▇█▅▅▆▆▅▆▇</td></tr><tr><td>specificity</td><td>█████████▇███▂▁▅▅▇▆▆▆██▇▆▇▇▆▇▄▇▅▅█▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▂▁▁▁▂▂▂▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▇▇▇▇▇▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▃▆▆▇█▆▇██▆▆▆█▆▆█▆█▇██▆▆▇▇▇▇█</td></tr><tr><td>validation_loss</td><td>████████▇▇▇▇▇▅▄▂▂▄▂▂▂▅▄▄▁▄▄▁▅▁▃▁▁▆▅▅▄▅▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.79899</td></tr><tr><td>precision</td><td>0.89326</td></tr><tr><td>recall</td><td>0.72273</td></tr><tr><td>specificity</td><td>0.91403</td></tr><tr><td>train_accuracy</td><td>0.92002</td></tr><tr><td>train_loss</td><td>0.2016</td></tr><tr><td>validation_accuracy</td><td>0.81859</td></tr><tr><td>validation_loss</td><td>0.45533</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/vsxuuq08' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/vsxuuq08</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_075053-vsxuuq08/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 08:00:47,915] Trial 8 finished with values: [0.45533365862710135, 0.81859410430839] and parameters: {'lr': 4.532989479228827e-05, 'wd': 2.7778806517326157e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: filters=120, lr=1.44e-05, wd=4.65e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_080047-62phk6cf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/62phk6cf' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/62phk6cf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/62phk6cf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7235 acc=0.4974 | val_loss=0.7062 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7195 acc=0.4991 | val_loss=0.7038 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7166 acc=0.5020 | val_loss=0.7090 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7156 acc=0.4872 | val_loss=0.7044 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7073 acc=0.5235 | val_loss=0.7109 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7104 acc=0.5111 | val_loss=0.6967 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7142 acc=0.5009 | val_loss=0.6960 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7096 acc=0.5167 | val_loss=0.6982 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7101 acc=0.4940 | val_loss=0.7038 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7156 acc=0.5082 | val_loss=0.7001 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7065 acc=0.4969 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7168 acc=0.4844 | val_loss=0.7003 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7035 acc=0.5252 | val_loss=0.7007 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 014 | train_loss=0.7123 acc=0.4906 | val_loss=0.6968 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7067 acc=0.4946 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7117 acc=0.5065 | val_loss=0.6933 acc=0.5034 | prec=0.5012 rec=0.9818 spec=0.0271 f1=0.6636 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7178 acc=0.4878 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7071 acc=0.5128 | val_loss=0.6929 acc=0.5125 | prec=0.5078 rec=0.7364 spec=0.2896 f1=0.6011 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7129 acc=0.4918 | val_loss=0.6934 acc=0.5011 | prec=0.5000 rec=0.9636 spec=0.0407 f1=0.6584 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7061 acc=0.4901 | val_loss=0.6951 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7098 acc=0.4940 | val_loss=0.6926 acc=0.5420 | prec=0.5652 rec=0.3545 spec=0.7285 f1=0.4358 | time=13.0s\n",
            "Epoch 022 | train_loss=0.7139 acc=0.5031 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 023 | train_loss=0.6983 acc=0.5162 | val_loss=0.6991 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 024 | train_loss=0.7039 acc=0.5020 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7030 acc=0.5111 | val_loss=0.6908 acc=0.5193 | prec=0.5093 rec=0.9955 spec=0.0452 f1=0.6738 | time=12.8s\n",
            "Epoch 026 | train_loss=0.7090 acc=0.4986 | val_loss=0.6891 acc=0.6032 | prec=0.6957 rec=0.3636 spec=0.8416 f1=0.4776 | time=12.9s\n",
            "Epoch 027 | train_loss=0.7099 acc=0.5014 | val_loss=0.6872 acc=0.5692 | prec=0.6923 rec=0.2455 spec=0.8914 f1=0.3624 | time=12.9s\n",
            "Epoch 028 | train_loss=0.7030 acc=0.5071 | val_loss=0.6796 acc=0.6349 | prec=0.8105 rec=0.3500 spec=0.9186 f1=0.4889 | time=12.8s\n",
            "Epoch 029 | train_loss=0.6854 acc=0.5542 | val_loss=0.6598 acc=0.6531 | prec=0.8252 rec=0.3864 spec=0.9186 f1=0.5263 | time=12.8s\n",
            "Epoch 030 | train_loss=0.6741 acc=0.5888 | val_loss=0.6407 acc=0.6939 | prec=0.8455 rec=0.4727 spec=0.9140 f1=0.6064 | time=12.9s\n",
            "Epoch 031 | train_loss=0.6428 acc=0.6427 | val_loss=0.6205 acc=0.7619 | prec=0.8177 rec=0.6727 spec=0.8507 f1=0.7382 | time=12.8s\n",
            "Epoch 032 | train_loss=0.6325 acc=0.6670 | val_loss=0.6047 acc=0.7234 | prec=0.8603 rec=0.5318 spec=0.9140 f1=0.6573 | time=12.8s\n",
            "Epoch 033 | train_loss=0.6201 acc=0.6897 | val_loss=0.5884 acc=0.7891 | prec=0.8508 rec=0.7000 spec=0.8778 f1=0.7681 | time=13.0s\n",
            "Epoch 034 | train_loss=0.6029 acc=0.7039 | val_loss=0.5750 acc=0.7755 | prec=0.8040 rec=0.7273 spec=0.8235 f1=0.7637 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5991 acc=0.7028 | val_loss=0.5621 acc=0.7846 | prec=0.8307 rec=0.7136 spec=0.8552 f1=0.7677 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5833 acc=0.7283 | val_loss=0.5535 acc=0.7755 | prec=0.7713 rec=0.7818 spec=0.7692 f1=0.7765 | time=12.7s\n",
            "Epoch 037 | train_loss=0.5692 acc=0.7215 | val_loss=0.5386 acc=0.7755 | prec=0.7951 rec=0.7409 spec=0.8100 f1=0.7671 | time=12.8s\n",
            "Epoch 038 | train_loss=0.5633 acc=0.7238 | val_loss=0.5287 acc=0.7823 | prec=0.7818 rec=0.7818 spec=0.7828 f1=0.7818 | time=12.9s\n",
            "Epoch 039 | train_loss=0.5390 acc=0.7453 | val_loss=0.5137 acc=0.7846 | prec=0.8109 rec=0.7409 spec=0.8281 f1=0.7743 | time=12.9s\n",
            "Epoch 040 | train_loss=0.5383 acc=0.7555 | val_loss=0.5090 acc=0.7868 | prec=0.8462 rec=0.7000 spec=0.8733 f1=0.7662 | time=12.8s\n",
            "Epoch 041 | train_loss=0.5280 acc=0.7657 | val_loss=0.5022 acc=0.7937 | prec=0.8449 rec=0.7182 spec=0.8688 f1=0.7764 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5274 acc=0.7646 | val_loss=0.4960 acc=0.7846 | prec=0.8049 rec=0.7500 spec=0.8190 f1=0.7765 | time=12.8s\n",
            "Epoch 043 | train_loss=0.5260 acc=0.7425 | val_loss=0.4990 acc=0.8005 | prec=0.7895 rec=0.8182 spec=0.7828 f1=0.8036 | time=12.8s\n",
            "Epoch 044 | train_loss=0.5070 acc=0.7737 | val_loss=0.4964 acc=0.7891 | prec=0.8547 rec=0.6955 spec=0.8824 f1=0.7669 | time=12.9s\n",
            "Epoch 045 | train_loss=0.5010 acc=0.7777 | val_loss=0.4896 acc=0.7959 | prec=0.7928 rec=0.8000 spec=0.7919 f1=0.7964 | time=12.8s\n",
            "Epoch 046 | train_loss=0.4961 acc=0.7765 | val_loss=0.4855 acc=0.7982 | prec=0.8659 rec=0.7045 spec=0.8914 f1=0.7769 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4930 acc=0.7833 | val_loss=0.4910 acc=0.7937 | prec=0.8564 rec=0.7045 spec=0.8824 f1=0.7731 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4783 acc=0.7918 | val_loss=0.4781 acc=0.7914 | prec=0.8556 rec=0.7000 spec=0.8824 f1=0.7700 | time=12.9s\n",
            "Epoch 049 | train_loss=0.4844 acc=0.8009 | val_loss=0.4815 acc=0.7891 | prec=0.8629 rec=0.6864 spec=0.8914 f1=0.7646 | time=12.9s\n",
            "Epoch 050 | train_loss=0.4763 acc=0.8026 | val_loss=0.4735 acc=0.8073 | prec=0.8261 rec=0.7773 spec=0.8371 f1=0.8009 | time=12.9s\n",
            "Epoch 051 | train_loss=0.4667 acc=0.8128 | val_loss=0.4698 acc=0.8005 | prec=0.8548 rec=0.7227 spec=0.8778 f1=0.7833 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4591 acc=0.8020 | val_loss=0.4689 acc=0.7914 | prec=0.8596 rec=0.6955 spec=0.8869 f1=0.7688 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4666 acc=0.7969 | val_loss=0.4672 acc=0.8073 | prec=0.8462 rec=0.7500 spec=0.8643 f1=0.7952 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4688 acc=0.7975 | val_loss=0.4590 acc=0.8005 | prec=0.8548 rec=0.7227 spec=0.8778 f1=0.7833 | time=12.9s\n",
            "Epoch 055 | train_loss=0.4571 acc=0.8020 | val_loss=0.4663 acc=0.8005 | prec=0.8708 rec=0.7045 spec=0.8959 f1=0.7789 | time=12.9s\n",
            "Epoch 056 | train_loss=0.4549 acc=0.8066 | val_loss=0.4577 acc=0.8073 | prec=0.8027 rec=0.8136 spec=0.8009 f1=0.8081 | time=12.9s\n",
            "Epoch 057 | train_loss=0.4521 acc=0.8145 | val_loss=0.4546 acc=0.8118 | prec=0.8246 rec=0.7909 spec=0.8326 f1=0.8074 | time=12.8s\n",
            "Epoch 058 | train_loss=0.4416 acc=0.8151 | val_loss=0.4551 acc=0.8141 | prec=0.8317 rec=0.7864 spec=0.8416 f1=0.8084 | time=12.9s\n",
            "Epoch 059 | train_loss=0.4445 acc=0.8219 | val_loss=0.4458 acc=0.8050 | prec=0.8526 rec=0.7364 spec=0.8733 f1=0.7902 | time=12.8s\n",
            "Epoch 060 | train_loss=0.4502 acc=0.8049 | val_loss=0.4547 acc=0.7982 | prec=0.8659 rec=0.7045 spec=0.8914 f1=0.7769 | time=12.8s\n",
            "Epoch 061 | train_loss=0.4307 acc=0.8157 | val_loss=0.4446 acc=0.8005 | prec=0.8267 rec=0.7591 spec=0.8416 f1=0.7915 | time=12.8s\n",
            "Epoch 062 | train_loss=0.4438 acc=0.8094 | val_loss=0.4558 acc=0.8005 | prec=0.8750 rec=0.7000 spec=0.9005 f1=0.7778 | time=12.9s\n",
            "Epoch 063 | train_loss=0.4223 acc=0.8264 | val_loss=0.4640 acc=0.7846 | prec=0.8981 rec=0.6409 spec=0.9276 f1=0.7480 | time=12.9s\n",
            "Epoch 064 | train_loss=0.4166 acc=0.8276 | val_loss=0.4428 acc=0.8095 | prec=0.8696 rec=0.7273 spec=0.8914 f1=0.7921 | time=12.8s\n",
            "Epoch 065 | train_loss=0.4131 acc=0.8293 | val_loss=0.4572 acc=0.7868 | prec=0.8841 rec=0.6591 spec=0.9140 f1=0.7552 | time=12.9s\n",
            "Epoch 066 | train_loss=0.4104 acc=0.8310 | val_loss=0.4419 acc=0.8095 | prec=0.8778 rec=0.7182 spec=0.9005 f1=0.7900 | time=12.8s\n",
            "Epoch 067 | train_loss=0.4048 acc=0.8349 | val_loss=0.4600 acc=0.7823 | prec=0.8974 rec=0.6364 spec=0.9276 f1=0.7447 | time=13.0s\n",
            "Epoch 068 | train_loss=0.4072 acc=0.8349 | val_loss=0.4410 acc=0.7982 | prec=0.8786 rec=0.6909 spec=0.9050 f1=0.7735 | time=12.9s\n",
            "Epoch 069 | train_loss=0.3972 acc=0.8400 | val_loss=0.4396 acc=0.7937 | prec=0.8686 rec=0.6909 spec=0.8959 f1=0.7696 | time=12.9s\n",
            "Epoch 070 | train_loss=0.4157 acc=0.8344 | val_loss=0.4680 acc=0.7732 | prec=0.9054 rec=0.6091 spec=0.9367 f1=0.7283 | time=12.8s\n",
            "Epoch 071 | train_loss=0.3974 acc=0.8287 | val_loss=0.4293 acc=0.8050 | prec=0.8807 rec=0.7045 spec=0.9050 f1=0.7828 | time=12.8s\n",
            "Epoch 072 | train_loss=0.3918 acc=0.8400 | val_loss=0.4304 acc=0.8027 | prec=0.8800 rec=0.7000 spec=0.9050 f1=0.7797 | time=12.8s\n",
            "Epoch 073 | train_loss=0.3969 acc=0.8463 | val_loss=0.4493 acc=0.7823 | prec=0.8875 rec=0.6455 spec=0.9186 f1=0.7474 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3851 acc=0.8389 | val_loss=0.4732 acc=0.7642 | prec=0.9085 rec=0.5864 spec=0.9412 f1=0.7127 | time=13.0s\n",
            "Epoch 075 | train_loss=0.4112 acc=0.8230 | val_loss=0.4726 acc=0.7619 | prec=0.9137 rec=0.5773 spec=0.9457 f1=0.7075 | time=12.9s\n",
            "Epoch 076 | train_loss=0.4007 acc=0.8355 | val_loss=0.4399 acc=0.7891 | prec=0.8944 rec=0.6545 spec=0.9231 f1=0.7559 | time=12.8s\n",
            "Epoch 077 | train_loss=0.4002 acc=0.8344 | val_loss=0.4183 acc=0.7959 | prec=0.8611 rec=0.7045 spec=0.8869 f1=0.7750 | time=12.9s\n",
            "Epoch 078 | train_loss=0.3817 acc=0.8520 | val_loss=0.4291 acc=0.7959 | prec=0.8963 rec=0.6682 spec=0.9231 f1=0.7656 | time=12.9s\n",
            "Epoch 079 | train_loss=0.3799 acc=0.8469 | val_loss=0.4491 acc=0.7823 | prec=0.9133 rec=0.6227 spec=0.9412 f1=0.7405 | time=12.8s\n",
            "Epoch 080 | train_loss=0.3792 acc=0.8508 | val_loss=0.4243 acc=0.8005 | prec=0.8976 rec=0.6773 spec=0.9231 f1=0.7720 | time=12.8s\n",
            "Epoch 081 | train_loss=0.3642 acc=0.8537 | val_loss=0.4297 acc=0.7891 | prec=0.8994 rec=0.6500 spec=0.9276 f1=0.7546 | time=12.9s\n",
            "Epoch 082 | train_loss=0.3600 acc=0.8508 | val_loss=0.4115 acc=0.8073 | prec=0.8947 rec=0.6955 spec=0.9186 f1=0.7826 | time=12.9s\n",
            "Epoch 083 | train_loss=0.3573 acc=0.8565 | val_loss=0.4435 acc=0.7868 | prec=0.9145 rec=0.6318 spec=0.9412 f1=0.7473 | time=12.8s\n",
            "Epoch 084 | train_loss=0.3740 acc=0.8531 | val_loss=0.4005 acc=0.8118 | prec=0.8624 rec=0.7409 spec=0.8824 f1=0.7971 | time=12.8s\n",
            "Epoch 085 | train_loss=0.3663 acc=0.8486 | val_loss=0.4352 acc=0.7823 | prec=0.8974 rec=0.6364 spec=0.9276 f1=0.7447 | time=13.1s\n",
            "Epoch 086 | train_loss=0.3683 acc=0.8491 | val_loss=0.4180 acc=0.8005 | prec=0.8929 rec=0.6818 spec=0.9186 f1=0.7732 | time=12.9s\n",
            "Epoch 087 | train_loss=0.3588 acc=0.8514 | val_loss=0.4191 acc=0.7959 | prec=0.8869 rec=0.6773 spec=0.9140 f1=0.7680 | time=12.8s\n",
            "Epoch 088 | train_loss=0.3537 acc=0.8503 | val_loss=0.4274 acc=0.7914 | prec=0.9051 rec=0.6500 spec=0.9321 f1=0.7566 | time=12.9s\n",
            "Epoch 089 | train_loss=0.3689 acc=0.8588 | val_loss=0.4315 acc=0.7891 | prec=0.9097 rec=0.6409 spec=0.9367 f1=0.7520 | time=12.9s\n",
            "Epoch 090 | train_loss=0.3532 acc=0.8571 | val_loss=0.4376 acc=0.7891 | prec=0.9150 rec=0.6364 spec=0.9412 f1=0.7507 | time=12.9s\n",
            "Epoch 091 | train_loss=0.3432 acc=0.8588 | val_loss=0.4149 acc=0.7937 | prec=0.8957 rec=0.6636 spec=0.9231 f1=0.7624 | time=12.9s\n",
            "Epoch 092 | train_loss=0.3427 acc=0.8542 | val_loss=0.4172 acc=0.7982 | prec=0.9018 rec=0.6682 spec=0.9276 f1=0.7676 | time=13.2s\n",
            "Epoch 093 | train_loss=0.3409 acc=0.8678 | val_loss=0.4653 acc=0.7778 | prec=0.9236 rec=0.6045 spec=0.9502 f1=0.7308 | time=13.2s\n",
            "Epoch 094 | train_loss=0.3655 acc=0.8610 | val_loss=0.4302 acc=0.7891 | prec=0.9097 rec=0.6409 spec=0.9367 f1=0.7520 | time=12.9s\n",
            "Epoch 095 | train_loss=0.3304 acc=0.8633 | val_loss=0.4146 acc=0.8027 | prec=0.9080 rec=0.6727 spec=0.9321 f1=0.7728 | time=13.0s\n",
            "Epoch 096 | train_loss=0.3331 acc=0.8678 | val_loss=0.4521 acc=0.7823 | prec=0.9247 rec=0.6136 spec=0.9502 f1=0.7377 | time=12.9s\n",
            "Epoch 097 | train_loss=0.3270 acc=0.8690 | val_loss=0.4118 acc=0.8005 | prec=0.9024 rec=0.6727 spec=0.9276 f1=0.7708 | time=12.9s\n",
            "Epoch 098 | train_loss=0.3423 acc=0.8780 | val_loss=0.4400 acc=0.7891 | prec=0.9150 rec=0.6364 spec=0.9412 f1=0.7507 | time=12.8s\n",
            "Epoch 099 | train_loss=0.3163 acc=0.8843 | val_loss=0.4481 acc=0.7868 | prec=0.9257 rec=0.6227 spec=0.9502 f1=0.7446 | time=12.9s\n",
            "Early stopping at epoch 99\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▆▆▆▆▆▆▆▅▆▆▆▁▅▇▇▇██████▇▆▇▇▇▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▄▆▇▆▇▆▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇█▇███</td></tr><tr><td>recall</td><td>█████████████▁▁▃▆▅▆▅▅▅▅▅▅▅▅▅▄▄▄▄▅▅▄▅▄▄▄▄</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▃▁▆▁██▇█▇▇█▇█▇▇▇█████████████████</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▂▁▅▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█████</td></tr><tr><td>train_loss</td><td>███████████▇▇▆▆▅▄▄▄▄▃▃▃▃▂▂▂▃▂▂▂▂▁▁▂▂▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▁▇▆▇▇▇█▇███▇█████▇▇▇▇██▇█▇███▇</td></tr><tr><td>validation_loss</td><td>███████████▆▆▆▅▅▄▄▃▃▃▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.74457</td></tr><tr><td>precision</td><td>0.92568</td></tr><tr><td>recall</td><td>0.62273</td></tr><tr><td>specificity</td><td>0.95023</td></tr><tr><td>train_accuracy</td><td>0.88429</td></tr><tr><td>train_loss</td><td>0.3163</td></tr><tr><td>validation_accuracy</td><td>0.78685</td></tr><tr><td>validation_loss</td><td>0.44815</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/62phk6cf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7/runs/62phk6cf</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_080047-62phk6cf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 08:22:04,232] Trial 9 finished with values: [0.44814891261713846, 0.7868480725623582] and parameters: {'lr': 1.4419843522360348e-05, 'wd': 4.6549935543509025e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pareto-optimal trials (val_loss, val_acc) and their params:\n",
            " Trial #1: values=[0.39670870133808683, 0.8526077097505669]\n",
            "              params={'lr': 2.4166877561293106e-05, 'wd': 8.262372253115548e-05}\n",
            " Trial #7: values=[0.3690581832613264, 0.8390022675736961]\n",
            "              params={'lr': 2.3089123428359085e-05, 'wd': 7.204776343575214e-06}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Return back to model_optimized_6.py\n",
        "- Block = 1, Head = 3\n",
        "- Decrease the search space\n",
        "- MAX Epoch = 150\n",
        "- Dropout rate = 0.2 (model_optimized_7.py) in CNNDecoder makes a bit worse in generalization\n",
        "- Again, increase the dropout rate = 0.2 to 0.3 in CNNDecoder"
      ],
      "metadata": {
        "id": "7mFPLgYStZoM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix\n",
        ")\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Prepare data ────────────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "# Undersample for balance\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "samp0 = random.sample(data0, k=min_count)\n",
        "samp1 = random.sample(data1, k=min_count)\n",
        "balanced_meta = samp0 + samp1\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# Map labels to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# Build dataset\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# Single train/validation split\n",
        "indices = np.arange(len(dataset))\n",
        "train_idx, val_idx = train_test_split(\n",
        "    indices, test_size=0.2, stratify=labels, random_state=0\n",
        ")\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # Sample hyperparameters\n",
        "    lr          = trial.suggest_float('lr', 1e-5, 2e-4, log=True)\n",
        "    wd          = trial.suggest_float('wd', 1e-6, 1e-4, log=True)\n",
        "    pct_start   = 0.2  # fixed\n",
        "    num_filters = 120\n",
        "\n",
        "    print(f\"Trial {trial.number}: filters={num_filters}, lr={lr:.2e}, wd={wd:.2e}, pct_start={pct_start:.2f}\")\n",
        "\n",
        "    # W&B init for this trial\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-binary-AD-CN-single-fold-8',\n",
        "        name=f'trial{trial.number}',\n",
        "        config={'lr': lr, 'wd': wd, 'pct_start': pct_start, 'filters': num_filters},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # DataLoaders\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # Model, optimizer, scheduler, criterion\n",
        "    input_len = dataset[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=num_filters,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    total_steps = MAX_EPOCHS * len(train_loader)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=lr,\n",
        "        total_steps=total_steps,\n",
        "        pct_start=pct_start,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0\n",
        "\n",
        "    # Epoch loop\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # Dynamic Weight Decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = wd * (cur_lr / lr)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        val_preds, val_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss  += loss.item()\n",
        "\n",
        "                preds   = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "\n",
        "                val_preds.append(preds.cpu().numpy())\n",
        "                val_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss   = vloss / len(val_loader)\n",
        "        val_preds  = np.concatenate(val_preds)\n",
        "        val_labels = np.concatenate(val_labels)\n",
        "        val_acc    = (val_preds == val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "\n",
        "        # Compute specificity = TN / (TN + FP)\n",
        "        tn, fp, fn, tp = confusion_matrix(val_labels, val_preds).ravel()\n",
        "        specificity     = tn / (tn + fp)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        # Terminal output & W&B logging\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={precision:.4f} rec={recall:.4f} spec={specificity:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            'train_loss': train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'validation_loss': val_loss,\n",
        "            'validation_accuracy': val_acc,\n",
        "            'precision': precision,\n",
        "            'recall': recall,\n",
        "            'specificity': specificity,\n",
        "            'f1_score': f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # Early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    return val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna study ─────────────────────────────────────────────\n",
        "study = optuna.create_study(directions=['minimize', 'maximize'])\n",
        "study.optimize(objective, n_trials=10)\n",
        "\n",
        "# ─── Print Pare토-optimal trials ──────────────────────────────────\n",
        "print(\"Pareto-optimal trials (val_loss, val_acc) and their params:\")\n",
        "for t in study.best_trials:\n",
        "    print(\n",
        "        f\" Trial #{t.number}: values={t.values}\\n\"\n",
        "        f\"              params={t.params}\"\n",
        "    )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qwrz2J3EtZxl",
        "outputId": "6e5b52ec-fca7-45a2-c788-aa36b241229a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 08:36:52,068] A new study created in memory with name: no-name-fe340887-6a2b-4636-a3c6-4016ff16809e\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 0: filters=120, lr=8.53e-05, wd=2.20e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_083652-jg0560sa</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/jg0560sa' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/jg0560sa' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/jg0560sa</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7195 acc=0.4991 | val_loss=0.6977 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=15.3s\n",
            "Epoch 002 | train_loss=0.7175 acc=0.5026 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7227 acc=0.4912 | val_loss=0.6957 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7071 acc=0.5122 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7095 acc=0.5133 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7154 acc=0.4997 | val_loss=0.6929 acc=0.5102 | prec=0.5062 rec=0.7409 spec=0.2805 f1=0.6015 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7127 acc=0.5031 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7070 acc=0.5167 | val_loss=0.6927 acc=0.5329 | prec=0.5814 rec=0.2273 spec=0.8371 f1=0.3268 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7050 acc=0.5128 | val_loss=0.6914 acc=0.5306 | prec=0.5867 rec=0.2000 spec=0.8597 f1=0.2983 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7083 acc=0.5060 | val_loss=0.6888 acc=0.5283 | prec=0.5208 rec=0.6818 spec=0.3756 f1=0.5906 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7113 acc=0.4991 | val_loss=0.6816 acc=0.5964 | prec=0.6438 rec=0.4273 spec=0.7647 f1=0.5137 | time=12.9s\n",
            "Epoch 012 | train_loss=0.6926 acc=0.5474 | val_loss=0.6850 acc=0.5533 | prec=0.8966 rec=0.1182 spec=0.9864 f1=0.2088 | time=12.8s\n",
            "Epoch 013 | train_loss=0.6697 acc=0.5837 | val_loss=0.6343 acc=0.6689 | prec=0.8190 rec=0.4318 spec=0.9050 f1=0.5655 | time=12.8s\n",
            "Epoch 014 | train_loss=0.6172 acc=0.6824 | val_loss=0.5706 acc=0.7823 | prec=0.7743 rec=0.7955 spec=0.7692 f1=0.7848 | time=12.8s\n",
            "Epoch 015 | train_loss=0.5790 acc=0.7175 | val_loss=0.5691 acc=0.7166 | prec=0.8992 rec=0.4864 spec=0.9457 f1=0.6313 | time=12.9s\n",
            "Epoch 016 | train_loss=0.5613 acc=0.7175 | val_loss=0.5063 acc=0.8141 | prec=0.8833 rec=0.7227 spec=0.9050 f1=0.7950 | time=12.8s\n",
            "Epoch 017 | train_loss=0.5486 acc=0.7385 | val_loss=0.5032 acc=0.8095 | prec=0.8908 rec=0.7045 spec=0.9140 f1=0.7868 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5279 acc=0.7612 | val_loss=0.4892 acc=0.8027 | prec=0.9030 rec=0.6773 spec=0.9276 f1=0.7740 | time=12.9s\n",
            "Epoch 019 | train_loss=0.4967 acc=0.7663 | val_loss=0.4707 acc=0.8005 | prec=0.7705 rec=0.8545 spec=0.7466 f1=0.8103 | time=12.8s\n",
            "Epoch 020 | train_loss=0.4824 acc=0.7867 | val_loss=0.4440 acc=0.8322 | prec=0.8883 rec=0.7591 spec=0.9050 f1=0.8186 | time=12.8s\n",
            "Epoch 021 | train_loss=0.4535 acc=0.8032 | val_loss=0.4361 acc=0.8390 | prec=0.8901 rec=0.7727 spec=0.9050 f1=0.8273 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4468 acc=0.8162 | val_loss=0.4396 acc=0.8299 | prec=0.8962 rec=0.7455 spec=0.9140 f1=0.8139 | time=13.0s\n",
            "Epoch 023 | train_loss=0.4403 acc=0.8100 | val_loss=0.4411 acc=0.8118 | prec=0.8663 rec=0.7364 spec=0.8869 f1=0.7961 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4022 acc=0.8406 | val_loss=0.4150 acc=0.8367 | prec=0.8246 rec=0.8545 spec=0.8190 f1=0.8393 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3986 acc=0.8315 | val_loss=0.4252 acc=0.8345 | prec=0.9153 rec=0.7364 spec=0.9321 f1=0.8161 | time=12.9s\n",
            "Epoch 026 | train_loss=0.3788 acc=0.8434 | val_loss=0.4088 acc=0.8231 | prec=0.7817 rec=0.8955 spec=0.7511 f1=0.8347 | time=12.9s\n",
            "Epoch 027 | train_loss=0.3737 acc=0.8503 | val_loss=0.3745 acc=0.8458 | prec=0.8585 rec=0.8273 spec=0.8643 f1=0.8426 | time=12.9s\n",
            "Epoch 028 | train_loss=0.3398 acc=0.8729 | val_loss=0.4392 acc=0.8095 | prec=0.9474 rec=0.6545 spec=0.9638 f1=0.7742 | time=12.7s\n",
            "Epoch 029 | train_loss=0.3277 acc=0.8724 | val_loss=0.3738 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.8s\n",
            "Epoch 030 | train_loss=0.3151 acc=0.8803 | val_loss=0.4415 acc=0.7914 | prec=0.7270 rec=0.9318 spec=0.6516 f1=0.8167 | time=12.7s\n",
            "Epoch 031 | train_loss=0.3120 acc=0.8741 | val_loss=0.3986 acc=0.8299 | prec=0.8680 rec=0.7773 spec=0.8824 f1=0.8201 | time=12.8s\n",
            "Epoch 032 | train_loss=0.3106 acc=0.8798 | val_loss=0.3748 acc=0.8254 | prec=0.9040 rec=0.7273 spec=0.9231 f1=0.8060 | time=12.9s\n",
            "Epoch 033 | train_loss=0.2894 acc=0.8888 | val_loss=0.3664 acc=0.8481 | prec=0.8341 rec=0.8682 spec=0.8281 f1=0.8508 | time=12.9s\n",
            "Epoch 034 | train_loss=0.2986 acc=0.8820 | val_loss=0.3671 acc=0.8503 | prec=0.8156 rec=0.9045 spec=0.7964 f1=0.8578 | time=12.9s\n",
            "Epoch 035 | train_loss=0.2855 acc=0.9024 | val_loss=0.3636 acc=0.8367 | prec=0.8978 rec=0.7591 spec=0.9140 f1=0.8227 | time=12.8s\n",
            "Epoch 036 | train_loss=0.2587 acc=0.9013 | val_loss=0.3540 acc=0.8503 | prec=0.8889 rec=0.8000 spec=0.9005 f1=0.8421 | time=12.8s\n",
            "Epoch 037 | train_loss=0.2580 acc=0.9036 | val_loss=0.3814 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=12.8s\n",
            "Epoch 038 | train_loss=0.2587 acc=0.9030 | val_loss=0.3849 acc=0.8413 | prec=0.8641 rec=0.8091 spec=0.8733 f1=0.8357 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2716 acc=0.8871 | val_loss=0.3686 acc=0.8526 | prec=0.8216 rec=0.9000 spec=0.8054 f1=0.8590 | time=13.0s\n",
            "Epoch 040 | train_loss=0.2397 acc=0.9183 | val_loss=0.3655 acc=0.8390 | prec=0.8782 rec=0.7864 spec=0.8914 f1=0.8297 | time=12.9s\n",
            "Epoch 041 | train_loss=0.2491 acc=0.9030 | val_loss=0.4520 acc=0.8027 | prec=0.9236 rec=0.6591 spec=0.9457 f1=0.7692 | time=12.8s\n",
            "Epoch 042 | train_loss=0.2996 acc=0.8922 | val_loss=0.3842 acc=0.8549 | prec=0.8197 rec=0.9091 spec=0.8009 f1=0.8621 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2221 acc=0.9200 | val_loss=0.4557 acc=0.8095 | prec=0.9198 rec=0.6773 spec=0.9412 f1=0.7801 | time=12.7s\n",
            "Epoch 044 | train_loss=0.2201 acc=0.9098 | val_loss=0.4285 acc=0.8209 | prec=0.9123 rec=0.7091 spec=0.9321 f1=0.7980 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2374 acc=0.9217 | val_loss=0.3637 acc=0.8617 | prec=0.8630 rec=0.8591 spec=0.8643 f1=0.8610 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2112 acc=0.9138 | val_loss=0.3509 acc=0.8549 | prec=0.8679 rec=0.8364 spec=0.8733 f1=0.8519 | time=12.8s\n",
            "Epoch 047 | train_loss=0.1755 acc=0.9387 | val_loss=0.3918 acc=0.8526 | prec=0.8934 rec=0.8000 spec=0.9050 f1=0.8441 | time=12.9s\n",
            "Epoch 048 | train_loss=0.1928 acc=0.9280 | val_loss=0.3635 acc=0.8594 | prec=0.8692 rec=0.8455 spec=0.8733 f1=0.8571 | time=12.9s\n",
            "Epoch 049 | train_loss=0.1763 acc=0.9376 | val_loss=0.3622 acc=0.8639 | prec=0.8810 rec=0.8409 spec=0.8869 f1=0.8605 | time=12.8s\n",
            "Epoch 050 | train_loss=0.1930 acc=0.9138 | val_loss=0.3699 acc=0.8571 | prec=0.8520 rec=0.8636 spec=0.8507 f1=0.8578 | time=12.8s\n",
            "Epoch 051 | train_loss=0.1789 acc=0.9280 | val_loss=0.3960 acc=0.8481 | prec=0.8370 rec=0.8636 spec=0.8326 f1=0.8501 | time=12.8s\n",
            "Epoch 052 | train_loss=0.1763 acc=0.9251 | val_loss=0.3977 acc=0.8413 | prec=0.8788 rec=0.7909 spec=0.8914 f1=0.8325 | time=13.2s\n",
            "Epoch 053 | train_loss=0.1689 acc=0.9314 | val_loss=0.3862 acc=0.8526 | prec=0.8974 rec=0.7955 spec=0.9095 f1=0.8434 | time=13.1s\n",
            "Epoch 054 | train_loss=0.1705 acc=0.9217 | val_loss=0.4203 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.9s\n",
            "Epoch 055 | train_loss=0.1691 acc=0.9319 | val_loss=0.3724 acc=0.8571 | prec=0.8618 rec=0.8500 spec=0.8643 f1=0.8558 | time=13.4s\n",
            "Epoch 056 | train_loss=0.1833 acc=0.9206 | val_loss=0.3827 acc=0.8617 | prec=0.8664 rec=0.8545 spec=0.8688 f1=0.8604 | time=13.7s\n",
            "Epoch 057 | train_loss=0.1459 acc=0.9359 | val_loss=0.4028 acc=0.8481 | prec=0.8732 rec=0.8136 spec=0.8824 f1=0.8424 | time=13.7s\n",
            "Epoch 058 | train_loss=0.1626 acc=0.9268 | val_loss=0.4366 acc=0.8435 | prec=0.8057 rec=0.9045 spec=0.7828 f1=0.8522 | time=13.7s\n",
            "Epoch 059 | train_loss=0.1504 acc=0.9359 | val_loss=0.4435 acc=0.8435 | prec=0.8297 rec=0.8636 spec=0.8235 f1=0.8463 | time=13.6s\n",
            "Epoch 060 | train_loss=0.1514 acc=0.9370 | val_loss=0.4202 acc=0.8503 | prec=0.8775 rec=0.8136 spec=0.8869 f1=0.8443 | time=13.7s\n",
            "Epoch 061 | train_loss=0.1690 acc=0.9325 | val_loss=0.4888 acc=0.8186 | prec=0.9118 rec=0.7045 spec=0.9321 f1=0.7949 | time=13.6s\n",
            "Early stopping at epoch 61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▄▃▆▇▇▇██████████████▇▇███████████▇</td></tr><tr><td>precision</td><td>▁▁▅▁▅▅▆█▇▇█████▇█▇▇█▆▇█▇█▇▇▇█▇█▇▇█▇▇█▇▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▃▃▆▄▇▅▆▆█▇▇██▇▇██▇▇█▆█▆▆▇█▇█▇▇██▇▆</td></tr><tr><td>specificity</td><td>███▁█▂▆▆▇▇▇▆▇▇▇▆▇█▇▅▇▆▆▇▆▆▇▇▆▇▇▇▇▇▇▇▇▆▆▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇████████████</td></tr><tr><td>train_loss</td><td>███████▇▇▆▆▆▅▅▅▄▃▃▃▃▃▃▂▂▃▂▃▂▂▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▂▂▄▅▇▇▇▇█▇▇▇▇█▇▇▇█▇██▇█▇█████▇████</td></tr><tr><td>validation_loss</td><td>█████████▇▄▄▄▃▃▃▃▃▂▁▂▁▁▁▁▂▃▁▁▂▁▁▂▂▂▁▂▂▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.79487</td></tr><tr><td>precision</td><td>0.91176</td></tr><tr><td>recall</td><td>0.70455</td></tr><tr><td>specificity</td><td>0.93213</td></tr><tr><td>train_accuracy</td><td>0.9325</td></tr><tr><td>train_loss</td><td>0.16903</td></tr><tr><td>validation_accuracy</td><td>0.81859</td></tr><tr><td>validation_loss</td><td>0.48875</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/jg0560sa' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/jg0560sa</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_083652-jg0560sa/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 08:50:06,102] Trial 0 finished with values: [0.48875192765678677, 0.81859410430839] and parameters: {'lr': 8.534653675580783e-05, 'wd': 2.2038899844018073e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 1: filters=120, lr=6.44e-05, wd=1.85e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_085006-lw64dq9d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/lw64dq9d' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/lw64dq9d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/lw64dq9d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8061 acc=0.4974 | val_loss=0.7112 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.8s\n",
            "Epoch 002 | train_loss=0.7622 acc=0.4872 | val_loss=0.6964 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 003 | train_loss=0.7488 acc=0.4974 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.2s\n",
            "Epoch 004 | train_loss=0.7206 acc=0.5315 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 005 | train_loss=0.7546 acc=0.4748 | val_loss=0.6968 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7402 acc=0.5009 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.2s\n",
            "Epoch 007 | train_loss=0.7339 acc=0.5020 | val_loss=0.6927 acc=0.4943 | prec=0.4963 rec=0.9091 spec=0.0814 f1=0.6421 | time=13.1s\n",
            "Epoch 008 | train_loss=0.7236 acc=0.4935 | val_loss=0.6941 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7166 acc=0.5082 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7129 acc=0.5071 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7136 acc=0.5031 | val_loss=0.6937 acc=0.4853 | prec=0.4894 rec=0.7364 spec=0.2353 f1=0.5880 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7222 acc=0.5031 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7252 acc=0.5088 | val_loss=0.7033 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7171 acc=0.4935 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7142 acc=0.5082 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7125 acc=0.5065 | val_loss=0.6994 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7124 acc=0.5128 | val_loss=0.6954 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7192 acc=0.5014 | val_loss=0.6980 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6909 acc=0.5406 | val_loss=0.6879 acc=0.5374 | prec=0.9444 rec=0.0773 spec=0.9955 f1=0.1429 | time=13.0s\n",
            "Epoch 020 | train_loss=0.6522 acc=0.6256 | val_loss=0.6027 acc=0.6961 | prec=0.9300 rec=0.4227 spec=0.9683 f1=0.5813 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5981 acc=0.7016 | val_loss=0.5571 acc=0.7528 | prec=0.8993 rec=0.5682 spec=0.9367 f1=0.6964 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5459 acc=0.7436 | val_loss=0.4977 acc=0.7937 | prec=0.8377 rec=0.7273 spec=0.8597 f1=0.7786 | time=12.8s\n",
            "Epoch 023 | train_loss=0.5036 acc=0.7691 | val_loss=0.4750 acc=0.8141 | prec=0.8450 rec=0.7682 spec=0.8597 f1=0.8048 | time=12.8s\n",
            "Epoch 024 | train_loss=0.4923 acc=0.7788 | val_loss=0.4532 acc=0.8413 | prec=0.8538 rec=0.8227 spec=0.8597 f1=0.8380 | time=12.9s\n",
            "Epoch 025 | train_loss=0.4647 acc=0.7941 | val_loss=0.4570 acc=0.8367 | prec=0.8854 rec=0.7727 spec=0.9005 f1=0.8252 | time=13.0s\n",
            "Epoch 026 | train_loss=0.4436 acc=0.8151 | val_loss=0.4414 acc=0.8503 | prec=0.8929 rec=0.7955 spec=0.9050 f1=0.8413 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4520 acc=0.8077 | val_loss=0.4081 acc=0.8435 | prec=0.8872 rec=0.7864 spec=0.9005 f1=0.8337 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4363 acc=0.7998 | val_loss=0.4207 acc=0.8435 | prec=0.8578 rec=0.8227 spec=0.8643 f1=0.8399 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3951 acc=0.8434 | val_loss=0.3892 acc=0.8549 | prec=0.8611 rec=0.8455 spec=0.8643 f1=0.8532 | time=12.9s\n",
            "Epoch 030 | train_loss=0.4070 acc=0.8287 | val_loss=0.5125 acc=0.7324 | prec=0.6584 rec=0.9636 spec=0.5023 f1=0.7823 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3635 acc=0.8565 | val_loss=0.3824 acc=0.8526 | prec=0.8189 rec=0.9045 spec=0.8009 f1=0.8596 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3534 acc=0.8537 | val_loss=0.4102 acc=0.8118 | prec=0.7605 rec=0.9091 spec=0.7149 f1=0.8282 | time=12.8s\n",
            "Epoch 033 | train_loss=0.3393 acc=0.8599 | val_loss=0.3563 acc=0.8639 | prec=0.8419 rec=0.8955 spec=0.8326 f1=0.8678 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3475 acc=0.8639 | val_loss=0.3778 acc=0.8345 | prec=0.8025 rec=0.8864 spec=0.7828 f1=0.8423 | time=12.9s\n",
            "Epoch 035 | train_loss=0.3250 acc=0.8701 | val_loss=0.3635 acc=0.8617 | prec=0.8664 rec=0.8545 spec=0.8688 f1=0.8604 | time=12.9s\n",
            "Epoch 036 | train_loss=0.3136 acc=0.8786 | val_loss=0.3705 acc=0.8367 | prec=0.8426 rec=0.8273 spec=0.8462 f1=0.8349 | time=13.0s\n",
            "Epoch 037 | train_loss=0.2889 acc=0.8888 | val_loss=0.3711 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=12.9s\n",
            "Epoch 038 | train_loss=0.2739 acc=0.8837 | val_loss=0.3542 acc=0.8549 | prec=0.8939 rec=0.8045 spec=0.9050 f1=0.8469 | time=12.8s\n",
            "Epoch 039 | train_loss=0.2893 acc=0.8780 | val_loss=0.3436 acc=0.8639 | prec=0.8604 rec=0.8682 spec=0.8597 f1=0.8643 | time=12.9s\n",
            "Epoch 040 | train_loss=0.2780 acc=0.8843 | val_loss=0.4506 acc=0.8073 | prec=0.7368 rec=0.9545 spec=0.6606 f1=0.8317 | time=12.8s\n",
            "Epoch 041 | train_loss=0.3132 acc=0.8599 | val_loss=0.4028 acc=0.8345 | prec=0.7816 rec=0.9273 spec=0.7421 f1=0.8482 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2638 acc=0.8951 | val_loss=0.3700 acc=0.8435 | prec=0.8057 rec=0.9045 spec=0.7828 f1=0.8522 | time=12.9s\n",
            "Epoch 043 | train_loss=0.2536 acc=0.8922 | val_loss=0.3552 acc=0.8413 | prec=0.8049 rec=0.9000 spec=0.7828 f1=0.8498 | time=12.9s\n",
            "Epoch 044 | train_loss=0.2192 acc=0.9178 | val_loss=0.3533 acc=0.8435 | prec=0.8213 rec=0.8773 spec=0.8100 f1=0.8484 | time=13.0s\n",
            "Epoch 045 | train_loss=0.2248 acc=0.9098 | val_loss=0.3530 acc=0.8503 | prec=0.8812 rec=0.8091 spec=0.8914 f1=0.8436 | time=13.0s\n",
            "Epoch 046 | train_loss=0.2357 acc=0.9030 | val_loss=0.3683 acc=0.8390 | prec=0.8117 rec=0.8818 spec=0.7964 f1=0.8453 | time=12.8s\n",
            "Epoch 047 | train_loss=0.2276 acc=0.9229 | val_loss=0.3582 acc=0.8458 | prec=0.8115 rec=0.9000 spec=0.7919 f1=0.8534 | time=12.9s\n",
            "Epoch 048 | train_loss=0.2131 acc=0.9223 | val_loss=0.5932 acc=0.7619 | prec=0.6803 rec=0.9864 spec=0.5385 f1=0.8052 | time=12.8s\n",
            "Epoch 049 | train_loss=0.2384 acc=0.9070 | val_loss=0.4871 acc=0.7982 | prec=0.7331 rec=0.9364 spec=0.6606 f1=0.8224 | time=12.9s\n",
            "Epoch 050 | train_loss=0.2327 acc=0.9144 | val_loss=0.3445 acc=0.8617 | prec=0.8299 rec=0.9091 spec=0.8145 f1=0.8677 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2152 acc=0.9251 | val_loss=0.4073 acc=0.8458 | prec=0.8016 rec=0.9182 spec=0.7738 f1=0.8559 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2118 acc=0.9161 | val_loss=0.4895 acc=0.8186 | prec=0.7482 rec=0.9591 spec=0.6787 f1=0.8406 | time=12.9s\n",
            "Epoch 053 | train_loss=0.1891 acc=0.9240 | val_loss=0.3662 acc=0.8322 | prec=0.7897 rec=0.9045 spec=0.7602 f1=0.8432 | time=12.9s\n",
            "Epoch 054 | train_loss=0.1890 acc=0.9246 | val_loss=0.3498 acc=0.8458 | prec=0.8167 rec=0.8909 spec=0.8009 f1=0.8522 | time=12.8s\n",
            "Early stopping at epoch 54\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▁▁▆▁▁▆▁▁▁▁▁▂▆▇▇█████▇█████████████████</td></tr><tr><td>precision</td><td>▅▅▅▁▁▅▅▁▁▅▁▁▁▁▁████████▆▇▇██▇█▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>recall</td><td>███▁▁▁▆▁▁▁▁▁▁▂▄▆▆▇▆▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇</td></tr><tr><td>specificity</td><td>▁▁▁██▁██▃███████▇▇▇▇▇▇▅▇▆▆▇▇▇▇▆▆▆▆▇▅▆▇▆▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▁▁▁▂▂▁▂▁▂▁▂▂▃▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇██████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▇▇█▇██▆█▇█▇███▇████▆▇██▇</td></tr><tr><td>validation_loss</td><td>███████████████▄▄▃▃▃▂▂▂▂▁▁▂▂▁▁▂▂▁▁▁▄▁▂▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85217</td></tr><tr><td>precision</td><td>0.81667</td></tr><tr><td>recall</td><td>0.89091</td></tr><tr><td>specificity</td><td>0.8009</td></tr><tr><td>train_accuracy</td><td>0.92456</td></tr><tr><td>train_loss</td><td>0.18895</td></tr><tr><td>validation_accuracy</td><td>0.8458</td></tr><tr><td>validation_loss</td><td>0.34976</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/lw64dq9d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/lw64dq9d</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_085006-lw64dq9d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 09:01:46,356] Trial 1 finished with values: [0.3497606185930116, 0.8458049886621315] and parameters: {'lr': 6.436530952928205e-05, 'wd': 1.8466323534899384e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 2: filters=120, lr=1.23e-04, wd=7.13e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_090146-t3k8la19</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/t3k8la19' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/t3k8la19' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/t3k8la19</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7218 acc=0.4827 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7306 acc=0.4861 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7172 acc=0.5060 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7149 acc=0.4980 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7043 acc=0.5082 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7114 acc=0.4918 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7066 acc=0.5167 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7111 acc=0.4867 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7089 acc=0.5105 | val_loss=0.6914 acc=0.6259 | prec=0.6087 rec=0.7000 spec=0.5520 f1=0.6512 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7133 acc=0.5065 | val_loss=0.6893 acc=0.5918 | prec=0.5532 rec=0.9455 spec=0.2398 f1=0.6980 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7040 acc=0.5207 | val_loss=0.6883 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 012 | train_loss=0.6992 acc=0.5286 | val_loss=0.6359 acc=0.7551 | prec=0.7240 rec=0.8227 spec=0.6878 f1=0.7702 | time=12.9s\n",
            "Epoch 013 | train_loss=0.6072 acc=0.6773 | val_loss=0.5348 acc=0.8050 | prec=0.7913 rec=0.8273 spec=0.7828 f1=0.8089 | time=12.9s\n",
            "Epoch 014 | train_loss=0.5414 acc=0.7362 | val_loss=0.4647 acc=0.8322 | prec=0.8476 rec=0.8091 spec=0.8552 f1=0.8279 | time=12.9s\n",
            "Epoch 015 | train_loss=0.5059 acc=0.7635 | val_loss=0.4499 acc=0.8209 | prec=0.8691 rec=0.7545 spec=0.8869 f1=0.8078 | time=13.0s\n",
            "Epoch 016 | train_loss=0.4813 acc=0.7890 | val_loss=0.4669 acc=0.7823 | prec=0.7153 rec=0.9364 spec=0.6290 f1=0.8110 | time=12.9s\n",
            "Epoch 017 | train_loss=0.4536 acc=0.8032 | val_loss=0.4027 acc=0.8413 | prec=0.8205 rec=0.8727 spec=0.8100 f1=0.8458 | time=12.9s\n",
            "Epoch 018 | train_loss=0.4218 acc=0.8179 | val_loss=0.3912 acc=0.8503 | prec=0.8235 rec=0.8909 spec=0.8100 f1=0.8559 | time=12.8s\n",
            "Epoch 019 | train_loss=0.4163 acc=0.8259 | val_loss=0.3824 acc=0.8367 | prec=0.8083 rec=0.8818 spec=0.7919 f1=0.8435 | time=13.0s\n",
            "Epoch 020 | train_loss=0.3874 acc=0.8327 | val_loss=0.3712 acc=0.8639 | prec=0.8670 rec=0.8591 spec=0.8688 f1=0.8630 | time=12.7s\n",
            "Epoch 021 | train_loss=0.3826 acc=0.8372 | val_loss=0.4982 acc=0.6984 | prec=0.6239 rec=0.9955 spec=0.4027 f1=0.7671 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4218 acc=0.8304 | val_loss=0.4792 acc=0.8050 | prec=0.9295 rec=0.6591 spec=0.9502 f1=0.7713 | time=13.0s\n",
            "Epoch 023 | train_loss=0.3752 acc=0.8480 | val_loss=0.3571 acc=0.8617 | prec=0.8565 rec=0.8682 spec=0.8552 f1=0.8623 | time=12.9s\n",
            "Epoch 024 | train_loss=0.3331 acc=0.8639 | val_loss=0.3475 acc=0.8481 | prec=0.8964 rec=0.7864 spec=0.9095 f1=0.8378 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3442 acc=0.8440 | val_loss=0.4179 acc=0.8277 | prec=0.9045 rec=0.7318 spec=0.9231 f1=0.8090 | time=13.0s\n",
            "Epoch 026 | train_loss=0.2884 acc=0.8888 | val_loss=0.3373 acc=0.8549 | prec=0.8223 rec=0.9045 spec=0.8054 f1=0.8615 | time=13.0s\n",
            "Epoch 027 | train_loss=0.3031 acc=0.8593 | val_loss=0.3448 acc=0.8390 | prec=0.7945 rec=0.9136 spec=0.7647 f1=0.8499 | time=13.0s\n",
            "Epoch 028 | train_loss=0.2933 acc=0.8877 | val_loss=0.3994 acc=0.8345 | prec=0.8769 rec=0.7773 spec=0.8914 f1=0.8241 | time=12.8s\n",
            "Epoch 029 | train_loss=0.2983 acc=0.8758 | val_loss=0.4005 acc=0.8413 | prec=0.9167 rec=0.7500 spec=0.9321 f1=0.8250 | time=12.9s\n",
            "Epoch 030 | train_loss=0.2567 acc=0.8894 | val_loss=0.4451 acc=0.8186 | prec=0.9268 rec=0.6909 spec=0.9457 f1=0.7917 | time=12.8s\n",
            "Epoch 031 | train_loss=0.2489 acc=0.8939 | val_loss=0.3608 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.8s\n",
            "Epoch 032 | train_loss=0.2311 acc=0.9058 | val_loss=0.3465 acc=0.8617 | prec=0.8732 rec=0.8455 spec=0.8778 f1=0.8591 | time=12.8s\n",
            "Epoch 033 | train_loss=0.2390 acc=0.8951 | val_loss=0.3621 acc=0.8503 | prec=0.8738 rec=0.8182 spec=0.8824 f1=0.8451 | time=12.9s\n",
            "Epoch 034 | train_loss=0.2267 acc=0.9036 | val_loss=0.4080 acc=0.8073 | prec=0.7509 rec=0.9182 spec=0.6968 f1=0.8262 | time=12.9s\n",
            "Epoch 035 | train_loss=0.2208 acc=0.9047 | val_loss=0.3740 acc=0.8413 | prec=0.8866 rec=0.7818 spec=0.9005 f1=0.8309 | time=12.9s\n",
            "Epoch 036 | train_loss=0.2109 acc=0.9251 | val_loss=0.3628 acc=0.8413 | prec=0.8261 rec=0.8636 spec=0.8190 f1=0.8444 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2196 acc=0.9070 | val_loss=0.4123 acc=0.8095 | prec=0.7556 rec=0.9136 spec=0.7059 f1=0.8272 | time=12.9s\n",
            "Epoch 038 | train_loss=0.1976 acc=0.9274 | val_loss=0.4103 acc=0.8141 | prec=0.7614 rec=0.9136 spec=0.7149 f1=0.8306 | time=13.0s\n",
            "Epoch 039 | train_loss=0.1820 acc=0.9206 | val_loss=0.4408 acc=0.8163 | prec=0.7663 rec=0.9091 spec=0.7240 f1=0.8316 | time=13.0s\n",
            "Epoch 040 | train_loss=0.2341 acc=0.9098 | val_loss=0.3889 acc=0.8367 | prec=0.8978 rec=0.7591 spec=0.9140 f1=0.8227 | time=13.1s\n",
            "Epoch 041 | train_loss=0.1800 acc=0.9246 | val_loss=0.3866 acc=0.8413 | prec=0.8151 rec=0.8818 spec=0.8009 f1=0.8472 | time=12.9s\n",
            "Early stopping at epoch 41\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▆▆▆▆▆▇▆▇████████▇▇███████▇██████████</td></tr><tr><td>precision</td><td>▁▁▁▁▅▅▅▅▆▅▅▆▇▇█▆▇▇▇█▆█▇██▇▇███▇██▇█▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁████▆██▇▇▇▆█▇▇▇▇█▆▇▇▆▇▇▆▆▆▇▇▇▇▆▇▇▇▇▇</td></tr><tr><td>specificity</td><td>████▁▁▁▁▅▃▁▆▆▇▇▅▇▇▇▇▄█▇▇▇▇▆▇██▇▇▇▆▇▇▆▆▆▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▁▁▁▂▂▄▅▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇█▇███████</td></tr><tr><td>train_loss</td><td>████████████▆▆▅▅▄▄▄▄▄▄▃▃▃▂▃▂▃▂▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▃▃▁▆▇▇▇▆██▇█▅▇██▇██▇█▇▇██▇██▇▇▇█</td></tr><tr><td>validation_loss</td><td>███████████▇▅▃▃▄▂▂▂▂▄▄▁▁▃▁▁▂▂▃▁▁▁▂▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84716</td></tr><tr><td>precision</td><td>0.81513</td></tr><tr><td>recall</td><td>0.88182</td></tr><tr><td>specificity</td><td>0.8009</td></tr><tr><td>train_accuracy</td><td>0.92456</td></tr><tr><td>train_loss</td><td>0.18</td></tr><tr><td>validation_accuracy</td><td>0.84127</td></tr><tr><td>validation_loss</td><td>0.38659</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/t3k8la19' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/t3k8la19</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_090146-t3k8la19/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 09:10:36,992] Trial 2 finished with values: [0.3865858508007867, 0.8412698412698413] and parameters: {'lr': 0.00012258747037716027, 'wd': 7.133392204022225e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 3: filters=120, lr=8.43e-05, wd=1.93e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_091036-3gkxo53u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/3gkxo53u' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/3gkxo53u' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/3gkxo53u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7185 acc=0.5128 | val_loss=0.6958 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7189 acc=0.5201 | val_loss=0.6941 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7206 acc=0.5088 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7108 acc=0.5003 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7230 acc=0.4872 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7228 acc=0.4986 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7113 acc=0.5071 | val_loss=0.6974 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7189 acc=0.4940 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7158 acc=0.5094 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7167 acc=0.4997 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7191 acc=0.5082 | val_loss=0.6954 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7113 acc=0.5088 | val_loss=0.6950 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7134 acc=0.5116 | val_loss=0.6908 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7054 acc=0.5275 | val_loss=0.6751 acc=0.5442 | prec=0.8276 rec=0.1091 spec=0.9774 f1=0.1928 | time=12.8s\n",
            "Epoch 015 | train_loss=0.6416 acc=0.6268 | val_loss=0.5691 acc=0.7166 | prec=0.9060 rec=0.4818 spec=0.9502 f1=0.6291 | time=12.9s\n",
            "Epoch 016 | train_loss=0.5557 acc=0.7136 | val_loss=0.5142 acc=0.7710 | prec=0.7179 rec=0.8909 spec=0.6516 f1=0.7951 | time=12.8s\n",
            "Epoch 017 | train_loss=0.5091 acc=0.7550 | val_loss=0.4716 acc=0.8141 | prec=0.7782 rec=0.8773 spec=0.7511 f1=0.8248 | time=12.8s\n",
            "Epoch 018 | train_loss=0.4895 acc=0.7635 | val_loss=0.4532 acc=0.8390 | prec=0.8706 rec=0.7955 spec=0.8824 f1=0.8314 | time=12.9s\n",
            "Epoch 019 | train_loss=0.4704 acc=0.7907 | val_loss=0.4250 acc=0.8186 | prec=0.9023 rec=0.7136 spec=0.9231 f1=0.7970 | time=12.8s\n",
            "Epoch 020 | train_loss=0.4693 acc=0.7862 | val_loss=0.4369 acc=0.8005 | prec=0.9125 rec=0.6636 spec=0.9367 f1=0.7684 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4365 acc=0.8049 | val_loss=0.4166 acc=0.8277 | prec=0.9000 rec=0.7364 spec=0.9186 f1=0.8100 | time=13.0s\n",
            "Epoch 022 | train_loss=0.4149 acc=0.8117 | val_loss=0.3933 acc=0.8503 | prec=0.8348 rec=0.8727 spec=0.8281 f1=0.8533 | time=12.7s\n",
            "Epoch 023 | train_loss=0.3970 acc=0.8264 | val_loss=0.4068 acc=0.8254 | prec=0.8178 rec=0.8364 spec=0.8145 f1=0.8270 | time=12.8s\n",
            "Epoch 024 | train_loss=0.3846 acc=0.8361 | val_loss=0.4275 acc=0.8141 | prec=0.9259 rec=0.6818 spec=0.9457 f1=0.7853 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3443 acc=0.8605 | val_loss=0.3928 acc=0.8413 | prec=0.8947 rec=0.7727 spec=0.9095 f1=0.8293 | time=12.8s\n",
            "Epoch 026 | train_loss=0.3386 acc=0.8639 | val_loss=0.4289 acc=0.8118 | prec=0.9255 rec=0.6773 spec=0.9457 f1=0.7822 | time=12.9s\n",
            "Epoch 027 | train_loss=0.3373 acc=0.8554 | val_loss=0.4486 acc=0.7959 | prec=0.9577 rec=0.6182 spec=0.9729 f1=0.7514 | time=12.9s\n",
            "Epoch 028 | train_loss=0.2915 acc=0.8866 | val_loss=0.3744 acc=0.8549 | prec=0.9062 rec=0.7909 spec=0.9186 f1=0.8447 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3028 acc=0.8956 | val_loss=0.3992 acc=0.7959 | prec=0.7462 rec=0.8955 spec=0.6968 f1=0.8140 | time=12.9s\n",
            "Epoch 030 | train_loss=0.3007 acc=0.8667 | val_loss=0.3803 acc=0.8299 | prec=0.8033 rec=0.8727 spec=0.7873 f1=0.8366 | time=12.9s\n",
            "Epoch 031 | train_loss=0.2811 acc=0.8894 | val_loss=0.3715 acc=0.8481 | prec=0.8341 rec=0.8682 spec=0.8281 f1=0.8508 | time=13.0s\n",
            "Epoch 032 | train_loss=0.2524 acc=0.9058 | val_loss=0.3796 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=12.9s\n",
            "Epoch 033 | train_loss=0.2702 acc=0.8877 | val_loss=0.3651 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.8s\n",
            "Epoch 034 | train_loss=0.2376 acc=0.9047 | val_loss=0.4090 acc=0.8390 | prec=0.9408 rec=0.7227 spec=0.9548 f1=0.8175 | time=12.9s\n",
            "Epoch 035 | train_loss=0.2385 acc=0.9053 | val_loss=0.4458 acc=0.8299 | prec=0.9240 rec=0.7182 spec=0.9412 f1=0.8082 | time=12.7s\n",
            "Epoch 036 | train_loss=0.2861 acc=0.8780 | val_loss=0.4355 acc=0.8231 | prec=0.9128 rec=0.7136 spec=0.9321 f1=0.8010 | time=13.0s\n",
            "Epoch 037 | train_loss=0.2226 acc=0.9132 | val_loss=0.3909 acc=0.8186 | prec=0.7966 rec=0.8545 spec=0.7828 f1=0.8246 | time=12.8s\n",
            "Epoch 038 | train_loss=0.2040 acc=0.9178 | val_loss=0.3746 acc=0.8299 | prec=0.7984 rec=0.8818 spec=0.7783 f1=0.8380 | time=13.0s\n",
            "Epoch 039 | train_loss=0.2100 acc=0.9047 | val_loss=0.3741 acc=0.8435 | prec=0.8268 rec=0.8682 spec=0.8190 f1=0.8470 | time=12.9s\n",
            "Epoch 040 | train_loss=0.2502 acc=0.8979 | val_loss=0.4074 acc=0.8299 | prec=0.8962 rec=0.7455 spec=0.9140 f1=0.8139 | time=12.8s\n",
            "Epoch 041 | train_loss=0.2055 acc=0.9098 | val_loss=0.4225 acc=0.8322 | prec=0.8763 rec=0.7727 spec=0.8914 f1=0.8213 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2139 acc=0.9109 | val_loss=0.4090 acc=0.7937 | prec=0.7329 rec=0.9227 spec=0.6652 f1=0.8169 | time=12.9s\n",
            "Epoch 043 | train_loss=0.2365 acc=0.8990 | val_loss=0.3839 acc=0.8435 | prec=0.8512 rec=0.8318 spec=0.8552 f1=0.8414 | time=12.9s\n",
            "Epoch 044 | train_loss=0.1983 acc=0.9183 | val_loss=0.4859 acc=0.8209 | prec=0.8983 rec=0.7227 spec=0.9186 f1=0.8010 | time=12.9s\n",
            "Epoch 045 | train_loss=0.1795 acc=0.9234 | val_loss=0.4804 acc=0.8186 | prec=0.9118 rec=0.7045 spec=0.9321 f1=0.7949 | time=12.8s\n",
            "Epoch 046 | train_loss=0.1680 acc=0.9302 | val_loss=0.4732 acc=0.8163 | prec=0.8971 rec=0.7136 spec=0.9186 f1=0.7949 | time=12.9s\n",
            "Epoch 047 | train_loss=0.1940 acc=0.9336 | val_loss=0.4877 acc=0.8231 | prec=0.8901 rec=0.7364 spec=0.9095 f1=0.8060 | time=12.7s\n",
            "Epoch 048 | train_loss=0.1869 acc=0.9331 | val_loss=0.4680 acc=0.8345 | prec=0.8769 rec=0.7773 spec=0.8914 f1=0.8241 | time=12.8s\n",
            "Early stopping at epoch 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▁▁▁▁▁▁▁▁▃▆███▇████▇▇█████████████████</td></tr><tr><td>precision</td><td>▅▅▅▁▁▁▁▁▁▁▁▇█▆▇███▇▇████▆▇▇▇██▇▇▇█▇▇███▇</td></tr><tr><td>recall</td><td>███▁▁▁▁▁▁▁▁▂▄▇▇▆▆▆▇▇▆▆▅▇▇▇▇▇▆▆▇▇▇▆▆▇▆▆▆▆</td></tr><tr><td>specificity</td><td>▁▁▁██████████▆▆▇█▇▇▇▇██▇▆▇▇▇██▆▆▇▇▇▇▇█▇▇</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▁▁▁▁▁▁▁▂▃▅▅▆▆▆▆▆▇▇▇▇▇▇█▇█████▇█▇████</td></tr><tr><td>train_loss</td><td>████████████▇▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▅▆▇▇▇▇█▇█▇▇█▇█████▇█████▇▇▇█</td></tr><tr><td>validation_loss</td><td>████████████▅▄▃▂▃▂▂▂▂▂▃▁▂▁▁▁▂▃▂▁▁▂▂▁▄▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.8241</td></tr><tr><td>precision</td><td>0.87692</td></tr><tr><td>recall</td><td>0.77727</td></tr><tr><td>specificity</td><td>0.8914</td></tr><tr><td>train_accuracy</td><td>0.93307</td></tr><tr><td>train_loss</td><td>0.18694</td></tr><tr><td>validation_accuracy</td><td>0.83447</td></tr><tr><td>validation_loss</td><td>0.46798</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/3gkxo53u' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/3gkxo53u</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_091036-3gkxo53u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 09:20:55,981] Trial 3 finished with values: [0.46797875847135273, 0.8344671201814059] and parameters: {'lr': 8.431594288851207e-05, 'wd': 1.932575181561339e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 4: filters=120, lr=9.64e-05, wd=5.00e-06, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_092055-5in0tby2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/5in0tby2' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/5in0tby2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/5in0tby2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7876 acc=0.4821 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7355 acc=0.4957 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7323 acc=0.4906 | val_loss=0.6935 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7283 acc=0.5139 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7221 acc=0.5020 | val_loss=0.6924 acc=0.5193 | prec=0.5351 rec=0.2773 spec=0.7602 f1=0.3653 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7338 acc=0.4889 | val_loss=0.6932 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7120 acc=0.5065 | val_loss=0.6921 acc=0.5125 | prec=0.5063 rec=0.9091 spec=0.1176 f1=0.6504 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7171 acc=0.4940 | val_loss=0.6921 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7138 acc=0.5026 | val_loss=0.6913 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7043 acc=0.5179 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7045 acc=0.5264 | val_loss=0.6908 acc=0.5215 | prec=0.5105 rec=0.9955 spec=0.0498 f1=0.6749 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7048 acc=0.5077 | val_loss=0.6892 acc=0.5805 | prec=0.5469 rec=0.9273 spec=0.2353 f1=0.6880 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7028 acc=0.5275 | val_loss=0.6834 acc=0.5261 | prec=0.9231 rec=0.0545 spec=0.9955 f1=0.1030 | time=12.9s\n",
            "Epoch 014 | train_loss=0.6931 acc=0.5440 | val_loss=0.6700 acc=0.5351 | prec=0.5179 rec=0.9864 spec=0.0860 f1=0.6792 | time=12.9s\n",
            "Epoch 015 | train_loss=0.6362 acc=0.6421 | val_loss=0.5535 acc=0.7551 | prec=0.7029 rec=0.8818 spec=0.6290 f1=0.7823 | time=12.9s\n",
            "Epoch 016 | train_loss=0.5565 acc=0.7136 | val_loss=0.5087 acc=0.7982 | prec=0.7764 rec=0.8364 spec=0.7602 f1=0.8053 | time=12.9s\n",
            "Epoch 017 | train_loss=0.5251 acc=0.7459 | val_loss=0.4606 acc=0.7914 | prec=0.8951 rec=0.6591 spec=0.9231 f1=0.7592 | time=12.9s\n",
            "Epoch 018 | train_loss=0.4836 acc=0.7771 | val_loss=0.4552 acc=0.7914 | prec=0.7424 rec=0.8909 spec=0.6923 f1=0.8099 | time=12.9s\n",
            "Epoch 019 | train_loss=0.4667 acc=0.7947 | val_loss=0.4267 acc=0.8367 | prec=0.8394 rec=0.8318 spec=0.8416 f1=0.8356 | time=12.8s\n",
            "Epoch 020 | train_loss=0.4529 acc=0.7901 | val_loss=0.4236 acc=0.8231 | prec=0.7773 rec=0.9045 spec=0.7421 f1=0.8361 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4162 acc=0.8219 | val_loss=0.4315 acc=0.8163 | prec=0.8971 rec=0.7136 spec=0.9186 f1=0.7949 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4568 acc=0.7896 | val_loss=0.4510 acc=0.7778 | prec=0.7194 rec=0.9091 spec=0.6471 f1=0.8032 | time=12.8s\n",
            "Epoch 023 | train_loss=0.4194 acc=0.8151 | val_loss=0.4264 acc=0.8163 | prec=0.9017 rec=0.7091 spec=0.9231 f1=0.7939 | time=12.8s\n",
            "Epoch 024 | train_loss=0.3738 acc=0.8486 | val_loss=0.3804 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=12.9s\n",
            "Epoch 025 | train_loss=0.3510 acc=0.8571 | val_loss=0.3750 acc=0.8458 | prec=0.8958 rec=0.7818 spec=0.9095 f1=0.8350 | time=12.9s\n",
            "Epoch 026 | train_loss=0.3407 acc=0.8559 | val_loss=0.3601 acc=0.8481 | prec=0.8525 rec=0.8409 spec=0.8552 f1=0.8467 | time=12.9s\n",
            "Epoch 027 | train_loss=0.3534 acc=0.8571 | val_loss=0.3643 acc=0.8390 | prec=0.8225 rec=0.8636 spec=0.8145 f1=0.8426 | time=12.9s\n",
            "Epoch 028 | train_loss=0.3223 acc=0.8735 | val_loss=0.3422 acc=0.8776 | prec=0.8915 rec=0.8591 spec=0.8959 f1=0.8750 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3010 acc=0.8849 | val_loss=0.3413 acc=0.8503 | prec=0.8182 rec=0.9000 spec=0.8009 f1=0.8571 | time=12.8s\n",
            "Epoch 030 | train_loss=0.3031 acc=0.8724 | val_loss=0.3264 acc=0.8617 | prec=0.8732 rec=0.8455 spec=0.8778 f1=0.8591 | time=13.0s\n",
            "Epoch 031 | train_loss=0.2863 acc=0.8849 | val_loss=0.3304 acc=0.8503 | prec=0.8291 rec=0.8818 spec=0.8190 f1=0.8546 | time=12.9s\n",
            "Epoch 032 | train_loss=0.2702 acc=0.9087 | val_loss=0.4611 acc=0.8005 | prec=0.9459 rec=0.6364 spec=0.9638 f1=0.7609 | time=12.9s\n",
            "Epoch 033 | train_loss=0.2629 acc=0.8877 | val_loss=0.3537 acc=0.8413 | prec=0.7953 rec=0.9182 spec=0.7647 f1=0.8523 | time=12.9s\n",
            "Epoch 034 | train_loss=0.2497 acc=0.8979 | val_loss=0.4102 acc=0.8073 | prec=0.7419 rec=0.9409 spec=0.6742 f1=0.8297 | time=12.9s\n",
            "Epoch 035 | train_loss=0.2578 acc=0.9024 | val_loss=0.3300 acc=0.8571 | prec=0.8204 rec=0.9136 spec=0.8009 f1=0.8645 | time=12.9s\n",
            "Epoch 036 | train_loss=0.2203 acc=0.9092 | val_loss=0.3446 acc=0.8503 | prec=0.8235 rec=0.8909 spec=0.8100 f1=0.8559 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2272 acc=0.9121 | val_loss=0.6135 acc=0.7551 | prec=0.6783 rec=0.9682 spec=0.5430 f1=0.7978 | time=12.9s\n",
            "Epoch 038 | train_loss=0.2216 acc=0.9070 | val_loss=0.3695 acc=0.8435 | prec=0.8008 rec=0.9136 spec=0.7738 f1=0.8535 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2169 acc=0.9183 | val_loss=0.4483 acc=0.8277 | prec=0.9286 rec=0.7091 spec=0.9457 f1=0.8041 | time=12.9s\n",
            "Epoch 040 | train_loss=0.2476 acc=0.8968 | val_loss=0.3589 acc=0.8277 | prec=0.7835 rec=0.9045 spec=0.7511 f1=0.8397 | time=13.0s\n",
            "Epoch 041 | train_loss=0.1941 acc=0.9268 | val_loss=0.4142 acc=0.8118 | prec=0.7473 rec=0.9409 spec=0.6833 f1=0.8330 | time=12.9s\n",
            "Epoch 042 | train_loss=0.1898 acc=0.9268 | val_loss=0.3353 acc=0.8458 | prec=0.8089 rec=0.9045 spec=0.7873 f1=0.8541 | time=12.9s\n",
            "Epoch 043 | train_loss=0.1900 acc=0.9200 | val_loss=0.3439 acc=0.8435 | prec=0.8186 rec=0.8818 spec=0.8054 f1=0.8490 | time=12.9s\n",
            "Epoch 044 | train_loss=0.1887 acc=0.9291 | val_loss=0.4165 acc=0.8231 | prec=0.7710 rec=0.9182 spec=0.7285 f1=0.8382 | time=12.9s\n",
            "Epoch 045 | train_loss=0.2278 acc=0.9075 | val_loss=0.6337 acc=0.7619 | prec=0.6837 rec=0.9727 spec=0.5520 f1=0.8030 | time=13.0s\n",
            "Early stopping at epoch 45\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▁▄▆▆▁▆▆▇▂▆▇▇▇██▇▇▇███████▇███▇█▇████▇</td></tr><tr><td>precision</td><td>▅▅▅▁▅▅▅▁▅▅▅█▅▆▇█▇▇█▆█▇█▇█▇▇▇█▇▆▇▆▇█▇▇▇▇▆</td></tr><tr><td>recall</td><td>███▁▃█▇▁██▇▁█▇▇▆▇▇▆▇▆▇▆▇▇▇▇▇▅▇█▇█▇▆▇█▇▇█</td></tr><tr><td>specificity</td><td>▁▁▁█▆▁▂█▁▁▃█▂▅▆▇▇▆▇▆▇▇▇▇▇▇▇▇█▆▆▇▅▆█▆▆▇▇▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▂▂▁▂▂▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇█▇██████████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▇▇▇▆▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▂▂▆▇▆▇▇▇▆▇▇▇▇█▇█▇▇▇▇█▆▇▇▇▇▇▇▆</td></tr><tr><td>validation_loss</td><td>█████████████▅▄▄▃▃▃▃▃▂▂▂▁▁▁▁▄▂▃▁▆▂▃▂▃▁▁▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.803</td></tr><tr><td>precision</td><td>0.68371</td></tr><tr><td>recall</td><td>0.97273</td></tr><tr><td>specificity</td><td>0.55204</td></tr><tr><td>train_accuracy</td><td>0.90754</td></tr><tr><td>train_loss</td><td>0.22782</td></tr><tr><td>validation_accuracy</td><td>0.7619</td></tr><tr><td>validation_loss</td><td>0.63371</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/5in0tby2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/5in0tby2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_092055-5in0tby2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 09:30:37,747] Trial 4 finished with values: [0.633709437080792, 0.7619047619047619] and parameters: {'lr': 9.638088087992014e-05, 'wd': 4.999187267001703e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 5: filters=120, lr=3.79e-05, wd=4.54e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_093037-f65pykh7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/f65pykh7' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/f65pykh7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/f65pykh7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7352 acc=0.4929 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7301 acc=0.4929 | val_loss=0.7000 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7289 acc=0.4974 | val_loss=0.6942 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7290 acc=0.4980 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7247 acc=0.4838 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7255 acc=0.4963 | val_loss=0.6927 acc=0.4989 | prec=0.3333 rec=0.0045 spec=0.9910 f1=0.0090 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7184 acc=0.5071 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7164 acc=0.5037 | val_loss=0.6934 acc=0.5011 | prec=0.5000 rec=0.7864 spec=0.2172 f1=0.6113 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7066 acc=0.4980 | val_loss=0.6930 acc=0.5147 | prec=0.5214 rec=0.3318 spec=0.6968 f1=0.4056 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7132 acc=0.4952 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7041 acc=0.5201 | val_loss=0.6927 acc=0.5125 | prec=0.5064 rec=0.9000 spec=0.1267 f1=0.6481 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7072 acc=0.4986 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7078 acc=0.4889 | val_loss=0.6923 acc=0.5034 | prec=0.5011 rec=1.0000 spec=0.0090 f1=0.6677 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7088 acc=0.5088 | val_loss=0.6921 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7091 acc=0.4833 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7005 acc=0.5156 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.6998 acc=0.5077 | val_loss=0.6905 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.6963 acc=0.5201 | val_loss=0.6883 acc=0.6508 | prec=0.8113 rec=0.3909 spec=0.9095 f1=0.5276 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6975 acc=0.5196 | val_loss=0.6841 acc=0.5283 | prec=0.8333 rec=0.0682 spec=0.9864 f1=0.1261 | time=12.9s\n",
            "Epoch 020 | train_loss=0.6803 acc=0.5701 | val_loss=0.6669 acc=0.5420 | prec=0.8750 rec=0.0955 spec=0.9864 f1=0.1721 | time=12.9s\n",
            "Epoch 021 | train_loss=0.6617 acc=0.6001 | val_loss=0.5878 acc=0.7755 | prec=0.7867 rec=0.7545 spec=0.7964 f1=0.7703 | time=12.9s\n",
            "Epoch 022 | train_loss=0.6044 acc=0.6773 | val_loss=0.5297 acc=0.8141 | prec=0.8255 rec=0.7955 spec=0.8326 f1=0.8102 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5903 acc=0.6954 | val_loss=0.5305 acc=0.8095 | prec=0.7957 rec=0.8318 spec=0.7873 f1=0.8133 | time=12.8s\n",
            "Epoch 024 | train_loss=0.5534 acc=0.7357 | val_loss=0.4953 acc=0.8186 | prec=0.8097 rec=0.8318 spec=0.8054 f1=0.8206 | time=13.0s\n",
            "Epoch 025 | train_loss=0.5322 acc=0.7459 | val_loss=0.4833 acc=0.8254 | prec=0.8783 rec=0.7545 spec=0.8959 f1=0.8117 | time=12.8s\n",
            "Epoch 026 | train_loss=0.5203 acc=0.7459 | val_loss=0.5218 acc=0.7347 | prec=0.6635 rec=0.9500 spec=0.5204 f1=0.7813 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5021 acc=0.7606 | val_loss=0.4498 acc=0.8435 | prec=0.8647 rec=0.8136 spec=0.8733 f1=0.8384 | time=12.8s\n",
            "Epoch 028 | train_loss=0.5208 acc=0.7328 | val_loss=0.4410 acc=0.8367 | prec=0.8394 rec=0.8318 spec=0.8416 f1=0.8356 | time=13.0s\n",
            "Epoch 029 | train_loss=0.4926 acc=0.7822 | val_loss=0.4296 acc=0.8435 | prec=0.8240 rec=0.8727 spec=0.8145 f1=0.8477 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4591 acc=0.7913 | val_loss=0.4344 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=13.0s\n",
            "Epoch 031 | train_loss=0.4543 acc=0.8032 | val_loss=0.4523 acc=0.8367 | prec=0.8008 rec=0.8955 spec=0.7783 f1=0.8455 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4387 acc=0.8242 | val_loss=0.4212 acc=0.8458 | prec=0.8220 rec=0.8818 spec=0.8100 f1=0.8509 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4528 acc=0.8106 | val_loss=0.4321 acc=0.8435 | prec=0.8057 rec=0.9045 spec=0.7828 f1=0.8522 | time=13.0s\n",
            "Epoch 034 | train_loss=0.4310 acc=0.8191 | val_loss=0.3870 acc=0.8526 | prec=0.8673 rec=0.8318 spec=0.8733 f1=0.8492 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4387 acc=0.8071 | val_loss=0.4186 acc=0.8322 | prec=0.7967 rec=0.8909 spec=0.7738 f1=0.8412 | time=12.9s\n",
            "Epoch 036 | train_loss=0.4150 acc=0.8225 | val_loss=0.4177 acc=0.8413 | prec=0.8000 rec=0.9091 spec=0.7738 f1=0.8511 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4085 acc=0.8446 | val_loss=0.3888 acc=0.8503 | prec=0.8598 rec=0.8364 spec=0.8643 f1=0.8479 | time=13.1s\n",
            "Epoch 038 | train_loss=0.4018 acc=0.8372 | val_loss=0.3916 acc=0.8503 | prec=0.9096 rec=0.7773 spec=0.9231 f1=0.8382 | time=13.0s\n",
            "Epoch 039 | train_loss=0.3807 acc=0.8491 | val_loss=0.4253 acc=0.8254 | prec=0.9181 rec=0.7136 spec=0.9367 f1=0.8031 | time=13.0s\n",
            "Epoch 040 | train_loss=0.3983 acc=0.8457 | val_loss=0.4020 acc=0.8367 | prec=0.8274 rec=0.8500 spec=0.8235 f1=0.8386 | time=13.0s\n",
            "Epoch 041 | train_loss=0.3748 acc=0.8400 | val_loss=0.4163 acc=0.8209 | prec=0.7722 rec=0.9091 spec=0.7330 f1=0.8351 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3719 acc=0.8491 | val_loss=0.3644 acc=0.8549 | prec=0.8939 rec=0.8045 spec=0.9050 f1=0.8469 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3992 acc=0.8389 | val_loss=0.3937 acc=0.8503 | prec=0.8235 rec=0.8909 spec=0.8100 f1=0.8559 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3633 acc=0.8542 | val_loss=0.4254 acc=0.8118 | prec=0.7566 rec=0.9182 spec=0.7059 f1=0.8296 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3421 acc=0.8718 | val_loss=0.3702 acc=0.8458 | prec=0.8193 rec=0.8864 spec=0.8054 f1=0.8515 | time=13.0s\n",
            "Epoch 046 | train_loss=0.3552 acc=0.8565 | val_loss=0.3939 acc=0.8390 | prec=0.7922 rec=0.9182 spec=0.7602 f1=0.8505 | time=13.0s\n",
            "Epoch 047 | train_loss=0.3171 acc=0.8883 | val_loss=0.3948 acc=0.8413 | prec=0.8024 rec=0.9045 spec=0.7783 f1=0.8504 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3247 acc=0.8678 | val_loss=0.3718 acc=0.8413 | prec=0.8750 rec=0.7955 spec=0.8869 f1=0.8333 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3213 acc=0.8809 | val_loss=0.3666 acc=0.8458 | prec=0.8689 rec=0.8136 spec=0.8778 f1=0.8404 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3392 acc=0.8667 | val_loss=0.3861 acc=0.8413 | prec=0.8074 rec=0.8955 spec=0.7873 f1=0.8491 | time=12.9s\n",
            "Epoch 051 | train_loss=0.3080 acc=0.8894 | val_loss=0.3638 acc=0.8503 | prec=0.8667 rec=0.8273 spec=0.8733 f1=0.8465 | time=12.9s\n",
            "Epoch 052 | train_loss=0.3285 acc=0.8695 | val_loss=0.3740 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3069 acc=0.8769 | val_loss=0.3799 acc=0.8413 | prec=0.8151 rec=0.8818 spec=0.8009 f1=0.8472 | time=12.8s\n",
            "Epoch 054 | train_loss=0.2934 acc=0.8917 | val_loss=0.3837 acc=0.8345 | prec=0.8025 rec=0.8864 spec=0.7828 f1=0.8423 | time=13.0s\n",
            "Epoch 055 | train_loss=0.3298 acc=0.8798 | val_loss=0.3712 acc=0.8413 | prec=0.8538 rec=0.8227 spec=0.8597 f1=0.8380 | time=12.9s\n",
            "Epoch 056 | train_loss=0.3110 acc=0.8798 | val_loss=0.3765 acc=0.8526 | prec=0.8216 rec=0.9000 spec=0.8054 f1=0.8590 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3094 acc=0.8860 | val_loss=0.3771 acc=0.8526 | prec=0.8216 rec=0.9000 spec=0.8054 f1=0.8590 | time=12.9s\n",
            "Epoch 058 | train_loss=0.2731 acc=0.8968 | val_loss=0.3819 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2896 acc=0.8928 | val_loss=0.3817 acc=0.8367 | prec=0.8058 rec=0.8864 spec=0.7873 f1=0.8442 | time=12.8s\n",
            "Epoch 060 | train_loss=0.2841 acc=0.8843 | val_loss=0.3760 acc=0.8345 | prec=0.8296 rec=0.8409 spec=0.8281 f1=0.8352 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2667 acc=0.8951 | val_loss=0.3603 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=12.9s\n",
            "Epoch 062 | train_loss=0.2657 acc=0.8968 | val_loss=0.3607 acc=0.8413 | prec=0.8319 rec=0.8545 spec=0.8281 f1=0.8430 | time=12.9s\n",
            "Epoch 063 | train_loss=0.2547 acc=0.9053 | val_loss=0.3809 acc=0.8458 | prec=0.8089 rec=0.9045 spec=0.7873 f1=0.8541 | time=12.9s\n",
            "Epoch 064 | train_loss=0.2535 acc=0.9013 | val_loss=0.3730 acc=0.8458 | prec=0.8654 rec=0.8182 spec=0.8733 f1=0.8411 | time=12.9s\n",
            "Epoch 065 | train_loss=0.2933 acc=0.8894 | val_loss=0.3627 acc=0.8594 | prec=0.8911 rec=0.8182 spec=0.9005 f1=0.8531 | time=12.8s\n",
            "Epoch 066 | train_loss=0.2344 acc=0.9178 | val_loss=0.4055 acc=0.8209 | prec=0.7809 rec=0.8909 spec=0.7511 f1=0.8323 | time=12.9s\n",
            "Epoch 067 | train_loss=0.2483 acc=0.9058 | val_loss=0.3722 acc=0.8526 | prec=0.8818 rec=0.8136 spec=0.8914 f1=0.8463 | time=12.8s\n",
            "Epoch 068 | train_loss=0.2449 acc=0.9058 | val_loss=0.3995 acc=0.8254 | prec=0.7804 rec=0.9045 spec=0.7466 f1=0.8379 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2429 acc=0.9030 | val_loss=0.3735 acc=0.8571 | prec=0.8905 rec=0.8136 spec=0.9005 f1=0.8504 | time=12.9s\n",
            "Epoch 070 | train_loss=0.2249 acc=0.9155 | val_loss=0.4078 acc=0.8209 | prec=0.7809 rec=0.8909 spec=0.7511 f1=0.8323 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2427 acc=0.9075 | val_loss=0.3769 acc=0.8345 | prec=0.8419 rec=0.8227 spec=0.8462 f1=0.8322 | time=12.8s\n",
            "Epoch 072 | train_loss=0.2357 acc=0.9064 | val_loss=0.3870 acc=0.8503 | prec=0.8812 rec=0.8091 spec=0.8914 f1=0.8436 | time=12.8s\n",
            "Epoch 073 | train_loss=0.2395 acc=0.9092 | val_loss=0.3706 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2322 acc=0.9183 | val_loss=0.3708 acc=0.8435 | prec=0.8545 rec=0.8273 spec=0.8597 f1=0.8406 | time=12.8s\n",
            "Epoch 075 | train_loss=0.2484 acc=0.9166 | val_loss=0.4161 acc=0.8231 | prec=0.9128 rec=0.7136 spec=0.9321 f1=0.8010 | time=12.9s\n",
            "Epoch 076 | train_loss=0.2106 acc=0.9229 | val_loss=0.3736 acc=0.8549 | prec=0.8714 rec=0.8318 spec=0.8778 f1=0.8512 | time=12.9s\n",
            "Early stopping at epoch 76\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▆▄▆▁▁▁▂██████████████████████████████</td></tr><tr><td>precision</td><td>▁▁▄▅▅▅▁▁▇▇▇██▇▇▇█▇▇██▇█▇▇▇██▇▇▇▇▇██▇▇█▇█</td></tr><tr><td>recall</td><td>▁▁▁▇▃██▁▁▄▂▆▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>██▃▆▁▁▁█▇█▇▇▇▇▅▇▇▇▆▇█▆▇▇▆▆▆▇▇▇▆▇▇▇▆▇▆▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▄▅▅▅▅▆▆▆▆▇▇▇▇█▇▇███████████</td></tr><tr><td>train_loss</td><td>█████████▇▆▆▅▅▅▄▄▄▄▄▃▄▃▃▂▃▂▃▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▄▂▆▇▆█████▇██▇█████████████▇███</td></tr><tr><td>validation_loss</td><td>███████████▇▆▅▄▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.85116</td></tr><tr><td>precision</td><td>0.87143</td></tr><tr><td>recall</td><td>0.83182</td></tr><tr><td>specificity</td><td>0.87783</td></tr><tr><td>train_accuracy</td><td>0.92286</td></tr><tr><td>train_loss</td><td>0.21061</td></tr><tr><td>validation_accuracy</td><td>0.85488</td></tr><tr><td>validation_loss</td><td>0.37359</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/f65pykh7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/f65pykh7</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_093037-f65pykh7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 09:46:58,605] Trial 5 finished with values: [0.3735910728573799, 0.854875283446712] and parameters: {'lr': 3.793344453740315e-05, 'wd': 4.5415221838197576e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 6: filters=120, lr=3.52e-05, wd=4.77e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_094658-miidkbk7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/miidkbk7' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/miidkbk7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/miidkbk7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7140 acc=0.5207 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7190 acc=0.5020 | val_loss=0.6960 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7114 acc=0.4991 | val_loss=0.6968 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7061 acc=0.5269 | val_loss=0.6969 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7092 acc=0.5145 | val_loss=0.7012 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7038 acc=0.5026 | val_loss=0.6984 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7227 acc=0.4906 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7110 acc=0.5037 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7137 acc=0.4912 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7139 acc=0.4980 | val_loss=0.6976 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7179 acc=0.4810 | val_loss=0.6969 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7104 acc=0.4872 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7071 acc=0.5009 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7076 acc=0.5105 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7133 acc=0.4997 | val_loss=0.6954 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7152 acc=0.4838 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 017 | train_loss=0.7061 acc=0.5031 | val_loss=0.6932 acc=0.4921 | prec=0.4722 rec=0.1545 spec=0.8281 f1=0.2329 | time=13.0s\n",
            "Epoch 018 | train_loss=0.7108 acc=0.5054 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7044 acc=0.5065 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7073 acc=0.5162 | val_loss=0.6908 acc=0.5420 | prec=0.7647 rec=0.1182 spec=0.9638 f1=0.2047 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7099 acc=0.4884 | val_loss=0.6887 acc=0.5782 | prec=0.5423 rec=0.9909 spec=0.1674 f1=0.7010 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7057 acc=0.5060 | val_loss=0.6842 acc=0.6304 | prec=0.8800 rec=0.3000 spec=0.9593 f1=0.4475 | time=13.0s\n",
            "Epoch 023 | train_loss=0.6850 acc=0.5406 | val_loss=0.6818 acc=0.6100 | prec=0.5678 rec=0.9136 spec=0.3077 f1=0.7003 | time=12.9s\n",
            "Epoch 024 | train_loss=0.6795 acc=0.5740 | val_loss=0.6445 acc=0.7415 | prec=0.8442 rec=0.5909 spec=0.8914 f1=0.6952 | time=12.8s\n",
            "Epoch 025 | train_loss=0.6360 acc=0.6495 | val_loss=0.5692 acc=0.7823 | prec=0.8780 rec=0.6545 spec=0.9095 f1=0.7500 | time=13.0s\n",
            "Epoch 026 | train_loss=0.6101 acc=0.6880 | val_loss=0.5864 acc=0.7234 | prec=0.6561 rec=0.9364 spec=0.5113 f1=0.7715 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5584 acc=0.7368 | val_loss=0.5314 acc=0.7982 | prec=0.7610 rec=0.8682 spec=0.7285 f1=0.8110 | time=13.0s\n",
            "Epoch 028 | train_loss=0.5436 acc=0.7436 | val_loss=0.4840 acc=0.8163 | prec=0.8971 rec=0.7136 spec=0.9186 f1=0.7949 | time=12.8s\n",
            "Epoch 029 | train_loss=0.5138 acc=0.7691 | val_loss=0.4650 acc=0.8322 | prec=0.8802 rec=0.7682 spec=0.8959 f1=0.8204 | time=12.8s\n",
            "Epoch 030 | train_loss=0.5067 acc=0.7640 | val_loss=0.4480 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4904 acc=0.7958 | val_loss=0.4430 acc=0.8413 | prec=0.8788 rec=0.7909 spec=0.8914 f1=0.8325 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4835 acc=0.7924 | val_loss=0.4386 acc=0.8481 | prec=0.8923 rec=0.7909 spec=0.9050 f1=0.8386 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4699 acc=0.8060 | val_loss=0.4359 acc=0.8413 | prec=0.8788 rec=0.7909 spec=0.8914 f1=0.8325 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4521 acc=0.8264 | val_loss=0.4244 acc=0.8299 | prec=0.8166 rec=0.8500 spec=0.8100 f1=0.8330 | time=12.9s\n",
            "Epoch 035 | train_loss=0.4428 acc=0.8242 | val_loss=0.4113 acc=0.8390 | prec=0.8311 rec=0.8500 spec=0.8281 f1=0.8404 | time=12.9s\n",
            "Epoch 036 | train_loss=0.4196 acc=0.8406 | val_loss=0.4170 acc=0.8322 | prec=0.8147 rec=0.8591 spec=0.8054 f1=0.8363 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4258 acc=0.8355 | val_loss=0.4205 acc=0.8254 | prec=0.8783 rec=0.7545 spec=0.8959 f1=0.8117 | time=12.8s\n",
            "Epoch 038 | train_loss=0.4225 acc=0.8332 | val_loss=0.4083 acc=0.8345 | prec=0.8267 rec=0.8455 spec=0.8235 f1=0.8360 | time=13.0s\n",
            "Epoch 039 | train_loss=0.4047 acc=0.8423 | val_loss=0.4066 acc=0.8367 | prec=0.9022 rec=0.7545 spec=0.9186 f1=0.8218 | time=12.9s\n",
            "Epoch 040 | train_loss=0.4020 acc=0.8463 | val_loss=0.4075 acc=0.8345 | prec=0.8731 rec=0.7818 spec=0.8869 f1=0.8249 | time=13.0s\n",
            "Epoch 041 | train_loss=0.3763 acc=0.8548 | val_loss=0.3993 acc=0.8322 | prec=0.8318 rec=0.8318 spec=0.8326 f1=0.8318 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3667 acc=0.8571 | val_loss=0.3962 acc=0.8322 | prec=0.8259 rec=0.8409 spec=0.8235 f1=0.8333 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3646 acc=0.8701 | val_loss=0.3928 acc=0.8345 | prec=0.8419 rec=0.8227 spec=0.8462 f1=0.8322 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3516 acc=0.8792 | val_loss=0.4215 acc=0.8299 | prec=0.9006 rec=0.7409 spec=0.9186 f1=0.8130 | time=13.0s\n",
            "Epoch 045 | train_loss=0.3646 acc=0.8491 | val_loss=0.3900 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=13.0s\n",
            "Epoch 046 | train_loss=0.3363 acc=0.8780 | val_loss=0.3950 acc=0.8367 | prec=0.8627 rec=0.8000 spec=0.8733 f1=0.8302 | time=12.9s\n",
            "Epoch 047 | train_loss=0.3317 acc=0.8792 | val_loss=0.4098 acc=0.8254 | prec=0.8865 rec=0.7455 spec=0.9050 f1=0.8099 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3230 acc=0.8780 | val_loss=0.4070 acc=0.8299 | prec=0.8718 rec=0.7727 spec=0.8869 f1=0.8193 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3094 acc=0.8917 | val_loss=0.4012 acc=0.8277 | prec=0.8130 rec=0.8500 spec=0.8054 f1=0.8311 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3060 acc=0.8866 | val_loss=0.4121 acc=0.8345 | prec=0.8973 rec=0.7545 spec=0.9140 f1=0.8198 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3063 acc=0.8786 | val_loss=0.4023 acc=0.8254 | prec=0.8907 rec=0.7409 spec=0.9095 f1=0.8089 | time=13.0s\n",
            "Epoch 052 | train_loss=0.2954 acc=0.8905 | val_loss=0.4608 acc=0.8118 | prec=0.9255 rec=0.6773 spec=0.9457 f1=0.7822 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2872 acc=0.8888 | val_loss=0.3702 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=13.0s\n",
            "Epoch 054 | train_loss=0.2842 acc=0.9013 | val_loss=0.3723 acc=0.8549 | prec=0.8786 rec=0.8227 spec=0.8869 f1=0.8498 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2935 acc=0.8985 | val_loss=0.3796 acc=0.8435 | prec=0.8794 rec=0.7955 spec=0.8914 f1=0.8353 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2790 acc=0.9007 | val_loss=0.3704 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=13.0s\n",
            "Epoch 057 | train_loss=0.2611 acc=0.9053 | val_loss=0.3584 acc=0.8526 | prec=0.8539 rec=0.8500 spec=0.8552 f1=0.8519 | time=12.9s\n",
            "Epoch 058 | train_loss=0.2591 acc=0.9041 | val_loss=0.3755 acc=0.8390 | prec=0.8253 rec=0.8591 spec=0.8190 f1=0.8419 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2573 acc=0.9070 | val_loss=0.3705 acc=0.8345 | prec=0.8000 rec=0.8909 spec=0.7783 f1=0.8430 | time=12.9s\n",
            "Epoch 060 | train_loss=0.2814 acc=0.8968 | val_loss=0.3688 acc=0.8435 | prec=0.8213 rec=0.8773 spec=0.8100 f1=0.8484 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2414 acc=0.9075 | val_loss=0.3609 acc=0.8413 | prec=0.8538 rec=0.8227 spec=0.8597 f1=0.8380 | time=13.0s\n",
            "Epoch 062 | train_loss=0.2451 acc=0.9087 | val_loss=0.3616 acc=0.8435 | prec=0.8545 rec=0.8273 spec=0.8597 f1=0.8406 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2477 acc=0.8990 | val_loss=0.3539 acc=0.8435 | prec=0.8386 rec=0.8500 spec=0.8371 f1=0.8442 | time=12.8s\n",
            "Epoch 064 | train_loss=0.2400 acc=0.9041 | val_loss=0.3694 acc=0.8345 | prec=0.8731 rec=0.7818 spec=0.8869 f1=0.8249 | time=12.9s\n",
            "Epoch 065 | train_loss=0.2615 acc=0.9081 | val_loss=0.3602 acc=0.8503 | prec=0.8667 rec=0.8273 spec=0.8733 f1=0.8465 | time=12.8s\n",
            "Epoch 066 | train_loss=0.2369 acc=0.9104 | val_loss=0.3673 acc=0.8503 | prec=0.8929 rec=0.7955 spec=0.9050 f1=0.8413 | time=13.0s\n",
            "Epoch 067 | train_loss=0.2459 acc=0.9138 | val_loss=0.3553 acc=0.8481 | prec=0.8341 rec=0.8682 spec=0.8281 f1=0.8508 | time=13.0s\n",
            "Epoch 068 | train_loss=0.2521 acc=0.8922 | val_loss=0.3741 acc=0.8526 | prec=0.8744 rec=0.8227 spec=0.8824 f1=0.8478 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2334 acc=0.9104 | val_loss=0.3772 acc=0.8413 | prec=0.8947 rec=0.7727 spec=0.9095 f1=0.8293 | time=12.9s\n",
            "Epoch 070 | train_loss=0.2230 acc=0.9234 | val_loss=0.3804 acc=0.8435 | prec=0.8872 rec=0.7864 spec=0.9005 f1=0.8337 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2322 acc=0.9081 | val_loss=0.3704 acc=0.8458 | prec=0.8551 rec=0.8318 spec=0.8597 f1=0.8433 | time=12.9s\n",
            "Epoch 072 | train_loss=0.2187 acc=0.9155 | val_loss=0.3733 acc=0.8458 | prec=0.8800 rec=0.8000 spec=0.8914 f1=0.8381 | time=12.8s\n",
            "Epoch 073 | train_loss=0.2132 acc=0.9251 | val_loss=0.3864 acc=0.8413 | prec=0.8788 rec=0.7909 spec=0.8914 f1=0.8325 | time=13.0s\n",
            "Epoch 074 | train_loss=0.2178 acc=0.9200 | val_loss=0.3851 acc=0.8299 | prec=0.8166 rec=0.8500 spec=0.8100 f1=0.8330 | time=12.9s\n",
            "Epoch 075 | train_loss=0.2215 acc=0.9132 | val_loss=0.3614 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=12.9s\n",
            "Epoch 076 | train_loss=0.2267 acc=0.9098 | val_loss=0.3979 acc=0.8435 | prec=0.9037 rec=0.7682 spec=0.9186 f1=0.8305 | time=12.8s\n",
            "Epoch 077 | train_loss=0.2294 acc=0.9183 | val_loss=0.3678 acc=0.8390 | prec=0.8565 rec=0.8136 spec=0.8643 f1=0.8345 | time=12.9s\n",
            "Epoch 078 | train_loss=0.2236 acc=0.9047 | val_loss=0.3590 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=12.9s\n",
            "Early stopping at epoch 78\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▃▁▃▇▅▇▇████████████▇██████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▇▅██▇███▇▇█▇▇███████▇▇██▇██████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁█▆▇▇▇█▇▇▇▇▇▇▇█▇▇█████▇▇▇█▇▇▇▇▇</td></tr><tr><td>specificity</td><td>████████████▁▇▇▅▇▇▆▇▆▇▆▇▆▆▇▇▆▇▇▇▆▆▇▇▇▇▆▇</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▁▁▁▁▁▂▁▂▂▄▅▆▅▆▆▆▇▇▇▇▇▇▇▇▇▇██████▇███</td></tr><tr><td>train_loss</td><td>███████████████▇▇▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▂▃▆▅▇▇███▇█████▇▇███████████████</td></tr><tr><td>validation_loss</td><td>████████████▅▆▅▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84932</td></tr><tr><td>precision</td><td>0.85321</td></tr><tr><td>recall</td><td>0.84545</td></tr><tr><td>specificity</td><td>0.8552</td></tr><tr><td>train_accuracy</td><td>0.90471</td></tr><tr><td>train_loss</td><td>0.22358</td></tr><tr><td>validation_accuracy</td><td>0.85034</td></tr><tr><td>validation_loss</td><td>0.35903</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/miidkbk7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/miidkbk7</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_094658-miidkbk7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 10:03:46,643] Trial 6 finished with values: [0.3590279668569565, 0.8503401360544217] and parameters: {'lr': 3.5210516472168496e-05, 'wd': 4.774224777773625e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 7: filters=120, lr=1.02e-05, wd=4.00e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_100346-c8kkeqzp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/c8kkeqzp' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/c8kkeqzp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/c8kkeqzp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7947 acc=0.4765 | val_loss=0.7028 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7917 acc=0.5043 | val_loss=0.6987 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7723 acc=0.4980 | val_loss=0.7011 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7940 acc=0.4821 | val_loss=0.7031 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7729 acc=0.5071 | val_loss=0.7048 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7797 acc=0.4991 | val_loss=0.6976 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7637 acc=0.5020 | val_loss=0.7006 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7540 acc=0.4918 | val_loss=0.6982 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7511 acc=0.5026 | val_loss=0.6971 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7341 acc=0.5139 | val_loss=0.6967 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7445 acc=0.4952 | val_loss=0.6962 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7432 acc=0.5082 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7511 acc=0.4952 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7413 acc=0.4974 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7575 acc=0.4765 | val_loss=0.6926 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7455 acc=0.4918 | val_loss=0.6946 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7336 acc=0.5009 | val_loss=0.6922 acc=0.5261 | prec=0.5141 rec=0.9091 spec=0.1448 f1=0.6568 | time=13.0s\n",
            "Epoch 018 | train_loss=0.7423 acc=0.4963 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7245 acc=0.5128 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7312 acc=0.5139 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7412 acc=0.4906 | val_loss=0.6929 acc=0.4943 | prec=0.4966 rec=0.9864 spec=0.0045 f1=0.6606 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7376 acc=0.4997 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 023 | train_loss=0.7408 acc=0.5082 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 024 | train_loss=0.7272 acc=0.5150 | val_loss=0.6964 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7266 acc=0.5031 | val_loss=0.6926 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 026 | train_loss=0.7203 acc=0.5037 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 027 | train_loss=0.7255 acc=0.5065 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 028 | train_loss=0.7369 acc=0.4918 | val_loss=0.6901 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 029 | train_loss=0.7356 acc=0.5184 | val_loss=0.6855 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 030 | train_loss=0.7184 acc=0.5179 | val_loss=0.6754 acc=0.6327 | prec=0.5833 rec=0.9227 spec=0.3439 f1=0.7148 | time=13.0s\n",
            "Epoch 031 | train_loss=0.7120 acc=0.5343 | val_loss=0.6630 acc=0.7120 | prec=0.6782 rec=0.8045 spec=0.6199 f1=0.7360 | time=12.9s\n",
            "Epoch 032 | train_loss=0.6914 acc=0.5627 | val_loss=0.6461 acc=0.7347 | prec=0.7537 rec=0.6955 spec=0.7738 f1=0.7234 | time=12.9s\n",
            "Epoch 033 | train_loss=0.6718 acc=0.5916 | val_loss=0.6128 acc=0.7506 | prec=0.7007 rec=0.8727 spec=0.6290 f1=0.7773 | time=12.9s\n",
            "Epoch 034 | train_loss=0.6563 acc=0.6126 | val_loss=0.5897 acc=0.7143 | prec=0.8730 rec=0.5000 spec=0.9276 f1=0.6358 | time=12.9s\n",
            "Epoch 035 | train_loss=0.6357 acc=0.6268 | val_loss=0.5539 acc=0.7551 | prec=0.8590 rec=0.6091 spec=0.9005 f1=0.7128 | time=12.9s\n",
            "Epoch 036 | train_loss=0.6163 acc=0.6506 | val_loss=0.5310 acc=0.7914 | prec=0.7963 rec=0.7818 spec=0.8009 f1=0.7890 | time=12.8s\n",
            "Epoch 037 | train_loss=0.5881 acc=0.6778 | val_loss=0.5223 acc=0.7868 | prec=0.7480 rec=0.8636 spec=0.7104 f1=0.8017 | time=12.7s\n",
            "Epoch 038 | train_loss=0.5780 acc=0.6756 | val_loss=0.5069 acc=0.8095 | prec=0.8091 rec=0.8091 spec=0.8100 f1=0.8091 | time=12.8s\n",
            "Epoch 039 | train_loss=0.5638 acc=0.6999 | val_loss=0.5053 acc=0.8095 | prec=0.7906 rec=0.8409 spec=0.7783 f1=0.8150 | time=12.9s\n",
            "Epoch 040 | train_loss=0.5594 acc=0.7130 | val_loss=0.4951 acc=0.8118 | prec=0.8072 rec=0.8182 spec=0.8054 f1=0.8126 | time=13.0s\n",
            "Epoch 041 | train_loss=0.5426 acc=0.7113 | val_loss=0.4853 acc=0.8163 | prec=0.8117 rec=0.8227 spec=0.8100 f1=0.8172 | time=12.9s\n",
            "Epoch 042 | train_loss=0.5420 acc=0.7147 | val_loss=0.4783 acc=0.8141 | prec=0.8255 rec=0.7955 spec=0.8326 f1=0.8102 | time=12.9s\n",
            "Epoch 043 | train_loss=0.5456 acc=0.7249 | val_loss=0.4618 acc=0.8073 | prec=0.8426 rec=0.7545 spec=0.8597 f1=0.7962 | time=13.0s\n",
            "Epoch 044 | train_loss=0.5164 acc=0.7323 | val_loss=0.4668 acc=0.8050 | prec=0.8526 rec=0.7364 spec=0.8733 f1=0.7902 | time=12.9s\n",
            "Epoch 045 | train_loss=0.5119 acc=0.7436 | val_loss=0.4634 acc=0.8254 | prec=0.8357 rec=0.8091 spec=0.8416 f1=0.8222 | time=12.9s\n",
            "Epoch 046 | train_loss=0.5079 acc=0.7470 | val_loss=0.4500 acc=0.8254 | prec=0.8295 rec=0.8182 spec=0.8326 f1=0.8238 | time=12.9s\n",
            "Epoch 047 | train_loss=0.5014 acc=0.7652 | val_loss=0.4543 acc=0.8050 | prec=0.7792 rec=0.8500 spec=0.7602 f1=0.8130 | time=12.9s\n",
            "Epoch 048 | train_loss=0.4891 acc=0.7629 | val_loss=0.4488 acc=0.8254 | prec=0.8326 rec=0.8136 spec=0.8371 f1=0.8230 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4806 acc=0.7618 | val_loss=0.4540 acc=0.8163 | prec=0.7957 rec=0.8500 spec=0.7828 f1=0.8220 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4899 acc=0.7720 | val_loss=0.4483 acc=0.8186 | prec=0.8465 rec=0.7773 spec=0.8597 f1=0.8104 | time=12.9s\n",
            "Epoch 051 | train_loss=0.4911 acc=0.7618 | val_loss=0.4473 acc=0.8322 | prec=0.8476 rec=0.8091 spec=0.8552 f1=0.8279 | time=12.9s\n",
            "Epoch 052 | train_loss=0.4906 acc=0.7629 | val_loss=0.4493 acc=0.8209 | prec=0.8079 rec=0.8409 spec=0.8009 f1=0.8241 | time=13.0s\n",
            "Epoch 053 | train_loss=0.4853 acc=0.7708 | val_loss=0.4337 acc=0.8209 | prec=0.8473 rec=0.7818 spec=0.8597 f1=0.8132 | time=13.0s\n",
            "Epoch 054 | train_loss=0.4757 acc=0.7669 | val_loss=0.4461 acc=0.8231 | prec=0.8087 rec=0.8455 spec=0.8009 f1=0.8267 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4910 acc=0.7646 | val_loss=0.4390 acc=0.8254 | prec=0.8150 rec=0.8409 spec=0.8100 f1=0.8277 | time=12.9s\n",
            "Epoch 056 | train_loss=0.4978 acc=0.7499 | val_loss=0.4286 acc=0.8277 | prec=0.8495 rec=0.7955 spec=0.8597 f1=0.8216 | time=12.9s\n",
            "Epoch 057 | train_loss=0.4772 acc=0.7720 | val_loss=0.4316 acc=0.8277 | prec=0.8495 rec=0.7955 spec=0.8597 f1=0.8216 | time=13.0s\n",
            "Epoch 058 | train_loss=0.4745 acc=0.7737 | val_loss=0.4393 acc=0.8141 | prec=0.7949 rec=0.8455 spec=0.7828 f1=0.8194 | time=12.8s\n",
            "Epoch 059 | train_loss=0.4648 acc=0.7771 | val_loss=0.4312 acc=0.8254 | prec=0.8557 rec=0.7818 spec=0.8688 f1=0.8171 | time=13.0s\n",
            "Epoch 060 | train_loss=0.4620 acc=0.7788 | val_loss=0.4251 acc=0.8277 | prec=0.8333 rec=0.8182 spec=0.8371 f1=0.8257 | time=12.9s\n",
            "Epoch 061 | train_loss=0.4842 acc=0.7708 | val_loss=0.4333 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.9s\n",
            "Epoch 062 | train_loss=0.4752 acc=0.7748 | val_loss=0.4452 acc=0.8322 | prec=0.8724 rec=0.7773 spec=0.8869 f1=0.8221 | time=12.8s\n",
            "Epoch 063 | train_loss=0.4645 acc=0.7782 | val_loss=0.4272 acc=0.8277 | prec=0.8273 rec=0.8273 spec=0.8281 f1=0.8273 | time=12.9s\n",
            "Epoch 064 | train_loss=0.4613 acc=0.7794 | val_loss=0.4391 acc=0.8118 | prec=0.7890 rec=0.8500 spec=0.7738 f1=0.8184 | time=12.8s\n",
            "Epoch 065 | train_loss=0.4341 acc=0.8100 | val_loss=0.4352 acc=0.8322 | prec=0.8578 rec=0.7955 spec=0.8688 f1=0.8255 | time=12.9s\n",
            "Epoch 066 | train_loss=0.4449 acc=0.7896 | val_loss=0.4306 acc=0.8277 | prec=0.8273 rec=0.8273 spec=0.8281 f1=0.8273 | time=12.9s\n",
            "Epoch 067 | train_loss=0.4431 acc=0.7935 | val_loss=0.4234 acc=0.8345 | prec=0.8621 rec=0.7955 spec=0.8733 f1=0.8274 | time=12.9s\n",
            "Epoch 068 | train_loss=0.4692 acc=0.7862 | val_loss=0.4324 acc=0.8277 | prec=0.8214 rec=0.8364 spec=0.8190 f1=0.8288 | time=12.9s\n",
            "Epoch 069 | train_loss=0.4510 acc=0.7918 | val_loss=0.4320 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.8s\n",
            "Epoch 070 | train_loss=0.4400 acc=0.7981 | val_loss=0.4297 acc=0.8299 | prec=0.8436 rec=0.8091 spec=0.8507 f1=0.8260 | time=12.9s\n",
            "Epoch 071 | train_loss=0.4243 acc=0.8049 | val_loss=0.4270 acc=0.8435 | prec=0.8832 rec=0.7909 spec=0.8959 f1=0.8345 | time=12.9s\n",
            "Epoch 072 | train_loss=0.4280 acc=0.8037 | val_loss=0.4295 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=12.9s\n",
            "Epoch 073 | train_loss=0.4236 acc=0.8009 | val_loss=0.4176 acc=0.8277 | prec=0.8429 rec=0.8045 spec=0.8507 f1=0.8233 | time=12.9s\n",
            "Epoch 074 | train_loss=0.4294 acc=0.8054 | val_loss=0.4212 acc=0.8322 | prec=0.8476 rec=0.8091 spec=0.8552 f1=0.8279 | time=12.9s\n",
            "Epoch 075 | train_loss=0.4428 acc=0.7952 | val_loss=0.4246 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=12.9s\n",
            "Epoch 076 | train_loss=0.4309 acc=0.8054 | val_loss=0.4154 acc=0.8345 | prec=0.8451 rec=0.8182 spec=0.8507 f1=0.8314 | time=12.8s\n",
            "Epoch 077 | train_loss=0.4220 acc=0.8157 | val_loss=0.4199 acc=0.8299 | prec=0.8166 rec=0.8500 spec=0.8100 f1=0.8330 | time=12.9s\n",
            "Epoch 078 | train_loss=0.4235 acc=0.8145 | val_loss=0.4229 acc=0.8277 | prec=0.8495 rec=0.7955 spec=0.8597 f1=0.8216 | time=12.9s\n",
            "Epoch 079 | train_loss=0.4298 acc=0.7947 | val_loss=0.4181 acc=0.8277 | prec=0.8186 rec=0.8409 spec=0.8145 f1=0.8296 | time=12.8s\n",
            "Epoch 080 | train_loss=0.4098 acc=0.8276 | val_loss=0.4244 acc=0.8367 | prec=0.8491 rec=0.8182 spec=0.8552 f1=0.8333 | time=13.0s\n",
            "Epoch 081 | train_loss=0.4318 acc=0.8151 | val_loss=0.4198 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=12.9s\n",
            "Epoch 082 | train_loss=0.4119 acc=0.8259 | val_loss=0.4269 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.8s\n",
            "Epoch 083 | train_loss=0.4031 acc=0.8202 | val_loss=0.4145 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=12.8s\n",
            "Epoch 084 | train_loss=0.4108 acc=0.8196 | val_loss=0.4189 acc=0.8322 | prec=0.8802 rec=0.7682 spec=0.8959 f1=0.8204 | time=12.9s\n",
            "Epoch 085 | train_loss=0.4199 acc=0.8162 | val_loss=0.4111 acc=0.8277 | prec=0.8600 rec=0.7818 spec=0.8733 f1=0.8190 | time=13.0s\n",
            "Epoch 086 | train_loss=0.3950 acc=0.8264 | val_loss=0.4242 acc=0.8299 | prec=0.8341 rec=0.8227 spec=0.8371 f1=0.8284 | time=12.9s\n",
            "Epoch 087 | train_loss=0.4065 acc=0.8179 | val_loss=0.4171 acc=0.8322 | prec=0.8802 rec=0.7682 spec=0.8959 f1=0.8204 | time=12.9s\n",
            "Epoch 088 | train_loss=0.4161 acc=0.8145 | val_loss=0.4043 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=12.8s\n",
            "Epoch 089 | train_loss=0.3950 acc=0.8151 | val_loss=0.4208 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=12.9s\n",
            "Epoch 090 | train_loss=0.3908 acc=0.8213 | val_loss=0.4086 acc=0.8299 | prec=0.8537 rec=0.7955 spec=0.8643 f1=0.8235 | time=12.9s\n",
            "Epoch 091 | train_loss=0.3954 acc=0.8236 | val_loss=0.4141 acc=0.8254 | prec=0.8421 rec=0.8000 spec=0.8507 f1=0.8205 | time=12.9s\n",
            "Epoch 092 | train_loss=0.3918 acc=0.8140 | val_loss=0.4110 acc=0.8254 | prec=0.8488 rec=0.7909 spec=0.8597 f1=0.8188 | time=12.9s\n",
            "Epoch 093 | train_loss=0.3696 acc=0.8412 | val_loss=0.4258 acc=0.8277 | prec=0.8750 rec=0.7636 spec=0.8914 f1=0.8155 | time=13.0s\n",
            "Epoch 094 | train_loss=0.3868 acc=0.8361 | val_loss=0.4192 acc=0.8299 | prec=0.8372 rec=0.8182 spec=0.8416 f1=0.8276 | time=12.9s\n",
            "Epoch 095 | train_loss=0.3918 acc=0.8293 | val_loss=0.4142 acc=0.8322 | prec=0.8724 rec=0.7773 spec=0.8869 f1=0.8221 | time=12.9s\n",
            "Epoch 096 | train_loss=0.3907 acc=0.8168 | val_loss=0.4094 acc=0.8322 | prec=0.8288 rec=0.8364 spec=0.8281 f1=0.8326 | time=12.8s\n",
            "Epoch 097 | train_loss=0.3893 acc=0.8230 | val_loss=0.4028 acc=0.8322 | prec=0.8510 rec=0.8045 spec=0.8597 f1=0.8271 | time=12.9s\n",
            "Epoch 098 | train_loss=0.3861 acc=0.8327 | val_loss=0.4070 acc=0.8277 | prec=0.8711 rec=0.7682 spec=0.8869 f1=0.8164 | time=12.9s\n",
            "Epoch 099 | train_loss=0.3853 acc=0.8276 | val_loss=0.4097 acc=0.8367 | prec=0.8814 rec=0.7773 spec=0.8959 f1=0.8261 | time=12.8s\n",
            "Epoch 100 | train_loss=0.3771 acc=0.8372 | val_loss=0.4075 acc=0.8231 | prec=0.8413 rec=0.7955 spec=0.8507 f1=0.8178 | time=12.9s\n",
            "Epoch 101 | train_loss=0.3911 acc=0.8332 | val_loss=0.4034 acc=0.8231 | prec=0.8586 rec=0.7727 spec=0.8733 f1=0.8134 | time=12.9s\n",
            "Epoch 102 | train_loss=0.3775 acc=0.8372 | val_loss=0.4130 acc=0.8345 | prec=0.8731 rec=0.7818 spec=0.8869 f1=0.8249 | time=12.8s\n",
            "Epoch 103 | train_loss=0.3753 acc=0.8344 | val_loss=0.4075 acc=0.8345 | prec=0.8693 rec=0.7864 spec=0.8824 f1=0.8258 | time=13.0s\n",
            "Epoch 104 | train_loss=0.3816 acc=0.8395 | val_loss=0.4082 acc=0.8345 | prec=0.8808 rec=0.7727 spec=0.8959 f1=0.8232 | time=12.9s\n",
            "Epoch 105 | train_loss=0.3803 acc=0.8315 | val_loss=0.4070 acc=0.8277 | prec=0.8564 rec=0.7864 spec=0.8688 f1=0.8199 | time=12.9s\n",
            "Epoch 106 | train_loss=0.3812 acc=0.8366 | val_loss=0.3991 acc=0.8277 | prec=0.8529 rec=0.7909 spec=0.8643 f1=0.8208 | time=12.9s\n",
            "Epoch 107 | train_loss=0.3712 acc=0.8429 | val_loss=0.4089 acc=0.8277 | prec=0.8529 rec=0.7909 spec=0.8643 f1=0.8208 | time=12.9s\n",
            "Epoch 108 | train_loss=0.3825 acc=0.8361 | val_loss=0.4021 acc=0.8299 | prec=0.8372 rec=0.8182 spec=0.8416 f1=0.8276 | time=12.9s\n",
            "Epoch 109 | train_loss=0.3741 acc=0.8400 | val_loss=0.4058 acc=0.8299 | prec=0.8877 rec=0.7545 spec=0.9050 f1=0.8157 | time=12.9s\n",
            "Epoch 110 | train_loss=0.3641 acc=0.8429 | val_loss=0.4032 acc=0.8299 | prec=0.8756 rec=0.7682 spec=0.8914 f1=0.8184 | time=13.0s\n",
            "Epoch 111 | train_loss=0.3713 acc=0.8491 | val_loss=0.4150 acc=0.8163 | prec=0.8883 rec=0.7227 spec=0.9095 f1=0.7970 | time=13.0s\n",
            "Epoch 112 | train_loss=0.3819 acc=0.8327 | val_loss=0.4043 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=12.9s\n",
            "Epoch 113 | train_loss=0.3774 acc=0.8310 | val_loss=0.4036 acc=0.8277 | prec=0.8529 rec=0.7909 spec=0.8643 f1=0.8208 | time=12.9s\n",
            "Epoch 114 | train_loss=0.3609 acc=0.8429 | val_loss=0.4078 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=12.9s\n",
            "Epoch 115 | train_loss=0.3708 acc=0.8440 | val_loss=0.4067 acc=0.8299 | prec=0.8436 rec=0.8091 spec=0.8507 f1=0.8260 | time=12.8s\n",
            "Epoch 116 | train_loss=0.3706 acc=0.8434 | val_loss=0.4033 acc=0.8277 | prec=0.8673 rec=0.7727 spec=0.8824 f1=0.8173 | time=12.8s\n",
            "Epoch 117 | train_loss=0.3568 acc=0.8434 | val_loss=0.4150 acc=0.8299 | prec=0.8469 rec=0.8045 spec=0.8552 f1=0.8252 | time=12.9s\n",
            "Epoch 118 | train_loss=0.3896 acc=0.8395 | val_loss=0.4048 acc=0.8277 | prec=0.8364 rec=0.8136 spec=0.8416 f1=0.8249 | time=12.8s\n",
            "Epoch 119 | train_loss=0.3500 acc=0.8571 | val_loss=0.4114 acc=0.8231 | prec=0.8586 rec=0.7727 spec=0.8733 f1=0.8134 | time=13.0s\n",
            "Epoch 120 | train_loss=0.3674 acc=0.8469 | val_loss=0.4005 acc=0.8299 | prec=0.8281 rec=0.8318 spec=0.8281 f1=0.8299 | time=12.9s\n",
            "Epoch 121 | train_loss=0.3604 acc=0.8520 | val_loss=0.4047 acc=0.8299 | prec=0.8222 rec=0.8409 spec=0.8190 f1=0.8315 | time=12.8s\n",
            "Early stopping at epoch 121\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▂▂▂▂▂▂▂▂▂▂▁▆▇███▇██▇▇████████▇███▇██▇▇▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁█▇▇▇▆▇▇▇▇▆█▇▇▇█▇▇▇▇▇██▇█▇█▇▇█▇</td></tr><tr><td>recall</td><td>██████████▇▆▁▃▅▄▅▅▄▄▅▅▅▅▅▅▄▄▅▄▄▄▄▄▃▅▅▅▅▅</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁▁▄▆█▇▇█▇▇█▇▇█████████████████▇</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▁▂▁▁▄▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▇▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▆▆▇█▇█▇██▇██████████████████▇████</td></tr><tr><td>validation_loss</td><td>██████████▇▆▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.83146</td></tr><tr><td>precision</td><td>0.82222</td></tr><tr><td>recall</td><td>0.84091</td></tr><tr><td>specificity</td><td>0.819</td></tr><tr><td>train_accuracy</td><td>0.85196</td></tr><tr><td>train_loss</td><td>0.36039</td></tr><tr><td>validation_accuracy</td><td>0.82993</td></tr><tr><td>validation_loss</td><td>0.40474</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/c8kkeqzp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/c8kkeqzp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_100346-c8kkeqzp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 10:29:47,508] Trial 7 finished with values: [0.40474107222897665, 0.8299319727891157] and parameters: {'lr': 1.0178107575465798e-05, 'wd': 4.001596696808962e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 8: filters=120, lr=3.39e-05, wd=3.04e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_102947-82gl0q85</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/82gl0q85' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/82gl0q85' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/82gl0q85</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7436 acc=0.4861 | val_loss=0.6968 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7415 acc=0.4918 | val_loss=0.6973 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7430 acc=0.4719 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7202 acc=0.4952 | val_loss=0.6963 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7301 acc=0.4986 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7161 acc=0.5150 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7213 acc=0.4935 | val_loss=0.6970 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7170 acc=0.5162 | val_loss=0.6953 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7116 acc=0.4980 | val_loss=0.6959 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7125 acc=0.5048 | val_loss=0.6960 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7196 acc=0.4918 | val_loss=0.6974 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7105 acc=0.5196 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7067 acc=0.5213 | val_loss=0.6947 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7096 acc=0.4969 | val_loss=0.6838 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 015 | train_loss=0.6827 acc=0.5655 | val_loss=0.7048 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 016 | train_loss=0.6774 acc=0.5735 | val_loss=0.5953 acc=0.7438 | prec=0.8129 rec=0.6318 spec=0.8552 f1=0.7110 | time=13.0s\n",
            "Epoch 017 | train_loss=0.6387 acc=0.6427 | val_loss=0.5650 acc=0.7732 | prec=0.8226 rec=0.6955 spec=0.8507 f1=0.7537 | time=12.9s\n",
            "Epoch 018 | train_loss=0.6015 acc=0.6750 | val_loss=0.5653 acc=0.7415 | prec=0.6743 rec=0.9318 spec=0.5520 f1=0.7824 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5621 acc=0.7266 | val_loss=0.5275 acc=0.7800 | prec=0.8683 rec=0.6591 spec=0.9005 f1=0.7494 | time=12.8s\n",
            "Epoch 020 | train_loss=0.5510 acc=0.7351 | val_loss=0.5267 acc=0.7800 | prec=0.8917 rec=0.6364 spec=0.9231 f1=0.7427 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5540 acc=0.7255 | val_loss=0.4917 acc=0.7959 | prec=0.7851 rec=0.8136 spec=0.7783 f1=0.7991 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5130 acc=0.7578 | val_loss=0.4829 acc=0.8118 | prec=0.7965 rec=0.8364 spec=0.7873 f1=0.8160 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5031 acc=0.7697 | val_loss=0.4978 acc=0.7823 | prec=0.7296 rec=0.8955 spec=0.6697 f1=0.8041 | time=13.0s\n",
            "Epoch 024 | train_loss=0.4962 acc=0.7765 | val_loss=0.4862 acc=0.7846 | prec=0.7323 rec=0.8955 spec=0.6742 f1=0.8057 | time=12.9s\n",
            "Epoch 025 | train_loss=0.4918 acc=0.7856 | val_loss=0.4713 acc=0.8095 | prec=0.7636 rec=0.8955 spec=0.7240 f1=0.8243 | time=12.9s\n",
            "Epoch 026 | train_loss=0.4875 acc=0.7890 | val_loss=0.4607 acc=0.8118 | prec=0.7890 rec=0.8500 spec=0.7738 f1=0.8184 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4632 acc=0.7935 | val_loss=0.4770 acc=0.7800 | prec=0.7269 rec=0.8955 spec=0.6652 f1=0.8024 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4539 acc=0.8100 | val_loss=0.4581 acc=0.8050 | prec=0.8722 rec=0.7136 spec=0.8959 f1=0.7850 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4433 acc=0.8111 | val_loss=0.4430 acc=0.8299 | prec=0.8311 rec=0.8273 spec=0.8326 f1=0.8292 | time=12.9s\n",
            "Epoch 030 | train_loss=0.4336 acc=0.8315 | val_loss=0.4397 acc=0.8209 | prec=0.8895 rec=0.7318 spec=0.9095 f1=0.8030 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4147 acc=0.8219 | val_loss=0.4233 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=13.1s\n",
            "Epoch 032 | train_loss=0.4112 acc=0.8423 | val_loss=0.4473 acc=0.8050 | prec=0.7680 rec=0.8727 spec=0.7376 f1=0.8170 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4082 acc=0.8236 | val_loss=0.4336 acc=0.8209 | prec=0.8190 rec=0.8227 spec=0.8190 f1=0.8209 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3922 acc=0.8446 | val_loss=0.4556 acc=0.8005 | prec=0.7558 rec=0.8864 spec=0.7149 f1=0.8159 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3956 acc=0.8412 | val_loss=0.4217 acc=0.8254 | prec=0.8421 rec=0.8000 spec=0.8507 f1=0.8205 | time=12.9s\n",
            "Epoch 036 | train_loss=0.3881 acc=0.8446 | val_loss=0.4588 acc=0.7755 | prec=0.7200 rec=0.9000 spec=0.6516 f1=0.8000 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3605 acc=0.8752 | val_loss=0.4208 acc=0.8231 | prec=0.7934 rec=0.8727 spec=0.7738 f1=0.8312 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3511 acc=0.8792 | val_loss=0.4194 acc=0.8277 | prec=0.8600 rec=0.7818 spec=0.8733 f1=0.8190 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3574 acc=0.8622 | val_loss=0.4088 acc=0.8299 | prec=0.8194 rec=0.8455 spec=0.8145 f1=0.8322 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3475 acc=0.8729 | val_loss=0.4144 acc=0.8163 | prec=0.7814 rec=0.8773 spec=0.7557 f1=0.8266 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3289 acc=0.8803 | val_loss=0.4150 acc=0.8118 | prec=0.7686 rec=0.8909 spec=0.7330 f1=0.8253 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3342 acc=0.8712 | val_loss=0.3944 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3311 acc=0.8758 | val_loss=0.4036 acc=0.8435 | prec=0.8794 rec=0.7955 spec=0.8914 f1=0.8353 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3147 acc=0.8866 | val_loss=0.4071 acc=0.8254 | prec=0.7942 rec=0.8773 spec=0.7738 f1=0.8337 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3147 acc=0.8803 | val_loss=0.3970 acc=0.8322 | prec=0.8120 rec=0.8636 spec=0.8009 f1=0.8370 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2975 acc=0.8877 | val_loss=0.3867 acc=0.8481 | prec=0.8525 rec=0.8409 spec=0.8552 f1=0.8467 | time=12.9s\n",
            "Epoch 047 | train_loss=0.3043 acc=0.8849 | val_loss=0.3980 acc=0.8345 | prec=0.8693 rec=0.7864 spec=0.8824 f1=0.8258 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3075 acc=0.8758 | val_loss=0.3882 acc=0.8367 | prec=0.8058 rec=0.8864 spec=0.7873 f1=0.8442 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2791 acc=0.9087 | val_loss=0.3951 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2947 acc=0.8934 | val_loss=0.4180 acc=0.8186 | prec=0.7652 rec=0.9182 spec=0.7195 f1=0.8347 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2722 acc=0.8951 | val_loss=0.3861 acc=0.8435 | prec=0.8297 rec=0.8636 spec=0.8235 f1=0.8463 | time=12.9s\n",
            "Epoch 052 | train_loss=0.2618 acc=0.9121 | val_loss=0.4130 acc=0.8345 | prec=0.7928 rec=0.9045 spec=0.7647 f1=0.8450 | time=12.8s\n",
            "Epoch 053 | train_loss=0.2582 acc=0.9075 | val_loss=0.3891 acc=0.8435 | prec=0.8159 rec=0.8864 spec=0.8009 f1=0.8497 | time=12.8s\n",
            "Epoch 054 | train_loss=0.2811 acc=0.8956 | val_loss=0.4046 acc=0.8367 | prec=0.8627 rec=0.8000 spec=0.8733 f1=0.8302 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2967 acc=0.8968 | val_loss=0.3843 acc=0.8413 | prec=0.8289 rec=0.8591 spec=0.8235 f1=0.8438 | time=12.8s\n",
            "Epoch 056 | train_loss=0.2578 acc=0.8962 | val_loss=0.4111 acc=0.8367 | prec=0.8083 rec=0.8818 spec=0.7919 f1=0.8435 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2601 acc=0.9144 | val_loss=0.4303 acc=0.8277 | prec=0.8789 rec=0.7591 spec=0.8959 f1=0.8146 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2839 acc=0.8917 | val_loss=0.3928 acc=0.8435 | prec=0.8008 rec=0.9136 spec=0.7738 f1=0.8535 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2796 acc=0.9115 | val_loss=0.4086 acc=0.8322 | prec=0.7829 rec=0.9182 spec=0.7466 f1=0.8452 | time=12.9s\n",
            "Epoch 060 | train_loss=0.2274 acc=0.9126 | val_loss=0.3871 acc=0.8458 | prec=0.8248 rec=0.8773 spec=0.8145 f1=0.8502 | time=12.8s\n",
            "Epoch 061 | train_loss=0.2456 acc=0.9229 | val_loss=0.3888 acc=0.8481 | prec=0.8228 rec=0.8864 spec=0.8100 f1=0.8534 | time=12.8s\n",
            "Epoch 062 | train_loss=0.2429 acc=0.9104 | val_loss=0.4027 acc=0.8345 | prec=0.8075 rec=0.8773 spec=0.7919 f1=0.8410 | time=12.9s\n",
            "Epoch 063 | train_loss=0.2648 acc=0.8985 | val_loss=0.3987 acc=0.8277 | prec=0.7975 rec=0.8773 spec=0.7783 f1=0.8355 | time=13.0s\n",
            "Epoch 064 | train_loss=0.2418 acc=0.9024 | val_loss=0.3937 acc=0.8435 | prec=0.8240 rec=0.8727 spec=0.8145 f1=0.8477 | time=13.0s\n",
            "Epoch 065 | train_loss=0.2508 acc=0.9098 | val_loss=0.3975 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2545 acc=0.9109 | val_loss=0.4199 acc=0.8186 | prec=0.7734 rec=0.9000 spec=0.7376 f1=0.8319 | time=12.9s\n",
            "Epoch 067 | train_loss=0.2218 acc=0.9189 | val_loss=0.3888 acc=0.8413 | prec=0.8261 rec=0.8636 spec=0.8190 f1=0.8444 | time=12.8s\n",
            "Epoch 068 | train_loss=0.2322 acc=0.9240 | val_loss=0.4098 acc=0.8277 | prec=0.7951 rec=0.8818 spec=0.7738 f1=0.8362 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2342 acc=0.9058 | val_loss=0.3795 acc=0.8526 | prec=0.8384 rec=0.8727 spec=0.8326 f1=0.8552 | time=12.9s\n",
            "Epoch 070 | train_loss=0.2182 acc=0.9223 | val_loss=0.4119 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2261 acc=0.9138 | val_loss=0.3812 acc=0.8458 | prec=0.8304 rec=0.8682 spec=0.8235 f1=0.8489 | time=12.9s\n",
            "Epoch 072 | train_loss=0.2088 acc=0.9217 | val_loss=0.3906 acc=0.8413 | prec=0.8049 rec=0.9000 spec=0.7828 f1=0.8498 | time=12.9s\n",
            "Epoch 073 | train_loss=0.2191 acc=0.9212 | val_loss=0.3929 acc=0.8435 | prec=0.8356 rec=0.8545 spec=0.8326 f1=0.8449 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2318 acc=0.8956 | val_loss=0.4264 acc=0.8186 | prec=0.7692 rec=0.9091 spec=0.7285 f1=0.8333 | time=12.9s\n",
            "Epoch 075 | train_loss=0.2090 acc=0.9144 | val_loss=0.3966 acc=0.8503 | prec=0.8468 rec=0.8545 spec=0.8462 f1=0.8507 | time=12.8s\n",
            "Epoch 076 | train_loss=0.2005 acc=0.9189 | val_loss=0.4076 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=13.0s\n",
            "Epoch 077 | train_loss=0.2253 acc=0.9257 | val_loss=0.4125 acc=0.8345 | prec=0.7952 rec=0.9000 spec=0.7692 f1=0.8443 | time=12.9s\n",
            "Epoch 078 | train_loss=0.2107 acc=0.9189 | val_loss=0.4025 acc=0.8549 | prec=0.8514 rec=0.8591 spec=0.8507 f1=0.8552 | time=13.0s\n",
            "Epoch 079 | train_loss=0.1948 acc=0.9285 | val_loss=0.4064 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=12.8s\n",
            "Epoch 080 | train_loss=0.2158 acc=0.9342 | val_loss=0.3981 acc=0.8549 | prec=0.8333 rec=0.8864 spec=0.8235 f1=0.8590 | time=12.9s\n",
            "Epoch 081 | train_loss=0.2041 acc=0.9189 | val_loss=0.4083 acc=0.8413 | prec=0.8378 rec=0.8455 spec=0.8371 f1=0.8416 | time=12.8s\n",
            "Epoch 082 | train_loss=0.1999 acc=0.9285 | val_loss=0.4104 acc=0.8390 | prec=0.8091 rec=0.8864 spec=0.7919 f1=0.8460 | time=12.8s\n",
            "Epoch 083 | train_loss=0.2038 acc=0.9212 | val_loss=0.4111 acc=0.8481 | prec=0.8312 rec=0.8727 spec=0.8235 f1=0.8514 | time=12.9s\n",
            "Epoch 084 | train_loss=0.1903 acc=0.9302 | val_loss=0.4149 acc=0.8435 | prec=0.8159 rec=0.8864 spec=0.8009 f1=0.8497 | time=13.0s\n",
            "Early stopping at epoch 84\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▄▄▇▆▆▅▇▇▇▇▇▇▇████▇██▇█▇█▇▇███▇██</td></tr><tr><td>precision</td><td>▁▁▁▁▁▇▄█▆▅▆▅██▆▆▇▅▆▆█▇▆▆▆█▆▆▇▇▇█▇▇▆▆▇▇▇▇</td></tr><tr><td>recall</td><td>████████▁▁▅▆▆▆▅▅▄▆▄▅▄▅▅▄▆▆▄▆▆▆▆▄▅▆▅▅▅▅▅▆</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁██▆▇▇▇█▇█▇▇▇█▇▇▇▇▇▇▇▇▇▇▇▇█▇▇█▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇█████▇█████▇████</td></tr><tr><td>train_loss</td><td>█████████▇▆▆▅▅▄▄▄▄▄▃▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▆▇▇▇▇▇█▇▇▆▇▇████▇███▇███▇██▇████</td></tr><tr><td>validation_loss</td><td>████████▅▅▃▄▃▃▃▂▂▂▃▂▂▂▁▁▂▂▂▂▂▁▁▁▁▁▁▂▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.84967</td></tr><tr><td>precision</td><td>0.8159</td></tr><tr><td>recall</td><td>0.88636</td></tr><tr><td>specificity</td><td>0.8009</td></tr><tr><td>train_accuracy</td><td>0.93023</td></tr><tr><td>train_loss</td><td>0.19025</td></tr><tr><td>validation_accuracy</td><td>0.84354</td></tr><tr><td>validation_loss</td><td>0.41488</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/82gl0q85' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/82gl0q85</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_102947-82gl0q85/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 10:47:51,987] Trial 8 finished with values: [0.4148822958980288, 0.8435374149659864] and parameters: {'lr': 3.392518113282836e-05, 'wd': 3.044530844454165e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trial 9: filters=120, lr=2.38e-05, wd=8.19e-05, pct_start=0.20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_104751-9l2p9vxj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/9l2p9vxj' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/9l2p9vxj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/9l2p9vxj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7661 acc=0.4918 | val_loss=0.6968 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7619 acc=0.4861 | val_loss=0.6994 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7373 acc=0.5145 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7394 acc=0.4957 | val_loss=0.6966 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7349 acc=0.5014 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7447 acc=0.4844 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7252 acc=0.4957 | val_loss=0.6931 acc=0.5125 | prec=0.8571 rec=0.0273 spec=0.9955 f1=0.0529 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7205 acc=0.5020 | val_loss=0.6931 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7219 acc=0.4991 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7154 acc=0.5105 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7190 acc=0.5139 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7037 acc=0.5105 | val_loss=0.6922 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7151 acc=0.5162 | val_loss=0.6918 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7165 acc=0.5077 | val_loss=0.6905 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7119 acc=0.5014 | val_loss=0.6874 acc=0.5397 | prec=0.9474 rec=0.0818 spec=0.9955 f1=0.1506 | time=12.7s\n",
            "Epoch 016 | train_loss=0.7089 acc=0.5196 | val_loss=0.6812 acc=0.5442 | prec=0.9524 rec=0.0909 spec=0.9955 f1=0.1660 | time=12.8s\n",
            "Epoch 017 | train_loss=0.6951 acc=0.5428 | val_loss=0.6694 acc=0.5646 | prec=0.8684 rec=0.1500 spec=0.9774 f1=0.2558 | time=13.0s\n",
            "Epoch 018 | train_loss=0.6883 acc=0.5445 | val_loss=0.6379 acc=0.6780 | prec=0.6211 rec=0.9091 spec=0.4480 f1=0.7380 | time=12.9s\n",
            "Epoch 019 | train_loss=0.6340 acc=0.6324 | val_loss=0.5898 acc=0.7347 | prec=0.6734 rec=0.9091 spec=0.5611 f1=0.7737 | time=12.9s\n",
            "Epoch 020 | train_loss=0.6013 acc=0.6869 | val_loss=0.5439 acc=0.7710 | prec=0.8742 rec=0.6318 spec=0.9095 f1=0.7335 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5693 acc=0.7260 | val_loss=0.5289 acc=0.8095 | prec=0.8119 rec=0.8045 spec=0.8145 f1=0.8082 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5546 acc=0.7334 | val_loss=0.5362 acc=0.8073 | prec=0.8689 rec=0.7227 spec=0.8914 f1=0.7891 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5290 acc=0.7476 | val_loss=0.5132 acc=0.7823 | prec=0.7296 rec=0.8955 spec=0.6697 f1=0.8041 | time=12.8s\n",
            "Epoch 024 | train_loss=0.5185 acc=0.7635 | val_loss=0.4900 acc=0.8186 | prec=0.8398 rec=0.7864 spec=0.8507 f1=0.8122 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5024 acc=0.7748 | val_loss=0.4774 acc=0.8209 | prec=0.8249 rec=0.8136 spec=0.8281 f1=0.8192 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5125 acc=0.7612 | val_loss=0.4636 acc=0.8367 | prec=0.8458 rec=0.8227 spec=0.8507 f1=0.8341 | time=13.0s\n",
            "Epoch 027 | train_loss=0.4819 acc=0.7907 | val_loss=0.4598 acc=0.8345 | prec=0.8419 rec=0.8227 spec=0.8462 f1=0.8322 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4650 acc=0.8049 | val_loss=0.4593 acc=0.8118 | prec=0.7819 rec=0.8636 spec=0.7602 f1=0.8207 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4557 acc=0.8071 | val_loss=0.4609 acc=0.8231 | prec=0.8660 rec=0.7636 spec=0.8824 f1=0.8116 | time=12.9s\n",
            "Epoch 030 | train_loss=0.4549 acc=0.8043 | val_loss=0.4505 acc=0.8254 | prec=0.8150 rec=0.8409 spec=0.8100 f1=0.8277 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4387 acc=0.8123 | val_loss=0.4559 acc=0.8186 | prec=0.7823 rec=0.8818 spec=0.7557 f1=0.8291 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4322 acc=0.8270 | val_loss=0.4420 acc=0.8299 | prec=0.8139 rec=0.8545 spec=0.8054 f1=0.8337 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4275 acc=0.8236 | val_loss=0.4435 acc=0.8163 | prec=0.7957 rec=0.8500 spec=0.7828 f1=0.8220 | time=12.8s\n",
            "Epoch 034 | train_loss=0.4175 acc=0.8259 | val_loss=0.4273 acc=0.8345 | prec=0.8238 rec=0.8500 spec=0.8190 f1=0.8367 | time=12.9s\n",
            "Epoch 035 | train_loss=0.3960 acc=0.8412 | val_loss=0.4253 acc=0.8526 | prec=0.8638 rec=0.8364 spec=0.8688 f1=0.8499 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4054 acc=0.8434 | val_loss=0.4224 acc=0.8299 | prec=0.8251 rec=0.8364 spec=0.8235 f1=0.8307 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4042 acc=0.8344 | val_loss=0.4290 acc=0.8118 | prec=0.7708 rec=0.8864 spec=0.7376 f1=0.8245 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3897 acc=0.8469 | val_loss=0.4156 acc=0.8322 | prec=0.8259 rec=0.8409 spec=0.8235 f1=0.8333 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3769 acc=0.8434 | val_loss=0.4106 acc=0.8345 | prec=0.8238 rec=0.8500 spec=0.8190 f1=0.8367 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3615 acc=0.8644 | val_loss=0.4093 acc=0.8345 | prec=0.8238 rec=0.8500 spec=0.8190 f1=0.8367 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3785 acc=0.8542 | val_loss=0.4102 acc=0.8390 | prec=0.8091 rec=0.8864 spec=0.7919 f1=0.8460 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3611 acc=0.8599 | val_loss=0.3957 acc=0.8435 | prec=0.8512 rec=0.8318 spec=0.8552 f1=0.8414 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3453 acc=0.8661 | val_loss=0.3957 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3547 acc=0.8593 | val_loss=0.3934 acc=0.8231 | prec=0.7934 rec=0.8727 spec=0.7738 f1=0.8312 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3370 acc=0.8695 | val_loss=0.4079 acc=0.8254 | prec=0.7895 rec=0.8864 spec=0.7647 f1=0.8351 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3478 acc=0.8707 | val_loss=0.4982 acc=0.7279 | prec=0.6572 rec=0.9500 spec=0.5068 f1=0.7770 | time=12.9s\n",
            "Epoch 047 | train_loss=0.3359 acc=0.8622 | val_loss=0.3951 acc=0.8345 | prec=0.8050 rec=0.8818 spec=0.7873 f1=0.8416 | time=13.0s\n",
            "Epoch 048 | train_loss=0.3319 acc=0.8746 | val_loss=0.4572 acc=0.7732 | prec=0.7055 rec=0.9364 spec=0.6109 f1=0.8047 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3174 acc=0.8843 | val_loss=0.3993 acc=0.8209 | prec=0.7809 rec=0.8909 spec=0.7511 f1=0.8323 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3308 acc=0.8667 | val_loss=0.3677 acc=0.8526 | prec=0.8708 rec=0.8273 spec=0.8778 f1=0.8485 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3211 acc=0.8826 | val_loss=0.3926 acc=0.8299 | prec=0.7866 rec=0.9045 spec=0.7557 f1=0.8414 | time=12.9s\n",
            "Epoch 052 | train_loss=0.2943 acc=0.8939 | val_loss=0.3857 acc=0.8413 | prec=0.8233 rec=0.8682 spec=0.8145 f1=0.8451 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2911 acc=0.8939 | val_loss=0.3834 acc=0.8367 | prec=0.8109 rec=0.8773 spec=0.7964 f1=0.8428 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3126 acc=0.8837 | val_loss=0.3620 acc=0.8503 | prec=0.8598 rec=0.8364 spec=0.8643 f1=0.8479 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2834 acc=0.8968 | val_loss=0.4058 acc=0.8209 | prec=0.7640 rec=0.9273 spec=0.7149 f1=0.8378 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2816 acc=0.9007 | val_loss=0.3982 acc=0.8254 | prec=0.7761 rec=0.9136 spec=0.7376 f1=0.8392 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2677 acc=0.9075 | val_loss=0.4393 acc=0.7914 | prec=0.7238 rec=0.9409 spec=0.6425 f1=0.8182 | time=12.9s\n",
            "Epoch 058 | train_loss=0.2806 acc=0.9002 | val_loss=0.3984 acc=0.8254 | prec=0.7782 rec=0.9091 spec=0.7421 f1=0.8386 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2855 acc=0.9002 | val_loss=0.3830 acc=0.8345 | prec=0.7882 rec=0.9136 spec=0.7557 f1=0.8463 | time=12.9s\n",
            "Epoch 060 | train_loss=0.2889 acc=0.8973 | val_loss=0.4486 acc=0.7891 | prec=0.7197 rec=0.9455 spec=0.6335 f1=0.8173 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2895 acc=0.8860 | val_loss=0.3842 acc=0.8322 | prec=0.8017 rec=0.8818 spec=0.7828 f1=0.8398 | time=12.9s\n",
            "Epoch 062 | train_loss=0.2779 acc=0.9041 | val_loss=0.3690 acc=0.8435 | prec=0.8213 rec=0.8773 spec=0.8100 f1=0.8484 | time=13.0s\n",
            "Epoch 063 | train_loss=0.3025 acc=0.8888 | val_loss=0.3859 acc=0.8322 | prec=0.7920 rec=0.9000 spec=0.7647 f1=0.8426 | time=12.9s\n",
            "Epoch 064 | train_loss=0.2753 acc=0.8877 | val_loss=0.3878 acc=0.8277 | prec=0.7857 rec=0.9000 spec=0.7557 f1=0.8390 | time=12.8s\n",
            "Epoch 065 | train_loss=0.2638 acc=0.9098 | val_loss=0.3829 acc=0.8322 | prec=0.7897 rec=0.9045 spec=0.7602 f1=0.8432 | time=12.8s\n",
            "Epoch 066 | train_loss=0.2587 acc=0.9087 | val_loss=0.3705 acc=0.8458 | prec=0.8220 rec=0.8818 spec=0.8100 f1=0.8509 | time=12.9s\n",
            "Epoch 067 | train_loss=0.2634 acc=0.8962 | val_loss=0.3913 acc=0.8367 | prec=0.7960 rec=0.9045 spec=0.7692 f1=0.8468 | time=12.9s\n",
            "Epoch 068 | train_loss=0.2654 acc=0.8945 | val_loss=0.3679 acc=0.8367 | prec=0.8109 rec=0.8773 spec=0.7964 f1=0.8428 | time=12.9s\n",
            "Epoch 069 | train_loss=0.2574 acc=0.9070 | val_loss=0.4480 acc=0.7914 | prec=0.7238 rec=0.9409 spec=0.6425 f1=0.8182 | time=12.8s\n",
            "Early stopping at epoch 69\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▇▇▇██████████▇██████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▇▁▁█▁█▇▅▆▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▇▇▇▆▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▂▂█▇▆█▇▇▇▇▇▇▇█▇▇▇▇█▇██▇▇▇█████▇▇</td></tr><tr><td>specificity</td><td>██████████▅▆▂▅▅▅▄▄▄▅▅▃▅▅▅▅▄▄▁▆▆▃▃▃▄▄▄▄▅▂</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▂▂▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██████████████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▇▆▅▅▅▅▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▂▅▆▆▇▇▇██▇▇███▇█▇▆█▇████▇▇████</td></tr><tr><td>validation_loss</td><td>█████████▇▆▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▄▂▃▂▁▁▂▁▁▁▁▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81818</td></tr><tr><td>precision</td><td>0.72378</td></tr><tr><td>recall</td><td>0.94091</td></tr><tr><td>specificity</td><td>0.64253</td></tr><tr><td>train_accuracy</td><td>0.90698</td></tr><tr><td>train_loss</td><td>0.25739</td></tr><tr><td>validation_accuracy</td><td>0.79138</td></tr><tr><td>validation_loss</td><td>0.44804</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/9l2p9vxj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8/runs/9l2p9vxj</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-binary-AD-CN-single-fold-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_104751-9l2p9vxj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-06 11:02:43,320] Trial 9 finished with values: [0.44804493870053974, 0.7913832199546486] and parameters: {'lr': 2.3847632993026627e-05, 'wd': 8.193296299960752e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pareto-optimal trials (val_loss, val_acc) and their params:\n",
            " Trial #1: values=[0.3497606185930116, 0.8458049886621315]\n",
            "              params={'lr': 6.436530952928205e-05, 'wd': 1.8466323534899384e-05}\n",
            " Trial #5: values=[0.3735910728573799, 0.854875283446712]\n",
            "              params={'lr': 3.793344453740315e-05, 'wd': 4.5415221838197576e-05}\n",
            " Trial #6: values=[0.3590279668569565, 0.8503401360544217]\n",
            "              params={'lr': 3.5210516472168496e-05, 'wd': 4.774224777773625e-05}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "####\n",
        "\n",
        "1] Trial 0: filters=120, lr=8.53e-05, wd=2.20e-05, pct_start=0.20\n",
        "Epoch 061 | train_loss=0.1690 acc=0.9325 | val_loss=0.4888 acc=0.8186 | prec=0.9118 rec=0.7045 spec=0.9321 f1=0.7949 | time=13.6s\n",
        "Early stopping at epoch 61\n",
        "\n",
        "2] Trial 1: filters=120, lr=6.44e-05, wd=1.85e-05, pct_start=0.20\n",
        "Epoch 054 | train_loss=0.1890 acc=0.9246 | val_loss=0.3498 acc=0.8458 | prec=0.8167 rec=0.8909 spec=0.8009 f1=0.8522 | time=12.8s\n",
        "Early stopping at epoch 54\n",
        "\n",
        "3] Trial 2: filters=120, lr=1.23e-04, wd=7.13e-05, pct_start=0.20\n",
        "Epoch 041 | train_loss=0.1800 acc=0.9246 | val_loss=0.3866 acc=0.8413 | prec=0.8151 rec=0.8818 spec=0.8009 f1=0.8472 | time=12.9s\n",
        "Early stopping at epoch 41\n",
        "\n",
        "4] Trial 3: filters=120, lr=8.43e-05, wd=1.93e-06, pct_start=0.20\n",
        "Epoch 048 | train_loss=0.1869 acc=0.9331 | val_loss=0.4680 acc=0.8345 | prec=0.8769 rec=0.7773 spec=0.8914 f1=0.8241 | time=12.8s\n",
        "Early stopping at epoch 48\n",
        "\n",
        "5] Trial 4: filters=120, lr=9.64e-05, wd=5.00e-06, pct_start=0.20\n",
        "Epoch 045 | train_loss=0.2278 acc=0.9075 | val_loss=0.6337 acc=0.7619 | prec=0.6837 rec=0.9727 spec=0.5520 f1=0.8030 | time=13.0s\n",
        "Early stopping at epoch 45\n",
        "\n",
        "6] Trial 5: filters=120, lr=3.79e-05, wd=4.54e-05, pct_start=0.20\n",
        "Epoch 076 | train_loss=0.2106 acc=0.9229 | val_loss=0.3736 acc=0.8549 | prec=0.8714 rec=0.8318 spec=0.8778 f1=0.8512 | time=12.9s\n",
        "Early stopping at epoch 76\n",
        "\n",
        "7] Trial 6: filters=120, lr=3.52e-05, wd=4.77e-05, pct_start=0.20\n",
        "Epoch 078 | train_loss=0.2236 acc=0.9047 | val_loss=0.3590 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=12.9s\n",
        "Early stopping at epoch 78\n",
        "\n",
        "8] Trial 7: filters=120, lr=1.02e-05, wd=4.00e-05, pct_start=0.20\n",
        "Epoch 121 | train_loss=0.3604 acc=0.8520 | val_loss=0.4047 acc=0.8299 | prec=0.8222 rec=0.8409 spec=0.8190 f1=0.8315 | time=12.8s\n",
        "Early stopping at epoch 121\n",
        "\n",
        "9] Trial 8: filters=120, lr=3.39e-05, wd=3.04e-05, pct_start=0.20\n",
        "Epoch 084 | train_loss=0.1903 acc=0.9302 | val_loss=0.4149 acc=0.8435 | prec=0.8159 rec=0.8864 spec=0.8009 f1=0.8497 | time=13.0s\n",
        "Early stopping at epoch 84\n",
        "\n",
        "10] Trial 9: filters=120, lr=2.38e-05, wd=8.19e-05, pct_start=0.20\n",
        "Epoch 069 | train_loss=0.2574 acc=0.9070 | val_loss=0.4480 acc=0.7914 | prec=0.7238 rec=0.9409 spec=0.6425 f1=0.8182 | time=12.8s\n",
        "Early stopping at epoch 69"
      ],
      "metadata": {
        "id": "GwhQ8oOT8Rmy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5-Fold Cross Validation Result with fixed parameter"
      ],
      "metadata": {
        "id": "vGvApLTzro2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "LR          = 5e-5\n",
        "WD          = 1e-5\n",
        "PCT_START   = 0.20\n",
        "NUM_FILTERS = 120\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & balance metadata ─────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# map to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# build dataset & labels array\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── 5-Fold Cross-Validation ─────────────────────────────────────\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# store per-fold metrics\n",
        "fold_metrics = {'loss':[], 'acc':[], 'prec':[], 'rec':[], 'spec':[], 'f1':[]}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), start=1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-fixed-hp\",\n",
        "        name=f\"fold{fold}\",\n",
        "        config={\n",
        "            'lr': LR, 'wd': WD,\n",
        "            'pct_start': PCT_START,\n",
        "            'filters': NUM_FILTERS,\n",
        "            'max_epochs': MAX_EPOCHS,\n",
        "            'batch_size': BATCH_SIZE\n",
        "        },\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # build model, optimizer, scheduler, criterion\n",
        "    input_len = dataset[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=NUM_FILTERS,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=LR,\n",
        "        total_steps=MAX_EPOCHS * len(train_loader),\n",
        "        pct_start=PCT_START,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0\n",
        "\n",
        "    # ─── Epoch Loop ───────────────────────────────────────────────\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = WD * (cur_lr / LR)\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss += loss.item()\n",
        "\n",
        "                preds    = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_preds  = np.concatenate(all_preds)\n",
        "        val_labels = np.concatenate(all_labels)\n",
        "        val_acc    = (val_preds == val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "        tn, fp, fn, tp = confusion_matrix(val_labels, val_preds).ravel()\n",
        "        specificity   = tn / (tn + fp)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        # print to terminal\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={precision:.4f} rec={recall:.4f} spec={specificity:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        # log to W&B\n",
        "        wandb.log({\n",
        "            'train_loss':     train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss':       val_loss,\n",
        "            'val_accuracy':   val_acc,\n",
        "            'precision':      precision,\n",
        "            'recall':         recall,\n",
        "            'specificity':    specificity,\n",
        "            'f1_score':       f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # collect best fold metrics\n",
        "    fold_metrics['loss'].append(best_loss)\n",
        "    fold_metrics['acc'].append(val_acc)\n",
        "    fold_metrics['prec'].append(precision)\n",
        "    fold_metrics['rec'].append(recall)\n",
        "    fold_metrics['spec'].append(specificity)\n",
        "    fold_metrics['f1'].append(f1)\n",
        "\n",
        "    wandb.finish()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ─── Final 5-Fold Summary ────────────────────────────────────────\n",
        "print(\"\\n=== 5-Fold Cross-Validation Averages ===\")\n",
        "for key, vals in fold_metrics.items():\n",
        "    print(f\"{key:4s} : {np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5-OPMIUXrnNz",
        "outputId": "81ebff43-4f94-4bd3-b623-da78efc126df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "=== Fold 1 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize?ref=models\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjh8032\u001b[0m (\u001b[33mjh8032-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_192231-zmq428en</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/zmq428en' target=\"_blank\">fold1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/zmq428en' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/zmq428en</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7360 acc=0.4906 | val_loss=0.6964 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=474.6s\n",
            "Epoch 002 | train_loss=0.7268 acc=0.4963 | val_loss=0.6958 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7229 acc=0.4980 | val_loss=0.6958 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7245 acc=0.4906 | val_loss=0.6969 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7241 acc=0.4997 | val_loss=0.7001 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7255 acc=0.4884 | val_loss=0.6945 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7191 acc=0.5065 | val_loss=0.6929 acc=0.5034 | prec=0.5025 rec=0.8959 spec=0.1091 f1=0.6439 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7171 acc=0.5190 | val_loss=0.6921 acc=0.5329 | prec=0.5200 rec=0.8824 spec=0.1818 f1=0.6544 | time=12.7s\n",
            "Epoch 009 | train_loss=0.7143 acc=0.5082 | val_loss=0.6941 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.7s\n",
            "Epoch 010 | train_loss=0.7143 acc=0.5099 | val_loss=0.6957 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7102 acc=0.4986 | val_loss=0.6901 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7050 acc=0.5111 | val_loss=0.7000 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.9s\n",
            "Epoch 013 | train_loss=0.6972 acc=0.5496 | val_loss=0.6628 acc=0.5737 | prec=0.7797 rec=0.2081 spec=0.9409 f1=0.3286 | time=12.9s\n",
            "Epoch 014 | train_loss=0.6587 acc=0.6251 | val_loss=0.5982 acc=0.7143 | prec=0.8188 rec=0.5520 spec=0.8773 f1=0.6595 | time=12.8s\n",
            "Epoch 015 | train_loss=0.5906 acc=0.6818 | val_loss=0.5256 acc=0.7551 | prec=0.8156 rec=0.6606 spec=0.8500 f1=0.7300 | time=13.2s\n",
            "Epoch 016 | train_loss=0.5627 acc=0.7272 | val_loss=0.5718 acc=0.6848 | prec=0.6171 rec=0.9774 spec=0.3909 f1=0.7566 | time=13.5s\n",
            "Epoch 017 | train_loss=0.5485 acc=0.7311 | val_loss=0.4952 acc=0.7914 | prec=0.7549 rec=0.8643 spec=0.7182 f1=0.8059 | time=13.6s\n",
            "Epoch 018 | train_loss=0.5269 acc=0.7589 | val_loss=0.5008 acc=0.7959 | prec=0.7399 rec=0.9140 spec=0.6773 f1=0.8178 | time=13.5s\n",
            "Epoch 019 | train_loss=0.5089 acc=0.7589 | val_loss=0.4723 acc=0.8005 | prec=0.8342 rec=0.7511 spec=0.8500 f1=0.7905 | time=13.6s\n",
            "Epoch 020 | train_loss=0.5003 acc=0.7816 | val_loss=0.4541 acc=0.8118 | prec=0.8485 rec=0.7602 spec=0.8636 f1=0.8019 | time=13.5s\n",
            "Epoch 021 | train_loss=0.4912 acc=0.7765 | val_loss=0.4562 acc=0.7982 | prec=0.7391 rec=0.9231 spec=0.6727 f1=0.8209 | time=13.7s\n",
            "Epoch 022 | train_loss=0.4984 acc=0.7680 | val_loss=0.4581 acc=0.7982 | prec=0.7374 rec=0.9276 spec=0.6682 f1=0.8216 | time=13.5s\n",
            "Epoch 023 | train_loss=0.4668 acc=0.7986 | val_loss=0.4340 acc=0.8322 | prec=0.7952 rec=0.8959 spec=0.7682 f1=0.8426 | time=13.7s\n",
            "Epoch 024 | train_loss=0.4672 acc=0.7958 | val_loss=0.4059 acc=0.8435 | prec=0.8276 rec=0.8688 spec=0.8182 f1=0.8477 | time=13.6s\n",
            "Epoch 025 | train_loss=0.4525 acc=0.8003 | val_loss=0.4320 acc=0.8095 | prec=0.7473 rec=0.9367 spec=0.6818 f1=0.8313 | time=13.6s\n",
            "Epoch 026 | train_loss=0.4253 acc=0.8236 | val_loss=0.4777 acc=0.7755 | prec=0.7020 rec=0.9593 spec=0.5909 f1=0.8107 | time=13.7s\n",
            "Epoch 027 | train_loss=0.4200 acc=0.8219 | val_loss=0.4207 acc=0.8073 | prec=0.7446 rec=0.9367 spec=0.6773 f1=0.8297 | time=13.5s\n",
            "Epoch 028 | train_loss=0.3995 acc=0.8293 | val_loss=0.3953 acc=0.8390 | prec=0.7907 rec=0.9231 spec=0.7545 f1=0.8518 | time=13.0s\n",
            "Epoch 029 | train_loss=0.3819 acc=0.8434 | val_loss=0.4011 acc=0.8345 | prec=0.7782 rec=0.9367 spec=0.7318 f1=0.8501 | time=13.0s\n",
            "Epoch 030 | train_loss=0.3826 acc=0.8571 | val_loss=0.3757 acc=0.8458 | prec=0.8201 rec=0.8869 spec=0.8045 f1=0.8522 | time=13.1s\n",
            "Epoch 031 | train_loss=0.3663 acc=0.8542 | val_loss=0.3812 acc=0.8345 | prec=0.8776 rec=0.7783 spec=0.8909 f1=0.8249 | time=13.1s\n",
            "Epoch 032 | train_loss=0.3955 acc=0.8242 | val_loss=0.3926 acc=0.8390 | prec=0.7885 rec=0.9276 spec=0.7500 f1=0.8524 | time=13.1s\n",
            "Epoch 033 | train_loss=0.3503 acc=0.8661 | val_loss=0.3976 acc=0.8299 | prec=0.7786 rec=0.9231 spec=0.7364 f1=0.8447 | time=13.0s\n",
            "Epoch 034 | train_loss=0.3317 acc=0.8729 | val_loss=0.4288 acc=0.8073 | prec=0.7429 rec=0.9412 spec=0.6727 f1=0.8303 | time=13.0s\n",
            "Epoch 035 | train_loss=0.3161 acc=0.8752 | val_loss=0.3817 acc=0.8322 | prec=0.8075 rec=0.8733 spec=0.7909 f1=0.8391 | time=13.0s\n",
            "Epoch 036 | train_loss=0.3200 acc=0.8786 | val_loss=0.3687 acc=0.8458 | prec=0.8370 rec=0.8597 spec=0.8318 f1=0.8482 | time=13.0s\n",
            "Epoch 037 | train_loss=0.3341 acc=0.8656 | val_loss=0.3911 acc=0.8345 | prec=0.7868 rec=0.9186 spec=0.7500 f1=0.8476 | time=12.9s\n",
            "Epoch 038 | train_loss=0.2936 acc=0.8883 | val_loss=0.4574 acc=0.7891 | prec=0.7192 rec=0.9502 spec=0.6273 f1=0.8187 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3084 acc=0.8837 | val_loss=0.3872 acc=0.8299 | prec=0.8544 rec=0.7964 spec=0.8636 f1=0.8244 | time=12.7s\n",
            "Epoch 040 | train_loss=0.2916 acc=0.8951 | val_loss=0.4239 acc=0.8186 | prec=0.7701 rec=0.9095 spec=0.7273 f1=0.8340 | time=12.9s\n",
            "Epoch 041 | train_loss=0.2660 acc=0.9019 | val_loss=0.3884 acc=0.8277 | prec=0.8194 rec=0.8416 spec=0.8136 f1=0.8304 | time=12.8s\n",
            "Epoch 042 | train_loss=0.2821 acc=0.8945 | val_loss=0.4000 acc=0.8231 | prec=0.8150 rec=0.8371 spec=0.8091 f1=0.8259 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2762 acc=0.8973 | val_loss=0.3755 acc=0.8322 | prec=0.8210 rec=0.8507 spec=0.8136 f1=0.8356 | time=12.9s\n",
            "Epoch 044 | train_loss=0.2448 acc=0.9098 | val_loss=0.3968 acc=0.8299 | prec=0.8174 rec=0.8507 spec=0.8091 f1=0.8337 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2268 acc=0.9223 | val_loss=0.4235 acc=0.8254 | prec=0.7812 rec=0.9050 spec=0.7455 f1=0.8386 | time=12.8s\n",
            "Epoch 046 | train_loss=0.2540 acc=0.9019 | val_loss=0.3901 acc=0.8322 | prec=0.8025 rec=0.8824 spec=0.7818 f1=0.8405 | time=12.8s\n",
            "Epoch 047 | train_loss=0.2543 acc=0.9047 | val_loss=0.3933 acc=0.8367 | prec=0.8433 rec=0.8281 spec=0.8455 f1=0.8356 | time=12.7s\n",
            "Epoch 048 | train_loss=0.2582 acc=0.8934 | val_loss=0.4363 acc=0.8277 | prec=0.8033 rec=0.8688 spec=0.7864 f1=0.8348 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2522 acc=0.9115 | val_loss=0.4276 acc=0.8118 | prec=0.8224 rec=0.7964 spec=0.8273 f1=0.8092 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2552 acc=0.8956 | val_loss=0.4155 acc=0.8299 | prec=0.8380 rec=0.8190 spec=0.8409 f1=0.8284 | time=12.8s\n",
            "Epoch 051 | train_loss=0.2327 acc=0.9280 | val_loss=0.4210 acc=0.8209 | prec=0.8227 rec=0.8190 spec=0.8227 f1=0.8209 | time=12.9s\n",
            "Early stopping at epoch 51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▆▆▆▅▆▆▆▆▁▆▇▇█▇███████████████████████▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▆▇▇▃▆▇▇▅▅▆▆▅▆▆▆█▆▆▅▆█▆▇▇▇▆▇▇▇▇</td></tr><tr><td>recall</td><td>█████▇▇███▁▄▅█▇▆▆▇▇▇█▇▇▇▇▇▇▇▇▇█▆▇▇▇▇▇▆▇▆</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▂▁▁▁▁██▇▆█▆▆▇▇▆▆▇▇▇█▇▆▇█▇▇▇▇▇▇▇█▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▄▅▅▅▆▆▅▆▆▆▆▆▇▇▇▇▇▇▇▇█▇██████</td></tr><tr><td>train_loss</td><td>██████████▇▆▆▅▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▂▁▁▁▂▆▅▇▇▇▇▇██▇▇████▇███▇▇███████▇▇</td></tr><tr><td>val_loss</td><td>██████████▇▆▄▅▄▃▃▃▃▂▂▃▂▁▂▁▁▁▂▁▁▂▁▂▁▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82086</td></tr><tr><td>precision</td><td>0.82273</td></tr><tr><td>recall</td><td>0.819</td></tr><tr><td>specificity</td><td>0.82273</td></tr><tr><td>train_accuracy</td><td>0.92796</td></tr><tr><td>train_loss</td><td>0.23275</td></tr><tr><td>val_accuracy</td><td>0.82086</td></tr><tr><td>val_loss</td><td>0.42104</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/zmq428en' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/zmq428en</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_192231-zmq428en/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 2 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_194140-kg9mg1s0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/kg9mg1s0' target=\"_blank\">fold2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/kg9mg1s0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/kg9mg1s0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7815 acc=0.4776 | val_loss=0.7026 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7537 acc=0.4838 | val_loss=0.6956 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.7s\n",
            "Epoch 003 | train_loss=0.7350 acc=0.4935 | val_loss=0.6976 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7360 acc=0.4906 | val_loss=0.6934 acc=0.4739 | prec=0.4678 rec=0.3620 spec=0.5864 f1=0.4082 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7276 acc=0.4861 | val_loss=0.6947 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7232 acc=0.5009 | val_loss=0.6954 acc=0.5011 | prec=0.5011 rec=1.0000 spec=0.0000 f1=0.6677 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7127 acc=0.5207 | val_loss=0.6958 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7165 acc=0.5116 | val_loss=0.6956 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7228 acc=0.4946 | val_loss=0.6942 acc=0.4830 | prec=0.3793 rec=0.0498 spec=0.9182 f1=0.0880 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7281 acc=0.4861 | val_loss=0.6957 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7196 acc=0.5094 | val_loss=0.6945 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7249 acc=0.4878 | val_loss=0.6947 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7138 acc=0.4991 | val_loss=0.6944 acc=0.4921 | prec=0.0000 rec=0.0000 spec=0.9864 f1=0.0000 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7196 acc=0.5145 | val_loss=0.6945 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7143 acc=0.4946 | val_loss=0.6952 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7190 acc=0.5014 | val_loss=0.6955 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7050 acc=0.5139 | val_loss=0.6956 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7056 acc=0.5201 | val_loss=0.6959 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7135 acc=0.4997 | val_loss=0.6964 acc=0.4966 | prec=0.4615 rec=0.0271 spec=0.9682 f1=0.0513 | time=12.9s\n",
            "Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▅██▁▁▂▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>precision</td><td>▁▁▁███▁▁▆▁▁▁▁▁▁▁▁▁▇</td></tr><tr><td>recall</td><td>▁▁▁▄██▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>specificity</td><td>███▅▁▁██▇██████████</td></tr><tr><td>train_accuracy</td><td>▁▂▄▃▂▅█▇▄▂▆▃▄▇▄▅▇█▅</td></tr><tr><td>train_loss</td><td>█▅▄▄▃▃▂▂▃▃▂▃▂▂▂▂▁▁▂</td></tr><tr><td>val_accuracy</td><td>▇▇▇▁██▇▇▃▇▇▇▆▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>█▃▄▁▂▂▃▃▂▃▂▂▂▂▂▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.05128</td></tr><tr><td>precision</td><td>0.46154</td></tr><tr><td>recall</td><td>0.02715</td></tr><tr><td>specificity</td><td>0.96818</td></tr><tr><td>train_accuracy</td><td>0.49972</td></tr><tr><td>train_loss</td><td>0.7135</td></tr><tr><td>val_accuracy</td><td>0.4966</td></tr><tr><td>val_loss</td><td>0.69643</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/kg9mg1s0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/kg9mg1s0</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_194140-kg9mg1s0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 3 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_194548-3ylm4luz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/3ylm4luz' target=\"_blank\">fold3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/3ylm4luz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/3ylm4luz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7262 acc=0.4929 | val_loss=0.7026 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7155 acc=0.5048 | val_loss=0.7007 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 003 | train_loss=0.7213 acc=0.5065 | val_loss=0.6999 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7157 acc=0.5105 | val_loss=0.7010 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7124 acc=0.4986 | val_loss=0.6981 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7168 acc=0.4929 | val_loss=0.6987 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7114 acc=0.4935 | val_loss=0.6975 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7028 acc=0.5077 | val_loss=0.6945 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 009 | train_loss=0.7065 acc=0.5150 | val_loss=0.6978 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.6997 acc=0.5184 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 011 | train_loss=0.7007 acc=0.5156 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7045 acc=0.4827 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.6997 acc=0.5196 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.6987 acc=0.5003 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7012 acc=0.4974 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7033 acc=0.4969 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7037 acc=0.4935 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7010 acc=0.4991 | val_loss=0.6926 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7011 acc=0.5094 | val_loss=0.6918 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7026 acc=0.4861 | val_loss=0.6865 acc=0.6712 | prec=0.7863 rec=0.4682 spec=0.8733 f1=0.5869 | time=12.8s\n",
            "Epoch 021 | train_loss=0.6988 acc=0.4997 | val_loss=0.6890 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.9s\n",
            "Epoch 022 | train_loss=0.6769 acc=0.5735 | val_loss=0.6718 acc=0.5941 | prec=0.9020 rec=0.2091 spec=0.9774 f1=0.3395 | time=12.9s\n",
            "Epoch 023 | train_loss=0.6499 acc=0.6330 | val_loss=0.6040 acc=0.7528 | prec=0.7265 rec=0.8091 spec=0.6968 f1=0.7656 | time=12.9s\n",
            "Epoch 024 | train_loss=0.6169 acc=0.6886 | val_loss=0.5846 acc=0.8209 | prec=0.8983 rec=0.7227 spec=0.9186 f1=0.8010 | time=12.8s\n",
            "Epoch 025 | train_loss=0.5924 acc=0.7181 | val_loss=0.5672 acc=0.8118 | prec=0.8663 rec=0.7364 spec=0.8869 f1=0.7961 | time=12.7s\n",
            "Epoch 026 | train_loss=0.5797 acc=0.7175 | val_loss=0.5429 acc=0.8186 | prec=0.8846 rec=0.7318 spec=0.9050 f1=0.8010 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5567 acc=0.7476 | val_loss=0.5371 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5345 acc=0.7635 | val_loss=0.5269 acc=0.8299 | prec=0.8919 rec=0.7500 spec=0.9095 f1=0.8148 | time=12.7s\n",
            "Epoch 029 | train_loss=0.5220 acc=0.7691 | val_loss=0.5174 acc=0.8186 | prec=0.8333 rec=0.7955 spec=0.8416 f1=0.8140 | time=12.9s\n",
            "Epoch 030 | train_loss=0.5114 acc=0.7805 | val_loss=0.4882 acc=0.8322 | prec=0.8650 rec=0.7864 spec=0.8778 f1=0.8238 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4925 acc=0.8003 | val_loss=0.4889 acc=0.8549 | prec=0.8451 rec=0.8682 spec=0.8416 f1=0.8565 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4839 acc=0.8060 | val_loss=0.4866 acc=0.8367 | prec=0.7984 rec=0.9000 spec=0.7738 f1=0.8462 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4658 acc=0.8077 | val_loss=0.4835 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4461 acc=0.8372 | val_loss=0.4515 acc=0.8413 | prec=0.9167 rec=0.7500 spec=0.9321 f1=0.8250 | time=13.0s\n",
            "Epoch 035 | train_loss=0.4321 acc=0.8361 | val_loss=0.4679 acc=0.8027 | prec=0.7436 rec=0.9227 spec=0.6833 f1=0.8235 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4253 acc=0.8469 | val_loss=0.4787 acc=0.8050 | prec=0.7410 rec=0.9364 spec=0.6742 f1=0.8273 | time=12.8s\n",
            "Epoch 037 | train_loss=0.4147 acc=0.8531 | val_loss=0.4256 acc=0.8322 | prec=0.8288 rec=0.8364 spec=0.8281 f1=0.8326 | time=12.9s\n",
            "Epoch 038 | train_loss=0.3708 acc=0.8735 | val_loss=0.4384 acc=0.8254 | prec=0.8357 rec=0.8091 spec=0.8416 f1=0.8222 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3690 acc=0.8707 | val_loss=0.4405 acc=0.8186 | prec=0.9167 rec=0.7000 spec=0.9367 f1=0.7938 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3483 acc=0.8803 | val_loss=0.4360 acc=0.8209 | prec=0.9123 rec=0.7091 spec=0.9321 f1=0.7980 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3404 acc=0.8894 | val_loss=0.4097 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3242 acc=0.8917 | val_loss=0.3994 acc=0.8413 | prec=0.8947 rec=0.7727 spec=0.9095 f1=0.8293 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3035 acc=0.8934 | val_loss=0.4012 acc=0.8435 | prec=0.8912 rec=0.7818 spec=0.9050 f1=0.8329 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3223 acc=0.8803 | val_loss=0.4051 acc=0.8367 | prec=0.8008 rec=0.8955 spec=0.7783 f1=0.8455 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3012 acc=0.8962 | val_loss=0.4004 acc=0.8390 | prec=0.8744 rec=0.7909 spec=0.8869 f1=0.8305 | time=12.8s\n",
            "Epoch 046 | train_loss=0.2940 acc=0.8945 | val_loss=0.3863 acc=0.8481 | prec=0.8844 rec=0.8000 spec=0.8959 f1=0.8401 | time=12.7s\n",
            "Epoch 047 | train_loss=0.3132 acc=0.8945 | val_loss=0.4187 acc=0.8254 | prec=0.8994 rec=0.7318 spec=0.9186 f1=0.8070 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3068 acc=0.8820 | val_loss=0.3973 acc=0.8345 | prec=0.8848 rec=0.7682 spec=0.9005 f1=0.8224 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2815 acc=0.8888 | val_loss=0.3919 acc=0.8435 | prec=0.8612 rec=0.8182 spec=0.8688 f1=0.8392 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2815 acc=0.9149 | val_loss=0.3773 acc=0.8435 | prec=0.8386 rec=0.8500 spec=0.8371 f1=0.8442 | time=12.7s\n",
            "Epoch 051 | train_loss=0.2517 acc=0.9075 | val_loss=0.3873 acc=0.8345 | prec=0.8210 rec=0.8545 spec=0.8145 f1=0.8374 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2511 acc=0.9070 | val_loss=0.4415 acc=0.8163 | prec=0.9017 rec=0.7091 spec=0.9231 f1=0.7939 | time=12.8s\n",
            "Epoch 053 | train_loss=0.2390 acc=0.9149 | val_loss=0.3931 acc=0.8413 | prec=0.8641 rec=0.8091 spec=0.8733 f1=0.8357 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2390 acc=0.9081 | val_loss=0.3863 acc=0.8413 | prec=0.8049 rec=0.9000 spec=0.7828 f1=0.8498 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2431 acc=0.9166 | val_loss=0.4504 acc=0.8231 | prec=0.8989 rec=0.7273 spec=0.9186 f1=0.8040 | time=12.8s\n",
            "Epoch 056 | train_loss=0.2511 acc=0.8928 | val_loss=0.4074 acc=0.8390 | prec=0.8860 rec=0.7773 spec=0.9005 f1=0.8281 | time=12.8s\n",
            "Epoch 057 | train_loss=0.2259 acc=0.9155 | val_loss=0.3827 acc=0.8458 | prec=0.8551 rec=0.8318 spec=0.8597 f1=0.8433 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2273 acc=0.9195 | val_loss=0.3822 acc=0.8435 | prec=0.8297 rec=0.8636 spec=0.8235 f1=0.8463 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2081 acc=0.9325 | val_loss=0.4340 acc=0.8186 | prec=0.9070 rec=0.7091 spec=0.9276 f1=0.7959 | time=12.8s\n",
            "Epoch 060 | train_loss=0.2461 acc=0.9047 | val_loss=0.3922 acc=0.8390 | prec=0.8433 rec=0.8318 spec=0.8462 f1=0.8375 | time=12.8s\n",
            "Epoch 061 | train_loss=0.1931 acc=0.9365 | val_loss=0.3962 acc=0.8231 | prec=0.8413 rec=0.7955 spec=0.8507 f1=0.8178 | time=12.8s\n",
            "Epoch 062 | train_loss=0.1936 acc=0.9297 | val_loss=0.4049 acc=0.8277 | prec=0.8600 rec=0.7818 spec=0.8733 f1=0.8190 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2104 acc=0.9217 | val_loss=0.4077 acc=0.8254 | prec=0.8454 rec=0.7955 spec=0.8552 f1=0.8197 | time=12.9s\n",
            "Epoch 064 | train_loss=0.2038 acc=0.9263 | val_loss=0.4426 acc=0.8322 | prec=0.8802 rec=0.7682 spec=0.8959 f1=0.8204 | time=12.7s\n",
            "Epoch 065 | train_loss=0.1816 acc=0.9348 | val_loss=0.4258 acc=0.8390 | prec=0.8942 rec=0.7682 spec=0.9095 f1=0.8264 | time=12.8s\n",
            "Early stopping at epoch 65\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▇██████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▇█▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▅▁▃▇▆▆▇▇▇▇▇▇██▇▆▇▇▇█▇▆▇▇▇▆█▆▇▇</td></tr><tr><td>specificity</td><td>██████████▅██▁▆▆▅▆▅▃▁▄▅▅▆▆▆▆▅▅▆▅▃▆▆▆▅▅▅▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▂▂▂▁▂▁▁▁▁▁▁▃▄▅▅▆▆▇▇▇▇▇▇▇▇▇▇▇████▇██</td></tr><tr><td>train_loss</td><td>█████████████▇▇▆▅▅▅▅▄▄▄▄▃▂▃▂▂▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▄▃▆▇▇████▇▇██▇████████▇██▇███</td></tr><tr><td>val_loss</td><td>█████████████▇▆▅▅▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82641</td></tr><tr><td>precision</td><td>0.89418</td></tr><tr><td>recall</td><td>0.76818</td></tr><tr><td>specificity</td><td>0.9095</td></tr><tr><td>train_accuracy</td><td>0.93477</td></tr><tr><td>train_loss</td><td>0.18164</td></tr><tr><td>val_accuracy</td><td>0.839</td></tr><tr><td>val_loss</td><td>0.42583</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/3ylm4luz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/3ylm4luz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_194548-3ylm4luz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 4 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_195945-sjpcudod</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/sjpcudod' target=\"_blank\">fold4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/sjpcudod' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/sjpcudod</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7358 acc=0.5111 | val_loss=0.6921 acc=0.5057 | prec=0.5050 rec=0.4636 spec=0.5475 f1=0.4834 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7291 acc=0.5020 | val_loss=0.6931 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7147 acc=0.5207 | val_loss=0.6929 acc=0.5057 | prec=1.0000 rec=0.0091 spec=1.0000 f1=0.0180 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7261 acc=0.4935 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7188 acc=0.5031 | val_loss=0.7036 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7013 acc=0.5343 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7160 acc=0.4923 | val_loss=0.6991 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7143 acc=0.5009 | val_loss=0.6988 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7220 acc=0.4855 | val_loss=0.7007 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7140 acc=0.5037 | val_loss=0.7031 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 011 | train_loss=0.7155 acc=0.4940 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7082 acc=0.5235 | val_loss=0.6957 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7048 acc=0.5088 | val_loss=0.6900 acc=0.5918 | prec=0.6389 rec=0.4182 spec=0.7647 f1=0.5055 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7035 acc=0.5201 | val_loss=0.6863 acc=0.6553 | prec=0.6269 rec=0.7636 spec=0.5475 f1=0.6885 | time=12.9s\n",
            "Epoch 015 | train_loss=0.6948 acc=0.5332 | val_loss=0.6690 acc=0.5850 | prec=0.8491 rec=0.2045 spec=0.9638 f1=0.3297 | time=12.7s\n",
            "Epoch 016 | train_loss=0.6459 acc=0.6149 | val_loss=0.6497 acc=0.6304 | prec=0.8904 rec=0.2955 spec=0.9638 f1=0.4437 | time=13.0s\n",
            "Epoch 017 | train_loss=0.6214 acc=0.6500 | val_loss=0.5755 acc=0.7370 | prec=0.8714 rec=0.5545 spec=0.9186 f1=0.6778 | time=12.8s\n",
            "Epoch 018 | train_loss=0.5873 acc=0.6869 | val_loss=0.5353 acc=0.7596 | prec=0.8701 rec=0.6091 spec=0.9095 f1=0.7166 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5599 acc=0.7170 | val_loss=0.5184 acc=0.7823 | prec=0.8647 rec=0.6682 spec=0.8959 f1=0.7538 | time=12.9s\n",
            "Epoch 020 | train_loss=0.5307 acc=0.7396 | val_loss=0.4741 acc=0.8186 | prec=0.8302 rec=0.8000 spec=0.8371 f1=0.8148 | time=12.8s\n",
            "Epoch 021 | train_loss=0.5084 acc=0.7652 | val_loss=0.4636 acc=0.8209 | prec=0.8770 rec=0.7455 spec=0.8959 f1=0.8059 | time=12.9s\n",
            "Epoch 022 | train_loss=0.4966 acc=0.7697 | val_loss=0.4583 acc=0.8367 | prec=0.8083 rec=0.8818 spec=0.7919 f1=0.8435 | time=12.9s\n",
            "Epoch 023 | train_loss=0.4926 acc=0.7913 | val_loss=0.4474 acc=0.8322 | prec=0.7920 rec=0.9000 spec=0.7647 f1=0.8426 | time=12.7s\n",
            "Epoch 024 | train_loss=0.4703 acc=0.7981 | val_loss=0.4250 acc=0.8503 | prec=0.8889 rec=0.8000 spec=0.9005 f1=0.8421 | time=12.8s\n",
            "Epoch 025 | train_loss=0.4703 acc=0.7890 | val_loss=0.4189 acc=0.8481 | prec=0.9005 rec=0.7818 spec=0.9140 f1=0.8370 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4380 acc=0.8128 | val_loss=0.4265 acc=0.8141 | prec=0.9157 rec=0.6909 spec=0.9367 f1=0.7876 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4506 acc=0.8077 | val_loss=0.4443 acc=0.8050 | prec=0.7343 rec=0.9545 spec=0.6561 f1=0.8300 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4260 acc=0.7986 | val_loss=0.3803 acc=0.8594 | prec=0.8990 rec=0.8091 spec=0.9095 f1=0.8517 | time=12.8s\n",
            "Epoch 029 | train_loss=0.3912 acc=0.8429 | val_loss=0.4128 acc=0.8163 | prec=0.9264 rec=0.6864 spec=0.9457 f1=0.7885 | time=12.8s\n",
            "Epoch 030 | train_loss=0.3847 acc=0.8395 | val_loss=0.3640 acc=0.8707 | prec=0.8756 rec=0.8636 spec=0.8778 f1=0.8696 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3822 acc=0.8400 | val_loss=0.3550 acc=0.8730 | prec=0.8942 rec=0.8455 spec=0.9005 f1=0.8692 | time=12.8s\n",
            "Epoch 032 | train_loss=0.3793 acc=0.8582 | val_loss=0.4033 acc=0.8231 | prec=0.7630 rec=0.9364 spec=0.7104 f1=0.8408 | time=12.7s\n",
            "Epoch 033 | train_loss=0.3613 acc=0.8491 | val_loss=0.3455 acc=0.8707 | prec=0.9095 rec=0.8227 spec=0.9186 f1=0.8640 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3445 acc=0.8559 | val_loss=0.3403 acc=0.8753 | prec=0.8910 rec=0.8545 spec=0.8959 f1=0.8724 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3383 acc=0.8661 | val_loss=0.3534 acc=0.8549 | prec=0.9286 rec=0.7682 spec=0.9412 f1=0.8408 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3252 acc=0.8678 | val_loss=0.3267 acc=0.8798 | prec=0.8711 rec=0.8909 spec=0.8688 f1=0.8809 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3231 acc=0.8627 | val_loss=0.3416 acc=0.8707 | prec=0.9267 rec=0.8045 spec=0.9367 f1=0.8613 | time=12.7s\n",
            "Epoch 038 | train_loss=0.3000 acc=0.8832 | val_loss=0.3356 acc=0.8571 | prec=0.8230 rec=0.9091 spec=0.8054 f1=0.8639 | time=12.7s\n",
            "Epoch 039 | train_loss=0.3051 acc=0.8820 | val_loss=0.3983 acc=0.8345 | prec=0.9401 rec=0.7136 spec=0.9548 f1=0.8114 | time=12.8s\n",
            "Epoch 040 | train_loss=0.2919 acc=0.9013 | val_loss=0.3183 acc=0.8798 | prec=0.8848 rec=0.8727 spec=0.8869 f1=0.8787 | time=12.8s\n",
            "Epoch 041 | train_loss=0.2976 acc=0.8996 | val_loss=0.3486 acc=0.8435 | prec=0.7984 rec=0.9182 spec=0.7692 f1=0.8541 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2859 acc=0.8854 | val_loss=0.3181 acc=0.8821 | prec=0.9118 rec=0.8455 spec=0.9186 f1=0.8774 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2689 acc=0.9041 | val_loss=0.3743 acc=0.8367 | prec=0.9353 rec=0.7227 spec=0.9502 f1=0.8154 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2830 acc=0.8911 | val_loss=0.3215 acc=0.8798 | prec=0.9073 rec=0.8455 spec=0.9140 f1=0.8753 | time=12.7s\n",
            "Epoch 045 | train_loss=0.2757 acc=0.8883 | val_loss=0.3424 acc=0.8617 | prec=0.8142 rec=0.9364 spec=0.7873 f1=0.8710 | time=12.8s\n",
            "Epoch 046 | train_loss=0.2790 acc=0.8973 | val_loss=0.3069 acc=0.8685 | prec=0.8522 rec=0.8909 spec=0.8462 f1=0.8711 | time=12.8s\n",
            "Epoch 047 | train_loss=0.2565 acc=0.9092 | val_loss=0.3231 acc=0.8776 | prec=0.8773 rec=0.8773 spec=0.8778 f1=0.8773 | time=12.8s\n",
            "Epoch 048 | train_loss=0.2338 acc=0.9087 | val_loss=0.3158 acc=0.8753 | prec=0.9024 rec=0.8409 spec=0.9095 f1=0.8706 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2424 acc=0.9161 | val_loss=0.3323 acc=0.8571 | prec=0.8178 rec=0.9182 spec=0.7964 f1=0.8651 | time=12.9s\n",
            "Epoch 050 | train_loss=0.2410 acc=0.8962 | val_loss=0.3228 acc=0.8730 | prec=0.8475 rec=0.9091 spec=0.8371 f1=0.8772 | time=12.8s\n",
            "Epoch 051 | train_loss=0.2308 acc=0.9013 | val_loss=0.2982 acc=0.8844 | prec=0.8690 rec=0.9045 spec=0.8643 f1=0.8864 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2284 acc=0.9121 | val_loss=0.3230 acc=0.8685 | prec=0.8785 rec=0.8545 spec=0.8824 f1=0.8664 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2108 acc=0.9257 | val_loss=0.3188 acc=0.8753 | prec=0.8634 rec=0.8909 spec=0.8597 f1=0.8770 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2050 acc=0.9246 | val_loss=0.3404 acc=0.8707 | prec=0.8410 rec=0.9136 spec=0.8281 f1=0.8758 | time=12.9s\n",
            "Epoch 055 | train_loss=0.1937 acc=0.9336 | val_loss=0.3200 acc=0.8662 | prec=0.8966 rec=0.8273 spec=0.9050 f1=0.8605 | time=12.8s\n",
            "Epoch 056 | train_loss=0.2235 acc=0.9092 | val_loss=0.3337 acc=0.8594 | prec=0.8990 rec=0.8091 spec=0.9095 f1=0.8517 | time=12.8s\n",
            "Epoch 057 | train_loss=0.2251 acc=0.9161 | val_loss=0.3445 acc=0.8594 | prec=0.8798 rec=0.8318 spec=0.8869 f1=0.8551 | time=12.8s\n",
            "Epoch 058 | train_loss=0.1984 acc=0.9212 | val_loss=0.3364 acc=0.8639 | prec=0.8279 rec=0.9182 spec=0.8100 f1=0.8707 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2124 acc=0.9240 | val_loss=0.3298 acc=0.8798 | prec=0.8553 rec=0.9136 spec=0.8462 f1=0.8835 | time=12.7s\n",
            "Epoch 060 | train_loss=0.2088 acc=0.9240 | val_loss=0.4831 acc=0.8231 | prec=0.7500 rec=0.9682 spec=0.6787 f1=0.8452 | time=12.8s\n",
            "Epoch 061 | train_loss=0.1957 acc=0.9183 | val_loss=0.3864 acc=0.8481 | prec=0.7931 rec=0.9409 spec=0.7557 f1=0.8607 | time=12.8s\n",
            "Epoch 062 | train_loss=0.1933 acc=0.9297 | val_loss=0.3362 acc=0.8571 | prec=0.8552 rec=0.8591 spec=0.8552 f1=0.8571 | time=12.8s\n",
            "Epoch 063 | train_loss=0.1853 acc=0.9246 | val_loss=0.3842 acc=0.8526 | prec=0.8088 rec=0.9227 spec=0.7828 f1=0.8620 | time=12.8s\n",
            "Epoch 064 | train_loss=0.2169 acc=0.9212 | val_loss=0.3314 acc=0.8707 | prec=0.8791 rec=0.8591 spec=0.8824 f1=0.8690 | time=12.7s\n",
            "Epoch 065 | train_loss=0.1837 acc=0.9257 | val_loss=0.4248 acc=0.8277 | prec=0.7667 rec=0.9409 spec=0.7149 f1=0.8449 | time=12.8s\n",
            "Epoch 066 | train_loss=0.1797 acc=0.9257 | val_loss=0.3315 acc=0.8707 | prec=0.8468 rec=0.9045 spec=0.8371 f1=0.8747 | time=12.9s\n",
            "Early stopping at epoch 66\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▅▄▅▇▇▇██████████▇██████████████</td></tr><tr><td>precision</td><td>██▁▁▁▁▁▅▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇█▇█▇▇▇▇▇▇▇▇▇▆▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▄▇▅▇▆▇█▇▆█▆▇█▇▇▇▇▆▇█▆▇█▇▇█▇▇▇██▇</td></tr><tr><td>specificity</td><td>▁█████████▇▇▇▅▆▄▆▇▃▇▆▄▇▆▇▇▅▇▄▇▆▅▆▅▇▆▃▅▆▅</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▁▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇███▇██████████</td></tr><tr><td>train_loss</td><td>██████████▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▄▃▃▆▇▇▇▇██▇█▇█▇▇█████████▇▇█▇</td></tr><tr><td>val_loss</td><td>████████▇▅▄▃▃▃▄▂▃▂▂▁▂▃▁▂▁▂▁▁▁▁▂▁▂▂▂▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.87473</td></tr><tr><td>precision</td><td>0.84681</td></tr><tr><td>recall</td><td>0.90455</td></tr><tr><td>specificity</td><td>0.8371</td></tr><tr><td>train_accuracy</td><td>0.92569</td></tr><tr><td>train_loss</td><td>0.17966</td></tr><tr><td>val_accuracy</td><td>0.87075</td></tr><tr><td>val_loss</td><td>0.3315</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/sjpcudod' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/sjpcudod</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_195945-sjpcudod/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 5 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_201355-uz7hxy2t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/uz7hxy2t' target=\"_blank\">fold5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/uz7hxy2t' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/uz7hxy2t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7344 acc=0.4722 | val_loss=0.6951 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7193 acc=0.5096 | val_loss=0.6949 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 003 | train_loss=0.7254 acc=0.5045 | val_loss=0.6943 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7274 acc=0.4932 | val_loss=0.6936 acc=0.4955 | prec=0.4444 rec=0.0364 spec=0.9545 f1=0.0672 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7239 acc=0.4949 | val_loss=0.6938 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7112 acc=0.5085 | val_loss=0.6949 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7177 acc=0.5040 | val_loss=0.6948 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7276 acc=0.4887 | val_loss=0.6930 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7351 acc=0.4864 | val_loss=0.6930 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7202 acc=0.5198 | val_loss=0.6911 acc=0.5750 | prec=0.5922 rec=0.4818 spec=0.6682 f1=0.5313 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7096 acc=0.5198 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7011 acc=0.5329 | val_loss=0.6937 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=13.1s\n",
            "Epoch 013 | train_loss=0.7056 acc=0.5255 | val_loss=0.6707 acc=0.6432 | prec=0.5994 rec=0.8636 spec=0.4227 f1=0.7076 | time=12.8s\n",
            "Epoch 014 | train_loss=0.6906 acc=0.5499 | val_loss=0.6414 acc=0.7205 | prec=0.8129 rec=0.5727 spec=0.8682 f1=0.6720 | time=13.0s\n",
            "Epoch 015 | train_loss=0.6694 acc=0.5947 | val_loss=0.6326 acc=0.6886 | prec=0.8879 rec=0.4318 spec=0.9455 f1=0.5810 | time=12.8s\n",
            "Epoch 016 | train_loss=0.6385 acc=0.6463 | val_loss=0.5926 acc=0.7795 | prec=0.8555 rec=0.6727 spec=0.8864 f1=0.7532 | time=12.9s\n",
            "Epoch 017 | train_loss=0.6204 acc=0.6684 | val_loss=0.5800 acc=0.7909 | prec=0.8810 rec=0.6727 spec=0.9091 f1=0.7629 | time=12.7s\n",
            "Epoch 018 | train_loss=0.5911 acc=0.7029 | val_loss=0.5368 acc=0.7909 | prec=0.8765 rec=0.6773 spec=0.9045 f1=0.7641 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5720 acc=0.7080 | val_loss=0.5370 acc=0.7636 | prec=0.8919 rec=0.6000 spec=0.9273 f1=0.7174 | time=12.9s\n",
            "Epoch 020 | train_loss=0.5349 acc=0.7534 | val_loss=0.5073 acc=0.7886 | prec=0.8671 rec=0.6818 spec=0.8955 f1=0.7634 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5164 acc=0.7568 | val_loss=0.4876 acc=0.7886 | prec=0.8191 rec=0.7409 spec=0.8364 f1=0.7780 | time=12.8s\n",
            "Epoch 022 | train_loss=0.5095 acc=0.7727 | val_loss=0.4926 acc=0.7886 | prec=0.8802 rec=0.6682 spec=0.9091 f1=0.7597 | time=12.7s\n",
            "Epoch 023 | train_loss=0.4812 acc=0.7971 | val_loss=0.4862 acc=0.7977 | prec=0.9068 rec=0.6636 spec=0.9318 f1=0.7664 | time=12.8s\n",
            "Epoch 024 | train_loss=0.4620 acc=0.8010 | val_loss=0.4794 acc=0.8023 | prec=0.9182 rec=0.6636 spec=0.9409 f1=0.7704 | time=12.8s\n",
            "Epoch 025 | train_loss=0.4505 acc=0.8107 | val_loss=0.4569 acc=0.7886 | prec=0.8547 rec=0.6955 spec=0.8818 f1=0.7669 | time=12.9s\n",
            "Epoch 026 | train_loss=0.4244 acc=0.8345 | val_loss=0.4294 acc=0.8000 | prec=0.8474 rec=0.7318 spec=0.8682 f1=0.7854 | time=12.8s\n",
            "Epoch 027 | train_loss=0.4050 acc=0.8362 | val_loss=0.4281 acc=0.8068 | prec=0.8082 rec=0.8045 spec=0.8091 f1=0.8064 | time=12.8s\n",
            "Epoch 028 | train_loss=0.3898 acc=0.8492 | val_loss=0.4106 acc=0.8227 | prec=0.8698 rec=0.7591 spec=0.8864 f1=0.8107 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3701 acc=0.8515 | val_loss=0.4131 acc=0.8205 | prec=0.8279 rec=0.8091 spec=0.8318 f1=0.8184 | time=12.7s\n",
            "Epoch 030 | train_loss=0.3692 acc=0.8441 | val_loss=0.4539 acc=0.7955 | prec=0.9452 rec=0.6273 spec=0.9636 f1=0.7541 | time=12.8s\n",
            "Epoch 031 | train_loss=0.3576 acc=0.8600 | val_loss=0.4410 acc=0.7864 | prec=0.7333 rec=0.9000 spec=0.6727 f1=0.8082 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3374 acc=0.8673 | val_loss=0.4371 acc=0.8045 | prec=0.9187 rec=0.6682 spec=0.9409 f1=0.7737 | time=12.8s\n",
            "Epoch 033 | train_loss=0.3351 acc=0.8583 | val_loss=0.5855 acc=0.7000 | prec=0.6317 rec=0.9591 spec=0.4409 f1=0.7617 | time=12.8s\n",
            "Epoch 034 | train_loss=0.3748 acc=0.8492 | val_loss=0.4131 acc=0.8068 | prec=0.7528 rec=0.9136 spec=0.7000 f1=0.8255 | time=12.8s\n",
            "Epoch 035 | train_loss=0.3286 acc=0.8730 | val_loss=0.3841 acc=0.8273 | prec=0.8333 rec=0.8182 spec=0.8364 f1=0.8257 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3161 acc=0.8764 | val_loss=0.4016 acc=0.8045 | prec=0.7939 rec=0.8227 spec=0.7864 f1=0.8080 | time=13.0s\n",
            "Epoch 037 | train_loss=0.2983 acc=0.8793 | val_loss=0.3980 acc=0.8159 | prec=0.8203 rec=0.8091 spec=0.8227 f1=0.8146 | time=12.9s\n",
            "Epoch 038 | train_loss=0.2952 acc=0.8838 | val_loss=0.3842 acc=0.8318 | prec=0.8120 rec=0.8636 spec=0.8000 f1=0.8370 | time=12.8s\n",
            "Epoch 039 | train_loss=0.2690 acc=0.8957 | val_loss=0.4282 acc=0.7977 | prec=0.7510 rec=0.8909 spec=0.7045 f1=0.8150 | time=12.8s\n",
            "Epoch 040 | train_loss=0.2651 acc=0.8991 | val_loss=0.3966 acc=0.8364 | prec=0.8737 rec=0.7864 spec=0.8864 f1=0.8278 | time=12.8s\n",
            "Epoch 041 | train_loss=0.2788 acc=0.8917 | val_loss=0.4032 acc=0.8182 | prec=0.7846 rec=0.8773 spec=0.7591 f1=0.8283 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2765 acc=0.8991 | val_loss=0.4083 acc=0.8159 | prec=0.7769 rec=0.8864 spec=0.7455 f1=0.8280 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2608 acc=0.8980 | val_loss=0.3862 acc=0.8159 | prec=0.8174 rec=0.8136 spec=0.8182 f1=0.8155 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2444 acc=0.9070 | val_loss=0.4127 acc=0.8114 | prec=0.8018 rec=0.8273 spec=0.7955 f1=0.8143 | time=12.8s\n",
            "Epoch 045 | train_loss=0.2340 acc=0.9076 | val_loss=0.4263 acc=0.8227 | prec=0.8114 rec=0.8409 spec=0.8045 f1=0.8259 | time=12.8s\n",
            "Epoch 046 | train_loss=0.2444 acc=0.8991 | val_loss=0.4158 acc=0.8136 | prec=0.7899 rec=0.8545 spec=0.7727 f1=0.8210 | time=12.8s\n",
            "Epoch 047 | train_loss=0.2257 acc=0.9110 | val_loss=0.4216 acc=0.8068 | prec=0.7755 rec=0.8636 spec=0.7500 f1=0.8172 | time=13.0s\n",
            "Epoch 048 | train_loss=0.2215 acc=0.9144 | val_loss=0.4223 acc=0.8250 | prec=0.7992 rec=0.8682 spec=0.7818 f1=0.8322 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2123 acc=0.9144 | val_loss=0.3858 acc=0.8409 | prec=0.8788 rec=0.7909 spec=0.8909 f1=0.8325 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2249 acc=0.9093 | val_loss=0.4824 acc=0.7795 | prec=0.7099 rec=0.9455 spec=0.6136 f1=0.8109 | time=12.8s\n",
            "Early stopping at epoch 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▂▁▁▁▁▅▇▇▇▆▇▇▇█▇▇▇████▇▇▇█████████████</td></tr><tr><td>precision</td><td>▁▁▁▄▁▁▁▅▅▅▇██▇█▇███▇▇▇▇█▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▄██▅▄▆▆▆▆▆▆▆▆▆▇▆▅▇█▇▇▇▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>specificity</td><td>███████▆▁▁▇█▇▇▇▇▇██▇▇▇▇█▆▄▆▇▇▇▆▇▆▆▇▇▆▆▆▅</td></tr><tr><td>train_accuracy</td><td>▁▂▂▁▁▂▁▁▂▂▂▂▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>███████████▇▇▇▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▃▁▄▆▅▇▇▆▇▇▇▇▇▇██▇▇▅▇█▇█▇██▇▇█▇▇█</td></tr><tr><td>val_loss</td><td>██████████▇▇▇▆▅▄▃▃▃▃▂▂▂▃▂▆▂▁▁▁▂▁▁▂▁▂▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81092</td></tr><tr><td>precision</td><td>0.7099</td></tr><tr><td>recall</td><td>0.94545</td></tr><tr><td>specificity</td><td>0.61364</td></tr><tr><td>train_accuracy</td><td>0.9093</td></tr><tr><td>train_loss</td><td>0.22488</td></tr><tr><td>val_accuracy</td><td>0.77955</td></tr><tr><td>val_loss</td><td>0.48245</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/uz7hxy2t' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp/runs/uz7hxy2t</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_201355-uz7hxy2t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 5-Fold Cross-Validation Averages ===\n",
            "loss : 0.4243 ± 0.1380\n",
            "acc  : 0.7614 ± 0.1356\n",
            "prec : 0.7470 ± 0.1551\n",
            "rec  : 0.6929 ± 0.3386\n",
            "spec : 0.8302 ± 0.1203\n",
            "f1   : 0.6768 ± 0.3136\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dyenjKKarng8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Change the LR and WD to 3.5e-5"
      ],
      "metadata": {
        "id": "hR9KO5u2Xg25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "LR          = 3.5e-5\n",
        "WD          = 3.5e-5\n",
        "PCT_START   = 0.20\n",
        "NUM_FILTERS = 120\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & balance metadata ─────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "train_meta = [d for d in all_meta if d['type']=='train']\n",
        "class0, class1 = 'A','C'\n",
        "\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# map to 0/1\n",
        "label_map = {class0:0, class1:1}\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "# build dataset & labels array\n",
        "raw_ds  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset = BinaryEEGDataset(raw_ds, balanced_meta)\n",
        "labels  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── 5-Fold Cross-Validation ─────────────────────────────────────\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "\n",
        "# store per-fold metrics\n",
        "fold_metrics = {'loss':[], 'acc':[], 'prec':[], 'rec':[], 'spec':[], 'f1':[]}\n",
        "\n",
        "for fold, (train_idx, val_idx) in enumerate(skf.split(np.zeros(len(labels)), labels), start=1):\n",
        "    print(f\"\\n=== Fold {fold} ===\")\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-fixed-hp-2\",\n",
        "        name=f\"fold{fold}\",\n",
        "        config={\n",
        "            'lr': LR, 'wd': WD,\n",
        "            'pct_start': PCT_START,\n",
        "            'filters': NUM_FILTERS,\n",
        "            'max_epochs': MAX_EPOCHS,\n",
        "            'batch_size': BATCH_SIZE\n",
        "        },\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(dataset, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # build model, optimizer, scheduler, criterion\n",
        "    input_len = dataset[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19,\n",
        "        input_length=input_len,\n",
        "        kernel_size=10,\n",
        "        num_filters=NUM_FILTERS,\n",
        "        num_heads=3,\n",
        "        num_blocks=1,\n",
        "        num_segments=5,\n",
        "        num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer,\n",
        "        max_lr=LR,\n",
        "        total_steps=MAX_EPOCHS * len(train_loader),\n",
        "        pct_start=PCT_START,\n",
        "        anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_loss = float('inf')\n",
        "    es_count  = 0\n",
        "\n",
        "    # ─── Epoch Loop ───────────────────────────────────────────────\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = WD * (cur_lr / LR)\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            preds     = logits.argmax(1)\n",
        "            tcorrect += (preds == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss += loss.item()\n",
        "\n",
        "                preds    = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_preds  = np.concatenate(all_preds)\n",
        "        val_labels = np.concatenate(all_labels)\n",
        "        val_acc    = (val_preds == val_labels).mean()\n",
        "        precision  = precision_score(val_labels, val_preds, zero_division=0)\n",
        "        recall     = recall_score(val_labels, val_preds, zero_division=0)\n",
        "        f1         = f1_score(val_labels, val_preds, zero_division=0)\n",
        "        tn, fp, fn, tp = confusion_matrix(val_labels, val_preds).ravel()\n",
        "        specificity   = tn / (tn + fp)\n",
        "\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        # print to terminal\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={precision:.4f} rec={recall:.4f} spec={specificity:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        # log to W&B\n",
        "        wandb.log({\n",
        "            'train_loss':     train_loss,\n",
        "            'train_accuracy': train_acc,\n",
        "            'val_loss':       val_loss,\n",
        "            'val_accuracy':   val_acc,\n",
        "            'precision':      precision,\n",
        "            'recall':         recall,\n",
        "            'specificity':    specificity,\n",
        "            'f1_score':       f1\n",
        "        }, step=epoch)\n",
        "\n",
        "        # early stopping\n",
        "        if val_loss < best_loss:\n",
        "            best_loss = val_loss\n",
        "            es_count  = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # collect best fold metrics\n",
        "    fold_metrics['loss'].append(best_loss)\n",
        "    fold_metrics['acc'].append(val_acc)\n",
        "    fold_metrics['prec'].append(precision)\n",
        "    fold_metrics['rec'].append(recall)\n",
        "    fold_metrics['spec'].append(specificity)\n",
        "    fold_metrics['f1'].append(f1)\n",
        "\n",
        "    wandb.finish()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ─── Final 5-Fold Summary ────────────────────────────────────────\n",
        "print(\"\\n=== 5-Fold Cross-Validation Averages ===\")\n",
        "for key, vals in fold_metrics.items():\n",
        "    print(f\"{key:4s} : {np.mean(vals):.4f} ± {np.std(vals):.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6Z74-QNornl2",
        "outputId": "bc113e71-0a52-4a57-982c-99256c1f2720"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 1 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_205143-ojz5w4ch</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/ojz5w4ch' target=\"_blank\">fold1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/ojz5w4ch' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/ojz5w4ch</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7644 acc=0.4952 | val_loss=0.6976 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=72.6s\n",
            "Epoch 002 | train_loss=0.7554 acc=0.4980 | val_loss=0.6944 acc=0.4807 | prec=0.4830 rec=0.5818 spec=0.3801 f1=0.5278 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7457 acc=0.4997 | val_loss=0.6950 acc=0.4875 | prec=0.4211 rec=0.0727 spec=0.9005 f1=0.1240 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7382 acc=0.5020 | val_loss=0.6945 acc=0.4921 | prec=0.4857 rec=0.3091 spec=0.6742 f1=0.3778 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7232 acc=0.4991 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7274 acc=0.5105 | val_loss=0.6966 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7163 acc=0.5065 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7173 acc=0.4912 | val_loss=0.7001 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7278 acc=0.4889 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7118 acc=0.5128 | val_loss=0.6968 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7216 acc=0.4799 | val_loss=0.6971 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7244 acc=0.4986 | val_loss=0.6911 acc=0.5261 | prec=0.8667 rec=0.0591 spec=0.9910 f1=0.1106 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7178 acc=0.5026 | val_loss=0.6893 acc=0.5397 | prec=0.5206 rec=0.9773 spec=0.1041 f1=0.6793 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7154 acc=0.5128 | val_loss=0.6818 acc=0.6100 | prec=0.7727 rec=0.3091 spec=0.9095 f1=0.4416 | time=12.7s\n",
            "Epoch 015 | train_loss=0.6983 acc=0.5445 | val_loss=0.6713 acc=0.6712 | prec=0.8049 rec=0.4500 spec=0.8914 f1=0.5773 | time=12.8s\n",
            "Epoch 016 | train_loss=0.6952 acc=0.5502 | val_loss=0.6491 acc=0.6961 | prec=0.8071 rec=0.5136 spec=0.8778 f1=0.6278 | time=12.8s\n",
            "Epoch 017 | train_loss=0.6626 acc=0.5825 | val_loss=0.6118 acc=0.7007 | prec=0.8385 rec=0.4955 spec=0.9050 f1=0.6229 | time=12.9s\n",
            "Epoch 018 | train_loss=0.6313 acc=0.6330 | val_loss=0.5964 acc=0.7120 | prec=0.6587 rec=0.8773 spec=0.5475 f1=0.7524 | time=12.9s\n",
            "Epoch 019 | train_loss=0.6249 acc=0.6404 | val_loss=0.5591 acc=0.7596 | prec=0.8800 rec=0.6000 spec=0.9186 f1=0.7135 | time=12.8s\n",
            "Epoch 020 | train_loss=0.5926 acc=0.6773 | val_loss=0.5434 acc=0.7619 | prec=0.7106 rec=0.8818 spec=0.6425 f1=0.7870 | time=12.9s\n",
            "Epoch 021 | train_loss=0.5683 acc=0.6982 | val_loss=0.4909 acc=0.8073 | prec=0.8325 rec=0.7682 spec=0.8462 f1=0.7991 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5480 acc=0.7198 | val_loss=0.4896 acc=0.7982 | prec=0.7631 rec=0.8636 spec=0.7330 f1=0.8102 | time=12.8s\n",
            "Epoch 023 | train_loss=0.5408 acc=0.7255 | val_loss=0.4626 acc=0.8073 | prec=0.7801 rec=0.8545 spec=0.7602 f1=0.8156 | time=12.8s\n",
            "Epoch 024 | train_loss=0.5324 acc=0.7499 | val_loss=0.4675 acc=0.8095 | prec=0.8820 rec=0.7136 spec=0.9050 f1=0.7889 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5047 acc=0.7527 | val_loss=0.4567 acc=0.8118 | prec=0.7796 rec=0.8682 spec=0.7557 f1=0.8215 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5114 acc=0.7720 | val_loss=0.4343 acc=0.8186 | prec=0.8763 rec=0.7409 spec=0.8959 f1=0.8030 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5048 acc=0.7646 | val_loss=0.4149 acc=0.8345 | prec=0.8621 rec=0.7955 spec=0.8733 f1=0.8274 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4837 acc=0.7811 | val_loss=0.4011 acc=0.8413 | prec=0.8641 rec=0.8091 spec=0.8733 f1=0.8357 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4696 acc=0.7748 | val_loss=0.3964 acc=0.8299 | prec=0.8372 rec=0.8182 spec=0.8416 f1=0.8276 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4687 acc=0.7981 | val_loss=0.4001 acc=0.8549 | prec=0.8980 rec=0.8000 spec=0.9095 f1=0.8462 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4410 acc=0.7964 | val_loss=0.4042 acc=0.8345 | prec=0.9016 rec=0.7500 spec=0.9186 f1=0.8189 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4446 acc=0.8083 | val_loss=0.4090 acc=0.8231 | prec=0.9226 rec=0.7045 spec=0.9412 f1=0.7990 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4332 acc=0.8003 | val_loss=0.4011 acc=0.8322 | prec=0.7920 rec=0.9000 spec=0.7647 f1=0.8426 | time=13.0s\n",
            "Epoch 034 | train_loss=0.4457 acc=0.7964 | val_loss=0.3791 acc=0.8458 | prec=0.8551 rec=0.8318 spec=0.8597 f1=0.8433 | time=12.7s\n",
            "Epoch 035 | train_loss=0.4113 acc=0.8270 | val_loss=0.3719 acc=0.8503 | prec=0.8438 rec=0.8591 spec=0.8416 f1=0.8514 | time=12.7s\n",
            "Epoch 036 | train_loss=0.3857 acc=0.8332 | val_loss=0.3742 acc=0.8481 | prec=0.8430 rec=0.8545 spec=0.8416 f1=0.8488 | time=12.7s\n",
            "Epoch 037 | train_loss=0.4031 acc=0.8338 | val_loss=0.3767 acc=0.8345 | prec=0.8210 rec=0.8545 spec=0.8145 f1=0.8374 | time=13.0s\n",
            "Epoch 038 | train_loss=0.3762 acc=0.8417 | val_loss=0.3651 acc=0.8458 | prec=0.8585 rec=0.8273 spec=0.8643 f1=0.8426 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3727 acc=0.8406 | val_loss=0.3880 acc=0.8231 | prec=0.9226 rec=0.7045 spec=0.9412 f1=0.7990 | time=12.9s\n",
            "Epoch 040 | train_loss=0.3501 acc=0.8622 | val_loss=0.3916 acc=0.8163 | prec=0.7814 rec=0.8773 spec=0.7557 f1=0.8266 | time=12.8s\n",
            "Epoch 041 | train_loss=0.3533 acc=0.8571 | val_loss=0.3585 acc=0.8549 | prec=0.8645 rec=0.8409 spec=0.8688 f1=0.8525 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3488 acc=0.8571 | val_loss=0.3741 acc=0.8345 | prec=0.8267 rec=0.8455 spec=0.8235 f1=0.8360 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3310 acc=0.8656 | val_loss=0.3536 acc=0.8571 | prec=0.8792 rec=0.8273 spec=0.8869 f1=0.8525 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3505 acc=0.8429 | val_loss=0.3639 acc=0.8435 | prec=0.9126 rec=0.7591 spec=0.9276 f1=0.8288 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3201 acc=0.8729 | val_loss=0.3653 acc=0.8413 | prec=0.8947 rec=0.7727 spec=0.9095 f1=0.8293 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3398 acc=0.8520 | val_loss=0.3558 acc=0.8458 | prec=0.9043 rec=0.7727 spec=0.9186 f1=0.8333 | time=12.9s\n",
            "Epoch 047 | train_loss=0.3269 acc=0.8735 | val_loss=0.3544 acc=0.8458 | prec=0.8725 rec=0.8091 spec=0.8824 f1=0.8396 | time=12.7s\n",
            "Epoch 048 | train_loss=0.2930 acc=0.8860 | val_loss=0.3583 acc=0.8458 | prec=0.8762 rec=0.8045 spec=0.8869 f1=0.8389 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3027 acc=0.8707 | val_loss=0.3713 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2900 acc=0.8843 | val_loss=0.3650 acc=0.8458 | prec=0.8689 rec=0.8136 spec=0.8778 f1=0.8404 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2822 acc=0.8934 | val_loss=0.3625 acc=0.8435 | prec=0.8995 rec=0.7727 spec=0.9140 f1=0.8313 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2699 acc=0.9013 | val_loss=0.3780 acc=0.8367 | prec=0.8394 rec=0.8318 spec=0.8416 f1=0.8356 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2815 acc=0.8917 | val_loss=0.3569 acc=0.8345 | prec=0.8356 rec=0.8318 spec=0.8371 f1=0.8337 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2879 acc=0.8888 | val_loss=0.3803 acc=0.8413 | prec=0.8319 rec=0.8545 spec=0.8281 f1=0.8430 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2560 acc=0.8939 | val_loss=0.3580 acc=0.8481 | prec=0.8806 rec=0.8045 spec=0.8914 f1=0.8409 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2728 acc=0.8877 | val_loss=0.3651 acc=0.8345 | prec=0.9061 rec=0.7455 spec=0.9231 f1=0.8180 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2495 acc=0.9002 | val_loss=0.3516 acc=0.8662 | prec=0.8966 rec=0.8273 spec=0.9050 f1=0.8605 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2904 acc=0.8758 | val_loss=0.3563 acc=0.8435 | prec=0.8953 rec=0.7773 spec=0.9095 f1=0.8321 | time=13.0s\n",
            "Epoch 059 | train_loss=0.2709 acc=0.8820 | val_loss=0.3652 acc=0.8435 | prec=0.8612 rec=0.8182 spec=0.8688 f1=0.8392 | time=12.8s\n",
            "Epoch 060 | train_loss=0.2387 acc=0.9087 | val_loss=0.3691 acc=0.8390 | prec=0.8782 rec=0.7864 spec=0.8914 f1=0.8297 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2796 acc=0.8837 | val_loss=0.3612 acc=0.8503 | prec=0.8738 rec=0.8182 spec=0.8824 f1=0.8451 | time=12.8s\n",
            "Epoch 062 | train_loss=0.2261 acc=0.9132 | val_loss=0.3720 acc=0.8435 | prec=0.8872 rec=0.7864 spec=0.9005 f1=0.8337 | time=12.9s\n",
            "Epoch 063 | train_loss=0.2271 acc=0.9138 | val_loss=0.3744 acc=0.8390 | prec=0.9027 rec=0.7591 spec=0.9186 f1=0.8247 | time=12.8s\n",
            "Epoch 064 | train_loss=0.2480 acc=0.9115 | val_loss=0.3784 acc=0.8367 | prec=0.9066 rec=0.7500 spec=0.9231 f1=0.8209 | time=13.0s\n",
            "Epoch 065 | train_loss=0.2346 acc=0.9070 | val_loss=0.3577 acc=0.8481 | prec=0.8768 rec=0.8091 spec=0.8869 f1=0.8416 | time=12.8s\n",
            "Epoch 066 | train_loss=0.2224 acc=0.9064 | val_loss=0.3585 acc=0.8458 | prec=0.8762 rec=0.8045 spec=0.8869 f1=0.8389 | time=12.9s\n",
            "Epoch 067 | train_loss=0.2233 acc=0.9115 | val_loss=0.3877 acc=0.8299 | prec=0.8877 rec=0.7545 spec=0.9050 f1=0.8157 | time=12.8s\n",
            "Epoch 068 | train_loss=0.2526 acc=0.9024 | val_loss=0.3890 acc=0.8345 | prec=0.8101 rec=0.8727 spec=0.7964 f1=0.8403 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2220 acc=0.9138 | val_loss=0.3621 acc=0.8367 | prec=0.8737 rec=0.7864 spec=0.8869 f1=0.8278 | time=12.8s\n",
            "Epoch 070 | train_loss=0.2274 acc=0.9036 | val_loss=0.3804 acc=0.8345 | prec=0.8101 rec=0.8727 spec=0.7964 f1=0.8403 | time=12.8s\n",
            "Epoch 071 | train_loss=0.1952 acc=0.9365 | val_loss=0.3793 acc=0.8435 | prec=0.8447 rec=0.8409 spec=0.8462 f1=0.8428 | time=12.9s\n",
            "Epoch 072 | train_loss=0.2468 acc=0.8894 | val_loss=0.3588 acc=0.8322 | prec=0.8883 rec=0.7591 spec=0.9050 f1=0.8186 | time=12.9s\n",
            "Early stopping at epoch 72\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▆▅▂▁▁▁▁▁▂▇▇█████████████████████████████</td></tr><tr><td>precision</td><td>▅▅▄▁▁▁▁█▅▇▇▆▇▇███▇▇▇█▇▇██████▇█████████▇</td></tr><tr><td>recall</td><td>█▂▁▁▁█▃▄▅▄▆▇▇▆▇▇▇▇▇▇▇▇▆▇▇▆▆▇▇▆▇▆▇▇▇▇▇▆▇▆</td></tr><tr><td>specificity</td><td>▁▄▇████▇▇▇▇▅▇▆▇█▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█████▇██</td></tr><tr><td>train_loss</td><td>███████▇▆▆▅▅▅▄▄▄▄▄▄▄▃▃▂▂▃▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▂▂▅▆▇▇▇▇█████████▇███████████████</td></tr><tr><td>val_loss</td><td>███████▇▇▆▅▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.81863</td></tr><tr><td>precision</td><td>0.8883</td></tr><tr><td>recall</td><td>0.75909</td></tr><tr><td>specificity</td><td>0.90498</td></tr><tr><td>train_accuracy</td><td>0.88939</td></tr><tr><td>train_loss</td><td>0.24679</td></tr><tr><td>val_accuracy</td><td>0.8322</td></tr><tr><td>val_loss</td><td>0.35878</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/ojz5w4ch' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/ojz5w4ch</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_205143-ojz5w4ch/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 2 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_210812-axzuoqcm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/axzuoqcm' target=\"_blank\">fold2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/axzuoqcm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/axzuoqcm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7757 acc=0.4793 | val_loss=0.7227 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7323 acc=0.5082 | val_loss=0.7067 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7354 acc=0.5020 | val_loss=0.7065 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7393 acc=0.4765 | val_loss=0.7032 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7136 acc=0.5048 | val_loss=0.6993 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7143 acc=0.5060 | val_loss=0.7004 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7160 acc=0.4844 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7206 acc=0.4935 | val_loss=0.6977 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 009 | train_loss=0.7190 acc=0.4912 | val_loss=0.6968 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7149 acc=0.5116 | val_loss=0.7002 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7152 acc=0.5054 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7219 acc=0.4742 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7113 acc=0.5355 | val_loss=0.6973 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7073 acc=0.5054 | val_loss=0.6913 acc=0.5760 | prec=0.5600 rec=0.7000 spec=0.4525 f1=0.6222 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7178 acc=0.4912 | val_loss=0.6987 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7031 acc=0.5207 | val_loss=0.6897 acc=0.5079 | prec=1.0000 rec=0.0136 spec=1.0000 f1=0.0269 | time=12.8s\n",
            "Epoch 017 | train_loss=0.6854 acc=0.5479 | val_loss=0.6761 acc=0.5306 | prec=0.9333 rec=0.0636 spec=0.9955 f1=0.1191 | time=12.7s\n",
            "Epoch 018 | train_loss=0.6343 acc=0.6341 | val_loss=0.6808 acc=0.5601 | prec=0.9333 rec=0.1273 spec=0.9910 f1=0.2240 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5856 acc=0.6920 | val_loss=0.5514 acc=0.7370 | prec=0.7097 rec=0.8000 spec=0.6742 f1=0.7521 | time=12.9s\n",
            "Epoch 020 | train_loss=0.5763 acc=0.6880 | val_loss=0.5399 acc=0.7506 | prec=0.8438 rec=0.6136 spec=0.8869 f1=0.7105 | time=12.8s\n",
            "Epoch 021 | train_loss=0.5653 acc=0.7028 | val_loss=0.5409 acc=0.7710 | prec=0.8148 rec=0.7000 spec=0.8416 f1=0.7531 | time=12.9s\n",
            "Epoch 022 | train_loss=0.5414 acc=0.7413 | val_loss=0.5257 acc=0.7642 | prec=0.8452 rec=0.6455 spec=0.8824 f1=0.7320 | time=12.8s\n",
            "Epoch 023 | train_loss=0.5306 acc=0.7584 | val_loss=0.5175 acc=0.7914 | prec=0.8137 rec=0.7545 spec=0.8281 f1=0.7830 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5274 acc=0.7595 | val_loss=0.5129 acc=0.7846 | prec=0.8272 rec=0.7182 spec=0.8507 f1=0.7689 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5063 acc=0.7640 | val_loss=0.5005 acc=0.7823 | prec=0.8333 rec=0.7045 spec=0.8597 f1=0.7635 | time=13.0s\n",
            "Epoch 026 | train_loss=0.4663 acc=0.7992 | val_loss=0.4871 acc=0.7891 | prec=0.7725 rec=0.8182 spec=0.7602 f1=0.7947 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4704 acc=0.7930 | val_loss=0.4776 acc=0.7914 | prec=0.7759 rec=0.8182 spec=0.7647 f1=0.7965 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4580 acc=0.7998 | val_loss=0.4924 acc=0.7619 | prec=0.8485 rec=0.6364 spec=0.8869 f1=0.7273 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4393 acc=0.8088 | val_loss=0.4701 acc=0.8027 | prec=0.7930 rec=0.8182 spec=0.7873 f1=0.8054 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4417 acc=0.8191 | val_loss=0.4617 acc=0.7959 | prec=0.7902 rec=0.8045 spec=0.7873 f1=0.7973 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4180 acc=0.8247 | val_loss=0.4591 acc=0.7891 | prec=0.8038 rec=0.7636 spec=0.8145 f1=0.7832 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4025 acc=0.8287 | val_loss=0.4546 acc=0.7891 | prec=0.8038 rec=0.7636 spec=0.8145 f1=0.7832 | time=12.8s\n",
            "Epoch 033 | train_loss=0.3884 acc=0.8344 | val_loss=0.4453 acc=0.7891 | prec=0.7822 rec=0.8000 spec=0.7783 f1=0.7910 | time=12.9s\n",
            "Epoch 034 | train_loss=0.3764 acc=0.8440 | val_loss=0.4503 acc=0.7891 | prec=0.7613 rec=0.8409 spec=0.7376 f1=0.7991 | time=13.0s\n",
            "Epoch 035 | train_loss=0.3721 acc=0.8531 | val_loss=0.4752 acc=0.7823 | prec=0.8605 rec=0.6727 spec=0.8914 f1=0.7551 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3905 acc=0.8389 | val_loss=0.4442 acc=0.7914 | prec=0.8107 rec=0.7591 spec=0.8235 f1=0.7840 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3591 acc=0.8576 | val_loss=0.4451 acc=0.8050 | prec=0.8018 rec=0.8091 spec=0.8009 f1=0.8054 | time=12.7s\n",
            "Epoch 038 | train_loss=0.3692 acc=0.8588 | val_loss=0.4521 acc=0.7891 | prec=0.8360 rec=0.7182 spec=0.8597 f1=0.7726 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3548 acc=0.8605 | val_loss=0.4396 acc=0.8095 | prec=0.7957 rec=0.8318 spec=0.7873 f1=0.8133 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3521 acc=0.8599 | val_loss=0.4578 acc=0.7891 | prec=0.8159 rec=0.7455 spec=0.8326 f1=0.7791 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3770 acc=0.8474 | val_loss=0.4697 acc=0.7732 | prec=0.7326 rec=0.8591 spec=0.6878 f1=0.7908 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3319 acc=0.8695 | val_loss=0.4489 acc=0.8005 | prec=0.8143 rec=0.7773 spec=0.8235 f1=0.7953 | time=13.1s\n",
            "Epoch 043 | train_loss=0.3165 acc=0.8786 | val_loss=0.4613 acc=0.7846 | prec=0.8307 rec=0.7136 spec=0.8552 f1=0.7677 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3254 acc=0.8622 | val_loss=0.4330 acc=0.8095 | prec=0.8063 rec=0.8136 spec=0.8054 f1=0.8100 | time=13.0s\n",
            "Epoch 045 | train_loss=0.3090 acc=0.8792 | val_loss=0.4388 acc=0.8163 | prec=0.8062 rec=0.8318 spec=0.8009 f1=0.8188 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2968 acc=0.8769 | val_loss=0.4394 acc=0.8118 | prec=0.8072 rec=0.8182 spec=0.8054 f1=0.8126 | time=12.9s\n",
            "Epoch 047 | train_loss=0.2872 acc=0.8780 | val_loss=0.4524 acc=0.7959 | prec=0.7642 rec=0.8545 spec=0.7376 f1=0.8069 | time=12.9s\n",
            "Epoch 048 | train_loss=0.2713 acc=0.9041 | val_loss=0.4404 acc=0.8050 | prec=0.7965 rec=0.8182 spec=0.7919 f1=0.8072 | time=12.8s\n",
            "Epoch 049 | train_loss=0.2970 acc=0.8956 | val_loss=0.4661 acc=0.7800 | prec=0.7303 rec=0.8864 spec=0.6742 f1=0.8008 | time=12.8s\n",
            "Epoch 050 | train_loss=0.2804 acc=0.8820 | val_loss=0.4526 acc=0.7937 | prec=0.7510 rec=0.8773 spec=0.7104 f1=0.8092 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2801 acc=0.8922 | val_loss=0.4520 acc=0.7982 | prec=0.8134 rec=0.7727 spec=0.8235 f1=0.7925 | time=13.1s\n",
            "Epoch 052 | train_loss=0.2632 acc=0.9064 | val_loss=0.5109 acc=0.7642 | prec=0.8452 rec=0.6455 spec=0.8824 f1=0.7320 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2833 acc=0.8996 | val_loss=0.4617 acc=0.7914 | prec=0.8232 rec=0.7409 spec=0.8416 f1=0.7799 | time=13.0s\n",
            "Epoch 054 | train_loss=0.2530 acc=0.9081 | val_loss=0.5159 acc=0.7755 | prec=0.8580 rec=0.6591 spec=0.8914 f1=0.7455 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2809 acc=0.8934 | val_loss=0.4578 acc=0.8095 | prec=0.8148 rec=0.8000 spec=0.8190 f1=0.8073 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2617 acc=0.9041 | val_loss=0.4699 acc=0.7891 | prec=0.8360 rec=0.7182 spec=0.8597 f1=0.7726 | time=12.8s\n",
            "Epoch 057 | train_loss=0.2669 acc=0.8968 | val_loss=0.4782 acc=0.7800 | prec=0.7321 rec=0.8818 spec=0.6787 f1=0.8000 | time=12.9s\n",
            "Epoch 058 | train_loss=0.2460 acc=0.9024 | val_loss=0.4582 acc=0.7959 | prec=0.7778 rec=0.8273 spec=0.7647 f1=0.8018 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2330 acc=0.9104 | val_loss=0.4722 acc=0.7891 | prec=0.8098 rec=0.7545 spec=0.8235 f1=0.7812 | time=12.9s\n",
            "Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▆▁▁▂▃▇▇███████████████████▇███</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▅▁█▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▆▇▇▇▇▇▆▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▇▁▂▇▆▇▇▇▇▆▇▇▇█▆▇▇▇█▇▇▇▇█▇██▆▆▇█▇</td></tr><tr><td>specificity</td><td>██████████████▁▅▅▄▅▅▆▃▃▄▃▆▅▄▅▄▄▂▄▂▄▆▄▅▁▄</td></tr><tr><td>train_accuracy</td><td>▂▁▁▁▂▁▁▂▂▁▁▂▂▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇████████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▁▂▁▂▂▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▃▂▂▆▇▇▇▇▇▇▇██▇▇▇▇█▇▇████▇▇▇█▇▇▇</td></tr><tr><td>val_loss</td><td>██▇▇▇▇▇▇▇▇▃▄▃▃▃▂▂▂▂▂▁▁▂▁▁▁▂▂▁▁▁▂▁▁▃▃▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.78118</td></tr><tr><td>precision</td><td>0.80976</td></tr><tr><td>recall</td><td>0.75455</td></tr><tr><td>specificity</td><td>0.82353</td></tr><tr><td>train_accuracy</td><td>0.91038</td></tr><tr><td>train_loss</td><td>0.23303</td></tr><tr><td>val_accuracy</td><td>0.78912</td></tr><tr><td>val_loss</td><td>0.47222</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/axzuoqcm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/axzuoqcm</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_210812-axzuoqcm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 3 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_212054-izd04a6k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/izd04a6k' target=\"_blank\">fold3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/izd04a6k' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/izd04a6k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7423 acc=0.4884 | val_loss=0.6946 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7319 acc=0.4969 | val_loss=0.6943 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7294 acc=0.5043 | val_loss=0.6943 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7140 acc=0.5105 | val_loss=0.6948 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7188 acc=0.5060 | val_loss=0.6945 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7215 acc=0.4997 | val_loss=0.6946 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7228 acc=0.4980 | val_loss=0.6944 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7131 acc=0.5077 | val_loss=0.6957 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7055 acc=0.5099 | val_loss=0.6937 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7143 acc=0.4901 | val_loss=0.6936 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7139 acc=0.5020 | val_loss=0.6949 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7020 acc=0.5355 | val_loss=0.6941 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7167 acc=0.5043 | val_loss=0.6933 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 014 | train_loss=0.7179 acc=0.4935 | val_loss=0.6942 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7123 acc=0.4946 | val_loss=0.6930 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7116 acc=0.5094 | val_loss=0.6940 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7096 acc=0.5054 | val_loss=0.6902 acc=0.5442 | prec=0.8333 rec=0.1131 spec=0.9773 f1=0.1992 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7001 acc=0.5162 | val_loss=0.6898 acc=0.5011 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.0s\n",
            "Epoch 019 | train_loss=0.6893 acc=0.5434 | val_loss=0.6937 acc=0.5057 | prec=1.0000 rec=0.0136 spec=1.0000 f1=0.0268 | time=12.9s\n",
            "Epoch 020 | train_loss=0.6661 acc=0.5973 | val_loss=0.6261 acc=0.7098 | prec=0.8843 rec=0.4842 spec=0.9364 f1=0.6257 | time=12.9s\n",
            "Epoch 021 | train_loss=0.6367 acc=0.6398 | val_loss=0.5715 acc=0.7687 | prec=0.8400 rec=0.6652 spec=0.8727 f1=0.7424 | time=13.0s\n",
            "Epoch 022 | train_loss=0.5860 acc=0.7011 | val_loss=0.5415 acc=0.7868 | prec=0.8360 rec=0.7149 spec=0.8591 f1=0.7707 | time=12.8s\n",
            "Epoch 023 | train_loss=0.5406 acc=0.7272 | val_loss=0.5119 acc=0.7891 | prec=0.8441 rec=0.7104 spec=0.8682 f1=0.7715 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5493 acc=0.7487 | val_loss=0.4853 acc=0.7868 | prec=0.7981 rec=0.7692 spec=0.8045 f1=0.7834 | time=13.0s\n",
            "Epoch 025 | train_loss=0.5153 acc=0.7691 | val_loss=0.4835 acc=0.7732 | prec=0.7553 rec=0.8100 spec=0.7364 f1=0.7817 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4994 acc=0.7646 | val_loss=0.4744 acc=0.7959 | prec=0.8701 rec=0.6968 spec=0.8955 f1=0.7739 | time=12.8s\n",
            "Epoch 027 | train_loss=0.4897 acc=0.7737 | val_loss=0.4503 acc=0.8254 | prec=0.8214 rec=0.8326 spec=0.8182 f1=0.8270 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4753 acc=0.7986 | val_loss=0.4580 acc=0.8186 | prec=0.8852 rec=0.7330 spec=0.9045 f1=0.8020 | time=12.9s\n",
            "Epoch 029 | train_loss=0.4411 acc=0.8032 | val_loss=0.4415 acc=0.8390 | prec=0.8827 rec=0.7828 spec=0.8955 f1=0.8297 | time=13.0s\n",
            "Epoch 030 | train_loss=0.4444 acc=0.7981 | val_loss=0.4132 acc=0.8458 | prec=0.8768 rec=0.8054 spec=0.8864 f1=0.8396 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4343 acc=0.8100 | val_loss=0.4371 acc=0.8277 | prec=0.8877 rec=0.7511 spec=0.9045 f1=0.8137 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4407 acc=0.8077 | val_loss=0.4149 acc=0.8458 | prec=0.8806 rec=0.8009 spec=0.8909 f1=0.8389 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4216 acc=0.8242 | val_loss=0.4033 acc=0.8413 | prec=0.8612 rec=0.8145 spec=0.8682 f1=0.8372 | time=13.0s\n",
            "Epoch 034 | train_loss=0.3984 acc=0.8321 | val_loss=0.4181 acc=0.8435 | prec=0.9176 rec=0.7557 spec=0.9318 f1=0.8288 | time=12.9s\n",
            "Epoch 035 | train_loss=0.3831 acc=0.8355 | val_loss=0.3871 acc=0.8390 | prec=0.8827 rec=0.7828 spec=0.8955 f1=0.8297 | time=13.0s\n",
            "Epoch 036 | train_loss=0.3973 acc=0.8497 | val_loss=0.3763 acc=0.8549 | prec=0.8618 rec=0.8462 spec=0.8636 f1=0.8539 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3715 acc=0.8559 | val_loss=0.3817 acc=0.8526 | prec=0.8578 rec=0.8462 spec=0.8591 f1=0.8519 | time=12.9s\n",
            "Epoch 038 | train_loss=0.3763 acc=0.8503 | val_loss=0.3808 acc=0.8594 | prec=0.8916 rec=0.8190 spec=0.9000 f1=0.8538 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3594 acc=0.8565 | val_loss=0.3787 acc=0.8549 | prec=0.8651 rec=0.8416 spec=0.8682 f1=0.8532 | time=12.9s\n",
            "Epoch 040 | train_loss=0.3561 acc=0.8554 | val_loss=0.4212 acc=0.8277 | prec=0.9341 rec=0.7059 spec=0.9500 f1=0.8041 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3490 acc=0.8746 | val_loss=0.4345 acc=0.8118 | prec=0.9481 rec=0.6606 spec=0.9636 f1=0.7787 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3343 acc=0.8718 | val_loss=0.3943 acc=0.8299 | prec=0.9056 rec=0.7376 spec=0.9227 f1=0.8130 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3168 acc=0.8837 | val_loss=0.3794 acc=0.8503 | prec=0.8414 rec=0.8643 spec=0.8364 f1=0.8527 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3259 acc=0.8746 | val_loss=0.3812 acc=0.8322 | prec=0.8889 rec=0.7602 spec=0.9045 f1=0.8195 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3347 acc=0.8650 | val_loss=0.3921 acc=0.8322 | prec=0.9016 rec=0.7466 spec=0.9182 f1=0.8168 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3043 acc=0.8866 | val_loss=0.3685 acc=0.8458 | prec=0.8525 rec=0.8371 spec=0.8545 f1=0.8447 | time=12.9s\n",
            "Epoch 047 | train_loss=0.2989 acc=0.8951 | val_loss=0.4070 acc=0.8299 | prec=0.9195 rec=0.7240 spec=0.9364 f1=0.8101 | time=12.9s\n",
            "Epoch 048 | train_loss=0.2837 acc=0.9041 | val_loss=0.3900 acc=0.8367 | prec=0.9027 rec=0.7557 spec=0.9182 f1=0.8227 | time=12.8s\n",
            "Epoch 049 | train_loss=0.2814 acc=0.8968 | val_loss=0.3751 acc=0.8345 | prec=0.8737 rec=0.7828 spec=0.8864 f1=0.8258 | time=12.9s\n",
            "Epoch 050 | train_loss=0.2966 acc=0.8917 | val_loss=0.3830 acc=0.8458 | prec=0.8122 rec=0.9005 spec=0.7909 f1=0.8541 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2834 acc=0.9013 | val_loss=0.3718 acc=0.8481 | prec=0.8775 rec=0.8100 spec=0.8864 f1=0.8424 | time=13.0s\n",
            "Epoch 052 | train_loss=0.2783 acc=0.8939 | val_loss=0.3853 acc=0.8299 | prec=0.8842 rec=0.7602 spec=0.9000 f1=0.8175 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2618 acc=0.9144 | val_loss=0.3785 acc=0.8435 | prec=0.8654 rec=0.8145 spec=0.8727 f1=0.8392 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2798 acc=0.8928 | val_loss=0.4435 acc=0.7959 | prec=0.9338 rec=0.6380 spec=0.9545 f1=0.7581 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2571 acc=0.9024 | val_loss=0.3683 acc=0.8458 | prec=0.8660 rec=0.8190 spec=0.8727 f1=0.8419 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2553 acc=0.9007 | val_loss=0.3891 acc=0.8367 | prec=0.9209 rec=0.7376 spec=0.9364 f1=0.8191 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2498 acc=0.9007 | val_loss=0.4049 acc=0.8277 | prec=0.9240 rec=0.7149 spec=0.9409 f1=0.8061 | time=13.0s\n",
            "Epoch 058 | train_loss=0.2550 acc=0.8979 | val_loss=0.4739 acc=0.7914 | prec=0.9510 rec=0.6154 spec=0.9682 f1=0.7473 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2350 acc=0.9144 | val_loss=0.3711 acc=0.8435 | prec=0.8654 rec=0.8145 spec=0.8727 f1=0.8392 | time=12.9s\n",
            "Epoch 060 | train_loss=0.2252 acc=0.9144 | val_loss=0.3597 acc=0.8549 | prec=0.8552 rec=0.8552 spec=0.8545 f1=0.8552 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2692 acc=0.9058 | val_loss=0.4106 acc=0.8322 | prec=0.9249 rec=0.7240 spec=0.9409 f1=0.8122 | time=12.9s\n",
            "Epoch 062 | train_loss=0.2377 acc=0.9087 | val_loss=0.3604 acc=0.8503 | prec=0.8744 rec=0.8190 spec=0.8818 f1=0.8458 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2740 acc=0.8945 | val_loss=0.3925 acc=0.8435 | prec=0.9222 rec=0.7511 spec=0.9364 f1=0.8279 | time=12.9s\n",
            "Epoch 064 | train_loss=0.2372 acc=0.9161 | val_loss=0.3590 acc=0.8503 | prec=0.8507 rec=0.8507 spec=0.8500 f1=0.8507 | time=12.8s\n",
            "Epoch 065 | train_loss=0.2239 acc=0.9183 | val_loss=0.4014 acc=0.8413 | prec=0.9266 rec=0.7421 spec=0.9409 f1=0.8241 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2111 acc=0.9183 | val_loss=0.3648 acc=0.8503 | prec=0.8974 rec=0.7919 spec=0.9091 f1=0.8413 | time=12.8s\n",
            "Epoch 067 | train_loss=0.2203 acc=0.9206 | val_loss=0.4320 acc=0.8345 | prec=0.9253 rec=0.7285 spec=0.9409 f1=0.8152 | time=12.9s\n",
            "Epoch 068 | train_loss=0.2172 acc=0.9053 | val_loss=0.3676 acc=0.8435 | prec=0.8304 rec=0.8643 spec=0.8227 f1=0.8470 | time=12.9s\n",
            "Epoch 069 | train_loss=0.2052 acc=0.9178 | val_loss=0.3739 acc=0.8503 | prec=0.8894 rec=0.8009 spec=0.9000 f1=0.8429 | time=12.8s\n",
            "Epoch 070 | train_loss=0.2096 acc=0.9240 | val_loss=0.4518 acc=0.8186 | prec=0.9273 rec=0.6923 spec=0.9455 f1=0.7927 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2141 acc=0.9178 | val_loss=0.3845 acc=0.8458 | prec=0.8493 rec=0.8416 spec=0.8500 f1=0.8455 | time=12.9s\n",
            "Epoch 072 | train_loss=0.1937 acc=0.9280 | val_loss=0.3952 acc=0.8322 | prec=0.8326 rec=0.8326 spec=0.8318 f1=0.8326 | time=12.9s\n",
            "Epoch 073 | train_loss=0.1893 acc=0.9297 | val_loss=0.4298 acc=0.8277 | prec=0.9096 rec=0.7285 spec=0.9273 f1=0.8090 | time=13.0s\n",
            "Epoch 074 | train_loss=0.2176 acc=0.9155 | val_loss=0.3968 acc=0.8481 | prec=0.8889 rec=0.7964 spec=0.9000 f1=0.8401 | time=12.8s\n",
            "Epoch 075 | train_loss=0.1938 acc=0.9280 | val_loss=0.3864 acc=0.8413 | prec=0.8545 rec=0.8235 spec=0.8591 f1=0.8387 | time=13.0s\n",
            "Epoch 076 | train_loss=0.2004 acc=0.9183 | val_loss=0.4182 acc=0.8413 | prec=0.9037 rec=0.7647 spec=0.9182 f1=0.8284 | time=12.9s\n",
            "Epoch 077 | train_loss=0.1789 acc=0.9297 | val_loss=0.3904 acc=0.8435 | prec=0.8689 rec=0.8100 spec=0.8773 f1=0.8384 | time=12.9s\n",
            "Epoch 078 | train_loss=0.1868 acc=0.9223 | val_loss=0.3893 acc=0.8299 | prec=0.8147 rec=0.8552 spec=0.8045 f1=0.8344 | time=12.9s\n",
            "Epoch 079 | train_loss=0.2164 acc=0.9280 | val_loss=0.4237 acc=0.8413 | prec=0.9126 rec=0.7557 spec=0.9273 f1=0.8267 | time=12.8s\n",
            "Early stopping at epoch 79\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▆▇▇██████████████▇██████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▇█▇▇▇▇▇▇█▇▇███▇█████▇███████▇▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▅▇▇▇▇▇▇▇▇█▆▇█▇█▇▆▇▆▇▇▇▇▇▇▇▆█▇▇</td></tr><tr><td>specificity</td><td>██████████▇▅▁▃▅▅▄▇▇▆▅▅▅▇▅▅▄▆▅▆▆▆▃▅▇▅▄▆▅▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▂▁▁▂▄▅▅▆▆▆▆▇▇▇▇▇▇▇██████████████</td></tr><tr><td>train_loss</td><td>████████▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▅▇▇▇███████▇▇███████████████▇█</td></tr><tr><td>val_loss</td><td>████████████▇▄▃▃▂▂▂▂▂▃▂▁▁▂▃▁▂▃▁▂▃▁▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.82673</td></tr><tr><td>precision</td><td>0.91257</td></tr><tr><td>recall</td><td>0.75566</td></tr><tr><td>specificity</td><td>0.92727</td></tr><tr><td>train_accuracy</td><td>0.92796</td></tr><tr><td>train_loss</td><td>0.21637</td></tr><tr><td>val_accuracy</td><td>0.84127</td></tr><tr><td>val_loss</td><td>0.42371</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/izd04a6k' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/izd04a6k</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_212054-izd04a6k/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 4 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_213757-88asreh0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/88asreh0' target=\"_blank\">fold4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/88asreh0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/88asreh0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7826 acc=0.4901 | val_loss=0.7215 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7678 acc=0.5122 | val_loss=0.7168 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7584 acc=0.5105 | val_loss=0.7231 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7496 acc=0.4986 | val_loss=0.7208 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7477 acc=0.4872 | val_loss=0.7164 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7301 acc=0.5014 | val_loss=0.6929 acc=0.5011 | prec=0.6000 rec=0.0136 spec=0.9909 f1=0.0265 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7391 acc=0.4935 | val_loss=0.6930 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7469 acc=0.4884 | val_loss=0.6940 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7304 acc=0.5105 | val_loss=0.6925 acc=0.5261 | prec=0.5170 rec=0.8235 spec=0.2273 f1=0.6353 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7343 acc=0.5020 | val_loss=0.6974 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7317 acc=0.5009 | val_loss=0.6979 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7206 acc=0.4935 | val_loss=0.6968 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7284 acc=0.4861 | val_loss=0.6931 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7379 acc=0.4776 | val_loss=0.6921 acc=0.5147 | prec=0.5614 rec=0.1448 spec=0.8864 f1=0.2302 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7180 acc=0.4997 | val_loss=0.6930 acc=0.4989 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7179 acc=0.4901 | val_loss=0.6918 acc=0.5306 | prec=0.5361 rec=0.4706 spec=0.5909 f1=0.5012 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7076 acc=0.5133 | val_loss=0.6909 acc=0.5646 | prec=0.5986 rec=0.3982 spec=0.7318 f1=0.4783 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7025 acc=0.5269 | val_loss=0.6917 acc=0.5034 | prec=0.5023 rec=1.0000 spec=0.0045 f1=0.6687 | time=13.0s\n",
            "Epoch 019 | train_loss=0.7132 acc=0.5213 | val_loss=0.6864 acc=0.5170 | prec=1.0000 rec=0.0362 spec=1.0000 f1=0.0699 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7097 acc=0.5349 | val_loss=0.6713 acc=0.6122 | prec=0.7451 rec=0.3439 spec=0.8818 f1=0.4706 | time=13.0s\n",
            "Epoch 021 | train_loss=0.6673 acc=0.5837 | val_loss=0.6423 acc=0.6621 | prec=0.7466 rec=0.4932 spec=0.8318 f1=0.5940 | time=12.9s\n",
            "Epoch 022 | train_loss=0.6392 acc=0.6234 | val_loss=0.6005 acc=0.6939 | prec=0.6378 rec=0.9005 spec=0.4864 f1=0.7467 | time=12.7s\n",
            "Epoch 023 | train_loss=0.6186 acc=0.6461 | val_loss=0.5641 acc=0.7596 | prec=0.7203 rec=0.8507 spec=0.6682 f1=0.7801 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5961 acc=0.6727 | val_loss=0.5627 acc=0.7143 | prec=0.6518 rec=0.9231 spec=0.5045 f1=0.7640 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5595 acc=0.7011 | val_loss=0.5089 acc=0.7642 | prec=0.8462 rec=0.6471 spec=0.8818 f1=0.7333 | time=13.1s\n",
            "Epoch 026 | train_loss=0.5433 acc=0.7294 | val_loss=0.5191 acc=0.7528 | prec=0.8784 rec=0.5882 spec=0.9182 f1=0.7046 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5521 acc=0.7249 | val_loss=0.5023 acc=0.7778 | prec=0.7412 rec=0.8552 spec=0.7000 f1=0.7941 | time=13.1s\n",
            "Epoch 028 | train_loss=0.5131 acc=0.7555 | val_loss=0.4713 acc=0.7868 | prec=0.7848 rec=0.7919 spec=0.7818 f1=0.7883 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4852 acc=0.7663 | val_loss=0.4702 acc=0.7823 | prec=0.7828 rec=0.7828 spec=0.7818 f1=0.7828 | time=12.7s\n",
            "Epoch 030 | train_loss=0.4883 acc=0.7516 | val_loss=0.4705 acc=0.7937 | prec=0.8571 rec=0.7059 spec=0.8818 f1=0.7742 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4850 acc=0.7720 | val_loss=0.4559 acc=0.7959 | prec=0.8075 rec=0.7783 spec=0.8136 f1=0.7926 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4784 acc=0.7714 | val_loss=0.4504 acc=0.7891 | prec=0.8333 rec=0.7240 spec=0.8545 f1=0.7748 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4467 acc=0.8037 | val_loss=0.4459 acc=0.8073 | prec=0.8579 rec=0.7376 spec=0.8773 f1=0.7932 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4388 acc=0.7873 | val_loss=0.4456 acc=0.7982 | prec=0.7920 rec=0.8100 spec=0.7864 f1=0.8009 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4124 acc=0.8191 | val_loss=0.4384 acc=0.8050 | prec=0.8140 rec=0.7919 spec=0.8182 f1=0.8028 | time=12.9s\n",
            "Epoch 036 | train_loss=0.4052 acc=0.8202 | val_loss=0.4639 acc=0.7868 | prec=0.7613 rec=0.8371 spec=0.7364 f1=0.7974 | time=12.8s\n",
            "Epoch 037 | train_loss=0.4116 acc=0.8310 | val_loss=0.4615 acc=0.8005 | prec=0.8982 rec=0.6787 spec=0.9227 f1=0.7732 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3857 acc=0.8383 | val_loss=0.4464 acc=0.7982 | prec=0.7727 rec=0.8462 spec=0.7500 f1=0.8078 | time=12.8s\n",
            "Epoch 039 | train_loss=0.4099 acc=0.8230 | val_loss=0.4220 acc=0.8005 | prec=0.7879 rec=0.8235 spec=0.7773 f1=0.8053 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3916 acc=0.8332 | val_loss=0.4366 acc=0.8073 | prec=0.8953 rec=0.6968 spec=0.9182 f1=0.7837 | time=12.9s\n",
            "Epoch 041 | train_loss=0.4083 acc=0.8185 | val_loss=0.4099 acc=0.8277 | prec=0.8341 rec=0.8190 spec=0.8364 f1=0.8265 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3620 acc=0.8480 | val_loss=0.4110 acc=0.8186 | prec=0.8106 rec=0.8326 spec=0.8045 f1=0.8214 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3494 acc=0.8582 | val_loss=0.4235 acc=0.8050 | prec=0.8169 rec=0.7873 spec=0.8227 f1=0.8018 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3402 acc=0.8508 | val_loss=0.4207 acc=0.8118 | prec=0.8632 rec=0.7421 spec=0.8818 f1=0.7981 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3346 acc=0.8684 | val_loss=0.4153 acc=0.8073 | prec=0.7881 rec=0.8416 spec=0.7727 f1=0.8140 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3578 acc=0.8593 | val_loss=0.4280 acc=0.7982 | prec=0.7973 rec=0.8009 spec=0.7955 f1=0.7991 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3298 acc=0.8593 | val_loss=0.4229 acc=0.8095 | prec=0.8586 rec=0.7421 spec=0.8773 f1=0.7961 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3038 acc=0.8752 | val_loss=0.4585 acc=0.7982 | prec=0.8837 rec=0.6878 spec=0.9091 f1=0.7735 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3188 acc=0.8610 | val_loss=0.4332 acc=0.8050 | prec=0.7711 rec=0.8688 spec=0.7409 f1=0.8170 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3089 acc=0.8695 | val_loss=0.4204 acc=0.7982 | prec=0.7973 rec=0.8009 spec=0.7955 f1=0.7991 | time=12.9s\n",
            "Epoch 051 | train_loss=0.2747 acc=0.8809 | val_loss=0.4472 acc=0.7891 | prec=0.7623 rec=0.8416 spec=0.7364 f1=0.8000 | time=12.9s\n",
            "Epoch 052 | train_loss=0.2897 acc=0.8809 | val_loss=0.4279 acc=0.8005 | prec=0.8410 rec=0.7421 spec=0.8591 f1=0.7885 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2707 acc=0.8917 | val_loss=0.4259 acc=0.8027 | prec=0.8131 rec=0.7873 spec=0.8182 f1=0.8000 | time=12.8s\n",
            "Epoch 054 | train_loss=0.2697 acc=0.8973 | val_loss=0.4490 acc=0.8005 | prec=0.8634 rec=0.7149 spec=0.8864 f1=0.7822 | time=12.8s\n",
            "Epoch 055 | train_loss=0.2710 acc=0.8956 | val_loss=0.4347 acc=0.8027 | prec=0.7965 rec=0.8145 spec=0.7909 f1=0.8054 | time=13.0s\n",
            "Epoch 056 | train_loss=0.2493 acc=0.8979 | val_loss=0.4500 acc=0.7959 | prec=0.7835 rec=0.8190 spec=0.7727 f1=0.8009 | time=12.8s\n",
            "Early stopping at epoch 56\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▁▁▁▆▁▁▁▃▁▅▇▂▆▇█▇▇████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▅▁▅▁▁▁▅▅█▆▆▆▆▇▇▆▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▆▇▇▆</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▇▁▁▁▂▁▄█▁▇▇▇▆▅▆▆▆▆▇▆▇▇▇▇▆▇▆▆▇▇▆▇▆▇</td></tr><tr><td>specificity</td><td>███████▁████▇▆▃▄▇▇▅▆▆▇▇▆▆▇▆▆▇▇▆▇▆▆▇▆▆▆▆▆</td></tr><tr><td>train_accuracy</td><td>▁▂▂▁▁▂▁▁▁▁▂▂▂▂▃▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇█▇▇█▇█████</td></tr><tr><td>train_loss</td><td>████▇█▇▇▇▇▇▇▇▇▇▆▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▅▇▇▆▇▇▇▇██▇▇█████▇█▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>█████▇▇▇▇▇▇▇▇▇▇▆▅▄▄▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.80088</td></tr><tr><td>precision</td><td>0.78355</td></tr><tr><td>recall</td><td>0.819</td></tr><tr><td>specificity</td><td>0.77273</td></tr><tr><td>train_accuracy</td><td>0.8979</td></tr><tr><td>train_loss</td><td>0.24934</td></tr><tr><td>val_accuracy</td><td>0.79592</td></tr><tr><td>val_loss</td><td>0.45004</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/88asreh0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/88asreh0</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_213757-88asreh0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Fold 5 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250506_215002-kuityj5v</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/kuityj5v' target=\"_blank\">fold5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/kuityj5v' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/kuityj5v</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7132 acc=0.4875 | val_loss=0.6936 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7074 acc=0.5142 | val_loss=0.6947 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7164 acc=0.4830 | val_loss=0.6939 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7066 acc=0.4938 | val_loss=0.6928 acc=0.5159 | prec=1.0000 rec=0.0318 spec=1.0000 f1=0.0617 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7019 acc=0.5283 | val_loss=0.6943 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7007 acc=0.5181 | val_loss=0.6956 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7118 acc=0.4915 | val_loss=0.6960 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7041 acc=0.5068 | val_loss=0.6931 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 009 | train_loss=0.6976 acc=0.5011 | val_loss=0.6946 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.6980 acc=0.5210 | val_loss=0.6950 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7097 acc=0.4904 | val_loss=0.6971 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7032 acc=0.4909 | val_loss=0.6940 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7017 acc=0.5125 | val_loss=0.6918 acc=0.5227 | prec=0.6190 rec=0.1182 spec=0.9273 f1=0.1985 | time=13.0s\n",
            "Epoch 014 | train_loss=0.7013 acc=0.5119 | val_loss=0.6997 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 015 | train_loss=0.6954 acc=0.5442 | val_loss=0.7006 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 016 | train_loss=0.7069 acc=0.5198 | val_loss=0.6999 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.2s\n",
            "Epoch 017 | train_loss=0.6963 acc=0.5238 | val_loss=0.6839 acc=0.5250 | prec=1.0000 rec=0.0500 spec=1.0000 f1=0.0952 | time=13.2s\n",
            "Epoch 018 | train_loss=0.6898 acc=0.5488 | val_loss=0.6830 acc=0.5182 | prec=1.0000 rec=0.0364 spec=1.0000 f1=0.0702 | time=13.3s\n",
            "Epoch 019 | train_loss=0.6368 acc=0.6417 | val_loss=0.5659 acc=0.7341 | prec=0.8084 rec=0.6136 spec=0.8545 f1=0.6977 | time=13.2s\n",
            "Epoch 020 | train_loss=0.5924 acc=0.7012 | val_loss=0.5404 acc=0.7568 | prec=0.8645 rec=0.6091 spec=0.9045 f1=0.7147 | time=13.0s\n",
            "Epoch 021 | train_loss=0.5539 acc=0.7302 | val_loss=0.5088 acc=0.8000 | prec=0.8511 rec=0.7273 spec=0.8727 f1=0.7843 | time=13.0s\n",
            "Epoch 022 | train_loss=0.5329 acc=0.7557 | val_loss=0.5021 acc=0.7909 | prec=0.8441 rec=0.7136 spec=0.8682 f1=0.7734 | time=12.9s\n",
            "Epoch 023 | train_loss=0.5310 acc=0.7625 | val_loss=0.4968 acc=0.7841 | prec=0.8453 rec=0.6955 spec=0.8727 f1=0.7631 | time=13.0s\n",
            "Epoch 024 | train_loss=0.5178 acc=0.7681 | val_loss=0.4824 acc=0.8182 | prec=0.8153 rec=0.8227 spec=0.8136 f1=0.8190 | time=13.0s\n",
            "Epoch 025 | train_loss=0.4959 acc=0.7778 | val_loss=0.4643 acc=0.8114 | prec=0.8246 rec=0.7909 spec=0.8318 f1=0.8074 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4969 acc=0.7868 | val_loss=0.4736 acc=0.7977 | prec=0.8701 rec=0.7000 spec=0.8955 f1=0.7758 | time=13.0s\n",
            "Epoch 027 | train_loss=0.4797 acc=0.7971 | val_loss=0.4647 acc=0.8023 | prec=0.8674 rec=0.7136 spec=0.8909 f1=0.7830 | time=12.9s\n",
            "Epoch 028 | train_loss=0.4692 acc=0.8033 | val_loss=0.4708 acc=0.8114 | prec=0.8914 rec=0.7091 spec=0.9136 f1=0.7899 | time=12.9s\n",
            "Epoch 029 | train_loss=0.4446 acc=0.8158 | val_loss=0.4322 acc=0.8205 | prec=0.8373 rec=0.7955 spec=0.8455 f1=0.8159 | time=12.9s\n",
            "Epoch 030 | train_loss=0.4352 acc=0.8152 | val_loss=0.4455 acc=0.8159 | prec=0.8840 rec=0.7273 spec=0.9045 f1=0.7980 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4120 acc=0.8345 | val_loss=0.4477 acc=0.8182 | prec=0.9167 rec=0.7000 spec=0.9364 f1=0.7938 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4172 acc=0.8316 | val_loss=0.4483 acc=0.8114 | prec=0.9102 rec=0.6909 spec=0.9318 f1=0.7855 | time=12.9s\n",
            "Epoch 033 | train_loss=0.3918 acc=0.8520 | val_loss=0.4319 acc=0.8205 | prec=0.8543 rec=0.7727 spec=0.8682 f1=0.8115 | time=12.9s\n",
            "Epoch 034 | train_loss=0.3912 acc=0.8447 | val_loss=0.4416 acc=0.8136 | prec=0.8876 rec=0.7182 spec=0.9091 f1=0.7940 | time=12.9s\n",
            "Epoch 035 | train_loss=0.3787 acc=0.8537 | val_loss=0.4876 acc=0.7818 | prec=0.9366 rec=0.6045 spec=0.9591 f1=0.7348 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3760 acc=0.8583 | val_loss=0.4211 acc=0.8159 | prec=0.8145 rec=0.8182 spec=0.8136 f1=0.8163 | time=13.0s\n",
            "Epoch 037 | train_loss=0.3590 acc=0.8560 | val_loss=0.4365 acc=0.8182 | prec=0.8684 rec=0.7500 spec=0.8864 f1=0.8049 | time=12.9s\n",
            "Epoch 038 | train_loss=0.3495 acc=0.8617 | val_loss=0.4510 acc=0.8114 | prec=0.9102 rec=0.6909 spec=0.9318 f1=0.7855 | time=12.8s\n",
            "Epoch 039 | train_loss=0.3480 acc=0.8798 | val_loss=0.4290 acc=0.8136 | prec=0.8416 rec=0.7727 spec=0.8545 f1=0.8057 | time=12.7s\n",
            "Epoch 040 | train_loss=0.3594 acc=0.8628 | val_loss=0.4139 acc=0.8136 | prec=0.8255 rec=0.7955 spec=0.8318 f1=0.8102 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3262 acc=0.8883 | val_loss=0.4414 acc=0.8091 | prec=0.9000 rec=0.6955 spec=0.9227 f1=0.7846 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3249 acc=0.8906 | val_loss=0.4208 acc=0.7932 | prec=0.7841 rec=0.8091 spec=0.7773 f1=0.7964 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3216 acc=0.8713 | val_loss=0.4468 acc=0.8136 | prec=0.9259 rec=0.6818 spec=0.9455 f1=0.7853 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3167 acc=0.8747 | val_loss=0.4231 acc=0.8205 | prec=0.8730 rec=0.7500 spec=0.8909 f1=0.8068 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3244 acc=0.8866 | val_loss=0.4416 acc=0.8273 | prec=0.9045 rec=0.7318 spec=0.9227 f1=0.8090 | time=13.0s\n",
            "Epoch 046 | train_loss=0.3050 acc=0.8827 | val_loss=0.4944 acc=0.7909 | prec=0.9507 rec=0.6136 spec=0.9682 f1=0.7459 | time=12.9s\n",
            "Epoch 047 | train_loss=0.2952 acc=0.8940 | val_loss=0.4861 acc=0.8045 | prec=0.9467 rec=0.6455 spec=0.9636 f1=0.7676 | time=12.9s\n",
            "Epoch 048 | train_loss=0.2713 acc=0.9036 | val_loss=0.4366 acc=0.8341 | prec=0.9153 rec=0.7364 spec=0.9318 f1=0.8161 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2687 acc=0.9042 | val_loss=0.4276 acc=0.8364 | prec=0.9066 rec=0.7500 spec=0.9227 f1=0.8209 | time=12.9s\n",
            "Epoch 050 | train_loss=0.2628 acc=0.9138 | val_loss=0.4528 acc=0.8250 | prec=0.9281 rec=0.7045 spec=0.9455 f1=0.8010 | time=12.8s\n",
            "Epoch 051 | train_loss=0.2796 acc=0.8900 | val_loss=0.4825 acc=0.8068 | prec=0.9299 rec=0.6636 spec=0.9500 f1=0.7745 | time=12.7s\n",
            "Epoch 052 | train_loss=0.2783 acc=0.9031 | val_loss=0.4783 acc=0.8045 | prec=0.9187 rec=0.6682 spec=0.9409 f1=0.7737 | time=12.8s\n",
            "Epoch 053 | train_loss=0.2637 acc=0.8963 | val_loss=0.4786 acc=0.8114 | prec=0.9202 rec=0.6818 spec=0.9409 f1=0.7833 | time=12.9s\n",
            "Epoch 054 | train_loss=0.2697 acc=0.8997 | val_loss=0.4276 acc=0.8068 | prec=0.8261 rec=0.7773 spec=0.8364 f1=0.8009 | time=12.8s\n",
            "Epoch 055 | train_loss=0.2491 acc=0.9093 | val_loss=0.4563 acc=0.8273 | prec=0.9045 rec=0.7318 spec=0.9227 f1=0.8090 | time=13.0s\n",
            "Early stopping at epoch 55\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>▁▁▁▂▁▁▁▁▁▃▁▁▂▂▇██████████▇██████▇███████</td></tr><tr><td>precision</td><td>▁▁▁█▁▁▁▁▁▁▁▁██▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇█▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▂▁▁▁▆▇▇██▇█▇▇▇█▆█▇██▇▇▇▆▆▇▇▇▇▇</td></tr><tr><td>specificity</td><td>█████████▆████▃▄▄▄▂▃▅▅▃▅▆▄▅▇▄▆▃▆▁▆▅▇▆▆▆▆</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▂▁▁▁▂▁▂▂▂▄▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇███████</td></tr><tr><td>train_loss</td><td>██████████████▇▅▅▅▅▄▄▃▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▆▇▇██▇█████████████▇██████</td></tr><tr><td>val_loss</td><td>████████████▅▄▃▃▂▂▂▂▂▂▂▁▂▁▂▂▁▁▁▂▁▂▃▁▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>f1_score</td><td>0.80905</td></tr><tr><td>precision</td><td>0.90449</td></tr><tr><td>recall</td><td>0.73182</td></tr><tr><td>specificity</td><td>0.92273</td></tr><tr><td>train_accuracy</td><td>0.9093</td></tr><tr><td>train_loss</td><td>0.24911</td></tr><tr><td>val_accuracy</td><td>0.82727</td></tr><tr><td>val_loss</td><td>0.45631</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/kuityj5v' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2/runs/kuityj5v</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-fixed-hp-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250506_215002-kuityj5v/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== 5-Fold Cross-Validation Averages ===\n",
            "loss : 0.3935 ± 0.0322\n",
            "acc  : 0.8172 ± 0.0207\n",
            "prec : 0.8597 ± 0.0527\n",
            "rec  : 0.7640 ± 0.0291\n",
            "spec : 0.8702 ± 0.0615\n",
            "f1   : 0.8073 ± 0.0157\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Next Step (AD vs CN)\n",
        "- 1] Fix the Block = 1, Head = 3, and Filter Size according to the best trial\n",
        "- 2] Try 5 Fold Cross Validation to see how the model generalize well\n",
        "- 3] Fix the parameter and train using the entire train dataset. Don't forget to save the model. Because we want to see the test performance.\n",
        "- 4] Testing the performance using test-within and test-cross\n",
        "\n",
        "\n",
        "#### Next Step (FTD vs CN)\n",
        "- 1] Optuna Search using the Block = 1, Head = 3, and Filter Size that we found out in AD vs CN result. (setting the same search space as we did on AD vs CN)\n",
        "- 2] Try 5 Fold Cross Validation to see how the model generalize well\n",
        "- 3] Fix the parameter and train using the entire train dataset. Save the model.\n",
        "- 4] Testing the performance using test-within and test-cross\n",
        "\n",
        "#### Next Step (AD vs FTD)\n",
        "- 1] Optuna Search same as before\n",
        "- 2] Try 5 Fold Cross Validation same as before. Using the fixed parameters.\n",
        "- 3] Fix the parameter and train using the entire train dataset. Save the model.\n",
        "- 4] Testing the performance using test-within and test-cross"
      ],
      "metadata": {
        "id": "iLoA1bMalOOe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AD vs CN Model Performance ('test_within' and 'test_cross')\n",
        "- Choose the best hyperparameter in 10 trials\n",
        "- Use the epoch stored in best trial\n",
        "- Train the model again using the entire train data\n",
        "- Evaluate the model in 'test_within' and 'test_cross'"
      ],
      "metadata": {
        "id": "iWJEdBN7q9MA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "PCT_START   = 0.2  # fixed\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & count splits ──────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "class0, class1 = 'A','C'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "# A vs C 만 필터링\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "def count_labels(meta_list):\n",
        "    cnt0 = cnt1 = 0\n",
        "    for d in meta_list:\n",
        "        lbl = d['label']\n",
        "        if isinstance(lbl, str):\n",
        "            if lbl == class0: cnt0 += 1\n",
        "            elif lbl == class1: cnt1 += 1\n",
        "        else:\n",
        "            if lbl == 0: cnt0 += 1\n",
        "            elif lbl == 1: cnt1 += 1\n",
        "    return cnt0, cnt1\n",
        "\n",
        "n_tr0, n_tr1 = count_labels(train_meta)\n",
        "n_tw0, n_tw1 = count_labels(test_within_meta)\n",
        "n_tc0, n_tc1 = count_labels(test_cross_meta)\n",
        "\n",
        "print(f\"--> Data counts before balancing:\")\n",
        "print(f\"    TRAIN         total={len(train_meta)}  A(D)={n_tr0}, C(N)={n_tr1}\")\n",
        "print(f\"    TEST_WITHIN   total={len(test_within_meta)}  A(D)={n_tw0}, C(N)={n_tw1}\")\n",
        "print(f\"    TEST_CROSS    total={len(test_cross_meta)}  A(D)={n_tc0}, C(N)={n_tc1}\\n\")\n",
        "\n",
        "# ─── Balance train set ───────────────────────────────────────────\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "balanced_meta = [copy.deepcopy(d) for d in balanced_meta]\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset_train = BinaryEEGDataset(raw_ds_train, balanced_meta)\n",
        "labels_train  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── Optuna Objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # 1) sample hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n",
        "    wd = trial.suggest_float('wd', 1e-6, 1e-4, log=True)\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-AD-CN-test-within-cross-2',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # train/validation split\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2,\n",
        "        stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset_train, tr_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset_train, va_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # model & optimizer & scheduler & loss\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc  = 0.0\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # ── train ──\n",
        "        model.train()\n",
        "        train_loss_sum = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight_decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = wd * (cur_lr / lr)\n",
        "            train_loss_sum += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            train_correct += (preds == y).sum().item()\n",
        "            train_total   += y.size(0)\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "        train_acc  = train_correct / train_total\n",
        "\n",
        "        # ── validate ──\n",
        "        model.eval()\n",
        "        vloss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss / len(val_loader)\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        val_acc = (preds == labs).sum() / labs.size\n",
        "\n",
        "        # ── metrics ──\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "        spec = tn / (tn+fp) if (tn+fp)>0 else 0.0\n",
        "        prec = precision_score(labs, preds, zero_division=0)\n",
        "        rec  = recall_score(labs, preds, zero_division=0)\n",
        "        f1   = f1_score(labs, preds, zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={prec:.4f} rec={rec:.4f} spec={spec:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':       epoch,\n",
        "            'train_loss':  train_loss,\n",
        "            'train_acc':   train_acc,\n",
        "            'val_loss':    val_loss,\n",
        "            'val_acc':     val_acc,\n",
        "            'specificity': spec,\n",
        "            'precision':   prec,\n",
        "            'recall':      rec,\n",
        "            'f1_score':    f1\n",
        "        })\n",
        "\n",
        "        # ── Early Stopping (val_loss 기준) ──\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc  = val_acc\n",
        "            es_count      = 0\n",
        "            trial.set_user_attr('best_epoch', epoch)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, train_loader, val_loader, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 다중목적 리턴\n",
        "    return best_val_loss, best_val_acc\n",
        "\n",
        "# ─── Run Optuna Study ────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=['minimize', 'maximize'],\n",
        "        study_name=\"eeg_multiobj\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Pareto front 중에서 val_acc가 가장 높은 trial 선택\n",
        "    best = max(study.best_trials, key=lambda t: t.values[1])\n",
        "    bv_loss, bv_acc = best.values\n",
        "    best_epoch      = best.user_attrs['best_epoch']\n",
        "    print(f\"\\n=== Selected Trial #{best.number} ===\")\n",
        "    print(\n",
        "        f\" val_loss={bv_loss:.4f}, val_acc={bv_acc:.4f}, \"\n",
        "        f\"params={best.params}, best_epoch={best_epoch}\"\n",
        "    )\n",
        "\n",
        "    # ─── Final Retrain on FULL BALANCED TRAIN SET ─────────────────────\n",
        "    final_loader = DataLoader(\n",
        "        dataset_train,\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    final_model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        final_model.parameters(),\n",
        "        lr=best.params['lr'], weight_decay=best.params['wd']\n",
        "    )\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=best.params['lr'],\n",
        "        epochs=best_epoch, steps_per_epoch=len(final_loader),\n",
        "        pct_start=PCT_START, anneal_strategy='cos',\n",
        "        cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(1, best_epoch+1):\n",
        "        final_model.train()\n",
        "        for X, y in final_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = final_model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    os.makedirs('models', exist_ok=True)\n",
        "    torch.save(final_model.state_dict(), 'models/eeg_best_AD_CN_1.pth')\n",
        "    print(f\"Saved final model to models/eeg_best_AD_CN_1.pth\\n\")\n",
        "\n",
        "    # ─── Evaluation 함수 ────────────────────────────────────────────\n",
        "    def evaluate(model, metas):\n",
        "        metas_copy = copy.deepcopy(metas)\n",
        "        for d in metas_copy:\n",
        "            if isinstance(d['label'], str):\n",
        "                d['label'] = label_map[d['label']]\n",
        "\n",
        "        ds     = BinaryEEGDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE,\n",
        "                            shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        model.eval()\n",
        "        vloss=vcorrect=vtotal=0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss    += criterion(logits, y).item()\n",
        "                preds     = logits.argmax(1)\n",
        "                vcorrect += (preds==y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "        return {\n",
        "            'loss': vloss/len(loader),\n",
        "            'acc':  vcorrect/vtotal,\n",
        "            'sensitivity': recall_score(labs, preds, zero_division=0),\n",
        "            'specificity': tn/(tn+fp) if (tn+fp)>0 else 0.0,\n",
        "            'f1':   f1_score(labs, preds, zero_division=0)\n",
        "        }\n",
        "\n",
        "    # ─── Final Evaluation on Test Sets ───────────────────────────────\n",
        "    print(\"=== Final Evaluation on Test Sets ===\")\n",
        "    for name, metas in [('test_within', test_within_meta), ('test_cross', test_cross_meta)]:\n",
        "        res = evaluate(final_model, metas)\n",
        "        n0, n1 = count_labels(metas)\n",
        "        print(f\"-- {name} -- total={len(metas)}  A(D)={n0}, C(N)={n1}\")\n",
        "        print(\n",
        "            f\" Accuracy={res['acc']:.4f} \"\n",
        "            f\"Sensitivity={res['sensitivity']:.4f} \"\n",
        "            f\"Specificity={res['specificity']:.4f} \"\n",
        "            f\"F1={res['f1']:.4f}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oR36frjZYYWD",
        "outputId": "6d5a482d-2830-4d93-fb20-077b9694cf43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 00:59:58,286] A new study created in memory with name: eeg_multiobj\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Data counts before balancing:\n",
            "    TRAIN         total=3219  A(D)=1388, C(N)=1102\n",
            "    TEST_WITHIN   total=272  A(D)=146, C(N)=126\n",
            "    TEST_CROSS    total=626  A(D)=319, C(N)=307\n",
            "\n",
            "\n",
            "--- Trial 0 hyperparams --> lr=2.75e-05, wd=8.57e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_005958-orcqgym8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/orcqgym8' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/orcqgym8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/orcqgym8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7493 acc=0.5201 | val_loss=0.6940 acc=0.4785 | prec=0.4728 rec=0.3955 spec=0.5611 f1=0.4307 | time=13.7s\n",
            "Epoch 002 | train_loss=0.7472 acc=0.4969 | val_loss=0.6935 acc=0.4807 | prec=0.4876 rec=0.8045 spec=0.1584 f1=0.6072 | time=13.1s\n",
            "Epoch 003 | train_loss=0.7550 acc=0.4765 | val_loss=0.6937 acc=0.4490 | prec=0.4633 rec=0.6591 spec=0.2398 f1=0.5441 | time=13.2s\n",
            "Epoch 004 | train_loss=0.7532 acc=0.4935 | val_loss=0.6939 acc=0.4830 | prec=0.4888 rec=0.7955 spec=0.1719 f1=0.6055 | time=13.1s\n",
            "Epoch 005 | train_loss=0.7400 acc=0.4963 | val_loss=0.6947 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=13.1s\n",
            "Epoch 006 | train_loss=0.7454 acc=0.4844 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7349 acc=0.4929 | val_loss=0.6943 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7243 acc=0.5111 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=0.9955 spec=0.0045 f1=0.6646 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7340 acc=0.5048 | val_loss=0.6933 acc=0.4875 | prec=0.4895 rec=0.6364 spec=0.3394 f1=0.5534 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7365 acc=0.4974 | val_loss=0.6932 acc=0.4989 | prec=0.4986 rec=0.7955 spec=0.2036 f1=0.6130 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7321 acc=0.4827 | val_loss=0.6966 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 012 | train_loss=0.7207 acc=0.5048 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7207 acc=0.5133 | val_loss=0.6934 acc=0.5057 | prec=0.6000 rec=0.0273 spec=0.9819 f1=0.0522 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7229 acc=0.4974 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7317 acc=0.4861 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7318 acc=0.4878 | val_loss=0.6958 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 017 | train_loss=0.7100 acc=0.5071 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7296 acc=0.5037 | val_loss=0.6918 acc=0.5170 | prec=0.5090 rec=0.8955 spec=0.1403 f1=0.6491 | time=13.0s\n",
            "Epoch 019 | train_loss=0.7208 acc=0.5026 | val_loss=0.6924 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7175 acc=0.5179 | val_loss=0.6907 acc=0.5692 | prec=0.7679 rec=0.1955 spec=0.9412 f1=0.3116 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7128 acc=0.5156 | val_loss=0.6862 acc=0.6395 | prec=0.6667 rec=0.5545 spec=0.7240 f1=0.6055 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7071 acc=0.5179 | val_loss=0.6748 acc=0.6395 | prec=0.7798 rec=0.3864 spec=0.8914 f1=0.5167 | time=13.0s\n",
            "Epoch 023 | train_loss=0.6883 acc=0.5610 | val_loss=0.6533 acc=0.7211 | prec=0.7680 rec=0.6318 spec=0.8100 f1=0.6933 | time=12.9s\n",
            "Epoch 024 | train_loss=0.6765 acc=0.5933 | val_loss=0.6406 acc=0.6304 | prec=0.8353 rec=0.3227 spec=0.9367 f1=0.4656 | time=12.9s\n",
            "Epoch 025 | train_loss=0.6454 acc=0.6205 | val_loss=0.6012 acc=0.7483 | prec=0.7137 rec=0.8273 spec=0.6697 f1=0.7663 | time=13.0s\n",
            "Epoch 026 | train_loss=0.6015 acc=0.6835 | val_loss=0.5341 acc=0.7619 | prec=0.8075 rec=0.6864 spec=0.8371 f1=0.7420 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5610 acc=0.7192 | val_loss=0.5776 acc=0.6803 | prec=0.6138 rec=0.9682 spec=0.3937 f1=0.7513 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5289 acc=0.7504 | val_loss=0.4943 acc=0.7755 | prec=0.7510 rec=0.8227 spec=0.7285 f1=0.7852 | time=12.9s\n",
            "Epoch 029 | train_loss=0.5048 acc=0.7578 | val_loss=0.5000 acc=0.7710 | prec=0.7333 rec=0.8500 spec=0.6923 f1=0.7874 | time=12.9s\n",
            "Epoch 030 | train_loss=0.4827 acc=0.7742 | val_loss=0.4954 acc=0.7868 | prec=0.7500 rec=0.8591 spec=0.7149 f1=0.8008 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4786 acc=0.7782 | val_loss=0.4841 acc=0.7846 | prec=0.7451 rec=0.8636 spec=0.7059 f1=0.8000 | time=13.0s\n",
            "Epoch 032 | train_loss=0.4721 acc=0.8032 | val_loss=0.4651 acc=0.8027 | prec=0.7714 rec=0.8591 spec=0.7466 f1=0.8129 | time=13.0s\n",
            "Epoch 033 | train_loss=0.4714 acc=0.7777 | val_loss=0.4696 acc=0.7937 | prec=0.7549 rec=0.8682 spec=0.7195 f1=0.8076 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4408 acc=0.8037 | val_loss=0.4594 acc=0.8073 | prec=0.7778 rec=0.8591 spec=0.7557 f1=0.8164 | time=13.1s\n",
            "Epoch 035 | train_loss=0.4294 acc=0.8015 | val_loss=0.4444 acc=0.8095 | prec=0.7957 rec=0.8318 spec=0.7873 f1=0.8133 | time=13.0s\n",
            "Epoch 036 | train_loss=0.4339 acc=0.8247 | val_loss=0.4350 acc=0.8141 | prec=0.8194 rec=0.8045 spec=0.8235 f1=0.8119 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4235 acc=0.8162 | val_loss=0.4577 acc=0.7982 | prec=0.7453 rec=0.9045 spec=0.6923 f1=0.8172 | time=12.9s\n",
            "Epoch 038 | train_loss=0.4088 acc=0.8213 | val_loss=0.4319 acc=0.8005 | prec=0.7870 rec=0.8227 spec=0.7783 f1=0.8044 | time=13.0s\n",
            "Epoch 039 | train_loss=0.4021 acc=0.8276 | val_loss=0.4434 acc=0.7959 | prec=0.7754 rec=0.8318 spec=0.7602 f1=0.8026 | time=13.0s\n",
            "Epoch 040 | train_loss=0.3851 acc=0.8383 | val_loss=0.4414 acc=0.7982 | prec=0.8325 rec=0.7455 spec=0.8507 f1=0.7866 | time=12.9s\n",
            "Epoch 041 | train_loss=0.3888 acc=0.8474 | val_loss=0.4354 acc=0.8095 | prec=0.8009 rec=0.8227 spec=0.7964 f1=0.8117 | time=12.9s\n",
            "Epoch 042 | train_loss=0.3726 acc=0.8457 | val_loss=0.4236 acc=0.8073 | prec=0.7801 rec=0.8545 spec=0.7602 f1=0.8156 | time=12.9s\n",
            "Epoch 043 | train_loss=0.3605 acc=0.8525 | val_loss=0.4563 acc=0.8005 | prec=0.7340 rec=0.9409 spec=0.6606 f1=0.8247 | time=12.9s\n",
            "Epoch 044 | train_loss=0.3657 acc=0.8525 | val_loss=0.4191 acc=0.8186 | prec=0.8182 rec=0.8182 spec=0.8190 f1=0.8182 | time=12.9s\n",
            "Epoch 045 | train_loss=0.3625 acc=0.8571 | val_loss=0.4044 acc=0.8141 | prec=0.8000 rec=0.8364 spec=0.7919 f1=0.8178 | time=12.9s\n",
            "Epoch 046 | train_loss=0.3560 acc=0.8452 | val_loss=0.4172 acc=0.8073 | prec=0.8293 rec=0.7727 spec=0.8416 f1=0.8000 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3423 acc=0.8627 | val_loss=0.4012 acc=0.8345 | prec=0.8075 rec=0.8773 spec=0.7919 f1=0.8410 | time=13.0s\n",
            "Epoch 048 | train_loss=0.3407 acc=0.8712 | val_loss=0.4096 acc=0.8231 | prec=0.7773 rec=0.9045 spec=0.7421 f1=0.8361 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3412 acc=0.8667 | val_loss=0.4277 acc=0.8118 | prec=0.7473 rec=0.9409 spec=0.6833 f1=0.8330 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3178 acc=0.8792 | val_loss=0.4390 acc=0.8050 | prec=0.7410 rec=0.9364 spec=0.6742 f1=0.8273 | time=12.9s\n",
            "Epoch 051 | train_loss=0.3269 acc=0.8599 | val_loss=0.4022 acc=0.8231 | prec=0.7934 rec=0.8727 spec=0.7738 f1=0.8312 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3078 acc=0.8809 | val_loss=0.3965 acc=0.8254 | prec=0.8593 rec=0.7773 spec=0.8733 f1=0.8162 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3267 acc=0.8633 | val_loss=0.4162 acc=0.8050 | prec=0.7500 rec=0.9136 spec=0.6968 f1=0.8238 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3022 acc=0.8809 | val_loss=0.3991 acc=0.8163 | prec=0.8089 rec=0.8273 spec=0.8054 f1=0.8180 | time=13.0s\n",
            "Epoch 055 | train_loss=0.2906 acc=0.8973 | val_loss=0.4008 acc=0.8299 | prec=0.8085 rec=0.8636 spec=0.7964 f1=0.8352 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2844 acc=0.8934 | val_loss=0.3932 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3056 acc=0.8928 | val_loss=0.4016 acc=0.8163 | prec=0.8492 rec=0.7682 spec=0.8643 f1=0.8067 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2827 acc=0.8877 | val_loss=0.3931 acc=0.8277 | prec=0.7857 rec=0.9000 spec=0.7557 f1=0.8390 | time=13.0s\n",
            "Epoch 059 | train_loss=0.2694 acc=0.8888 | val_loss=0.3915 acc=0.8141 | prec=0.8165 rec=0.8091 spec=0.8190 f1=0.8128 | time=12.9s\n",
            "Epoch 060 | train_loss=0.2764 acc=0.8843 | val_loss=0.3965 acc=0.8163 | prec=0.8357 rec=0.7864 spec=0.8462 f1=0.8103 | time=13.0s\n",
            "Epoch 061 | train_loss=0.2810 acc=0.8905 | val_loss=0.3874 acc=0.8209 | prec=0.7950 rec=0.8636 spec=0.7783 f1=0.8279 | time=12.8s\n",
            "Epoch 062 | train_loss=0.2729 acc=0.9002 | val_loss=0.4011 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2679 acc=0.9013 | val_loss=0.3993 acc=0.8005 | prec=0.8300 rec=0.7545 spec=0.8462 f1=0.7905 | time=12.8s\n",
            "Epoch 064 | train_loss=0.2507 acc=0.9064 | val_loss=0.4010 acc=0.8050 | prec=0.7939 rec=0.8227 spec=0.7873 f1=0.8080 | time=12.9s\n",
            "Epoch 065 | train_loss=0.2470 acc=0.8956 | val_loss=0.3927 acc=0.8095 | prec=0.8208 rec=0.7909 spec=0.8281 f1=0.8056 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2446 acc=0.9081 | val_loss=0.3851 acc=0.8299 | prec=0.8166 rec=0.8500 spec=0.8100 f1=0.8330 | time=12.8s\n",
            "Epoch 067 | train_loss=0.2345 acc=0.9172 | val_loss=0.4117 acc=0.8095 | prec=0.8301 rec=0.7773 spec=0.8416 f1=0.8028 | time=12.9s\n",
            "Epoch 068 | train_loss=0.2503 acc=0.9041 | val_loss=0.4032 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2314 acc=0.9138 | val_loss=0.3866 acc=0.8163 | prec=0.8145 rec=0.8182 spec=0.8145 f1=0.8163 | time=12.9s\n",
            "Epoch 070 | train_loss=0.2278 acc=0.9132 | val_loss=0.4055 acc=0.8050 | prec=0.8221 rec=0.7773 spec=0.8326 f1=0.7991 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2275 acc=0.9178 | val_loss=0.3887 acc=0.8299 | prec=0.7984 rec=0.8818 spec=0.7783 f1=0.8380 | time=12.9s\n",
            "Epoch 072 | train_loss=0.2368 acc=0.9104 | val_loss=0.3948 acc=0.8231 | prec=0.7983 rec=0.8636 spec=0.7828 f1=0.8297 | time=13.0s\n",
            "Epoch 073 | train_loss=0.2343 acc=0.9087 | val_loss=0.3908 acc=0.8209 | prec=0.8279 rec=0.8091 spec=0.8326 f1=0.8184 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2307 acc=0.9081 | val_loss=0.4014 acc=0.8118 | prec=0.8278 rec=0.7864 spec=0.8371 f1=0.8065 | time=13.1s\n",
            "Epoch 075 | train_loss=0.2215 acc=0.9121 | val_loss=0.4032 acc=0.8141 | prec=0.8350 rec=0.7818 spec=0.8462 f1=0.8075 | time=13.0s\n",
            "Epoch 076 | train_loss=0.2216 acc=0.9121 | val_loss=0.4088 acc=0.8050 | prec=0.8131 rec=0.7909 spec=0.8190 f1=0.8018 | time=13.0s\n",
            "Epoch 077 | train_loss=0.2029 acc=0.9331 | val_loss=0.3922 acc=0.8209 | prec=0.7878 rec=0.8773 spec=0.7647 f1=0.8301 | time=13.0s\n",
            "Epoch 078 | train_loss=0.2163 acc=0.9274 | val_loss=0.3951 acc=0.8141 | prec=0.7924 rec=0.8500 spec=0.7783 f1=0.8202 | time=12.9s\n",
            "Epoch 079 | train_loss=0.2095 acc=0.9126 | val_loss=0.3920 acc=0.8186 | prec=0.8182 rec=0.8182 spec=0.8190 f1=0.8182 | time=12.9s\n",
            "Epoch 080 | train_loss=0.2106 acc=0.9166 | val_loss=0.4243 acc=0.8163 | prec=0.8564 rec=0.7591 spec=0.8733 f1=0.8048 | time=12.9s\n",
            "Epoch 081 | train_loss=0.2132 acc=0.9217 | val_loss=0.3960 acc=0.8118 | prec=0.7940 rec=0.8409 spec=0.7828 f1=0.8168 | time=12.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▆▆▇▇▇▆▁▁▁▁▆▁▅▇▇█████████████████████████</td></tr><tr><td>precision</td><td>▄▄▄▄▄▁▁▁▅█▆▆▆▆▆▇▇▇▇▇▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▄▇▆▇██▁▁▁▇▅▅▆█▇▇▇▇▇▇▇▇▆▇█▆▇▇▆▇▇▇▇▆▇▇▇▆▇▆</td></tr><tr><td>specificity</td><td>▅▃▁▃▂████▇▆▆▆▇▇▆▇▇▆▇▇▇▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▆▆</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>██████████▇█▇▆▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▁▃▄▄▆▇▇▇▇▇█▇▇▇█████████████████</td></tr><tr><td>val_loss</td><td>████████████▇▇▅▃▃▃▃▃▃▂▂▂▃▁▂▁▂▂▁▂▁▁▁▁▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>81</td></tr><tr><td>f1_score</td><td>0.81678</td></tr><tr><td>precision</td><td>0.79399</td></tr><tr><td>recall</td><td>0.84091</td></tr><tr><td>specificity</td><td>0.78281</td></tr><tr><td>train_acc</td><td>0.92172</td></tr><tr><td>train_loss</td><td>0.21323</td></tr><tr><td>val_acc</td><td>0.81179</td></tr><tr><td>val_loss</td><td>0.396</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/orcqgym8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/orcqgym8</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_005958-orcqgym8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 01:17:32,383] Trial 0 finished with values: [0.38509778891290936, 0.8299319727891157] and parameters: {'lr': 2.7547885096132666e-05, 'wd': 8.574105446223684e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 1 hyperparams --> lr=1.04e-05, wd=3.93e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_011732-i63us6kp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/i63us6kp' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/i63us6kp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/i63us6kp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7717 acc=0.4952 | val_loss=0.7150 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7584 acc=0.4929 | val_loss=0.7056 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7672 acc=0.4957 | val_loss=0.7059 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7498 acc=0.5133 | val_loss=0.6963 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7305 acc=0.5184 | val_loss=0.6941 acc=0.4966 | prec=0.4977 rec=0.9727 spec=0.0226 f1=0.6585 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7331 acc=0.5037 | val_loss=0.6934 acc=0.4875 | prec=0.4850 rec=0.4409 spec=0.5339 f1=0.4619 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7254 acc=0.5201 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7071 acc=0.5167 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 009 | train_loss=0.7268 acc=0.4918 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7178 acc=0.4878 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7320 acc=0.4787 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 012 | train_loss=0.7081 acc=0.5281 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7163 acc=0.4969 | val_loss=0.6950 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7128 acc=0.5077 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7143 acc=0.5145 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7160 acc=0.5162 | val_loss=0.6983 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7215 acc=0.4838 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7129 acc=0.5026 | val_loss=0.6923 acc=0.5351 | prec=0.5484 rec=0.3864 spec=0.6833 f1=0.4533 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7193 acc=0.4952 | val_loss=0.6929 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7137 acc=0.5122 | val_loss=0.6930 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7105 acc=0.5048 | val_loss=0.6947 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 022 | train_loss=0.7104 acc=0.5054 | val_loss=0.6932 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.0s\n",
            "Epoch 023 | train_loss=0.7124 acc=0.4906 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 024 | train_loss=0.7143 acc=0.5054 | val_loss=0.6933 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.5s\n",
            "Epoch 025 | train_loss=0.7085 acc=0.5060 | val_loss=0.6888 acc=0.5102 | prec=1.0000 rec=0.0182 spec=1.0000 f1=0.0357 | time=13.3s\n",
            "Epoch 026 | train_loss=0.7070 acc=0.5111 | val_loss=0.6920 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.3s\n",
            "Epoch 027 | train_loss=0.7031 acc=0.5269 | val_loss=0.6864 acc=0.5170 | prec=1.0000 rec=0.0318 spec=1.0000 f1=0.0617 | time=13.5s\n",
            "Epoch 028 | train_loss=0.7000 acc=0.5156 | val_loss=0.6833 acc=0.5828 | prec=0.7647 rec=0.2364 spec=0.9276 f1=0.3611 | time=13.4s\n",
            "Epoch 029 | train_loss=0.6970 acc=0.5337 | val_loss=0.6851 acc=0.5261 | prec=1.0000 rec=0.0500 spec=1.0000 f1=0.0952 | time=13.7s\n",
            "Epoch 030 | train_loss=0.6657 acc=0.5905 | val_loss=0.6378 acc=0.6349 | prec=0.8391 rec=0.3318 spec=0.9367 f1=0.4756 | time=13.5s\n",
            "Epoch 031 | train_loss=0.6392 acc=0.6307 | val_loss=0.6285 acc=0.6553 | prec=0.8542 rec=0.3727 spec=0.9367 f1=0.5190 | time=13.1s\n",
            "Epoch 032 | train_loss=0.6143 acc=0.6495 | val_loss=0.5545 acc=0.7596 | prec=0.7615 rec=0.7545 spec=0.7647 f1=0.7580 | time=13.0s\n",
            "Epoch 033 | train_loss=0.6163 acc=0.6670 | val_loss=0.5583 acc=0.7642 | prec=0.8021 rec=0.7000 spec=0.8281 f1=0.7476 | time=13.0s\n",
            "Epoch 034 | train_loss=0.5812 acc=0.7107 | val_loss=0.5538 acc=0.7732 | prec=0.8659 rec=0.6455 spec=0.9005 f1=0.7396 | time=13.0s\n",
            "Epoch 035 | train_loss=0.5717 acc=0.7073 | val_loss=0.5428 acc=0.7687 | prec=0.8138 rec=0.6955 spec=0.8416 f1=0.7500 | time=13.2s\n",
            "Epoch 036 | train_loss=0.5667 acc=0.7226 | val_loss=0.5394 acc=0.7732 | prec=0.8333 rec=0.6818 spec=0.8643 f1=0.7500 | time=13.2s\n",
            "Epoch 037 | train_loss=0.5539 acc=0.7379 | val_loss=0.5467 acc=0.7755 | prec=0.8306 rec=0.6909 spec=0.8597 f1=0.7543 | time=13.0s\n",
            "Epoch 038 | train_loss=0.5521 acc=0.7187 | val_loss=0.5242 acc=0.7891 | prec=0.7953 rec=0.7773 spec=0.8009 f1=0.7862 | time=13.0s\n",
            "Epoch 039 | train_loss=0.5499 acc=0.7351 | val_loss=0.5217 acc=0.7846 | prec=0.7990 rec=0.7591 spec=0.8100 f1=0.7786 | time=13.1s\n",
            "Epoch 040 | train_loss=0.5500 acc=0.7362 | val_loss=0.5256 acc=0.7959 | prec=0.8155 rec=0.7636 spec=0.8281 f1=0.7887 | time=13.0s\n",
            "Epoch 041 | train_loss=0.5344 acc=0.7635 | val_loss=0.5289 acc=0.7687 | prec=0.8391 rec=0.6636 spec=0.8733 f1=0.7411 | time=13.0s\n",
            "Epoch 042 | train_loss=0.5284 acc=0.7465 | val_loss=0.5063 acc=0.7937 | prec=0.8209 rec=0.7500 spec=0.8371 f1=0.7838 | time=13.0s\n",
            "Epoch 043 | train_loss=0.5117 acc=0.7640 | val_loss=0.5174 acc=0.7755 | prec=0.8418 rec=0.6773 spec=0.8733 f1=0.7506 | time=13.0s\n",
            "Epoch 044 | train_loss=0.5328 acc=0.7623 | val_loss=0.5054 acc=0.7846 | prec=0.8378 rec=0.7045 spec=0.8643 f1=0.7654 | time=12.9s\n",
            "Epoch 045 | train_loss=0.5181 acc=0.7635 | val_loss=0.5051 acc=0.8005 | prec=0.8300 rec=0.7545 spec=0.8462 f1=0.7905 | time=12.9s\n",
            "Epoch 046 | train_loss=0.5108 acc=0.7725 | val_loss=0.5103 acc=0.7619 | prec=0.8528 rec=0.6318 spec=0.8914 f1=0.7258 | time=13.0s\n",
            "Epoch 047 | train_loss=0.5093 acc=0.7601 | val_loss=0.5010 acc=0.8050 | prec=0.8018 rec=0.8091 spec=0.8009 f1=0.8054 | time=13.0s\n",
            "Epoch 048 | train_loss=0.4907 acc=0.7901 | val_loss=0.5007 acc=0.8005 | prec=0.8333 rec=0.7500 spec=0.8507 f1=0.7895 | time=13.1s\n",
            "Epoch 049 | train_loss=0.5166 acc=0.7725 | val_loss=0.4883 acc=0.8118 | prec=0.8309 rec=0.7818 spec=0.8416 f1=0.8056 | time=12.8s\n",
            "Epoch 050 | train_loss=0.5051 acc=0.7748 | val_loss=0.4879 acc=0.8118 | prec=0.8246 rec=0.7909 spec=0.8326 f1=0.8074 | time=12.9s\n",
            "Epoch 051 | train_loss=0.4948 acc=0.7862 | val_loss=0.4854 acc=0.8027 | prec=0.8342 rec=0.7545 spec=0.8507 f1=0.7924 | time=13.0s\n",
            "Epoch 052 | train_loss=0.4913 acc=0.7771 | val_loss=0.4852 acc=0.8118 | prec=0.8374 rec=0.7727 spec=0.8507 f1=0.8038 | time=13.0s\n",
            "Epoch 053 | train_loss=0.4723 acc=0.7964 | val_loss=0.4812 acc=0.8141 | prec=0.8165 rec=0.8091 spec=0.8190 f1=0.8128 | time=13.0s\n",
            "Epoch 054 | train_loss=0.4817 acc=0.7935 | val_loss=0.4804 acc=0.8027 | prec=0.8308 rec=0.7591 spec=0.8462 f1=0.7933 | time=12.9s\n",
            "Epoch 055 | train_loss=0.4871 acc=0.7828 | val_loss=0.4799 acc=0.8118 | prec=0.8246 rec=0.7909 spec=0.8326 f1=0.8074 | time=13.0s\n",
            "Epoch 056 | train_loss=0.4829 acc=0.7822 | val_loss=0.4805 acc=0.8163 | prec=0.8089 rec=0.8273 spec=0.8054 f1=0.8180 | time=13.0s\n",
            "Epoch 057 | train_loss=0.4864 acc=0.7958 | val_loss=0.4672 acc=0.8186 | prec=0.8153 rec=0.8227 spec=0.8145 f1=0.8190 | time=12.9s\n",
            "Epoch 058 | train_loss=0.4741 acc=0.7947 | val_loss=0.4718 acc=0.8141 | prec=0.8000 rec=0.8364 spec=0.7919 f1=0.8178 | time=12.8s\n",
            "Epoch 059 | train_loss=0.4543 acc=0.8100 | val_loss=0.4610 acc=0.8231 | prec=0.8257 rec=0.8182 spec=0.8281 f1=0.8219 | time=13.0s\n",
            "Epoch 060 | train_loss=0.4666 acc=0.8049 | val_loss=0.4637 acc=0.8095 | prec=0.8366 rec=0.7682 spec=0.8507 f1=0.8009 | time=12.9s\n",
            "Epoch 061 | train_loss=0.4370 acc=0.8196 | val_loss=0.4530 acc=0.8254 | prec=0.8295 rec=0.8182 spec=0.8326 f1=0.8238 | time=12.9s\n",
            "Epoch 062 | train_loss=0.4616 acc=0.8140 | val_loss=0.4562 acc=0.8299 | prec=0.8139 rec=0.8545 spec=0.8054 f1=0.8337 | time=13.0s\n",
            "Epoch 063 | train_loss=0.4611 acc=0.8060 | val_loss=0.4508 acc=0.8231 | prec=0.8087 rec=0.8455 spec=0.8009 f1=0.8267 | time=13.0s\n",
            "Epoch 064 | train_loss=0.4487 acc=0.8128 | val_loss=0.4499 acc=0.8073 | prec=0.8392 rec=0.7591 spec=0.8552 f1=0.7971 | time=12.9s\n",
            "Epoch 065 | train_loss=0.4573 acc=0.8009 | val_loss=0.4514 acc=0.8277 | prec=0.8130 rec=0.8500 spec=0.8054 f1=0.8311 | time=12.9s\n",
            "Epoch 066 | train_loss=0.4373 acc=0.8236 | val_loss=0.4466 acc=0.8345 | prec=0.8267 rec=0.8455 spec=0.8235 f1=0.8360 | time=12.9s\n",
            "Epoch 067 | train_loss=0.4343 acc=0.8123 | val_loss=0.4420 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.8s\n",
            "Epoch 068 | train_loss=0.4602 acc=0.8242 | val_loss=0.4386 acc=0.8277 | prec=0.8214 rec=0.8364 spec=0.8190 f1=0.8288 | time=12.8s\n",
            "Epoch 069 | train_loss=0.4318 acc=0.8140 | val_loss=0.4339 acc=0.8231 | prec=0.8008 rec=0.8591 spec=0.7873 f1=0.8289 | time=12.9s\n",
            "Epoch 070 | train_loss=0.4184 acc=0.8264 | val_loss=0.4392 acc=0.8254 | prec=0.8017 rec=0.8636 spec=0.7873 f1=0.8315 | time=13.0s\n",
            "Epoch 071 | train_loss=0.4577 acc=0.8094 | val_loss=0.4289 acc=0.8345 | prec=0.8267 rec=0.8455 spec=0.8235 f1=0.8360 | time=12.9s\n",
            "Epoch 072 | train_loss=0.4281 acc=0.8202 | val_loss=0.4361 acc=0.8231 | prec=0.8034 rec=0.8545 spec=0.7919 f1=0.8282 | time=13.0s\n",
            "Epoch 073 | train_loss=0.4302 acc=0.8349 | val_loss=0.4293 acc=0.8254 | prec=0.8069 rec=0.8545 spec=0.7964 f1=0.8300 | time=12.9s\n",
            "Epoch 074 | train_loss=0.4406 acc=0.8191 | val_loss=0.4281 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=13.0s\n",
            "Epoch 075 | train_loss=0.4222 acc=0.8157 | val_loss=0.4241 acc=0.8277 | prec=0.8495 rec=0.7955 spec=0.8597 f1=0.8216 | time=12.8s\n",
            "Epoch 076 | train_loss=0.4169 acc=0.8242 | val_loss=0.4242 acc=0.8367 | prec=0.8364 rec=0.8364 spec=0.8371 f1=0.8364 | time=12.8s\n",
            "Epoch 077 | train_loss=0.4191 acc=0.8332 | val_loss=0.4212 acc=0.8390 | prec=0.8341 rec=0.8455 spec=0.8326 f1=0.8397 | time=12.9s\n",
            "Epoch 078 | train_loss=0.4322 acc=0.8310 | val_loss=0.4183 acc=0.8390 | prec=0.8371 rec=0.8409 spec=0.8371 f1=0.8390 | time=13.0s\n",
            "Epoch 079 | train_loss=0.4155 acc=0.8298 | val_loss=0.4156 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=13.0s\n",
            "Epoch 080 | train_loss=0.3961 acc=0.8378 | val_loss=0.4181 acc=0.8277 | prec=0.8564 rec=0.7864 spec=0.8688 f1=0.8199 | time=12.9s\n",
            "Epoch 081 | train_loss=0.4111 acc=0.8327 | val_loss=0.4191 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.8s\n",
            "Epoch 082 | train_loss=0.4135 acc=0.8293 | val_loss=0.4216 acc=0.8367 | prec=0.8136 rec=0.8727 spec=0.8009 f1=0.8421 | time=12.9s\n",
            "Epoch 083 | train_loss=0.4114 acc=0.8321 | val_loss=0.4121 acc=0.8413 | prec=0.8348 rec=0.8500 spec=0.8326 f1=0.8423 | time=12.9s\n",
            "Epoch 084 | train_loss=0.4111 acc=0.8395 | val_loss=0.4094 acc=0.8390 | prec=0.8311 rec=0.8500 spec=0.8281 f1=0.8404 | time=12.8s\n",
            "Epoch 085 | train_loss=0.3874 acc=0.8372 | val_loss=0.4120 acc=0.8390 | prec=0.8282 rec=0.8545 spec=0.8235 f1=0.8412 | time=12.9s\n",
            "Epoch 086 | train_loss=0.3927 acc=0.8344 | val_loss=0.4107 acc=0.8299 | prec=0.8607 rec=0.7864 spec=0.8733 f1=0.8219 | time=12.9s\n",
            "Epoch 087 | train_loss=0.3940 acc=0.8434 | val_loss=0.4057 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=13.0s\n",
            "Epoch 088 | train_loss=0.4022 acc=0.8452 | val_loss=0.4029 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=13.0s\n",
            "Epoch 089 | train_loss=0.3964 acc=0.8474 | val_loss=0.4070 acc=0.8254 | prec=0.8488 rec=0.7909 spec=0.8597 f1=0.8188 | time=13.1s\n",
            "Epoch 090 | train_loss=0.4023 acc=0.8446 | val_loss=0.4126 acc=0.8458 | prec=0.8220 rec=0.8818 spec=0.8100 f1=0.8509 | time=12.8s\n",
            "Epoch 091 | train_loss=0.3880 acc=0.8395 | val_loss=0.4009 acc=0.8390 | prec=0.8465 rec=0.8273 spec=0.8507 f1=0.8368 | time=12.8s\n",
            "Epoch 092 | train_loss=0.3898 acc=0.8446 | val_loss=0.4078 acc=0.8435 | prec=0.8268 rec=0.8682 spec=0.8190 f1=0.8470 | time=12.9s\n",
            "Epoch 093 | train_loss=0.3895 acc=0.8349 | val_loss=0.4010 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=13.0s\n",
            "Epoch 094 | train_loss=0.4002 acc=0.8508 | val_loss=0.3968 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=12.8s\n",
            "Epoch 095 | train_loss=0.3913 acc=0.8582 | val_loss=0.3987 acc=0.8503 | prec=0.8468 rec=0.8545 spec=0.8462 f1=0.8507 | time=12.9s\n",
            "Epoch 096 | train_loss=0.3924 acc=0.8349 | val_loss=0.3990 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=12.9s\n",
            "Epoch 097 | train_loss=0.3786 acc=0.8480 | val_loss=0.3920 acc=0.8458 | prec=0.8455 rec=0.8455 spec=0.8462 f1=0.8455 | time=12.9s\n",
            "Epoch 098 | train_loss=0.3668 acc=0.8633 | val_loss=0.4016 acc=0.8458 | prec=0.8455 rec=0.8455 spec=0.8462 f1=0.8455 | time=12.9s\n",
            "Epoch 099 | train_loss=0.3672 acc=0.8582 | val_loss=0.4018 acc=0.8481 | prec=0.8493 rec=0.8455 spec=0.8507 f1=0.8474 | time=12.9s\n",
            "Epoch 100 | train_loss=0.3710 acc=0.8457 | val_loss=0.3974 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=13.0s\n",
            "Epoch 101 | train_loss=0.3776 acc=0.8491 | val_loss=0.3884 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=12.9s\n",
            "Epoch 102 | train_loss=0.3960 acc=0.8355 | val_loss=0.3886 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=12.9s\n",
            "Epoch 103 | train_loss=0.3679 acc=0.8514 | val_loss=0.3885 acc=0.8367 | prec=0.8524 rec=0.8136 spec=0.8597 f1=0.8326 | time=12.9s\n",
            "Epoch 104 | train_loss=0.3762 acc=0.8503 | val_loss=0.3899 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=12.8s\n",
            "Epoch 105 | train_loss=0.3691 acc=0.8486 | val_loss=0.3931 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=12.9s\n",
            "Epoch 106 | train_loss=0.3703 acc=0.8582 | val_loss=0.3934 acc=0.8435 | prec=0.8416 rec=0.8455 spec=0.8416 f1=0.8435 | time=12.9s\n",
            "Epoch 107 | train_loss=0.3782 acc=0.8349 | val_loss=0.3904 acc=0.8435 | prec=0.8447 rec=0.8409 spec=0.8462 f1=0.8428 | time=12.8s\n",
            "Epoch 108 | train_loss=0.3819 acc=0.8582 | val_loss=0.3883 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=12.9s\n",
            "Epoch 109 | train_loss=0.3653 acc=0.8531 | val_loss=0.3911 acc=0.8503 | prec=0.8468 rec=0.8545 spec=0.8462 f1=0.8507 | time=12.7s\n",
            "Epoch 110 | train_loss=0.3694 acc=0.8520 | val_loss=0.3884 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=12.9s\n",
            "Epoch 111 | train_loss=0.3801 acc=0.8463 | val_loss=0.3893 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=12.9s\n",
            "Epoch 112 | train_loss=0.3678 acc=0.8610 | val_loss=0.3867 acc=0.8458 | prec=0.8423 rec=0.8500 spec=0.8416 f1=0.8462 | time=13.0s\n",
            "Epoch 113 | train_loss=0.3767 acc=0.8503 | val_loss=0.3899 acc=0.8367 | prec=0.8458 rec=0.8227 spec=0.8507 f1=0.8341 | time=13.0s\n",
            "Epoch 114 | train_loss=0.3691 acc=0.8554 | val_loss=0.3939 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=12.9s\n",
            "Epoch 115 | train_loss=0.3735 acc=0.8463 | val_loss=0.3888 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=13.0s\n",
            "Epoch 116 | train_loss=0.3574 acc=0.8593 | val_loss=0.3918 acc=0.8458 | prec=0.8393 rec=0.8545 spec=0.8371 f1=0.8468 | time=12.9s\n",
            "Epoch 117 | train_loss=0.3629 acc=0.8588 | val_loss=0.3865 acc=0.8503 | prec=0.8468 rec=0.8545 spec=0.8462 f1=0.8507 | time=13.0s\n",
            "Epoch 118 | train_loss=0.3603 acc=0.8537 | val_loss=0.3900 acc=0.8458 | prec=0.8455 rec=0.8455 spec=0.8462 f1=0.8455 | time=13.0s\n",
            "Epoch 119 | train_loss=0.3644 acc=0.8588 | val_loss=0.3877 acc=0.8390 | prec=0.8282 rec=0.8545 spec=0.8235 f1=0.8412 | time=12.8s\n",
            "Epoch 120 | train_loss=0.3704 acc=0.8548 | val_loss=0.3845 acc=0.8345 | prec=0.8451 rec=0.8182 spec=0.8507 f1=0.8314 | time=13.0s\n",
            "Epoch 121 | train_loss=0.3605 acc=0.8548 | val_loss=0.3887 acc=0.8435 | prec=0.8326 rec=0.8591 spec=0.8281 f1=0.8456 | time=12.8s\n",
            "Epoch 122 | train_loss=0.3674 acc=0.8661 | val_loss=0.3866 acc=0.8435 | prec=0.8326 rec=0.8591 spec=0.8281 f1=0.8456 | time=12.9s\n",
            "Epoch 123 | train_loss=0.3673 acc=0.8474 | val_loss=0.3899 acc=0.8367 | prec=0.8776 rec=0.7818 spec=0.8914 f1=0.8269 | time=12.9s\n",
            "Epoch 124 | train_loss=0.3652 acc=0.8480 | val_loss=0.3861 acc=0.8458 | prec=0.8423 rec=0.8500 spec=0.8416 f1=0.8462 | time=12.9s\n",
            "Epoch 125 | train_loss=0.3669 acc=0.8514 | val_loss=0.3856 acc=0.8458 | prec=0.8423 rec=0.8500 spec=0.8416 f1=0.8462 | time=12.9s\n",
            "Epoch 126 | train_loss=0.3602 acc=0.8537 | val_loss=0.3852 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=13.0s\n",
            "Epoch 127 | train_loss=0.3655 acc=0.8554 | val_loss=0.3944 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=12.9s\n",
            "Epoch 128 | train_loss=0.3611 acc=0.8593 | val_loss=0.3941 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=13.0s\n",
            "Epoch 129 | train_loss=0.3552 acc=0.8644 | val_loss=0.3897 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.9s\n",
            "Epoch 130 | train_loss=0.3552 acc=0.8582 | val_loss=0.3865 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=13.1s\n",
            "Epoch 131 | train_loss=0.3655 acc=0.8588 | val_loss=0.3851 acc=0.8435 | prec=0.8297 rec=0.8636 spec=0.8235 f1=0.8463 | time=13.1s\n",
            "Epoch 132 | train_loss=0.3621 acc=0.8582 | val_loss=0.3839 acc=0.8481 | prec=0.8462 rec=0.8500 spec=0.8462 f1=0.8481 | time=12.9s\n",
            "Epoch 133 | train_loss=0.3663 acc=0.8622 | val_loss=0.3866 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=12.9s\n",
            "Epoch 134 | train_loss=0.3595 acc=0.8571 | val_loss=0.3859 acc=0.8413 | prec=0.8319 rec=0.8545 spec=0.8281 f1=0.8430 | time=12.9s\n",
            "Epoch 135 | train_loss=0.3530 acc=0.8610 | val_loss=0.3857 acc=0.8503 | prec=0.8500 rec=0.8500 spec=0.8507 f1=0.8500 | time=12.9s\n",
            "Epoch 136 | train_loss=0.3587 acc=0.8593 | val_loss=0.3880 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=12.9s\n",
            "Epoch 137 | train_loss=0.3578 acc=0.8633 | val_loss=0.3878 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=12.8s\n",
            "Epoch 138 | train_loss=0.3492 acc=0.8650 | val_loss=0.3895 acc=0.8390 | prec=0.8465 rec=0.8273 spec=0.8507 f1=0.8368 | time=12.9s\n",
            "Epoch 139 | train_loss=0.3532 acc=0.8650 | val_loss=0.3852 acc=0.8481 | prec=0.8462 rec=0.8500 spec=0.8462 f1=0.8481 | time=13.0s\n",
            "Epoch 140 | train_loss=0.3662 acc=0.8576 | val_loss=0.3872 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=12.8s\n",
            "Epoch 141 | train_loss=0.3761 acc=0.8480 | val_loss=0.3849 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=12.9s\n",
            "Epoch 142 | train_loss=0.3802 acc=0.8588 | val_loss=0.3905 acc=0.8458 | prec=0.8333 rec=0.8636 spec=0.8281 f1=0.8482 | time=13.0s\n",
            "Epoch 143 | train_loss=0.3564 acc=0.8627 | val_loss=0.3888 acc=0.8390 | prec=0.8670 rec=0.8000 spec=0.8778 f1=0.8322 | time=12.9s\n",
            "Epoch 144 | train_loss=0.3752 acc=0.8582 | val_loss=0.3818 acc=0.8458 | prec=0.8455 rec=0.8455 spec=0.8462 f1=0.8455 | time=12.9s\n",
            "Epoch 145 | train_loss=0.3587 acc=0.8588 | val_loss=0.3884 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=13.0s\n",
            "Epoch 146 | train_loss=0.3574 acc=0.8520 | val_loss=0.3859 acc=0.8458 | prec=0.8393 rec=0.8545 spec=0.8371 f1=0.8468 | time=12.9s\n",
            "Epoch 147 | train_loss=0.3528 acc=0.8701 | val_loss=0.3869 acc=0.8435 | prec=0.8416 rec=0.8455 spec=0.8416 f1=0.8435 | time=13.0s\n",
            "Epoch 148 | train_loss=0.3538 acc=0.8741 | val_loss=0.3817 acc=0.8413 | prec=0.8319 rec=0.8545 spec=0.8281 f1=0.8430 | time=12.9s\n",
            "Epoch 149 | train_loss=0.3763 acc=0.8593 | val_loss=0.3868 acc=0.8413 | prec=0.8319 rec=0.8545 spec=0.8281 f1=0.8430 | time=13.0s\n",
            "Epoch 150 | train_loss=0.3590 acc=0.8599 | val_loss=0.3851 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇██████</td></tr><tr><td>f1_score</td><td>▆▆▁▁▁▁▁▁▁▁▇▇▇▇▇█████████████████████████</td></tr><tr><td>precision</td><td>▄▁▁▅█▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>████▁▁▄▁▃▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>▁▁██████▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▂▁▁▂▁▁▁▂▄▅▆▆▆▆▇▇▇▇▇▇▇▇▇████▇██████████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▅▅▃▄▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▁▁▄▄▆▆▇▆▇▇████████████████████████</td></tr><tr><td>val_loss</td><td>████████▇▅▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>150</td></tr><tr><td>f1_score</td><td>0.84211</td></tr><tr><td>precision</td><td>0.84793</td></tr><tr><td>recall</td><td>0.83636</td></tr><tr><td>specificity</td><td>0.85068</td></tr><tr><td>train_acc</td><td>0.8599</td></tr><tr><td>train_loss</td><td>0.35899</td></tr><tr><td>val_acc</td><td>0.84354</td></tr><tr><td>val_loss</td><td>0.38507</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/i63us6kp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/i63us6kp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_011732-i63us6kp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 01:49:59,753] Trial 1 finished with values: [0.3817240446805954, 0.8412698412698413] and parameters: {'lr': 1.0379121597342921e-05, 'wd': 3.925720192478132e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 2 hyperparams --> lr=2.72e-05, wd=4.81e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_014959-5nzemx9k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/5nzemx9k' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/5nzemx9k' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/5nzemx9k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7697 acc=0.4986 | val_loss=0.7164 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 002 | train_loss=0.7611 acc=0.5003 | val_loss=0.7220 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7559 acc=0.4952 | val_loss=0.7213 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7660 acc=0.4940 | val_loss=0.7183 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7471 acc=0.5088 | val_loss=0.7038 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7502 acc=0.4957 | val_loss=0.7047 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7386 acc=0.5184 | val_loss=0.7037 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 008 | train_loss=0.7623 acc=0.4804 | val_loss=0.7035 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7383 acc=0.5252 | val_loss=0.7025 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 010 | train_loss=0.7293 acc=0.5184 | val_loss=0.7026 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7464 acc=0.4827 | val_loss=0.7023 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7411 acc=0.4770 | val_loss=0.7040 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7299 acc=0.4997 | val_loss=0.7005 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 014 | train_loss=0.7350 acc=0.4986 | val_loss=0.6957 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7314 acc=0.5099 | val_loss=0.6961 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 016 | train_loss=0.7325 acc=0.5060 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7327 acc=0.4844 | val_loss=0.6918 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7314 acc=0.4935 | val_loss=0.6911 acc=0.5034 | prec=0.5011 rec=0.9955 spec=0.0136 f1=0.6667 | time=13.0s\n",
            "Epoch 019 | train_loss=0.7291 acc=0.4901 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 020 | train_loss=0.7153 acc=0.5173 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7213 acc=0.5071 | val_loss=0.6922 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 022 | train_loss=0.7126 acc=0.5128 | val_loss=0.6918 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 023 | train_loss=0.7397 acc=0.4810 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7258 acc=0.5235 | val_loss=0.7053 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 025 | train_loss=0.7015 acc=0.5377 | val_loss=0.6892 acc=0.5102 | prec=1.0000 rec=0.0182 spec=1.0000 f1=0.0357 | time=12.9s\n",
            "Epoch 026 | train_loss=0.6690 acc=0.5967 | val_loss=0.6261 acc=0.6961 | prec=0.8772 rec=0.4545 spec=0.9367 f1=0.5988 | time=13.1s\n",
            "Epoch 027 | train_loss=0.6083 acc=0.6733 | val_loss=0.5548 acc=0.7619 | prec=0.7833 rec=0.7227 spec=0.8009 f1=0.7518 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5655 acc=0.7124 | val_loss=0.5258 acc=0.7800 | prec=0.7470 rec=0.8455 spec=0.7149 f1=0.7932 | time=12.9s\n",
            "Epoch 029 | train_loss=0.5536 acc=0.7073 | val_loss=0.5143 acc=0.7846 | prec=0.7432 rec=0.8682 spec=0.7014 f1=0.8008 | time=13.0s\n",
            "Epoch 030 | train_loss=0.5363 acc=0.7283 | val_loss=0.4955 acc=0.7778 | prec=0.8506 rec=0.6727 spec=0.8824 f1=0.7513 | time=13.1s\n",
            "Epoch 031 | train_loss=0.5278 acc=0.7419 | val_loss=0.5016 acc=0.7664 | prec=0.8634 rec=0.6318 spec=0.9005 f1=0.7297 | time=12.9s\n",
            "Epoch 032 | train_loss=0.4986 acc=0.7612 | val_loss=0.4995 acc=0.7778 | prec=0.7179 rec=0.9136 spec=0.6425 f1=0.8040 | time=13.0s\n",
            "Epoch 033 | train_loss=0.4960 acc=0.7533 | val_loss=0.4712 acc=0.8186 | prec=0.8125 rec=0.8273 spec=0.8100 f1=0.8198 | time=13.1s\n",
            "Epoch 034 | train_loss=0.4696 acc=0.7771 | val_loss=0.4532 acc=0.8186 | prec=0.8182 rec=0.8182 spec=0.8190 f1=0.8182 | time=13.0s\n",
            "Epoch 035 | train_loss=0.4480 acc=0.7850 | val_loss=0.4613 acc=0.8095 | prec=0.7677 rec=0.8864 spec=0.7330 f1=0.8228 | time=13.0s\n",
            "Epoch 036 | train_loss=0.4350 acc=0.8083 | val_loss=0.4449 acc=0.8141 | prec=0.8286 rec=0.7909 spec=0.8371 f1=0.8093 | time=13.1s\n",
            "Epoch 037 | train_loss=0.4327 acc=0.8060 | val_loss=0.4447 acc=0.8118 | prec=0.7940 rec=0.8409 spec=0.7828 f1=0.8168 | time=13.2s\n",
            "Epoch 038 | train_loss=0.4421 acc=0.7935 | val_loss=0.4503 acc=0.8027 | prec=0.8595 rec=0.7227 spec=0.8824 f1=0.7852 | time=13.1s\n",
            "Epoch 039 | train_loss=0.4199 acc=0.8088 | val_loss=0.4365 acc=0.8209 | prec=0.8079 rec=0.8409 spec=0.8009 f1=0.8241 | time=13.1s\n",
            "Epoch 040 | train_loss=0.4064 acc=0.8174 | val_loss=0.4398 acc=0.8163 | prec=0.8174 rec=0.8136 spec=0.8190 f1=0.8155 | time=13.1s\n",
            "Epoch 041 | train_loss=0.4063 acc=0.8287 | val_loss=0.4365 acc=0.8141 | prec=0.7974 rec=0.8409 spec=0.7873 f1=0.8186 | time=13.0s\n",
            "Epoch 042 | train_loss=0.3910 acc=0.8327 | val_loss=0.4414 acc=0.8073 | prec=0.8462 rec=0.7500 spec=0.8643 f1=0.7952 | time=13.0s\n",
            "Epoch 043 | train_loss=0.3944 acc=0.8310 | val_loss=0.4670 acc=0.8027 | prec=0.8844 rec=0.6955 spec=0.9095 f1=0.7786 | time=13.1s\n",
            "Epoch 044 | train_loss=0.3929 acc=0.8389 | val_loss=0.4345 acc=0.8118 | prec=0.7890 rec=0.8500 spec=0.7738 f1=0.8184 | time=13.1s\n",
            "Epoch 045 | train_loss=0.3811 acc=0.8355 | val_loss=0.4199 acc=0.8163 | prec=0.8203 rec=0.8091 spec=0.8235 f1=0.8146 | time=13.1s\n",
            "Epoch 046 | train_loss=0.3836 acc=0.8491 | val_loss=0.4139 acc=0.8118 | prec=0.8018 rec=0.8273 spec=0.7964 f1=0.8143 | time=13.0s\n",
            "Epoch 047 | train_loss=0.3613 acc=0.8434 | val_loss=0.4145 acc=0.8231 | prec=0.8257 rec=0.8182 spec=0.8281 f1=0.8219 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3358 acc=0.8746 | val_loss=0.4118 acc=0.8231 | prec=0.8198 rec=0.8273 spec=0.8190 f1=0.8235 | time=13.1s\n",
            "Epoch 049 | train_loss=0.3438 acc=0.8548 | val_loss=0.4101 acc=0.8209 | prec=0.8439 rec=0.7864 spec=0.8552 f1=0.8141 | time=13.0s\n",
            "Epoch 050 | train_loss=0.3423 acc=0.8673 | val_loss=0.4124 acc=0.8209 | prec=0.8543 rec=0.7727 spec=0.8688 f1=0.8115 | time=12.9s\n",
            "Epoch 051 | train_loss=0.3674 acc=0.8622 | val_loss=0.3987 acc=0.8254 | prec=0.8357 rec=0.8091 spec=0.8416 f1=0.8222 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3298 acc=0.8633 | val_loss=0.4105 acc=0.8209 | prec=0.7975 rec=0.8591 spec=0.7828 f1=0.8271 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3376 acc=0.8786 | val_loss=0.4027 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3200 acc=0.8741 | val_loss=0.4019 acc=0.8186 | prec=0.7991 rec=0.8500 spec=0.7873 f1=0.8238 | time=12.9s\n",
            "Epoch 055 | train_loss=0.3171 acc=0.8826 | val_loss=0.4070 acc=0.8186 | prec=0.8500 rec=0.7727 spec=0.8643 f1=0.8095 | time=13.0s\n",
            "Epoch 056 | train_loss=0.3198 acc=0.8769 | val_loss=0.3980 acc=0.8209 | prec=0.8000 rec=0.8545 spec=0.7873 f1=0.8264 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3066 acc=0.8741 | val_loss=0.3937 acc=0.8299 | prec=0.8281 rec=0.8318 spec=0.8281 f1=0.8299 | time=13.1s\n",
            "Epoch 058 | train_loss=0.2947 acc=0.8826 | val_loss=0.3978 acc=0.8277 | prec=0.8333 rec=0.8182 spec=0.8371 f1=0.8257 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2857 acc=0.8905 | val_loss=0.3976 acc=0.8231 | prec=0.7983 rec=0.8636 spec=0.7828 f1=0.8297 | time=13.0s\n",
            "Epoch 060 | train_loss=0.2928 acc=0.8786 | val_loss=0.4104 acc=0.8209 | prec=0.7765 rec=0.9000 spec=0.7421 f1=0.8337 | time=12.9s\n",
            "Epoch 061 | train_loss=0.2776 acc=0.8956 | val_loss=0.3921 acc=0.8277 | prec=0.8243 rec=0.8318 spec=0.8235 f1=0.8281 | time=12.9s\n",
            "Epoch 062 | train_loss=0.2793 acc=0.8911 | val_loss=0.4042 acc=0.8209 | prec=0.8341 rec=0.8000 spec=0.8416 f1=0.8167 | time=13.1s\n",
            "Epoch 063 | train_loss=0.2745 acc=0.8905 | val_loss=0.3896 acc=0.8254 | prec=0.8178 rec=0.8364 spec=0.8145 f1=0.8270 | time=12.9s\n",
            "Epoch 064 | train_loss=0.2632 acc=0.9053 | val_loss=0.3871 acc=0.8390 | prec=0.8197 rec=0.8682 spec=0.8100 f1=0.8433 | time=13.0s\n",
            "Epoch 065 | train_loss=0.2526 acc=0.9075 | val_loss=0.3927 acc=0.8277 | prec=0.8130 rec=0.8500 spec=0.8054 f1=0.8311 | time=12.9s\n",
            "Epoch 066 | train_loss=0.2511 acc=0.9092 | val_loss=0.3973 acc=0.8345 | prec=0.8101 rec=0.8727 spec=0.7964 f1=0.8403 | time=13.0s\n",
            "Epoch 067 | train_loss=0.2564 acc=0.9013 | val_loss=0.3894 acc=0.8277 | prec=0.8364 rec=0.8136 spec=0.8416 f1=0.8249 | time=13.1s\n",
            "Epoch 068 | train_loss=0.2551 acc=0.8951 | val_loss=0.3883 acc=0.8231 | prec=0.8114 rec=0.8409 spec=0.8054 f1=0.8259 | time=12.9s\n",
            "Epoch 069 | train_loss=0.2326 acc=0.9075 | val_loss=0.3967 acc=0.8163 | prec=0.8325 rec=0.7909 spec=0.8416 f1=0.8112 | time=12.9s\n",
            "Epoch 070 | train_loss=0.2374 acc=0.9070 | val_loss=0.3915 acc=0.8231 | prec=0.7958 rec=0.8682 spec=0.7783 f1=0.8304 | time=13.0s\n",
            "Epoch 071 | train_loss=0.2529 acc=0.8968 | val_loss=0.4043 acc=0.8209 | prec=0.8507 rec=0.7773 spec=0.8643 f1=0.8124 | time=13.0s\n",
            "Epoch 072 | train_loss=0.2305 acc=0.9149 | val_loss=0.4132 acc=0.8367 | prec=0.8895 rec=0.7682 spec=0.9050 f1=0.8244 | time=13.1s\n",
            "Epoch 073 | train_loss=0.2599 acc=0.8996 | val_loss=0.3939 acc=0.8209 | prec=0.7743 rec=0.9045 spec=0.7376 f1=0.8344 | time=13.0s\n",
            "Epoch 074 | train_loss=0.2677 acc=0.9064 | val_loss=0.3858 acc=0.8277 | prec=0.8025 rec=0.8682 spec=0.7873 f1=0.8341 | time=12.8s\n",
            "Epoch 075 | train_loss=0.2447 acc=0.8939 | val_loss=0.4743 acc=0.8141 | prec=0.9107 rec=0.6955 spec=0.9321 f1=0.7887 | time=13.0s\n",
            "Epoch 076 | train_loss=0.2335 acc=0.9075 | val_loss=0.3847 acc=0.8231 | prec=0.8008 rec=0.8591 spec=0.7873 f1=0.8289 | time=13.0s\n",
            "Epoch 077 | train_loss=0.2261 acc=0.9149 | val_loss=0.3902 acc=0.8299 | prec=0.8194 rec=0.8455 spec=0.8145 f1=0.8322 | time=12.9s\n",
            "Epoch 078 | train_loss=0.2230 acc=0.9058 | val_loss=0.3953 acc=0.8141 | prec=0.8080 rec=0.8227 spec=0.8054 f1=0.8153 | time=13.0s\n",
            "Epoch 079 | train_loss=0.2179 acc=0.9115 | val_loss=0.3990 acc=0.8277 | prec=0.8495 rec=0.7955 spec=0.8597 f1=0.8216 | time=12.9s\n",
            "Epoch 080 | train_loss=0.2185 acc=0.9121 | val_loss=0.3828 acc=0.8254 | prec=0.8095 rec=0.8500 spec=0.8009 f1=0.8293 | time=13.0s\n",
            "Epoch 081 | train_loss=0.2421 acc=0.9206 | val_loss=0.3949 acc=0.8231 | prec=0.8287 rec=0.8136 spec=0.8326 f1=0.8211 | time=13.1s\n",
            "Epoch 082 | train_loss=0.2319 acc=0.9132 | val_loss=0.3939 acc=0.8186 | prec=0.8070 rec=0.8364 spec=0.8009 f1=0.8214 | time=12.9s\n",
            "Epoch 083 | train_loss=0.2265 acc=0.9092 | val_loss=0.4026 acc=0.8186 | prec=0.7846 rec=0.8773 spec=0.7602 f1=0.8283 | time=12.9s\n",
            "Epoch 084 | train_loss=0.2159 acc=0.9126 | val_loss=0.3934 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=13.0s\n",
            "Epoch 085 | train_loss=0.2137 acc=0.9183 | val_loss=0.4056 acc=0.8163 | prec=0.8263 rec=0.8000 spec=0.8326 f1=0.8129 | time=12.9s\n",
            "Epoch 086 | train_loss=0.2012 acc=0.9280 | val_loss=0.4027 acc=0.8186 | prec=0.8302 rec=0.8000 spec=0.8371 f1=0.8148 | time=13.1s\n",
            "Epoch 087 | train_loss=0.2092 acc=0.9149 | val_loss=0.4086 acc=0.8231 | prec=0.7910 rec=0.8773 spec=0.7692 f1=0.8319 | time=13.0s\n",
            "Epoch 088 | train_loss=0.2205 acc=0.9132 | val_loss=0.4098 acc=0.8163 | prec=0.8263 rec=0.8000 spec=0.8326 f1=0.8129 | time=13.1s\n",
            "Epoch 089 | train_loss=0.2207 acc=0.9263 | val_loss=0.4041 acc=0.8299 | prec=0.8059 rec=0.8682 spec=0.7919 f1=0.8359 | time=13.0s\n",
            "Epoch 090 | train_loss=0.2085 acc=0.9064 | val_loss=0.4108 acc=0.8231 | prec=0.7910 rec=0.8773 spec=0.7692 f1=0.8319 | time=12.8s\n",
            "Epoch 091 | train_loss=0.2047 acc=0.9138 | val_loss=0.4446 acc=0.8186 | prec=0.8933 rec=0.7227 spec=0.9140 f1=0.7990 | time=13.0s\n",
            "Epoch 092 | train_loss=0.2080 acc=0.9217 | val_loss=0.4082 acc=0.8299 | prec=0.8281 rec=0.8318 spec=0.8281 f1=0.8299 | time=13.0s\n",
            "Epoch 093 | train_loss=0.2009 acc=0.9200 | val_loss=0.4055 acc=0.8209 | prec=0.8310 rec=0.8045 spec=0.8371 f1=0.8176 | time=12.9s\n",
            "Epoch 094 | train_loss=0.2056 acc=0.9200 | val_loss=0.4099 acc=0.8277 | prec=0.8462 rec=0.8000 spec=0.8552 f1=0.8224 | time=12.9s\n",
            "Epoch 095 | train_loss=0.1999 acc=0.9234 | val_loss=0.4027 acc=0.8254 | prec=0.8206 rec=0.8318 spec=0.8190 f1=0.8262 | time=12.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>f1_score</td><td>▇▇▇▇▇▇▇▇▇▇▇▇▁▁█▇████▇███████████████████</td></tr><tr><td>precision</td><td>▅▅▅▅▅▅▁▁▁▁█▇█▇▇█▇████▇███▇██▇▇▇██▇▇█▇███</td></tr><tr><td>recall</td><td>█████████▁▁▄▇▇▅▇▇▇▆▆▇▇▆▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▆▇</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁█████▇▅▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▁▁▁▁▂▂▄▅▅▅▇▇▇▇▇▇▇█▇▇█████▇█████████</td></tr><tr><td>train_loss</td><td>██████████▇▇█▇▇▅▅▄▄▄▄▄▃▃▃▂▂▂▂▂▂▂▁▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▇▇▇▇███▇██████████████████████</td></tr><tr><td>val_loss</td><td>██████▇▇▇▇▇▆▅▃▂▂▂▃▂▂▂▂▁▁▁▁▁▁▁▁▃▁▁▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>95</td></tr><tr><td>f1_score</td><td>0.82619</td></tr><tr><td>precision</td><td>0.82063</td></tr><tr><td>recall</td><td>0.83182</td></tr><tr><td>specificity</td><td>0.819</td></tr><tr><td>train_acc</td><td>0.92343</td></tr><tr><td>train_loss</td><td>0.19993</td></tr><tr><td>val_acc</td><td>0.8254</td></tr><tr><td>val_loss</td><td>0.40271</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/5nzemx9k' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/5nzemx9k</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_014959-5nzemx9k/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 02:10:36,820] Trial 2 finished with values: [0.38283403856413706, 0.8253968253968254] and parameters: {'lr': 2.7229137262861128e-05, 'wd': 4.809973522202446e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 3 hyperparams --> lr=1.33e-05, wd=1.41e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_021036-6zttz7tj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/6zttz7tj' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/6zttz7tj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/6zttz7tj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7741 acc=0.4878 | val_loss=0.7007 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7668 acc=0.5071 | val_loss=0.7002 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7578 acc=0.5014 | val_loss=0.6958 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7595 acc=0.4906 | val_loss=0.6942 acc=0.5193 | prec=0.5098 rec=0.9500 spec=0.0905 f1=0.6635 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7460 acc=0.5122 | val_loss=0.6937 acc=0.4558 | prec=0.4621 rec=0.5545 spec=0.3575 f1=0.5041 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7606 acc=0.4940 | val_loss=0.6940 acc=0.4807 | prec=0.3846 rec=0.0682 spec=0.8914 f1=0.1158 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7318 acc=0.5286 | val_loss=0.6939 acc=0.5125 | prec=0.5062 rec=0.9318 spec=0.0950 f1=0.6560 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7351 acc=0.4952 | val_loss=0.6937 acc=0.4921 | prec=0.4000 rec=0.0364 spec=0.9457 f1=0.0667 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7397 acc=0.4850 | val_loss=0.6932 acc=0.5011 | prec=0.5000 rec=0.1318 spec=0.8688 f1=0.2086 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7377 acc=0.4997 | val_loss=0.6929 acc=0.5193 | prec=0.5455 rec=0.2182 spec=0.8190 f1=0.3117 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7413 acc=0.4912 | val_loss=0.6934 acc=0.4966 | prec=0.3750 rec=0.0136 spec=0.9774 f1=0.0263 | time=13.0s\n",
            "Epoch 012 | train_loss=0.7355 acc=0.4991 | val_loss=0.6933 acc=0.4989 | prec=0.4444 rec=0.0182 spec=0.9774 f1=0.0349 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7436 acc=0.4997 | val_loss=0.6933 acc=0.5034 | prec=0.6000 rec=0.0136 spec=0.9910 f1=0.0267 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7252 acc=0.5054 | val_loss=0.6940 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7385 acc=0.4889 | val_loss=0.6923 acc=0.5034 | prec=0.6000 rec=0.0136 spec=0.9910 f1=0.0267 | time=13.0s\n",
            "Epoch 016 | train_loss=0.7235 acc=0.5139 | val_loss=0.6890 acc=0.5442 | prec=0.6462 rec=0.1909 spec=0.8959 f1=0.2947 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7341 acc=0.5014 | val_loss=0.6878 acc=0.5329 | prec=0.6944 rec=0.1136 spec=0.9502 f1=0.1953 | time=13.0s\n",
            "Epoch 018 | train_loss=0.7234 acc=0.5201 | val_loss=0.6871 acc=0.5125 | prec=1.0000 rec=0.0227 spec=1.0000 f1=0.0444 | time=12.9s\n",
            "Epoch 019 | train_loss=0.7255 acc=0.5201 | val_loss=0.6766 acc=0.5510 | prec=0.7895 rec=0.1364 spec=0.9638 f1=0.2326 | time=13.0s\n",
            "Epoch 020 | train_loss=0.7083 acc=0.5372 | val_loss=0.6751 acc=0.5351 | prec=0.9412 rec=0.0727 spec=0.9955 f1=0.1350 | time=12.9s\n",
            "Epoch 021 | train_loss=0.6796 acc=0.5905 | val_loss=0.6518 acc=0.5941 | prec=0.8868 rec=0.2136 spec=0.9729 f1=0.3443 | time=12.9s\n",
            "Epoch 022 | train_loss=0.6341 acc=0.6336 | val_loss=0.6867 acc=0.5601 | prec=0.9643 rec=0.1227 spec=0.9955 f1=0.2177 | time=13.0s\n",
            "Epoch 023 | train_loss=0.6007 acc=0.6716 | val_loss=0.5734 acc=0.7460 | prec=0.8803 rec=0.5682 spec=0.9231 f1=0.6906 | time=13.0s\n",
            "Epoch 024 | train_loss=0.5691 acc=0.7062 | val_loss=0.5647 acc=0.7528 | prec=0.8675 rec=0.5955 spec=0.9095 f1=0.7062 | time=12.9s\n",
            "Epoch 025 | train_loss=0.5758 acc=0.7011 | val_loss=0.5918 acc=0.7075 | prec=0.9252 rec=0.4500 spec=0.9638 f1=0.6055 | time=13.0s\n",
            "Epoch 026 | train_loss=0.5575 acc=0.7209 | val_loss=0.5754 acc=0.7211 | prec=0.8880 rec=0.5045 spec=0.9367 f1=0.6435 | time=13.0s\n",
            "Epoch 027 | train_loss=0.5568 acc=0.7113 | val_loss=0.5444 acc=0.7755 | prec=0.8497 rec=0.6682 spec=0.8824 f1=0.7481 | time=13.0s\n",
            "Epoch 028 | train_loss=0.5492 acc=0.7436 | val_loss=0.5270 acc=0.7868 | prec=0.7972 rec=0.7682 spec=0.8054 f1=0.7824 | time=13.0s\n",
            "Epoch 029 | train_loss=0.5475 acc=0.7283 | val_loss=0.5139 acc=0.7959 | prec=0.8421 rec=0.7273 spec=0.8643 f1=0.7805 | time=13.0s\n",
            "Epoch 030 | train_loss=0.5197 acc=0.7623 | val_loss=0.5002 acc=0.8005 | prec=0.8367 rec=0.7455 spec=0.8552 f1=0.7885 | time=13.0s\n",
            "Epoch 031 | train_loss=0.5133 acc=0.7686 | val_loss=0.5058 acc=0.7982 | prec=0.8541 rec=0.7182 spec=0.8778 f1=0.7802 | time=13.0s\n",
            "Epoch 032 | train_loss=0.5116 acc=0.7635 | val_loss=0.4894 acc=0.7868 | prec=0.8706 rec=0.6727 spec=0.9005 f1=0.7590 | time=13.0s\n",
            "Epoch 033 | train_loss=0.5172 acc=0.7436 | val_loss=0.4941 acc=0.7823 | prec=0.8735 rec=0.6591 spec=0.9050 f1=0.7513 | time=13.0s\n",
            "Epoch 034 | train_loss=0.5100 acc=0.7533 | val_loss=0.5110 acc=0.7528 | prec=0.8936 rec=0.5727 spec=0.9321 f1=0.6981 | time=13.0s\n",
            "Epoch 035 | train_loss=0.4975 acc=0.7618 | val_loss=0.4922 acc=0.7778 | prec=0.8765 rec=0.6455 spec=0.9095 f1=0.7435 | time=13.0s\n",
            "Epoch 036 | train_loss=0.4901 acc=0.7737 | val_loss=0.4670 acc=0.8050 | prec=0.8602 rec=0.7273 spec=0.8824 f1=0.7882 | time=13.0s\n",
            "Epoch 037 | train_loss=0.4880 acc=0.7691 | val_loss=0.4813 acc=0.7982 | prec=0.7787 rec=0.8318 spec=0.7647 f1=0.8044 | time=12.8s\n",
            "Epoch 038 | train_loss=0.4696 acc=0.7816 | val_loss=0.4597 acc=0.8118 | prec=0.8624 rec=0.7409 spec=0.8824 f1=0.7971 | time=12.9s\n",
            "Epoch 039 | train_loss=0.4615 acc=0.7879 | val_loss=0.4664 acc=0.8163 | prec=0.8263 rec=0.8000 spec=0.8326 f1=0.8129 | time=13.0s\n",
            "Epoch 040 | train_loss=0.4656 acc=0.7816 | val_loss=0.4633 acc=0.8095 | prec=0.8238 rec=0.7864 spec=0.8326 f1=0.8047 | time=12.9s\n",
            "Epoch 041 | train_loss=0.4891 acc=0.7737 | val_loss=0.4530 acc=0.8141 | prec=0.8224 rec=0.8000 spec=0.8281 f1=0.8111 | time=12.9s\n",
            "Epoch 042 | train_loss=0.4545 acc=0.7850 | val_loss=0.4486 acc=0.8073 | prec=0.8325 rec=0.7682 spec=0.8462 f1=0.7991 | time=12.9s\n",
            "Epoch 043 | train_loss=0.4663 acc=0.7811 | val_loss=0.4608 acc=0.8209 | prec=0.8219 rec=0.8182 spec=0.8235 f1=0.8200 | time=13.0s\n",
            "Epoch 044 | train_loss=0.4485 acc=0.7975 | val_loss=0.4479 acc=0.8141 | prec=0.8485 rec=0.7636 spec=0.8643 f1=0.8038 | time=13.1s\n",
            "Epoch 045 | train_loss=0.4441 acc=0.8009 | val_loss=0.4396 acc=0.8186 | prec=0.8398 rec=0.7864 spec=0.8507 f1=0.8122 | time=13.0s\n",
            "Epoch 046 | train_loss=0.4419 acc=0.8015 | val_loss=0.4417 acc=0.8141 | prec=0.8108 rec=0.8182 spec=0.8100 f1=0.8145 | time=12.9s\n",
            "Epoch 047 | train_loss=0.4177 acc=0.8117 | val_loss=0.4514 acc=0.8209 | prec=0.8473 rec=0.7818 spec=0.8597 f1=0.8132 | time=13.0s\n",
            "Epoch 048 | train_loss=0.4237 acc=0.8009 | val_loss=0.4506 acc=0.8118 | prec=0.7965 rec=0.8364 spec=0.7873 f1=0.8160 | time=12.9s\n",
            "Epoch 049 | train_loss=0.4057 acc=0.8276 | val_loss=0.4430 acc=0.8118 | prec=0.8586 rec=0.7455 spec=0.8778 f1=0.7981 | time=12.9s\n",
            "Epoch 050 | train_loss=0.4168 acc=0.8185 | val_loss=0.4440 acc=0.8073 | prec=0.7897 rec=0.8364 spec=0.7783 f1=0.8124 | time=13.0s\n",
            "Epoch 051 | train_loss=0.4242 acc=0.8100 | val_loss=0.4317 acc=0.8118 | prec=0.8128 rec=0.8091 spec=0.8145 f1=0.8109 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3983 acc=0.8259 | val_loss=0.4348 acc=0.8118 | prec=0.8374 rec=0.7727 spec=0.8507 f1=0.8038 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3970 acc=0.8196 | val_loss=0.4456 acc=0.8095 | prec=0.8148 rec=0.8000 spec=0.8190 f1=0.8073 | time=12.9s\n",
            "Epoch 054 | train_loss=0.4001 acc=0.8179 | val_loss=0.4228 acc=0.8095 | prec=0.8208 rec=0.7909 spec=0.8281 f1=0.8056 | time=12.8s\n",
            "Epoch 055 | train_loss=0.3891 acc=0.8236 | val_loss=0.4240 acc=0.8141 | prec=0.8286 rec=0.7909 spec=0.8371 f1=0.8093 | time=12.8s\n",
            "Epoch 056 | train_loss=0.4101 acc=0.8225 | val_loss=0.4399 acc=0.8118 | prec=0.8072 rec=0.8182 spec=0.8054 f1=0.8126 | time=12.8s\n",
            "Epoch 057 | train_loss=0.4123 acc=0.8054 | val_loss=0.4283 acc=0.8095 | prec=0.8469 rec=0.7545 spec=0.8643 f1=0.7981 | time=12.9s\n",
            "Epoch 058 | train_loss=0.3954 acc=0.8236 | val_loss=0.4556 acc=0.7937 | prec=0.7529 rec=0.8727 spec=0.7149 f1=0.8084 | time=13.0s\n",
            "Epoch 059 | train_loss=0.4095 acc=0.8242 | val_loss=0.4381 acc=0.8118 | prec=0.7991 rec=0.8318 spec=0.7919 f1=0.8151 | time=13.0s\n",
            "Epoch 060 | train_loss=0.3869 acc=0.8202 | val_loss=0.4318 acc=0.8141 | prec=0.8350 rec=0.7818 spec=0.8462 f1=0.8075 | time=13.1s\n",
            "Epoch 061 | train_loss=0.3912 acc=0.8225 | val_loss=0.4327 acc=0.8186 | prec=0.7941 rec=0.8591 spec=0.7783 f1=0.8253 | time=13.2s\n",
            "Epoch 062 | train_loss=0.3633 acc=0.8395 | val_loss=0.4346 acc=0.7914 | prec=0.8516 rec=0.7045 spec=0.8778 f1=0.7711 | time=13.0s\n",
            "Epoch 063 | train_loss=0.3937 acc=0.8321 | val_loss=0.4196 acc=0.8141 | prec=0.8194 rec=0.8045 spec=0.8235 f1=0.8119 | time=12.9s\n",
            "Epoch 064 | train_loss=0.3957 acc=0.8225 | val_loss=0.4200 acc=0.8118 | prec=0.8216 rec=0.7955 spec=0.8281 f1=0.8083 | time=12.9s\n",
            "Epoch 065 | train_loss=0.3662 acc=0.8469 | val_loss=0.4245 acc=0.8118 | prec=0.8278 rec=0.7864 spec=0.8371 f1=0.8065 | time=12.9s\n",
            "Epoch 066 | train_loss=0.3585 acc=0.8400 | val_loss=0.4222 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=13.0s\n",
            "Epoch 067 | train_loss=0.3810 acc=0.8202 | val_loss=0.4120 acc=0.8186 | prec=0.8070 rec=0.8364 spec=0.8009 f1=0.8214 | time=12.9s\n",
            "Epoch 068 | train_loss=0.3689 acc=0.8412 | val_loss=0.4187 acc=0.8141 | prec=0.8165 rec=0.8091 spec=0.8190 f1=0.8128 | time=12.9s\n",
            "Epoch 069 | train_loss=0.3871 acc=0.8230 | val_loss=0.4291 acc=0.8163 | prec=0.8325 rec=0.7909 spec=0.8416 f1=0.8112 | time=13.0s\n",
            "Epoch 070 | train_loss=0.3629 acc=0.8349 | val_loss=0.4284 acc=0.8118 | prec=0.8513 rec=0.7545 spec=0.8688 f1=0.8000 | time=13.0s\n",
            "Epoch 071 | train_loss=0.3628 acc=0.8276 | val_loss=0.4224 acc=0.8027 | prec=0.8519 rec=0.7318 spec=0.8733 f1=0.7873 | time=13.0s\n",
            "Epoch 072 | train_loss=0.3624 acc=0.8400 | val_loss=0.4111 acc=0.8118 | prec=0.8278 rec=0.7864 spec=0.8371 f1=0.8065 | time=13.0s\n",
            "Epoch 073 | train_loss=0.3552 acc=0.8412 | val_loss=0.4153 acc=0.8163 | prec=0.8528 rec=0.7636 spec=0.8688 f1=0.8058 | time=12.9s\n",
            "Epoch 074 | train_loss=0.3676 acc=0.8508 | val_loss=0.4260 acc=0.8095 | prec=0.7677 rec=0.8864 spec=0.7330 f1=0.8228 | time=13.0s\n",
            "Epoch 075 | train_loss=0.3391 acc=0.8514 | val_loss=0.4081 acc=0.8231 | prec=0.8257 rec=0.8182 spec=0.8281 f1=0.8219 | time=12.9s\n",
            "Epoch 076 | train_loss=0.3461 acc=0.8395 | val_loss=0.4118 acc=0.8277 | prec=0.8243 rec=0.8318 spec=0.8235 f1=0.8281 | time=12.9s\n",
            "Epoch 077 | train_loss=0.3591 acc=0.8520 | val_loss=0.4137 acc=0.8186 | prec=0.8043 rec=0.8409 spec=0.7964 f1=0.8222 | time=12.9s\n",
            "Epoch 078 | train_loss=0.3472 acc=0.8412 | val_loss=0.4198 acc=0.8095 | prec=0.8542 rec=0.7455 spec=0.8733 f1=0.7961 | time=13.0s\n",
            "Epoch 079 | train_loss=0.3468 acc=0.8440 | val_loss=0.4203 acc=0.8073 | prec=0.8462 rec=0.7500 spec=0.8643 f1=0.7952 | time=13.1s\n",
            "Epoch 080 | train_loss=0.3504 acc=0.8412 | val_loss=0.4022 acc=0.8254 | prec=0.8357 rec=0.8091 spec=0.8416 f1=0.8222 | time=12.9s\n",
            "Epoch 081 | train_loss=0.3600 acc=0.8429 | val_loss=0.4011 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.9s\n",
            "Epoch 082 | train_loss=0.3290 acc=0.8610 | val_loss=0.4064 acc=0.8186 | prec=0.8431 rec=0.7818 spec=0.8552 f1=0.8113 | time=12.9s\n",
            "Epoch 083 | train_loss=0.3408 acc=0.8429 | val_loss=0.4065 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=13.0s\n",
            "Epoch 084 | train_loss=0.3390 acc=0.8474 | val_loss=0.4021 acc=0.8163 | prec=0.8424 rec=0.7773 spec=0.8552 f1=0.8085 | time=12.9s\n",
            "Epoch 085 | train_loss=0.3297 acc=0.8605 | val_loss=0.4021 acc=0.8231 | prec=0.8413 rec=0.7955 spec=0.8507 f1=0.8178 | time=12.9s\n",
            "Epoch 086 | train_loss=0.3326 acc=0.8576 | val_loss=0.4078 acc=0.8186 | prec=0.8571 rec=0.7636 spec=0.8733 f1=0.8077 | time=13.0s\n",
            "Epoch 087 | train_loss=0.3187 acc=0.8599 | val_loss=0.3984 acc=0.8254 | prec=0.8295 rec=0.8182 spec=0.8326 f1=0.8238 | time=13.0s\n",
            "Epoch 088 | train_loss=0.3237 acc=0.8605 | val_loss=0.3912 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.9s\n",
            "Epoch 089 | train_loss=0.3363 acc=0.8627 | val_loss=0.3942 acc=0.8231 | prec=0.8318 rec=0.8091 spec=0.8371 f1=0.8203 | time=13.0s\n",
            "Epoch 090 | train_loss=0.3432 acc=0.8355 | val_loss=0.3945 acc=0.8186 | prec=0.8500 rec=0.7727 spec=0.8643 f1=0.8095 | time=12.9s\n",
            "Epoch 091 | train_loss=0.3244 acc=0.8599 | val_loss=0.3940 acc=0.8186 | prec=0.8241 rec=0.8091 spec=0.8281 f1=0.8165 | time=12.9s\n",
            "Epoch 092 | train_loss=0.3175 acc=0.8673 | val_loss=0.3918 acc=0.8186 | prec=0.8302 rec=0.8000 spec=0.8371 f1=0.8148 | time=13.0s\n",
            "Epoch 093 | train_loss=0.3166 acc=0.8599 | val_loss=0.3953 acc=0.8141 | prec=0.8317 rec=0.7864 spec=0.8416 f1=0.8084 | time=13.0s\n",
            "Epoch 094 | train_loss=0.3227 acc=0.8610 | val_loss=0.3991 acc=0.8209 | prec=0.8507 rec=0.7773 spec=0.8643 f1=0.8124 | time=12.8s\n",
            "Epoch 095 | train_loss=0.3219 acc=0.8622 | val_loss=0.3954 acc=0.8277 | prec=0.8273 rec=0.8273 spec=0.8281 f1=0.8273 | time=12.9s\n",
            "Epoch 096 | train_loss=0.3213 acc=0.8610 | val_loss=0.3973 acc=0.8186 | prec=0.8465 rec=0.7773 spec=0.8597 f1=0.8104 | time=13.0s\n",
            "Epoch 097 | train_loss=0.3255 acc=0.8537 | val_loss=0.3969 acc=0.8186 | prec=0.8431 rec=0.7818 spec=0.8552 f1=0.8113 | time=12.9s\n",
            "Epoch 098 | train_loss=0.3110 acc=0.8707 | val_loss=0.3958 acc=0.8163 | prec=0.8390 rec=0.7818 spec=0.8507 f1=0.8094 | time=13.1s\n",
            "Epoch 099 | train_loss=0.3216 acc=0.8701 | val_loss=0.3915 acc=0.8186 | prec=0.8365 rec=0.7909 spec=0.8462 f1=0.8131 | time=13.0s\n",
            "Epoch 100 | train_loss=0.3117 acc=0.8633 | val_loss=0.3968 acc=0.8209 | prec=0.8473 rec=0.7818 spec=0.8597 f1=0.8132 | time=12.9s\n",
            "Epoch 101 | train_loss=0.3030 acc=0.8741 | val_loss=0.3952 acc=0.8163 | prec=0.8424 rec=0.7773 spec=0.8552 f1=0.8085 | time=12.9s\n",
            "Epoch 102 | train_loss=0.3049 acc=0.8633 | val_loss=0.3984 acc=0.8186 | prec=0.8465 rec=0.7773 spec=0.8597 f1=0.8104 | time=13.0s\n",
            "Epoch 103 | train_loss=0.3061 acc=0.8729 | val_loss=0.3945 acc=0.8186 | prec=0.8333 rec=0.7955 spec=0.8416 f1=0.8140 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>f1_score</td><td>▇▇▁▃▁▁▃▂▄▇█▇▇███████████████████████████</td></tr><tr><td>precision</td><td>▂▂▂▂▁▅▆█▇▇▆▇▆▆▆▆▆▆▆▆▆▆▅▆▆▆▇▆▇▆▆▆▇▆▇▆▇▆▆▆</td></tr><tr><td>recall</td><td>███▅█▁▁▁▁▁▆▆▆▆▆▆▆▇▆▆▇▇▇▇▆▆▇▇▆▆▇▇▇▇▆▇▇▆▆▆</td></tr><tr><td>specificity</td><td>▁▁█▇██▇█▇█▇▆▇▇▇▇▇▆▇▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▂▂▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇█▇██▇████</td></tr><tr><td>train_loss</td><td>███████▇▇▇▅▅▅▄▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▁▁▂▂▂▂▂▃▄▆▇▇▇▇▇▇█████████▇█████████████</td></tr><tr><td>val_loss</td><td>████████▄▄▃▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>103</td></tr><tr><td>f1_score</td><td>0.81395</td></tr><tr><td>precision</td><td>0.83333</td></tr><tr><td>recall</td><td>0.79545</td></tr><tr><td>specificity</td><td>0.84163</td></tr><tr><td>train_acc</td><td>0.87294</td></tr><tr><td>train_loss</td><td>0.30612</td></tr><tr><td>val_acc</td><td>0.81859</td></tr><tr><td>val_loss</td><td>0.39446</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/6zttz7tj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/6zttz7tj</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_021036-6zttz7tj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 02:32:55,157] Trial 3 finished with values: [0.391240303005491, 0.8276643990929705] and parameters: {'lr': 1.3326310517839097e-05, 'wd': 1.4101074056400748e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 4 hyperparams --> lr=2.11e-05, wd=1.77e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_023255-1zfi70mu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/1zfi70mu' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/1zfi70mu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/1zfi70mu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7669 acc=0.4957 | val_loss=0.7074 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.2s\n",
            "Epoch 002 | train_loss=0.7469 acc=0.5156 | val_loss=0.7050 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7479 acc=0.4997 | val_loss=0.6981 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 004 | train_loss=0.7201 acc=0.5281 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7463 acc=0.4855 | val_loss=0.6924 acc=0.5079 | prec=0.5484 rec=0.0773 spec=0.9367 f1=0.1355 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7155 acc=0.5128 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 007 | train_loss=0.7244 acc=0.5094 | val_loss=0.6919 acc=0.5283 | prec=0.5448 rec=0.3318 spec=0.7240 f1=0.4124 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7249 acc=0.5116 | val_loss=0.6929 acc=0.5011 | prec=0.5000 rec=0.0045 spec=0.9955 f1=0.0090 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7409 acc=0.4793 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7197 acc=0.5054 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7221 acc=0.4986 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 012 | train_loss=0.7319 acc=0.4748 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7328 acc=0.4912 | val_loss=0.6986 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 014 | train_loss=0.7241 acc=0.5026 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7287 acc=0.4991 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 016 | train_loss=0.7392 acc=0.4702 | val_loss=0.6943 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 017 | train_loss=0.7284 acc=0.4884 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7282 acc=0.4850 | val_loss=0.6950 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 019 | train_loss=0.7232 acc=0.4952 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7062 acc=0.5043 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7204 acc=0.4912 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 022 | train_loss=0.7210 acc=0.4986 | val_loss=0.6928 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▃▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>precision</td><td>▁▁▁▁▅▁▅▅▁▁▁▁▁▁▁▁▁▁▁▁▁█</td></tr><tr><td>recall</td><td>▁▁▁▁▃▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>specificity</td><td>████▆█▁███████████████</td></tr><tr><td>train_acc</td><td>▄▆▅█▃▆▆▆▂▅▄▂▄▅▅▁▃▃▄▅▄▄</td></tr><tr><td>train_loss</td><td>█▆▆▃▆▂▃▃▅▃▃▄▄▃▄▅▄▄▃▁▃▃</td></tr><tr><td>val_acc</td><td>▁▁▁▁▃▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>val_loss</td><td>█▇▄▂▁▂▁▁▂▃▃▂▄▂▃▂▂▂▂▄▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>f1_score</td><td>0.00905</td></tr><tr><td>precision</td><td>1</td></tr><tr><td>recall</td><td>0.00455</td></tr><tr><td>specificity</td><td>1</td></tr><tr><td>train_acc</td><td>0.49858</td></tr><tr><td>train_loss</td><td>0.72104</td></tr><tr><td>val_acc</td><td>0.5034</td></tr><tr><td>val_loss</td><td>0.69279</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/1zfi70mu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/1zfi70mu</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_023255-1zfi70mu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 02:37:44,709] Trial 4 finished with values: [0.6919034804616656, 0.528344671201814] and parameters: {'lr': 2.107484115009806e-05, 'wd': 1.7676336080751873e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 5 hyperparams --> lr=8.42e-05, wd=4.90e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_023744-jndqy4kv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/jndqy4kv' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/jndqy4kv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/jndqy4kv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7241 acc=0.5145 | val_loss=0.6978 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7162 acc=0.5026 | val_loss=0.6971 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7224 acc=0.4838 | val_loss=0.6983 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7207 acc=0.5026 | val_loss=0.6962 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7240 acc=0.4895 | val_loss=0.6933 acc=0.4989 | prec=0.4986 rec=0.7909 spec=0.2081 f1=0.6116 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7108 acc=0.5094 | val_loss=0.6946 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7139 acc=0.4855 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7145 acc=0.4906 | val_loss=0.6933 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7097 acc=0.4986 | val_loss=0.6931 acc=0.4966 | prec=0.4977 rec=0.9864 spec=0.0090 f1=0.6616 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7069 acc=0.5094 | val_loss=0.6929 acc=0.5102 | prec=1.0000 rec=0.0182 spec=1.0000 f1=0.0357 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7087 acc=0.5184 | val_loss=0.6925 acc=0.5578 | prec=0.5424 rec=0.7273 spec=0.3891 f1=0.6214 | time=13.0s\n",
            "Epoch 012 | train_loss=0.7100 acc=0.5150 | val_loss=0.6925 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7125 acc=0.4969 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 014 | train_loss=0.7128 acc=0.4923 | val_loss=0.6909 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 015 | train_loss=0.6965 acc=0.5349 | val_loss=0.6681 acc=0.7120 | prec=0.8298 rec=0.5318 spec=0.8914 f1=0.6482 | time=13.0s\n",
            "Epoch 016 | train_loss=0.6559 acc=0.6058 | val_loss=0.5615 acc=0.7687 | prec=0.8391 rec=0.6636 spec=0.8733 f1=0.7411 | time=13.0s\n",
            "Epoch 017 | train_loss=0.6157 acc=0.6517 | val_loss=0.5320 acc=0.7868 | prec=0.8119 rec=0.7455 spec=0.8281 f1=0.7773 | time=13.0s\n",
            "Epoch 018 | train_loss=0.5710 acc=0.7260 | val_loss=0.5041 acc=0.7982 | prec=0.8047 rec=0.7864 spec=0.8100 f1=0.7954 | time=12.8s\n",
            "Epoch 019 | train_loss=0.5533 acc=0.7368 | val_loss=0.5017 acc=0.8050 | prec=0.7617 rec=0.8864 spec=0.7240 f1=0.8193 | time=12.9s\n",
            "Epoch 020 | train_loss=0.5334 acc=0.7425 | val_loss=0.4636 acc=0.8322 | prec=0.8230 rec=0.8455 spec=0.8190 f1=0.8341 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4865 acc=0.7879 | val_loss=0.4826 acc=0.7959 | prec=0.8869 rec=0.6773 spec=0.9140 f1=0.7680 | time=12.9s\n",
            "Epoch 022 | train_loss=0.4854 acc=0.7850 | val_loss=0.4827 acc=0.7868 | prec=0.8987 rec=0.6455 spec=0.9276 f1=0.7513 | time=12.9s\n",
            "Epoch 023 | train_loss=0.4643 acc=0.8083 | val_loss=0.4731 acc=0.8005 | prec=0.7357 rec=0.9364 spec=0.6652 f1=0.8240 | time=12.9s\n",
            "Epoch 024 | train_loss=0.4339 acc=0.8162 | val_loss=0.4120 acc=0.8481 | prec=0.8493 rec=0.8455 spec=0.8507 f1=0.8474 | time=12.9s\n",
            "Epoch 025 | train_loss=0.4211 acc=0.8253 | val_loss=0.4173 acc=0.8345 | prec=0.7838 rec=0.9227 spec=0.7466 f1=0.8476 | time=13.0s\n",
            "Epoch 026 | train_loss=0.3942 acc=0.8327 | val_loss=0.4140 acc=0.8322 | prec=0.7897 rec=0.9045 spec=0.7602 f1=0.8432 | time=12.9s\n",
            "Epoch 027 | train_loss=0.3876 acc=0.8491 | val_loss=0.4054 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=13.0s\n",
            "Epoch 028 | train_loss=0.3891 acc=0.8372 | val_loss=0.4137 acc=0.8254 | prec=0.7804 rec=0.9045 spec=0.7466 f1=0.8379 | time=12.9s\n",
            "Epoch 029 | train_loss=0.3406 acc=0.8622 | val_loss=0.3996 acc=0.8345 | prec=0.7928 rec=0.9045 spec=0.7647 f1=0.8450 | time=13.0s\n",
            "Epoch 030 | train_loss=0.3223 acc=0.8650 | val_loss=0.3802 acc=0.8435 | prec=0.8032 rec=0.9091 spec=0.7783 f1=0.8529 | time=12.9s\n",
            "Epoch 031 | train_loss=0.3043 acc=0.8803 | val_loss=0.4361 acc=0.8095 | prec=0.8908 rec=0.7045 spec=0.9140 f1=0.7868 | time=12.9s\n",
            "Epoch 032 | train_loss=0.2973 acc=0.8894 | val_loss=0.3799 acc=0.8413 | prec=0.8348 rec=0.8500 spec=0.8326 f1=0.8423 | time=12.9s\n",
            "Epoch 033 | train_loss=0.2798 acc=0.8928 | val_loss=0.3821 acc=0.8231 | prec=0.8227 rec=0.8227 spec=0.8235 f1=0.8227 | time=12.9s\n",
            "Epoch 034 | train_loss=0.2703 acc=0.9019 | val_loss=0.5215 acc=0.7483 | prec=0.6708 rec=0.9727 spec=0.5249 f1=0.7941 | time=12.9s\n",
            "Epoch 035 | train_loss=0.2896 acc=0.9036 | val_loss=0.3807 acc=0.8254 | prec=0.8095 rec=0.8500 spec=0.8009 f1=0.8293 | time=13.0s\n",
            "Epoch 036 | train_loss=0.2559 acc=0.9024 | val_loss=0.6207 acc=0.7460 | prec=0.9219 rec=0.5364 spec=0.9548 f1=0.6782 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2541 acc=0.8939 | val_loss=0.3868 acc=0.8231 | prec=0.8413 rec=0.7955 spec=0.8507 f1=0.8178 | time=12.8s\n",
            "Epoch 038 | train_loss=0.2327 acc=0.9138 | val_loss=0.3742 acc=0.8390 | prec=0.8225 rec=0.8636 spec=0.8145 f1=0.8426 | time=13.0s\n",
            "Epoch 039 | train_loss=0.2261 acc=0.9075 | val_loss=0.3791 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=13.0s\n",
            "Epoch 040 | train_loss=0.2419 acc=0.9058 | val_loss=0.4022 acc=0.8345 | prec=0.8356 rec=0.8318 spec=0.8371 f1=0.8337 | time=13.0s\n",
            "Epoch 041 | train_loss=0.2150 acc=0.9109 | val_loss=0.3858 acc=0.8345 | prec=0.8050 rec=0.8818 spec=0.7873 f1=0.8416 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2093 acc=0.9092 | val_loss=0.4165 acc=0.8050 | prec=0.7445 rec=0.9273 spec=0.6833 f1=0.8259 | time=13.0s\n",
            "Epoch 043 | train_loss=0.2083 acc=0.9195 | val_loss=0.4493 acc=0.8277 | prec=0.8789 rec=0.7591 spec=0.8959 f1=0.8146 | time=12.9s\n",
            "Epoch 044 | train_loss=0.1980 acc=0.9234 | val_loss=0.4442 acc=0.8322 | prec=0.8763 rec=0.7727 spec=0.8914 f1=0.8213 | time=13.0s\n",
            "Epoch 045 | train_loss=0.2240 acc=0.9058 | val_loss=0.4130 acc=0.8141 | prec=0.7899 rec=0.8545 spec=0.7738 f1=0.8210 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2153 acc=0.9138 | val_loss=0.4256 acc=0.8186 | prec=0.8333 rec=0.7955 spec=0.8416 f1=0.8140 | time=12.9s\n",
            "Epoch 047 | train_loss=0.2182 acc=0.9081 | val_loss=0.4352 acc=0.8277 | prec=0.8636 rec=0.7773 spec=0.8778 f1=0.8182 | time=13.0s\n",
            "Epoch 048 | train_loss=0.2319 acc=0.9070 | val_loss=0.3968 acc=0.8390 | prec=0.8197 rec=0.8682 spec=0.8100 f1=0.8433 | time=13.0s\n",
            "Epoch 049 | train_loss=0.1809 acc=0.9280 | val_loss=0.4082 acc=0.8231 | prec=0.7983 rec=0.8636 spec=0.7828 f1=0.8297 | time=13.0s\n",
            "Epoch 050 | train_loss=0.1724 acc=0.9263 | val_loss=0.4492 acc=0.8254 | prec=0.8557 rec=0.7818 spec=0.8688 f1=0.8171 | time=13.0s\n",
            "Epoch 051 | train_loss=0.1722 acc=0.9223 | val_loss=0.4833 acc=0.8073 | prec=0.8534 rec=0.7409 spec=0.8733 f1=0.7932 | time=13.0s\n",
            "Epoch 052 | train_loss=0.1842 acc=0.9263 | val_loss=0.3976 acc=0.8367 | prec=0.8136 rec=0.8727 spec=0.8009 f1=0.8421 | time=13.0s\n",
            "Epoch 053 | train_loss=0.1550 acc=0.9319 | val_loss=0.4253 acc=0.8231 | prec=0.7773 rec=0.9045 spec=0.7421 f1=0.8361 | time=13.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▆▆▆▆▆▆▆▆▁▆▆▇▇██▇▇███████▇▇█▇██████████▇█</td></tr><tr><td>precision</td><td>▄▄▄▄▄▄▄█▅▁▇▇▇▇▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▆</td></tr><tr><td>recall</td><td>███▇██▁▆█▁▅▆▆▇▇▆▆█▇▇▇▇▇▇▆█▇▇▇▇▇▇▆▆▇▇▇▆▆▇</td></tr><tr><td>specificity</td><td>▁▁▁▁▂▁▁█▁█▇▇▇▆▇▇▆▆▆▇▆▆▇▇▇▇█▇▇▇▇▇▇▆▇▇▆▇▇▆</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▄▅▅▆▆▆▆▇▇▇▇▇██▇████████████</td></tr><tr><td>train_loss</td><td>████████████▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▁▅▆▇▇▇▇▇▇███████▆█▆███▇██▇█████</td></tr><tr><td>val_loss</td><td>███████████▅▄▄▄▃▃▃▂▂▂▁▂▁▁▁▆▁▁▁▁▂▃▃▂▂▁▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>53</td></tr><tr><td>f1_score</td><td>0.83613</td></tr><tr><td>precision</td><td>0.77734</td></tr><tr><td>recall</td><td>0.90455</td></tr><tr><td>specificity</td><td>0.74208</td></tr><tr><td>train_acc</td><td>0.93193</td></tr><tr><td>train_loss</td><td>0.15497</td></tr><tr><td>val_acc</td><td>0.82313</td></tr><tr><td>val_loss</td><td>0.4253</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/jndqy4kv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/jndqy4kv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_023744-jndqy4kv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 02:49:14,957] Trial 5 finished with values: [0.3742484056523868, 0.8390022675736961] and parameters: {'lr': 8.415656424242612e-05, 'wd': 4.895174415396328e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 6 hyperparams --> lr=2.06e-05, wd=2.61e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_024914-lhsuk7bn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/lhsuk7bn' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/lhsuk7bn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/lhsuk7bn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7707 acc=0.4974 | val_loss=0.6938 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=13.1s\n",
            "Epoch 002 | train_loss=0.7569 acc=0.5082 | val_loss=0.6972 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7523 acc=0.5043 | val_loss=0.6979 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 004 | train_loss=0.7473 acc=0.4901 | val_loss=0.6972 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7374 acc=0.5133 | val_loss=0.6994 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 006 | train_loss=0.7354 acc=0.5190 | val_loss=0.6970 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 007 | train_loss=0.7298 acc=0.4957 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 008 | train_loss=0.7163 acc=0.5201 | val_loss=0.6936 acc=0.4989 | prec=0.3333 rec=0.0045 spec=0.9910 f1=0.0090 | time=13.1s\n",
            "Epoch 009 | train_loss=0.7191 acc=0.5224 | val_loss=0.6936 acc=0.5011 | prec=0.5000 rec=0.0045 spec=0.9955 f1=0.0090 | time=13.1s\n",
            "Epoch 010 | train_loss=0.7118 acc=0.4997 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.3s\n",
            "Epoch 011 | train_loss=0.7191 acc=0.5054 | val_loss=0.6952 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 012 | train_loss=0.7211 acc=0.5026 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 013 | train_loss=0.7173 acc=0.5054 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 014 | train_loss=0.7200 acc=0.4952 | val_loss=0.6934 acc=0.4989 | prec=0.4286 rec=0.0136 spec=0.9819 f1=0.0264 | time=13.1s\n",
            "Epoch 015 | train_loss=0.7179 acc=0.4991 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 016 | train_loss=0.7050 acc=0.5218 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7176 acc=0.4867 | val_loss=0.6934 acc=0.4989 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7157 acc=0.4969 | val_loss=0.6929 acc=0.4989 | prec=0.4960 rec=0.2818 spec=0.7149 f1=0.3594 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7065 acc=0.5173 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 020 | train_loss=0.7066 acc=0.5247 | val_loss=0.6926 acc=0.5125 | prec=0.5059 rec=0.9682 spec=0.0588 f1=0.6646 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7153 acc=0.4935 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 022 | train_loss=0.7097 acc=0.4974 | val_loss=0.6912 acc=0.5102 | prec=0.5047 rec=0.9682 spec=0.0543 f1=0.6636 | time=13.0s\n",
            "Epoch 023 | train_loss=0.7082 acc=0.5128 | val_loss=0.6886 acc=0.5351 | prec=0.8571 rec=0.0818 spec=0.9864 f1=0.1494 | time=13.0s\n",
            "Epoch 024 | train_loss=0.6875 acc=0.5553 | val_loss=0.6837 acc=0.5964 | prec=0.5583 rec=0.9136 spec=0.2805 f1=0.6931 | time=13.2s\n",
            "Epoch 025 | train_loss=0.6563 acc=0.6001 | val_loss=0.6172 acc=0.6395 | prec=0.8675 rec=0.3273 spec=0.9502 f1=0.4752 | time=12.9s\n",
            "Epoch 026 | train_loss=0.6348 acc=0.6466 | val_loss=0.6755 acc=0.5737 | prec=0.5394 rec=0.9955 spec=0.1538 f1=0.6997 | time=12.9s\n",
            "Epoch 027 | train_loss=0.6094 acc=0.6863 | val_loss=0.6147 acc=0.7710 | prec=0.7179 rec=0.8909 spec=0.6516 f1=0.7951 | time=13.0s\n",
            "Epoch 028 | train_loss=0.6013 acc=0.6880 | val_loss=0.5805 acc=0.7800 | prec=0.7338 rec=0.8773 spec=0.6833 f1=0.7992 | time=13.0s\n",
            "Epoch 029 | train_loss=0.5823 acc=0.6999 | val_loss=0.5524 acc=0.7868 | prec=0.7351 rec=0.8955 spec=0.6787 f1=0.8074 | time=13.0s\n",
            "Epoch 030 | train_loss=0.5627 acc=0.7164 | val_loss=0.5282 acc=0.7710 | prec=0.8287 rec=0.6818 spec=0.8597 f1=0.7481 | time=12.9s\n",
            "Epoch 031 | train_loss=0.5515 acc=0.7294 | val_loss=0.5078 acc=0.8095 | prec=0.8238 rec=0.7864 spec=0.8326 f1=0.8047 | time=12.9s\n",
            "Epoch 032 | train_loss=0.5468 acc=0.7272 | val_loss=0.5422 acc=0.7778 | prec=0.7148 rec=0.9227 spec=0.6335 f1=0.8056 | time=12.9s\n",
            "Epoch 033 | train_loss=0.5246 acc=0.7362 | val_loss=0.4969 acc=0.8163 | prec=0.8325 rec=0.7909 spec=0.8416 f1=0.8112 | time=12.9s\n",
            "Epoch 034 | train_loss=0.5070 acc=0.7720 | val_loss=0.4934 acc=0.8322 | prec=0.8349 rec=0.8273 spec=0.8371 f1=0.8311 | time=13.0s\n",
            "Epoch 035 | train_loss=0.4974 acc=0.7833 | val_loss=0.4775 acc=0.8209 | prec=0.7950 rec=0.8636 spec=0.7783 f1=0.8279 | time=12.9s\n",
            "Epoch 036 | train_loss=0.4878 acc=0.7737 | val_loss=0.4862 acc=0.8186 | prec=0.7652 rec=0.9182 spec=0.7195 f1=0.8347 | time=12.9s\n",
            "Epoch 037 | train_loss=0.4581 acc=0.8043 | val_loss=0.4724 acc=0.8073 | prec=0.7586 rec=0.9000 spec=0.7149 f1=0.8233 | time=13.0s\n",
            "Epoch 038 | train_loss=0.4646 acc=0.7998 | val_loss=0.4655 acc=0.8095 | prec=0.7720 rec=0.8773 spec=0.7421 f1=0.8213 | time=12.9s\n",
            "Epoch 039 | train_loss=0.4590 acc=0.8032 | val_loss=0.4750 acc=0.8141 | prec=0.8382 rec=0.7773 spec=0.8507 f1=0.8066 | time=13.0s\n",
            "Epoch 040 | train_loss=0.4441 acc=0.8094 | val_loss=0.4588 acc=0.8141 | prec=0.8194 rec=0.8045 spec=0.8235 f1=0.8119 | time=13.0s\n",
            "Epoch 041 | train_loss=0.4342 acc=0.8088 | val_loss=0.4625 acc=0.8231 | prec=0.7910 rec=0.8773 spec=0.7692 f1=0.8319 | time=13.0s\n",
            "Epoch 042 | train_loss=0.4342 acc=0.8253 | val_loss=0.4609 acc=0.8209 | prec=0.7854 rec=0.8818 spec=0.7602 f1=0.8308 | time=13.0s\n",
            "Epoch 043 | train_loss=0.4072 acc=0.8355 | val_loss=0.4474 acc=0.8163 | prec=0.8325 rec=0.7909 spec=0.8416 f1=0.8112 | time=13.0s\n",
            "Epoch 044 | train_loss=0.4237 acc=0.8134 | val_loss=0.4591 acc=0.8231 | prec=0.8480 rec=0.7864 spec=0.8597 f1=0.8160 | time=12.9s\n",
            "Epoch 045 | train_loss=0.4092 acc=0.8338 | val_loss=0.4516 acc=0.8277 | prec=0.8429 rec=0.8045 spec=0.8507 f1=0.8233 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4131 acc=0.8355 | val_loss=0.4384 acc=0.8073 | prec=0.8358 rec=0.7636 spec=0.8507 f1=0.7981 | time=13.0s\n",
            "Epoch 047 | train_loss=0.4067 acc=0.8281 | val_loss=0.4478 acc=0.8209 | prec=0.7831 rec=0.8864 spec=0.7557 f1=0.8316 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3972 acc=0.8423 | val_loss=0.4347 acc=0.8254 | prec=0.8017 rec=0.8636 spec=0.7873 f1=0.8315 | time=13.0s\n",
            "Epoch 049 | train_loss=0.3897 acc=0.8508 | val_loss=0.4348 acc=0.8254 | prec=0.8357 rec=0.8091 spec=0.8416 f1=0.8222 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3708 acc=0.8537 | val_loss=0.4278 acc=0.8299 | prec=0.8404 rec=0.8136 spec=0.8462 f1=0.8268 | time=13.0s\n",
            "Epoch 051 | train_loss=0.3682 acc=0.8531 | val_loss=0.4208 acc=0.8277 | prec=0.8429 rec=0.8045 spec=0.8507 f1=0.8233 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3707 acc=0.8622 | val_loss=0.4336 acc=0.8277 | prec=0.8636 rec=0.7773 spec=0.8778 f1=0.8182 | time=12.9s\n",
            "Epoch 053 | train_loss=0.3489 acc=0.8622 | val_loss=0.4208 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3415 acc=0.8690 | val_loss=0.4167 acc=0.8367 | prec=0.8491 rec=0.8182 spec=0.8552 f1=0.8333 | time=12.9s\n",
            "Epoch 055 | train_loss=0.3405 acc=0.8639 | val_loss=0.4370 acc=0.8231 | prec=0.8660 rec=0.7636 spec=0.8824 f1=0.8116 | time=12.9s\n",
            "Epoch 056 | train_loss=0.3429 acc=0.8656 | val_loss=0.4107 acc=0.8231 | prec=0.8227 rec=0.8227 spec=0.8235 f1=0.8227 | time=13.0s\n",
            "Epoch 057 | train_loss=0.3493 acc=0.8724 | val_loss=0.4060 acc=0.8367 | prec=0.8426 rec=0.8273 spec=0.8462 f1=0.8349 | time=13.0s\n",
            "Epoch 058 | train_loss=0.3272 acc=0.8707 | val_loss=0.4648 acc=0.7959 | prec=0.8869 rec=0.6773 spec=0.9140 f1=0.7680 | time=13.0s\n",
            "Epoch 059 | train_loss=0.3231 acc=0.8820 | val_loss=0.4431 acc=0.8186 | prec=0.8723 rec=0.7455 spec=0.8914 f1=0.8039 | time=12.9s\n",
            "Epoch 060 | train_loss=0.3293 acc=0.8724 | val_loss=0.4504 acc=0.8005 | prec=0.8750 rec=0.7000 spec=0.9005 f1=0.7778 | time=13.0s\n",
            "Epoch 061 | train_loss=0.3130 acc=0.8837 | val_loss=0.4307 acc=0.8209 | prec=0.8691 rec=0.7545 spec=0.8869 f1=0.8078 | time=13.0s\n",
            "Epoch 062 | train_loss=0.3069 acc=0.8849 | val_loss=0.4245 acc=0.8277 | prec=0.8673 rec=0.7727 spec=0.8824 f1=0.8173 | time=13.0s\n",
            "Epoch 063 | train_loss=0.3084 acc=0.8860 | val_loss=0.4073 acc=0.8390 | prec=0.8706 rec=0.7955 spec=0.8824 f1=0.8314 | time=12.8s\n",
            "Epoch 064 | train_loss=0.3141 acc=0.8775 | val_loss=0.4001 acc=0.8367 | prec=0.8394 rec=0.8318 spec=0.8416 f1=0.8356 | time=13.0s\n",
            "Epoch 065 | train_loss=0.3190 acc=0.8763 | val_loss=0.4094 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=13.0s\n",
            "Epoch 066 | train_loss=0.2857 acc=0.8922 | val_loss=0.4765 acc=0.7959 | prec=0.8736 rec=0.6909 spec=0.9005 f1=0.7716 | time=12.9s\n",
            "Epoch 067 | train_loss=0.2898 acc=0.8962 | val_loss=0.4874 acc=0.7914 | prec=0.8951 rec=0.6591 spec=0.9231 f1=0.7592 | time=12.9s\n",
            "Epoch 068 | train_loss=0.3051 acc=0.9019 | val_loss=0.4147 acc=0.8435 | prec=0.8756 rec=0.8000 spec=0.8869 f1=0.8361 | time=13.0s\n",
            "Epoch 069 | train_loss=0.2948 acc=0.8820 | val_loss=0.4899 acc=0.7868 | prec=0.8841 rec=0.6591 spec=0.9140 f1=0.7552 | time=13.0s\n",
            "Epoch 070 | train_loss=0.2944 acc=0.8860 | val_loss=0.4000 acc=0.8458 | prec=0.8619 rec=0.8227 spec=0.8688 f1=0.8419 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2957 acc=0.8911 | val_loss=0.4325 acc=0.8231 | prec=0.8859 rec=0.7409 spec=0.9050 f1=0.8069 | time=12.9s\n",
            "Epoch 072 | train_loss=0.2907 acc=0.8894 | val_loss=0.5080 acc=0.7982 | prec=0.9068 rec=0.6636 spec=0.9321 f1=0.7664 | time=12.9s\n",
            "Epoch 073 | train_loss=0.2855 acc=0.8854 | val_loss=0.4208 acc=0.8299 | prec=0.8796 rec=0.7636 spec=0.8959 f1=0.8175 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2892 acc=0.8934 | val_loss=0.5291 acc=0.7755 | prec=0.9116 rec=0.6091 spec=0.9412 f1=0.7302 | time=13.0s\n",
            "Epoch 075 | train_loss=0.2818 acc=0.8894 | val_loss=0.3962 acc=0.8435 | prec=0.8794 rec=0.7955 spec=0.8914 f1=0.8353 | time=12.9s\n",
            "Epoch 076 | train_loss=0.2859 acc=0.8894 | val_loss=0.4316 acc=0.8277 | prec=0.9045 rec=0.7318 spec=0.9231 f1=0.8090 | time=12.9s\n",
            "Epoch 077 | train_loss=0.2894 acc=0.8951 | val_loss=0.4374 acc=0.8209 | prec=0.9029 rec=0.7182 spec=0.9231 f1=0.8000 | time=13.2s\n",
            "Epoch 078 | train_loss=0.2601 acc=0.8962 | val_loss=0.4703 acc=0.7914 | prec=0.9103 rec=0.6455 spec=0.9367 f1=0.7553 | time=13.0s\n",
            "Epoch 079 | train_loss=0.2976 acc=0.8866 | val_loss=0.3960 acc=0.8435 | prec=0.8578 rec=0.8227 spec=0.8643 f1=0.8399 | time=13.1s\n",
            "Epoch 080 | train_loss=0.3021 acc=0.8837 | val_loss=0.5014 acc=0.8073 | prec=0.9042 rec=0.6864 spec=0.9276 f1=0.7804 | time=13.1s\n",
            "Epoch 081 | train_loss=0.2624 acc=0.8973 | val_loss=0.4464 acc=0.8141 | prec=0.8920 rec=0.7136 spec=0.9140 f1=0.7929 | time=13.1s\n",
            "Epoch 082 | train_loss=0.2647 acc=0.9087 | val_loss=0.4489 acc=0.8209 | prec=0.8939 rec=0.7273 spec=0.9140 f1=0.8020 | time=13.1s\n",
            "Epoch 083 | train_loss=0.2693 acc=0.8990 | val_loss=0.4460 acc=0.8254 | prec=0.9086 rec=0.7227 spec=0.9276 f1=0.8051 | time=13.0s\n",
            "Epoch 084 | train_loss=0.2449 acc=0.9144 | val_loss=0.4233 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=13.0s\n",
            "Epoch 085 | train_loss=0.2484 acc=0.9098 | val_loss=0.4487 acc=0.8186 | prec=0.9118 rec=0.7045 spec=0.9321 f1=0.7949 | time=13.0s\n",
            "Epoch 086 | train_loss=0.2468 acc=0.9104 | val_loss=0.4318 acc=0.8367 | prec=0.8895 rec=0.7682 spec=0.9050 f1=0.8244 | time=13.0s\n",
            "Epoch 087 | train_loss=0.2459 acc=0.9200 | val_loss=0.4994 acc=0.8005 | prec=0.9125 rec=0.6636 spec=0.9367 f1=0.7684 | time=12.9s\n",
            "Epoch 088 | train_loss=0.2397 acc=0.9144 | val_loss=0.4687 acc=0.8050 | prec=0.9085 rec=0.6773 spec=0.9321 f1=0.7760 | time=12.9s\n",
            "Epoch 089 | train_loss=0.2260 acc=0.9155 | val_loss=0.4101 acc=0.8390 | prec=0.8901 rec=0.7727 spec=0.9050 f1=0.8273 | time=12.9s\n",
            "Epoch 090 | train_loss=0.2432 acc=0.9098 | val_loss=0.4186 acc=0.8413 | prec=0.8989 rec=0.7682 spec=0.9140 f1=0.8284 | time=12.9s\n",
            "Epoch 091 | train_loss=0.2322 acc=0.9155 | val_loss=0.4821 acc=0.8005 | prec=0.9024 rec=0.6727 spec=0.9276 f1=0.7708 | time=12.9s\n",
            "Epoch 092 | train_loss=0.2419 acc=0.9195 | val_loss=0.4857 acc=0.8005 | prec=0.9024 rec=0.6727 spec=0.9276 f1=0.7708 | time=13.0s\n",
            "Epoch 093 | train_loss=0.2363 acc=0.9036 | val_loss=0.4736 acc=0.8095 | prec=0.9048 rec=0.6909 spec=0.9276 f1=0.7835 | time=13.1s\n",
            "Epoch 094 | train_loss=0.2315 acc=0.9183 | val_loss=0.4836 acc=0.8027 | prec=0.9030 rec=0.6773 spec=0.9276 f1=0.7740 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▇▇▇▇▇▁▁▁▁▁████████████▇█▇██▇▇▇▇█████▇▇██</td></tr><tr><td>precision</td><td>▅▅▅▁▁▅▅▅▅█▇▇▇▇▇▇▇▇▇▇▇█▇█▇████▇██████████</td></tr><tr><td>recall</td><td>███▁▁▁▁▁▁▁▆▇▇▇▇▆▇▇▇▇▆▇▆▆▆▇▇▆▆▇▆▇▆▆▇▆▆▆▆▆</td></tr><tr><td>specificity</td><td>▁▁▁▁▁█████▁▁█▃█▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇█▇█▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇███▇▇█▇█████</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▇▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▄▇▇▇▇▇▇▇███████▇██▇█▇▇█▇███▇▇█▇</td></tr><tr><td>val_loss</td><td>███████████▅▄▄▃▃▃▃▂▂▂▂▁▂▁▁▁▁▃▃▁▄▂▂▃▂▂▃▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>94</td></tr><tr><td>f1_score</td><td>0.77403</td></tr><tr><td>precision</td><td>0.90303</td></tr><tr><td>recall</td><td>0.67727</td></tr><tr><td>specificity</td><td>0.9276</td></tr><tr><td>train_acc</td><td>0.91832</td></tr><tr><td>train_loss</td><td>0.23147</td></tr><tr><td>val_acc</td><td>0.80272</td></tr><tr><td>val_loss</td><td>0.48363</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/lhsuk7bn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/lhsuk7bn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_024914-lhsuk7bn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 03:09:39,040] Trial 6 finished with values: [0.39603661958660397, 0.8435374149659864] and parameters: {'lr': 2.057824570055627e-05, 'wd': 2.6146249296257286e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 7 hyperparams --> lr=2.20e-05, wd=2.30e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_030939-14s7wk3d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/14s7wk3d' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/14s7wk3d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/14s7wk3d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.8204 acc=0.4991 | val_loss=0.7469 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 002 | train_loss=0.7982 acc=0.4821 | val_loss=0.7072 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7445 acc=0.5099 | val_loss=0.6996 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7604 acc=0.4776 | val_loss=0.7026 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 005 | train_loss=0.7474 acc=0.4838 | val_loss=0.6959 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7386 acc=0.5094 | val_loss=0.6967 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7321 acc=0.4997 | val_loss=0.6970 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7350 acc=0.4986 | val_loss=0.6976 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7336 acc=0.4935 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7299 acc=0.4974 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7143 acc=0.5077 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7205 acc=0.5031 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7096 acc=0.5286 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 014 | train_loss=0.7126 acc=0.5009 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 015 | train_loss=0.7096 acc=0.5111 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 016 | train_loss=0.7119 acc=0.5071 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7106 acc=0.5088 | val_loss=0.6922 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7150 acc=0.5026 | val_loss=0.6925 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 019 | train_loss=0.7032 acc=0.5094 | val_loss=0.6908 acc=0.5420 | prec=0.8214 rec=0.1045 spec=0.9774 f1=0.1855 | time=13.1s\n",
            "Epoch 020 | train_loss=0.7129 acc=0.5060 | val_loss=0.6899 acc=0.5147 | prec=0.8750 rec=0.0318 spec=0.9955 f1=0.0614 | time=13.0s\n",
            "Epoch 021 | train_loss=0.7022 acc=0.5355 | val_loss=0.6878 acc=0.5556 | prec=0.8333 rec=0.1364 spec=0.9729 f1=0.2344 | time=13.0s\n",
            "Epoch 022 | train_loss=0.7084 acc=0.5190 | val_loss=0.6804 acc=0.7007 | prec=0.6849 rec=0.7409 spec=0.6606 f1=0.7118 | time=12.9s\n",
            "Epoch 023 | train_loss=0.6901 acc=0.5462 | val_loss=0.6520 acc=0.7075 | prec=0.7241 rec=0.6682 spec=0.7466 f1=0.6950 | time=12.9s\n",
            "Epoch 024 | train_loss=0.6655 acc=0.5956 | val_loss=0.6477 acc=0.5941 | prec=0.5522 rec=0.9864 spec=0.2036 f1=0.7080 | time=13.0s\n",
            "Epoch 025 | train_loss=0.6530 acc=0.6211 | val_loss=0.5874 acc=0.7392 | prec=0.7966 rec=0.6409 spec=0.8371 f1=0.7103 | time=12.9s\n",
            "Epoch 026 | train_loss=0.5951 acc=0.6954 | val_loss=0.5745 acc=0.7846 | prec=0.7682 rec=0.8136 spec=0.7557 f1=0.7903 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5766 acc=0.7260 | val_loss=0.5406 acc=0.8027 | prec=0.7714 rec=0.8591 spec=0.7466 f1=0.8129 | time=13.0s\n",
            "Epoch 028 | train_loss=0.5499 acc=0.7379 | val_loss=0.5228 acc=0.7732 | prec=0.8191 rec=0.7000 spec=0.8462 f1=0.7549 | time=13.0s\n",
            "Epoch 029 | train_loss=0.5456 acc=0.7317 | val_loss=0.5357 acc=0.7959 | prec=0.8186 rec=0.7591 spec=0.8326 f1=0.7877 | time=12.9s\n",
            "Epoch 030 | train_loss=0.5248 acc=0.7561 | val_loss=0.5097 acc=0.8050 | prec=0.7888 rec=0.8318 spec=0.7783 f1=0.8097 | time=13.0s\n",
            "Epoch 031 | train_loss=0.5212 acc=0.7521 | val_loss=0.5186 acc=0.7687 | prec=0.8391 rec=0.6636 spec=0.8733 f1=0.7411 | time=12.9s\n",
            "Epoch 032 | train_loss=0.5179 acc=0.7686 | val_loss=0.5001 acc=0.8118 | prec=0.8044 rec=0.8227 spec=0.8009 f1=0.8135 | time=12.9s\n",
            "Epoch 033 | train_loss=0.4823 acc=0.7845 | val_loss=0.4827 acc=0.8118 | prec=0.8246 rec=0.7909 spec=0.8326 f1=0.8074 | time=13.0s\n",
            "Epoch 034 | train_loss=0.4821 acc=0.7828 | val_loss=0.4850 acc=0.8209 | prec=0.8079 rec=0.8409 spec=0.8009 f1=0.8241 | time=13.0s\n",
            "Epoch 035 | train_loss=0.4928 acc=0.7850 | val_loss=0.4804 acc=0.8186 | prec=0.8097 rec=0.8318 spec=0.8054 f1=0.8206 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4673 acc=0.7907 | val_loss=0.4662 acc=0.8277 | prec=0.8273 rec=0.8273 spec=0.8281 f1=0.8273 | time=13.0s\n",
            "Epoch 037 | train_loss=0.4650 acc=0.7845 | val_loss=0.4629 acc=0.8367 | prec=0.8274 rec=0.8500 spec=0.8235 f1=0.8386 | time=13.0s\n",
            "Epoch 038 | train_loss=0.4500 acc=0.8123 | val_loss=0.4522 acc=0.8186 | prec=0.8398 rec=0.7864 spec=0.8507 f1=0.8122 | time=12.9s\n",
            "Epoch 039 | train_loss=0.4560 acc=0.7981 | val_loss=0.4514 acc=0.7937 | prec=0.8486 rec=0.7136 spec=0.8733 f1=0.7753 | time=13.0s\n",
            "Epoch 040 | train_loss=0.4484 acc=0.8077 | val_loss=0.4604 acc=0.8345 | prec=0.7816 rec=0.9273 spec=0.7421 f1=0.8482 | time=12.9s\n",
            "Epoch 041 | train_loss=0.4298 acc=0.8287 | val_loss=0.4442 acc=0.8209 | prec=0.8439 rec=0.7864 spec=0.8552 f1=0.8141 | time=13.0s\n",
            "Epoch 042 | train_loss=0.4242 acc=0.8157 | val_loss=0.4293 acc=0.8299 | prec=0.8404 rec=0.8136 spec=0.8462 f1=0.8268 | time=13.0s\n",
            "Epoch 043 | train_loss=0.4231 acc=0.8168 | val_loss=0.4396 acc=0.8367 | prec=0.8190 rec=0.8636 spec=0.8100 f1=0.8407 | time=13.0s\n",
            "Epoch 044 | train_loss=0.4265 acc=0.8230 | val_loss=0.4221 acc=0.8277 | prec=0.8396 rec=0.8091 spec=0.8462 f1=0.8241 | time=12.9s\n",
            "Epoch 045 | train_loss=0.4181 acc=0.8242 | val_loss=0.4181 acc=0.8345 | prec=0.8451 rec=0.8182 spec=0.8507 f1=0.8314 | time=12.9s\n",
            "Epoch 046 | train_loss=0.4238 acc=0.8293 | val_loss=0.4147 acc=0.8367 | prec=0.8394 rec=0.8318 spec=0.8416 f1=0.8356 | time=12.9s\n",
            "Epoch 047 | train_loss=0.4001 acc=0.8378 | val_loss=0.4201 acc=0.8277 | prec=0.8429 rec=0.8045 spec=0.8507 f1=0.8233 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3993 acc=0.8344 | val_loss=0.4191 acc=0.8458 | prec=0.7901 rec=0.9409 spec=0.7511 f1=0.8589 | time=13.0s\n",
            "Epoch 049 | train_loss=0.3984 acc=0.8446 | val_loss=0.3940 acc=0.8299 | prec=0.8469 rec=0.8045 spec=0.8552 f1=0.8252 | time=12.9s\n",
            "Epoch 050 | train_loss=0.3964 acc=0.8259 | val_loss=0.3985 acc=0.8549 | prec=0.8071 rec=0.9318 spec=0.7783 f1=0.8650 | time=13.0s\n",
            "Epoch 051 | train_loss=0.4156 acc=0.8327 | val_loss=0.3875 acc=0.8390 | prec=0.8599 rec=0.8091 spec=0.8688 f1=0.8337 | time=13.0s\n",
            "Epoch 052 | train_loss=0.3843 acc=0.8344 | val_loss=0.4007 acc=0.8413 | prec=0.8641 rec=0.8091 spec=0.8733 f1=0.8357 | time=13.0s\n",
            "Epoch 053 | train_loss=0.3725 acc=0.8469 | val_loss=0.3891 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=13.1s\n",
            "Epoch 054 | train_loss=0.3822 acc=0.8491 | val_loss=0.3894 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=13.0s\n",
            "Epoch 055 | train_loss=0.3641 acc=0.8537 | val_loss=0.3776 acc=0.8481 | prec=0.8462 rec=0.8500 spec=0.8462 f1=0.8481 | time=12.9s\n",
            "Epoch 056 | train_loss=0.3614 acc=0.8605 | val_loss=0.3887 acc=0.8571 | prec=0.8127 rec=0.9273 spec=0.7873 f1=0.8662 | time=12.9s\n",
            "Epoch 057 | train_loss=0.3565 acc=0.8588 | val_loss=0.3828 acc=0.8571 | prec=0.8520 rec=0.8636 spec=0.8507 f1=0.8578 | time=12.8s\n",
            "Epoch 058 | train_loss=0.3557 acc=0.8735 | val_loss=0.3694 acc=0.8662 | prec=0.8455 rec=0.8955 spec=0.8371 f1=0.8698 | time=12.9s\n",
            "Epoch 059 | train_loss=0.3595 acc=0.8440 | val_loss=0.3660 acc=0.8617 | prec=0.8412 rec=0.8909 spec=0.8326 f1=0.8653 | time=12.9s\n",
            "Epoch 060 | train_loss=0.3262 acc=0.8815 | val_loss=0.3705 acc=0.8594 | prec=0.8496 rec=0.8727 spec=0.8462 f1=0.8610 | time=13.1s\n",
            "Epoch 061 | train_loss=0.3398 acc=0.8701 | val_loss=0.3693 acc=0.8617 | prec=0.8272 rec=0.9136 spec=0.8100 f1=0.8683 | time=13.0s\n",
            "Epoch 062 | train_loss=0.3479 acc=0.8622 | val_loss=0.3774 acc=0.8481 | prec=0.8048 rec=0.9182 spec=0.7783 f1=0.8577 | time=13.1s\n",
            "Epoch 063 | train_loss=0.3209 acc=0.8684 | val_loss=0.3620 acc=0.8594 | prec=0.8726 rec=0.8409 spec=0.8778 f1=0.8565 | time=12.9s\n",
            "Epoch 064 | train_loss=0.3378 acc=0.8690 | val_loss=0.3570 acc=0.8549 | prec=0.8305 rec=0.8909 spec=0.8190 f1=0.8596 | time=12.9s\n",
            "Epoch 065 | train_loss=0.3342 acc=0.8735 | val_loss=0.3889 acc=0.8231 | prec=0.8901 rec=0.7364 spec=0.9095 f1=0.8060 | time=12.9s\n",
            "Epoch 066 | train_loss=0.3109 acc=0.8854 | val_loss=0.3652 acc=0.8503 | prec=0.8156 rec=0.9045 spec=0.7964 f1=0.8578 | time=12.9s\n",
            "Epoch 067 | train_loss=0.3277 acc=0.8554 | val_loss=0.3668 acc=0.8549 | prec=0.8145 rec=0.9182 spec=0.7919 f1=0.8632 | time=12.9s\n",
            "Epoch 068 | train_loss=0.3094 acc=0.8860 | val_loss=0.3539 acc=0.8526 | prec=0.8189 rec=0.9045 spec=0.8009 f1=0.8596 | time=12.9s\n",
            "Epoch 069 | train_loss=0.3311 acc=0.8815 | val_loss=0.3443 acc=0.8617 | prec=0.8533 rec=0.8727 spec=0.8507 f1=0.8629 | time=13.0s\n",
            "Epoch 070 | train_loss=0.3143 acc=0.8786 | val_loss=0.3492 acc=0.8594 | prec=0.8559 rec=0.8636 spec=0.8552 f1=0.8597 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2821 acc=0.9013 | val_loss=0.3486 acc=0.8549 | prec=0.8786 rec=0.8227 spec=0.8869 f1=0.8498 | time=13.1s\n",
            "Epoch 072 | train_loss=0.3168 acc=0.8809 | val_loss=0.3418 acc=0.8753 | prec=0.8873 rec=0.8591 spec=0.8914 f1=0.8730 | time=13.1s\n",
            "Epoch 073 | train_loss=0.2986 acc=0.8786 | val_loss=0.3381 acc=0.8685 | prec=0.8616 rec=0.8773 spec=0.8597 f1=0.8694 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2991 acc=0.8798 | val_loss=0.3299 acc=0.8685 | prec=0.8553 rec=0.8864 spec=0.8507 f1=0.8705 | time=12.9s\n",
            "Epoch 075 | train_loss=0.2889 acc=0.8962 | val_loss=0.3385 acc=0.8685 | prec=0.8584 rec=0.8818 spec=0.8552 f1=0.8700 | time=12.9s\n",
            "Epoch 076 | train_loss=0.2958 acc=0.8849 | val_loss=0.3334 acc=0.8662 | prec=0.8815 rec=0.8455 spec=0.8869 f1=0.8631 | time=12.9s\n",
            "Epoch 077 | train_loss=0.2808 acc=0.8934 | val_loss=0.3499 acc=0.8503 | prec=0.7962 rec=0.9409 spec=0.7602 f1=0.8625 | time=12.9s\n",
            "Epoch 078 | train_loss=0.2787 acc=0.8860 | val_loss=0.3327 acc=0.8594 | prec=0.8264 rec=0.9091 spec=0.8100 f1=0.8658 | time=12.9s\n",
            "Epoch 079 | train_loss=0.2879 acc=0.8826 | val_loss=0.3502 acc=0.8503 | prec=0.8008 rec=0.9318 spec=0.7692 f1=0.8613 | time=13.1s\n",
            "Epoch 080 | train_loss=0.2655 acc=0.9075 | val_loss=0.3538 acc=0.8571 | prec=0.8127 rec=0.9273 spec=0.7873 f1=0.8662 | time=13.0s\n",
            "Epoch 081 | train_loss=0.2753 acc=0.8968 | val_loss=0.3326 acc=0.8685 | prec=0.8750 rec=0.8591 spec=0.8778 f1=0.8670 | time=12.9s\n",
            "Epoch 082 | train_loss=0.2702 acc=0.8973 | val_loss=0.3296 acc=0.8639 | prec=0.8670 rec=0.8591 spec=0.8688 f1=0.8630 | time=13.0s\n",
            "Epoch 083 | train_loss=0.2619 acc=0.9047 | val_loss=0.3418 acc=0.8526 | prec=0.8216 rec=0.9000 spec=0.8054 f1=0.8590 | time=13.0s\n",
            "Epoch 084 | train_loss=0.2455 acc=0.9024 | val_loss=0.3267 acc=0.8639 | prec=0.8604 rec=0.8682 spec=0.8597 f1=0.8643 | time=13.0s\n",
            "Epoch 085 | train_loss=0.2931 acc=0.8922 | val_loss=0.3224 acc=0.8594 | prec=0.8624 rec=0.8545 spec=0.8643 f1=0.8584 | time=13.0s\n",
            "Epoch 086 | train_loss=0.2750 acc=0.8877 | val_loss=0.3425 acc=0.8549 | prec=0.8197 rec=0.9091 spec=0.8009 f1=0.8621 | time=13.0s\n",
            "Epoch 087 | train_loss=0.2511 acc=0.9024 | val_loss=0.3333 acc=0.8571 | prec=0.8489 rec=0.8682 spec=0.8462 f1=0.8584 | time=13.0s\n",
            "Epoch 088 | train_loss=0.2575 acc=0.8951 | val_loss=0.3367 acc=0.8549 | prec=0.8250 rec=0.9000 spec=0.8100 f1=0.8609 | time=12.9s\n",
            "Epoch 089 | train_loss=0.2433 acc=0.9064 | val_loss=0.3252 acc=0.8571 | prec=0.8520 rec=0.8636 spec=0.8507 f1=0.8578 | time=13.1s\n",
            "Epoch 090 | train_loss=0.2638 acc=0.8939 | val_loss=0.3259 acc=0.8617 | prec=0.8472 rec=0.8818 spec=0.8416 f1=0.8641 | time=13.0s\n",
            "Epoch 091 | train_loss=0.2581 acc=0.9036 | val_loss=0.3314 acc=0.8571 | prec=0.8369 rec=0.8864 spec=0.8281 f1=0.8609 | time=13.0s\n",
            "Epoch 092 | train_loss=0.2435 acc=0.9104 | val_loss=0.3387 acc=0.8549 | prec=0.8197 rec=0.9091 spec=0.8009 f1=0.8621 | time=12.9s\n",
            "Epoch 093 | train_loss=0.2380 acc=0.9183 | val_loss=0.3298 acc=0.8617 | prec=0.8442 rec=0.8864 spec=0.8371 f1=0.8647 | time=13.0s\n",
            "Epoch 094 | train_loss=0.2483 acc=0.8934 | val_loss=0.3255 acc=0.8594 | prec=0.8527 rec=0.8682 spec=0.8507 f1=0.8604 | time=13.0s\n",
            "Epoch 095 | train_loss=0.2650 acc=0.8996 | val_loss=0.3254 acc=0.8594 | prec=0.8496 rec=0.8727 spec=0.8462 f1=0.8610 | time=12.9s\n",
            "Epoch 096 | train_loss=0.2547 acc=0.9092 | val_loss=0.3478 acc=0.8526 | prec=0.8243 rec=0.8955 spec=0.8100 f1=0.8584 | time=12.9s\n",
            "Epoch 097 | train_loss=0.2378 acc=0.9036 | val_loss=0.3340 acc=0.8549 | prec=0.8333 rec=0.8864 spec=0.8235 f1=0.8590 | time=13.0s\n",
            "Epoch 098 | train_loss=0.2287 acc=0.9132 | val_loss=0.3325 acc=0.8594 | prec=0.8624 rec=0.8545 spec=0.8643 f1=0.8584 | time=12.9s\n",
            "Epoch 099 | train_loss=0.2406 acc=0.8996 | val_loss=0.3525 acc=0.8481 | prec=0.8024 rec=0.9227 spec=0.7738 f1=0.8584 | time=12.9s\n",
            "Epoch 100 | train_loss=0.2444 acc=0.8985 | val_loss=0.3300 acc=0.8549 | prec=0.8333 rec=0.8864 spec=0.8235 f1=0.8590 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▇▇███████▇████████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁█▆▇▇██▇██▇█████████▇██████████████▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▂▁▇▇▇▇▇▇██▇▇██▇▆█▇▇▇▇███▇▇██▇███</td></tr><tr><td>specificity</td><td>████████▇▁▅▄▄▅▅▅▅▅▃▅▅▅▅▄▃▄▅▅▆▆▅▆▅▅▅▅▄▅▅▃</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▁▁▁▁▂▃▅▅▆▆▆▆▇▇▇▇▇▇▇███████████████</td></tr><tr><td>train_loss</td><td>████████▇▇▄▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▅▆▇▆▇▇▆▇▇▇▇▇▇█████▇▇██████████████</td></tr><tr><td>val_loss</td><td>██████████▇▅▅▅▅▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>100</td></tr><tr><td>f1_score</td><td>0.85903</td></tr><tr><td>precision</td><td>0.83333</td></tr><tr><td>recall</td><td>0.88636</td></tr><tr><td>specificity</td><td>0.82353</td></tr><tr><td>train_acc</td><td>0.89847</td></tr><tr><td>train_loss</td><td>0.24445</td></tr><tr><td>val_acc</td><td>0.85488</td></tr><tr><td>val_loss</td><td>0.33004</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/14s7wk3d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/14s7wk3d</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_030939-14s7wk3d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 03:31:18,729] Trial 7 finished with values: [0.3224097618034908, 0.8594104308390023] and parameters: {'lr': 2.1990910603368945e-05, 'wd': 2.299085583693177e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 8 hyperparams --> lr=7.30e-05, wd=2.30e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_033118-s62iuqtp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/s62iuqtp' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/s62iuqtp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/s62iuqtp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7357 acc=0.4963 | val_loss=0.6975 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 002 | train_loss=0.7217 acc=0.4935 | val_loss=0.7001 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 003 | train_loss=0.7333 acc=0.4787 | val_loss=0.6972 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 004 | train_loss=0.7147 acc=0.5116 | val_loss=0.6975 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 005 | train_loss=0.7183 acc=0.5048 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 006 | train_loss=0.7186 acc=0.4991 | val_loss=0.6932 acc=0.5170 | prec=0.5376 rec=0.2273 spec=0.8054 f1=0.3195 | time=13.1s\n",
            "Epoch 007 | train_loss=0.7251 acc=0.4912 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.1s\n",
            "Epoch 008 | train_loss=0.7120 acc=0.5060 | val_loss=0.6958 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7072 acc=0.5162 | val_loss=0.6930 acc=0.5011 | prec=0.5000 rec=0.9955 spec=0.0090 f1=0.6657 | time=13.0s\n",
            "Epoch 010 | train_loss=0.7062 acc=0.5082 | val_loss=0.6925 acc=0.5238 | prec=0.5163 rec=0.7182 spec=0.3303 f1=0.6008 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7126 acc=0.5037 | val_loss=0.6923 acc=0.5329 | prec=0.6750 rec=0.1227 spec=0.9412 f1=0.2077 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7140 acc=0.5116 | val_loss=0.6926 acc=0.5420 | prec=0.5346 rec=0.6318 spec=0.4525 f1=0.5792 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7100 acc=0.4940 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7182 acc=0.4861 | val_loss=0.6932 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7073 acc=0.5162 | val_loss=0.6862 acc=0.5147 | prec=0.5071 rec=0.9682 spec=0.0633 f1=0.6656 | time=13.0s\n",
            "Epoch 016 | train_loss=0.6802 acc=0.5485 | val_loss=0.6313 acc=0.6712 | prec=0.8378 rec=0.4227 spec=0.9186 f1=0.5619 | time=12.9s\n",
            "Epoch 017 | train_loss=0.6191 acc=0.6653 | val_loss=0.5470 acc=0.7664 | prec=0.7826 rec=0.7364 spec=0.7964 f1=0.7588 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5482 acc=0.7317 | val_loss=0.5180 acc=0.7460 | prec=0.8600 rec=0.5864 spec=0.9050 f1=0.6973 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5020 acc=0.7538 | val_loss=0.4761 acc=0.8163 | prec=0.8145 rec=0.8182 spec=0.8145 f1=0.8163 | time=13.0s\n",
            "Epoch 020 | train_loss=0.4727 acc=0.7918 | val_loss=0.4698 acc=0.8163 | prec=0.8117 rec=0.8227 spec=0.8100 f1=0.8172 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4838 acc=0.7794 | val_loss=0.4785 acc=0.7868 | prec=0.8706 rec=0.6727 spec=0.9005 f1=0.7590 | time=12.8s\n",
            "Epoch 022 | train_loss=0.4497 acc=0.8037 | val_loss=0.4540 acc=0.8299 | prec=0.8033 rec=0.8727 spec=0.7873 f1=0.8366 | time=12.9s\n",
            "Epoch 023 | train_loss=0.4270 acc=0.8106 | val_loss=0.5452 acc=0.7075 | prec=0.6383 rec=0.9545 spec=0.4615 f1=0.7650 | time=13.0s\n",
            "Epoch 024 | train_loss=0.4203 acc=0.8162 | val_loss=0.4104 acc=0.8413 | prec=0.8538 rec=0.8227 spec=0.8597 f1=0.8380 | time=13.1s\n",
            "Epoch 025 | train_loss=0.4073 acc=0.8179 | val_loss=0.4027 acc=0.8367 | prec=0.8364 rec=0.8364 spec=0.8371 f1=0.8364 | time=13.0s\n",
            "Epoch 026 | train_loss=0.3785 acc=0.8417 | val_loss=0.4146 acc=0.8231 | prec=0.8257 rec=0.8182 spec=0.8281 f1=0.8219 | time=12.9s\n",
            "Epoch 027 | train_loss=0.3905 acc=0.8474 | val_loss=0.4198 acc=0.8186 | prec=0.8763 rec=0.7409 spec=0.8959 f1=0.8030 | time=13.0s\n",
            "Epoch 028 | train_loss=0.3748 acc=0.8423 | val_loss=0.4306 acc=0.8254 | prec=0.8907 rec=0.7409 spec=0.9095 f1=0.8089 | time=12.7s\n",
            "Epoch 029 | train_loss=0.3816 acc=0.8417 | val_loss=0.4077 acc=0.8299 | prec=0.8059 rec=0.8682 spec=0.7919 f1=0.8359 | time=13.0s\n",
            "Epoch 030 | train_loss=0.3684 acc=0.8491 | val_loss=0.4004 acc=0.8277 | prec=0.8333 rec=0.8182 spec=0.8371 f1=0.8257 | time=13.1s\n",
            "Epoch 031 | train_loss=0.3484 acc=0.8537 | val_loss=0.4371 acc=0.8073 | prec=0.7491 rec=0.9227 spec=0.6923 f1=0.8269 | time=12.9s\n",
            "Epoch 032 | train_loss=0.3163 acc=0.8695 | val_loss=0.3887 acc=0.8435 | prec=0.8512 rec=0.8318 spec=0.8552 f1=0.8414 | time=12.9s\n",
            "Epoch 033 | train_loss=0.3475 acc=0.8684 | val_loss=0.4234 acc=0.8050 | prec=0.8851 rec=0.7000 spec=0.9095 f1=0.7817 | time=12.9s\n",
            "Epoch 034 | train_loss=0.3147 acc=0.8735 | val_loss=0.3828 acc=0.8435 | prec=0.8512 rec=0.8318 spec=0.8552 f1=0.8414 | time=12.9s\n",
            "Epoch 035 | train_loss=0.3022 acc=0.8803 | val_loss=0.3724 acc=0.8390 | prec=0.8197 rec=0.8682 spec=0.8100 f1=0.8433 | time=12.9s\n",
            "Epoch 036 | train_loss=0.3059 acc=0.8673 | val_loss=0.4253 acc=0.8005 | prec=0.7409 rec=0.9227 spec=0.6787 f1=0.8219 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2964 acc=0.8684 | val_loss=0.3950 acc=0.8277 | prec=0.8750 rec=0.7636 spec=0.8914 f1=0.8155 | time=13.0s\n",
            "Epoch 038 | train_loss=0.2966 acc=0.8928 | val_loss=0.3863 acc=0.8299 | prec=0.7821 rec=0.9136 spec=0.7466 f1=0.8428 | time=13.1s\n",
            "Epoch 039 | train_loss=0.2815 acc=0.8990 | val_loss=0.4243 acc=0.8095 | prec=0.7500 rec=0.9273 spec=0.6923 f1=0.8293 | time=12.8s\n",
            "Epoch 040 | train_loss=0.2600 acc=0.8928 | val_loss=0.3686 acc=0.8390 | prec=0.8253 rec=0.8591 spec=0.8190 f1=0.8419 | time=13.0s\n",
            "Epoch 041 | train_loss=0.2436 acc=0.9030 | val_loss=0.4218 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.9s\n",
            "Epoch 042 | train_loss=0.2292 acc=0.9081 | val_loss=0.3952 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=13.0s\n",
            "Epoch 043 | train_loss=0.2155 acc=0.9161 | val_loss=0.4234 acc=0.8141 | prec=0.7738 rec=0.8864 spec=0.7421 f1=0.8263 | time=13.0s\n",
            "Epoch 044 | train_loss=0.2337 acc=0.9053 | val_loss=0.4281 acc=0.8118 | prec=0.7686 rec=0.8909 spec=0.7330 f1=0.8253 | time=13.0s\n",
            "Epoch 045 | train_loss=0.2276 acc=0.9030 | val_loss=0.3978 acc=0.8277 | prec=0.7880 rec=0.8955 spec=0.7602 f1=0.8383 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2172 acc=0.9041 | val_loss=0.4017 acc=0.8322 | prec=0.8510 rec=0.8045 spec=0.8597 f1=0.8271 | time=12.9s\n",
            "Epoch 047 | train_loss=0.2015 acc=0.9195 | val_loss=0.3889 acc=0.8481 | prec=0.8341 rec=0.8682 spec=0.8281 f1=0.8508 | time=12.9s\n",
            "Epoch 048 | train_loss=0.2100 acc=0.9206 | val_loss=0.4587 acc=0.8163 | prec=0.8840 rec=0.7273 spec=0.9050 f1=0.7980 | time=13.0s\n",
            "Epoch 049 | train_loss=0.2001 acc=0.9280 | val_loss=0.4762 acc=0.8163 | prec=0.8927 rec=0.7182 spec=0.9140 f1=0.7960 | time=12.9s\n",
            "Epoch 050 | train_loss=0.1921 acc=0.9138 | val_loss=0.4362 acc=0.8277 | prec=0.8789 rec=0.7591 spec=0.8959 f1=0.8146 | time=13.0s\n",
            "Epoch 051 | train_loss=0.1868 acc=0.9274 | val_loss=0.4031 acc=0.8413 | prec=0.8505 rec=0.8273 spec=0.8552 f1=0.8387 | time=12.9s\n",
            "Epoch 052 | train_loss=0.1687 acc=0.9257 | val_loss=0.4563 acc=0.8231 | prec=0.8859 rec=0.7409 spec=0.9050 f1=0.8069 | time=13.0s\n",
            "Epoch 053 | train_loss=0.1705 acc=0.9348 | val_loss=0.4416 acc=0.8277 | prec=0.8495 rec=0.7955 spec=0.8597 f1=0.8216 | time=12.8s\n",
            "Epoch 054 | train_loss=0.1806 acc=0.9285 | val_loss=0.4239 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=13.0s\n",
            "Epoch 055 | train_loss=0.1587 acc=0.9382 | val_loss=0.4308 acc=0.8254 | prec=0.8178 rec=0.8364 spec=0.8145 f1=0.8270 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▆▆▃▁▁▆▆▇█▇███████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▅▅▆▁▁▅█▇█▇▆████▇█▇██▇█▇█▇▇▇█████▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁█▆▂▁█▄▆▅▇▆▇█▇▆▆▇▇▇▇▇▇▇█▆▇▇▇▇▆▆▆▇▇</td></tr><tr><td>specificity</td><td>█████▁▃█▄█▁▇▇▇▇▇▆▄▇▇▇▇▇▆▇▇▇▇▆▇▇▆▆▆▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▂▁▂▁▁▂▁▂▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇██████</td></tr><tr><td>train_loss</td><td>███████████▇▇▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▁▄▆▆▇█▅██████▇██▇██▇██▇███████</td></tr><tr><td>val_loss</td><td>███████████▇▅▄▃▅▂▂▂▂▂▂▁▂▁▂▂▁▂▁▂▂▂▂▁▃▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>55</td></tr><tr><td>f1_score</td><td>0.82697</td></tr><tr><td>precision</td><td>0.81778</td></tr><tr><td>recall</td><td>0.83636</td></tr><tr><td>specificity</td><td>0.81448</td></tr><tr><td>train_acc</td><td>0.93817</td></tr><tr><td>train_loss</td><td>0.15872</td></tr><tr><td>val_acc</td><td>0.8254</td></tr><tr><td>val_loss</td><td>0.43076</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/s62iuqtp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/s62iuqtp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_033118-s62iuqtp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 03:43:15,062] Trial 8 finished with values: [0.3686079223241125, 0.8390022675736961] and parameters: {'lr': 7.304507825587855e-05, 'wd': 2.3003282688898533e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 9 hyperparams --> lr=4.26e-05, wd=6.14e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_034315-hlrdfgoz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/hlrdfgoz' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/hlrdfgoz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/hlrdfgoz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=0.7261 acc=0.5139 | val_loss=0.6958 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.0s\n",
            "Epoch 002 | train_loss=0.7296 acc=0.5077 | val_loss=0.6941 acc=0.4966 | prec=0.4977 rec=0.9909 spec=0.0045 f1=0.6626 | time=13.0s\n",
            "Epoch 003 | train_loss=0.7331 acc=0.5167 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.1s\n",
            "Epoch 004 | train_loss=0.7407 acc=0.4884 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=0.9864 spec=0.0136 f1=0.6626 | time=13.0s\n",
            "Epoch 005 | train_loss=0.7183 acc=0.5162 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7221 acc=0.4946 | val_loss=0.6933 acc=0.4921 | prec=0.4954 rec=0.9864 spec=0.0000 f1=0.6596 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7251 acc=0.5082 | val_loss=0.6931 acc=0.5283 | prec=0.5208 rec=0.6818 spec=0.3756 f1=0.5906 | time=13.0s\n",
            "Epoch 008 | train_loss=0.7173 acc=0.5150 | val_loss=0.6926 acc=0.5238 | prec=0.5373 rec=0.3273 spec=0.7195 f1=0.4068 | time=13.0s\n",
            "Epoch 009 | train_loss=0.7146 acc=0.4946 | val_loss=0.6920 acc=0.5125 | prec=0.6087 rec=0.0636 spec=0.9593 f1=0.1152 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7154 acc=0.4963 | val_loss=0.6909 acc=0.4921 | prec=0.4954 rec=0.9773 spec=0.0090 f1=0.6575 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7095 acc=0.5275 | val_loss=0.6902 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7039 acc=0.5241 | val_loss=0.6801 acc=0.6939 | prec=0.7114 rec=0.6500 spec=0.7376 f1=0.6793 | time=13.0s\n",
            "Epoch 013 | train_loss=0.7027 acc=0.5366 | val_loss=0.6671 acc=0.6236 | prec=0.8462 rec=0.3000 spec=0.9457 f1=0.4430 | time=13.1s\n",
            "Epoch 014 | train_loss=0.6815 acc=0.5729 | val_loss=0.6398 acc=0.6961 | prec=0.6443 rec=0.8727 spec=0.5204 f1=0.7413 | time=13.0s\n",
            "Epoch 015 | train_loss=0.6465 acc=0.6177 | val_loss=0.6017 acc=0.7551 | prec=0.7240 rec=0.8227 spec=0.6878 f1=0.7702 | time=12.9s\n",
            "Epoch 016 | train_loss=0.6212 acc=0.6534 | val_loss=0.5473 acc=0.7642 | prec=0.8372 rec=0.6545 spec=0.8733 f1=0.7347 | time=13.0s\n",
            "Epoch 017 | train_loss=0.5967 acc=0.6892 | val_loss=0.5976 acc=0.7029 | prec=0.6320 rec=0.9682 spec=0.4389 f1=0.7648 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5515 acc=0.7311 | val_loss=0.5154 acc=0.8050 | prec=0.8045 rec=0.8045 spec=0.8054 f1=0.8045 | time=12.9s\n",
            "Epoch 019 | train_loss=0.5477 acc=0.7470 | val_loss=0.4893 acc=0.7937 | prec=0.8525 rec=0.7091 spec=0.8778 f1=0.7742 | time=13.4s\n",
            "Epoch 020 | train_loss=0.5220 acc=0.7640 | val_loss=0.4821 acc=0.7823 | prec=0.8563 rec=0.6773 spec=0.8869 f1=0.7563 | time=13.2s\n",
            "Epoch 021 | train_loss=0.4913 acc=0.7765 | val_loss=0.4895 acc=0.8209 | prec=0.7743 rec=0.9045 spec=0.7376 f1=0.8344 | time=13.4s\n",
            "Epoch 022 | train_loss=0.4770 acc=0.7850 | val_loss=0.4567 acc=0.8050 | prec=0.8641 rec=0.7227 spec=0.8869 f1=0.7871 | time=13.3s\n",
            "Epoch 023 | train_loss=0.4684 acc=0.7986 | val_loss=0.4465 acc=0.8322 | prec=0.8093 rec=0.8682 spec=0.7964 f1=0.8377 | time=13.6s\n",
            "Epoch 024 | train_loss=0.4922 acc=0.7691 | val_loss=0.4652 acc=0.8345 | prec=0.8155 rec=0.8636 spec=0.8054 f1=0.8389 | time=13.5s\n",
            "Epoch 025 | train_loss=0.4515 acc=0.8003 | val_loss=0.4412 acc=0.8209 | prec=0.8190 rec=0.8227 spec=0.8190 f1=0.8209 | time=14.1s\n",
            "Epoch 026 | train_loss=0.4241 acc=0.8162 | val_loss=0.4249 acc=0.8277 | prec=0.8333 rec=0.8182 spec=0.8371 f1=0.8257 | time=13.8s\n",
            "Epoch 027 | train_loss=0.4051 acc=0.8298 | val_loss=0.4188 acc=0.8209 | prec=0.8653 rec=0.7591 spec=0.8824 f1=0.8087 | time=13.8s\n",
            "Epoch 028 | train_loss=0.4101 acc=0.8145 | val_loss=0.4132 acc=0.8322 | prec=0.8476 rec=0.8091 spec=0.8552 f1=0.8279 | time=13.9s\n",
            "Epoch 029 | train_loss=0.4000 acc=0.8429 | val_loss=0.4654 acc=0.7868 | prec=0.7158 rec=0.9500 spec=0.6244 f1=0.8164 | time=13.9s\n",
            "Epoch 030 | train_loss=0.3790 acc=0.8514 | val_loss=0.4024 acc=0.8254 | prec=0.8557 rec=0.7818 spec=0.8688 f1=0.8171 | time=13.8s\n",
            "Epoch 031 | train_loss=0.3630 acc=0.8440 | val_loss=0.4017 acc=0.8594 | prec=0.8405 rec=0.8864 spec=0.8326 f1=0.8628 | time=13.8s\n",
            "Epoch 032 | train_loss=0.3599 acc=0.8480 | val_loss=0.3854 acc=0.8549 | prec=0.8451 rec=0.8682 spec=0.8416 f1=0.8565 | time=13.8s\n",
            "Epoch 033 | train_loss=0.3560 acc=0.8678 | val_loss=0.4961 acc=0.7506 | prec=0.6763 rec=0.9591 spec=0.5430 f1=0.7932 | time=13.9s\n",
            "Epoch 034 | train_loss=0.3353 acc=0.8718 | val_loss=0.3885 acc=0.8345 | prec=0.8025 rec=0.8864 spec=0.7828 f1=0.8423 | time=13.8s\n",
            "Epoch 035 | train_loss=0.3150 acc=0.8917 | val_loss=0.3819 acc=0.8413 | prec=0.8440 rec=0.8364 spec=0.8462 f1=0.8402 | time=13.9s\n",
            "Epoch 036 | train_loss=0.3630 acc=0.8559 | val_loss=0.3873 acc=0.8209 | prec=0.8691 rec=0.7545 spec=0.8869 f1=0.8078 | time=13.9s\n",
            "Epoch 037 | train_loss=0.3136 acc=0.8769 | val_loss=0.3686 acc=0.8458 | prec=0.8248 rec=0.8773 spec=0.8145 f1=0.8502 | time=13.8s\n",
            "Epoch 038 | train_loss=0.3135 acc=0.8826 | val_loss=0.3824 acc=0.8299 | prec=0.8537 rec=0.7955 spec=0.8643 f1=0.8235 | time=13.2s\n",
            "Epoch 039 | train_loss=0.3103 acc=0.8769 | val_loss=0.3814 acc=0.8390 | prec=0.7922 rec=0.9182 spec=0.7602 f1=0.8505 | time=13.1s\n",
            "Epoch 040 | train_loss=0.3087 acc=0.8832 | val_loss=0.3721 acc=0.8231 | prec=0.8381 rec=0.8000 spec=0.8462 f1=0.8186 | time=13.1s\n",
            "Epoch 041 | train_loss=0.2949 acc=0.8826 | val_loss=0.3789 acc=0.8118 | prec=0.8586 rec=0.7455 spec=0.8778 f1=0.7981 | time=13.1s\n",
            "Epoch 042 | train_loss=0.3100 acc=0.8894 | val_loss=0.3749 acc=0.8141 | prec=0.8520 rec=0.7591 spec=0.8688 f1=0.8029 | time=13.1s\n",
            "Epoch 043 | train_loss=0.2979 acc=0.8922 | val_loss=0.3565 acc=0.8435 | prec=0.8326 rec=0.8591 spec=0.8281 f1=0.8456 | time=13.0s\n",
            "Epoch 044 | train_loss=0.2990 acc=0.8877 | val_loss=0.3659 acc=0.8231 | prec=0.8737 rec=0.7545 spec=0.8914 f1=0.8098 | time=13.0s\n",
            "Epoch 045 | train_loss=0.2793 acc=0.8996 | val_loss=0.3497 acc=0.8435 | prec=0.8512 rec=0.8318 spec=0.8552 f1=0.8414 | time=12.9s\n",
            "Epoch 046 | train_loss=0.2619 acc=0.9036 | val_loss=0.4908 acc=0.7732 | prec=0.9167 rec=0.6000 spec=0.9457 f1=0.7253 | time=13.0s\n",
            "Epoch 047 | train_loss=0.2519 acc=0.9047 | val_loss=0.3630 acc=0.8322 | prec=0.8411 rec=0.8182 spec=0.8462 f1=0.8295 | time=13.0s\n",
            "Epoch 048 | train_loss=0.2399 acc=0.9149 | val_loss=0.4877 acc=0.7619 | prec=0.9021 rec=0.5864 spec=0.9367 f1=0.7107 | time=12.9s\n",
            "Epoch 049 | train_loss=0.2573 acc=0.8973 | val_loss=0.3997 acc=0.8095 | prec=0.8864 rec=0.7091 spec=0.9095 f1=0.7879 | time=12.9s\n",
            "Epoch 050 | train_loss=0.2474 acc=0.8939 | val_loss=0.4504 acc=0.7937 | prec=0.8957 rec=0.6636 spec=0.9231 f1=0.7624 | time=13.0s\n",
            "Epoch 051 | train_loss=0.2400 acc=0.9138 | val_loss=0.3770 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=13.0s\n",
            "Epoch 052 | train_loss=0.2227 acc=0.9172 | val_loss=0.4080 acc=0.8050 | prec=0.8941 rec=0.6909 spec=0.9186 f1=0.7795 | time=12.9s\n",
            "Epoch 053 | train_loss=0.2284 acc=0.9149 | val_loss=0.3405 acc=0.8526 | prec=0.8163 rec=0.9091 spec=0.7964 f1=0.8602 | time=13.0s\n",
            "Epoch 054 | train_loss=0.2344 acc=0.9223 | val_loss=0.3520 acc=0.8367 | prec=0.8333 rec=0.8409 spec=0.8326 f1=0.8371 | time=12.9s\n",
            "Epoch 055 | train_loss=0.2335 acc=0.9166 | val_loss=0.3556 acc=0.8413 | prec=0.7976 rec=0.9136 spec=0.7692 f1=0.8517 | time=12.9s\n",
            "Epoch 056 | train_loss=0.2393 acc=0.9121 | val_loss=0.3544 acc=0.8299 | prec=0.8372 rec=0.8182 spec=0.8416 f1=0.8276 | time=13.0s\n",
            "Epoch 057 | train_loss=0.2415 acc=0.9189 | val_loss=0.3550 acc=0.8299 | prec=0.8166 rec=0.8500 spec=0.8100 f1=0.8330 | time=12.9s\n",
            "Epoch 058 | train_loss=0.2027 acc=0.9285 | val_loss=0.4087 acc=0.8209 | prec=0.8895 rec=0.7318 spec=0.9095 f1=0.8030 | time=13.0s\n",
            "Epoch 059 | train_loss=0.2390 acc=0.9064 | val_loss=0.4296 acc=0.8027 | prec=0.9030 rec=0.6773 spec=0.9276 f1=0.7740 | time=12.9s\n",
            "Epoch 060 | train_loss=0.2131 acc=0.9195 | val_loss=0.3633 acc=0.8345 | prec=0.8483 rec=0.8136 spec=0.8552 f1=0.8306 | time=13.0s\n",
            "Epoch 061 | train_loss=0.2088 acc=0.9212 | val_loss=0.3902 acc=0.8231 | prec=0.8817 rec=0.7455 spec=0.9005 f1=0.8079 | time=13.0s\n",
            "Epoch 062 | train_loss=0.2045 acc=0.9149 | val_loss=0.4902 acc=0.7937 | prec=0.9161 rec=0.6455 spec=0.9412 f1=0.7573 | time=13.0s\n",
            "Epoch 063 | train_loss=0.2124 acc=0.9138 | val_loss=0.4185 acc=0.8209 | prec=0.9029 rec=0.7182 spec=0.9231 f1=0.8000 | time=13.0s\n",
            "Epoch 064 | train_loss=0.1965 acc=0.9206 | val_loss=0.4326 acc=0.8141 | prec=0.9059 rec=0.7000 spec=0.9276 f1=0.7897 | time=13.0s\n",
            "Epoch 065 | train_loss=0.1902 acc=0.9325 | val_loss=0.3765 acc=0.8322 | prec=0.8687 rec=0.7818 spec=0.8824 f1=0.8230 | time=13.0s\n",
            "Epoch 066 | train_loss=0.1800 acc=0.9302 | val_loss=0.3643 acc=0.8345 | prec=0.8621 rec=0.7955 spec=0.8733 f1=0.8274 | time=12.9s\n",
            "Epoch 067 | train_loss=0.1934 acc=0.9365 | val_loss=0.4643 acc=0.8095 | prec=0.9146 rec=0.6818 spec=0.9367 f1=0.7812 | time=12.9s\n",
            "Epoch 068 | train_loss=0.2156 acc=0.9132 | val_loss=0.3756 acc=0.8254 | prec=0.8389 rec=0.8045 spec=0.8462 f1=0.8213 | time=13.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▆▆▆▆▅▁▆▇▇▇█▇███▇█████▇███▇██▇█████▇█▇▇██</td></tr><tr><td>precision</td><td>▁▁▁▁▁▃▁▅▃▅▇▆▆▇▇▇▇▄▇▇▇▆▇▇▇▇▇█▇█▇▇▇▆█▇▇█▇█</td></tr><tr><td>recall</td><td>█████▁█▅▃▇▅█▆▆▇▆▇▆▇▇▆▆▇▇▆▇▅▆▅▆▇▇▆▆▇▅▆▆▆▇</td></tr><tr><td>specificity</td><td>▁▁▁▆██▅▆▇▄▇▇▇▇▇▇▅▇▇▇▇▇▇▇▇▇████▇▇▇▇████▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▂▃▄▄▆▆▆▅▆▆▇▇▇▇▇▇▇▇▇████▇██████████</td></tr><tr><td>train_loss</td><td>██████▇▆▆▅▅▄▄▄▄▃▃▃▃▃▂▃▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▁▅▆▆▇▇▇▇▇▇▇▇██▇▇▇▇▇▆▇▆▇▇█▇▇▇▇▇▇▇█▇</td></tr><tr><td>val_loss</td><td>████████▆▅▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▄▁▂▃▂▁▁▁▁▃▂▄▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>68</td></tr><tr><td>f1_score</td><td>0.82135</td></tr><tr><td>precision</td><td>0.83886</td></tr><tr><td>recall</td><td>0.80455</td></tr><tr><td>specificity</td><td>0.84615</td></tr><tr><td>train_acc</td><td>0.91322</td></tr><tr><td>train_loss</td><td>0.21562</td></tr><tr><td>val_acc</td><td>0.8254</td></tr><tr><td>val_loss</td><td>0.37559</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/hlrdfgoz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2/runs/hlrdfgoz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_034315-hlrdfgoz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 03:58:16,100] Trial 9 finished with values: [0.34045001970870153, 0.8526077097505669] and parameters: {'lr': 4.256281022561698e-05, 'wd': 6.137703745483915e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Selected Trial #7 ===\n",
            " val_loss=0.3224, val_acc=0.8594, params={'lr': 2.1990910603368945e-05, 'wd': 2.299085583693177e-06}, best_epoch=85\n",
            "Saved final model to models/eeg_best_AD_CN_1.pth\n",
            "\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=272  A(D)=146, C(N)=126\n",
            " Accuracy=0.8272 Sensitivity=0.9206 Specificity=0.7466 F1=0.8315\n",
            "\n",
            "-- test_cross -- total=626  A(D)=319, C(N)=307\n",
            " Accuracy=0.6534 Sensitivity=0.8306 Specificity=0.4828 F1=0.7015\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Without Full Train dataset\n",
        "- Remove the full train (train + val) process\n",
        "- Instead of using the model at early stopping epoch, we are going to use the best performance model (lowest validation loss) before early stopping\n"
      ],
      "metadata": {
        "id": "Y-KSSvqOkTgH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "PCT_START   = 0.2  # fixed\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & count splits ──────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "class0, class1 = 'A','C'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "# A vs C Filtering\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "def count_labels(meta_list):\n",
        "    cnt0 = cnt1 = 0\n",
        "    for d in meta_list:\n",
        "        lbl = d['label']\n",
        "        if isinstance(lbl, str):\n",
        "            if lbl == class0: cnt0 += 1\n",
        "            elif lbl == class1: cnt1 += 1\n",
        "        else:\n",
        "            if lbl == 0: cnt0 += 1\n",
        "            elif lbl == 1: cnt1 += 1\n",
        "    return cnt0, cnt1\n",
        "\n",
        "n_tr0, n_tr1 = count_labels(train_meta)\n",
        "n_tw0, n_tw1 = count_labels(test_within_meta)\n",
        "n_tc0, n_tc1 = count_labels(test_cross_meta)\n",
        "\n",
        "print(f\"--> Data counts before balancing:\")\n",
        "print(f\"    TRAIN         total={len(train_meta)}  A(D)={n_tr0}, C(N)={n_tr1}\")\n",
        "print(f\"    TEST_WITHIN   total={len(test_within_meta)}  A(D)={n_tw0}, C(N)={n_tw1}\")\n",
        "print(f\"    TEST_CROSS    total={len(test_cross_meta)}  A(D)={n_tc0}, C(N)={n_tc1}\\n\")\n",
        "\n",
        "# ─── Balance train set ───────────────────────────────────────────\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "balanced_meta = [copy.deepcopy(d) for d in balanced_meta]\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# ─── After Balancing ───────────────────────────────────────────\n",
        "bal_AD, bal_CN = count_labels(balanced_meta)\n",
        "print(f\"[BALANCED TRAIN] total={len(balanced_meta)}  AD={bal_AD}, CN={bal_CN}\\n\")\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset_train = BinaryEEGDataset(raw_ds_train, balanced_meta)\n",
        "labels_train  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── Optuna Objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # 1) sample hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-4, log=True)\n",
        "    wd = trial.suggest_float('wd', 1e-6, 1e-4, log=True)\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-AD-CN-test-within-cross-3',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # train/validation split\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2,\n",
        "        stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "\n",
        "    tr_AD = np.sum(labels_train[tr_idx] == 0)\n",
        "    tr_CN = np.sum(labels_train[tr_idx] == 1)\n",
        "    va_AD = np.sum(labels_train[va_idx] == 0)\n",
        "    va_CN = np.sum(labels_train[va_idx] == 1)\n",
        "    print(f\"[Trial {trial.number}]  \"\n",
        "          f\"TRAIN n={len(tr_idx)} (AD={tr_AD}, CN={tr_CN}) | \"\n",
        "          f\"VAL n={len(va_idx)} (AD={va_AD}, CN={va_CN})\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset_train, tr_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset_train, va_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # model & optimizer & scheduler & loss\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc  = 0.0\n",
        "    best_state= None\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # ── train ──\n",
        "        model.train()\n",
        "        train_loss_sum = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight_decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = wd * (cur_lr / lr)\n",
        "            train_loss_sum += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            train_correct += (preds == y).sum().item()\n",
        "            train_total   += y.size(0)\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "        train_acc  = train_correct / train_total\n",
        "\n",
        "        # ── validate ──\n",
        "        model.eval()\n",
        "        vloss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss / len(val_loader)\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        val_acc = (preds == labs).sum() / labs.size\n",
        "\n",
        "        # ── metrics ──\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "        spec = tn / (tn+fp) if (tn+fp)>0 else 0.0\n",
        "        prec = precision_score(labs, preds, zero_division=0)\n",
        "        rec  = recall_score(labs, preds, zero_division=0)\n",
        "        f1   = f1_score(labs, preds, zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={prec:.4f} rec={rec:.4f} spec={spec:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':       epoch,\n",
        "            'train_loss':  train_loss,\n",
        "            'train_acc':   train_acc,\n",
        "            'val_loss':    val_loss,\n",
        "            'val_acc':     val_acc,\n",
        "            'specificity': spec,\n",
        "            'precision':   prec,\n",
        "            'recall':      rec,\n",
        "            'f1_score':    f1\n",
        "        })\n",
        "\n",
        "        # ── Early Stopping (val_loss 기준) ──\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc  = val_acc\n",
        "            es_count      = 0\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            os.makedirs('ckpts', exist_ok=True)\n",
        "            ckpt_path = f'ckpts/trial{trial.number}_optimal_AD_CN_1.pth'\n",
        "            torch.save(best_state, ckpt_path)\n",
        "            trial.set_user_attr('ckpt_path', ckpt_path)\n",
        "            trial.set_user_attr('best_epoch', epoch)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, train_loader, val_loader, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 다중목적 리턴\n",
        "    return best_val_loss, best_val_acc\n",
        "\n",
        "# ─── Run Optuna Study ────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=[\"minimize\", \"maximize\"],\n",
        "        study_name=\"eeg_multiobj\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Pareto front 중 val_acc 가 가장 높은 trial 선택\n",
        "    best       = max(study.best_trials, key=lambda t: t.values[1])\n",
        "    bv_loss, bv_acc = best.values\n",
        "    best_epoch = best.user_attrs[\"best_epoch\"]\n",
        "    ckpt_path  = best.user_attrs[\"ckpt_path\"]\n",
        "\n",
        "    print(f\"\\n=== Selected Trial #{best.number} ===\")\n",
        "    print(\n",
        "        f\" val_loss={bv_loss:.4f}, val_acc={bv_acc:.4f}, \"\n",
        "        f\"params={best.params}, best_epoch={best_epoch}, ckpt={ckpt_path}\"\n",
        "    )\n",
        "\n",
        "    # ─── Search‑model 로드 ───────────────────────────────────────\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    search_model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    search_model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    search_model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── Evaluation 함수 ─────────────────────────────────────────\n",
        "    def evaluate(model, metas, criterion):\n",
        "        metas_copy = copy.deepcopy(metas)\n",
        "        for d in metas_copy:\n",
        "            if isinstance(d[\"label\"], str):\n",
        "                d[\"label\"] = label_map[d[\"label\"]]\n",
        "\n",
        "        ds = BinaryEEGDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal += y.size(0)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0, 1]).ravel()\n",
        "        return {\n",
        "            \"loss\":        vloss / len(loader),\n",
        "            \"acc\":         vcorrect / vtotal,\n",
        "            \"sensitivity\": recall_score(labs, preds, zero_division=0),\n",
        "            \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
        "            \"f1\":          f1_score(labs, preds, zero_division=0),\n",
        "        }\n",
        "\n",
        "    # ─── Final Evaluation on Test Sets ────────────────────────────\n",
        "    print(\"=== Final Evaluation on Test Sets ===\")\n",
        "    for name, metas in [\n",
        "        (\"test_within\", test_within_meta),\n",
        "        (\"test_cross\",  test_cross_meta),\n",
        "    ]:\n",
        "        res = evaluate(search_model, metas, criterion)\n",
        "        n0, n1 = count_labels(metas)\n",
        "        print(f\"-- {name} -- total={len(metas)}  A(D)={n0}, C(N)={n1}\")\n",
        "        print(\n",
        "            f\" Accuracy={res['acc']:.4f} \"\n",
        "            f\"Sensitivity={res['sensitivity']:.4f} \"\n",
        "            f\"Specificity={res['specificity']:.4f} \"\n",
        "            f\"F1={res['f1']:.4f}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "oGYa9L4bYYYP",
        "outputId": "3f44fbc8-a0f5-4d3c-dc2b-b652c36d4c88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 21:22:50,544] A new study created in memory with name: eeg_multiobj\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Data counts before balancing:\n",
            "    TRAIN         total=3219  A(D)=1388, C(N)=1102\n",
            "    TEST_WITHIN   total=272  A(D)=146, C(N)=126\n",
            "    TEST_CROSS    total=626  A(D)=319, C(N)=307\n",
            "\n",
            "[BALANCED TRAIN] total=2204  AD=1102, CN=1102\n",
            "\n",
            "\n",
            "--- Trial 0 hyperparams --> lr=1.95e-05, wd=1.48e-06\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_212250-2bfm4juy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/2bfm4juy' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/2bfm4juy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/2bfm4juy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 0]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7506 acc=0.5173 | val_loss=0.6942 acc=0.4898 | prec=0.4935 rec=0.8682 spec=0.1131 f1=0.6293 | time=171.5s\n",
            "Epoch 002 | train_loss=0.7479 acc=0.4929 | val_loss=0.6946 acc=0.5147 | prec=0.5069 rec=0.9955 spec=0.0362 f1=0.6718 | time=13.5s\n",
            "Epoch 003 | train_loss=0.7577 acc=0.4793 | val_loss=0.6945 acc=0.5011 | prec=0.5000 rec=0.9955 spec=0.0090 f1=0.6657 | time=13.6s\n",
            "Epoch 004 | train_loss=0.7551 acc=0.5009 | val_loss=0.6942 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=13.5s\n",
            "Epoch 005 | train_loss=0.7405 acc=0.5014 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.5s\n",
            "Epoch 006 | train_loss=0.7480 acc=0.4872 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.5s\n",
            "Epoch 007 | train_loss=0.7377 acc=0.4952 | val_loss=0.6942 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=13.6s\n",
            "Epoch 008 | train_loss=0.7241 acc=0.5167 | val_loss=0.6937 acc=0.5034 | prec=0.5011 rec=0.9955 spec=0.0136 f1=0.6667 | time=13.4s\n",
            "Epoch 009 | train_loss=0.7382 acc=0.4980 | val_loss=0.6930 acc=0.5034 | prec=0.5015 rec=0.7682 spec=0.2398 f1=0.6068 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7416 acc=0.4901 | val_loss=0.6937 acc=0.4966 | prec=0.4977 rec=0.9909 spec=0.0045 f1=0.6626 | time=12.7s\n",
            "Epoch 011 | train_loss=0.7349 acc=0.5003 | val_loss=0.6929 acc=0.4853 | prec=0.4867 rec=0.5818 spec=0.3891 f1=0.5300 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7181 acc=0.4997 | val_loss=0.6932 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7217 acc=0.4969 | val_loss=0.6938 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7333 acc=0.4923 | val_loss=0.6927 acc=0.5034 | prec=0.6667 rec=0.0091 spec=0.9955 f1=0.0179 | time=12.7s\n",
            "Epoch 015 | train_loss=0.7401 acc=0.4770 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7375 acc=0.4782 | val_loss=0.6920 acc=0.5351 | prec=0.5243 rec=0.7364 spec=0.3348 f1=0.6125 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7102 acc=0.5207 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7237 acc=0.5111 | val_loss=0.6925 acc=0.5351 | prec=0.5330 rec=0.5500 spec=0.5204 f1=0.5414 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7210 acc=0.4963 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7214 acc=0.5139 | val_loss=0.6908 acc=0.5147 | prec=0.5069 rec=1.0000 spec=0.0317 f1=0.6728 | time=12.7s\n",
            "Epoch 021 | train_loss=0.7168 acc=0.5122 | val_loss=0.6907 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 022 | train_loss=0.7197 acc=0.4935 | val_loss=0.6902 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.7s\n",
            "Epoch 023 | train_loss=0.7150 acc=0.5235 | val_loss=0.6809 acc=0.6395 | prec=0.7163 rec=0.4591 spec=0.8190 f1=0.5596 | time=12.7s\n",
            "Epoch 024 | train_loss=0.7067 acc=0.5417 | val_loss=0.6803 acc=0.6304 | prec=0.5882 rec=0.8636 spec=0.3982 f1=0.6998 | time=12.8s\n",
            "Epoch 025 | train_loss=0.6950 acc=0.5434 | val_loss=0.6492 acc=0.6327 | prec=0.5890 rec=0.8727 spec=0.3937 f1=0.7033 | time=12.7s\n",
            "Epoch 026 | train_loss=0.6841 acc=0.5610 | val_loss=0.6196 acc=0.7211 | prec=0.6902 rec=0.8000 spec=0.6425 f1=0.7411 | time=12.8s\n",
            "Epoch 027 | train_loss=0.6451 acc=0.6529 | val_loss=0.6696 acc=0.5261 | prec=0.5128 rec=1.0000 spec=0.0543 f1=0.6780 | time=12.8s\n",
            "Epoch 028 | train_loss=0.6484 acc=0.6171 | val_loss=0.5808 acc=0.7619 | prec=0.7533 rec=0.7773 spec=0.7466 f1=0.7651 | time=12.8s\n",
            "Epoch 029 | train_loss=0.6107 acc=0.6767 | val_loss=0.5458 acc=0.7846 | prec=0.8079 rec=0.7455 spec=0.8235 f1=0.7754 | time=12.7s\n",
            "Epoch 030 | train_loss=0.5876 acc=0.6886 | val_loss=0.5503 acc=0.7868 | prec=0.8247 rec=0.7273 spec=0.8462 f1=0.7729 | time=12.7s\n",
            "Epoch 031 | train_loss=0.5743 acc=0.6999 | val_loss=0.5228 acc=0.7823 | prec=0.8069 rec=0.7409 spec=0.8235 f1=0.7725 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5718 acc=0.7243 | val_loss=0.5472 acc=0.7868 | prec=0.7333 rec=0.9000 spec=0.6742 f1=0.8082 | time=12.8s\n",
            "Epoch 033 | train_loss=0.5704 acc=0.7204 | val_loss=0.5038 acc=0.7914 | prec=0.8077 rec=0.7636 spec=0.8190 f1=0.7850 | time=12.7s\n",
            "Epoch 034 | train_loss=0.5326 acc=0.7589 | val_loss=0.5187 acc=0.8073 | prec=0.8082 rec=0.8045 spec=0.8100 f1=0.8064 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5393 acc=0.7419 | val_loss=0.5061 acc=0.8005 | prec=0.8143 rec=0.7773 spec=0.8235 f1=0.7953 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5302 acc=0.7555 | val_loss=0.5000 acc=0.7868 | prec=0.8500 rec=0.6955 spec=0.8778 f1=0.7650 | time=12.7s\n",
            "Epoch 037 | train_loss=0.5165 acc=0.7601 | val_loss=0.4832 acc=0.8073 | prec=0.8111 rec=0.8000 spec=0.8145 f1=0.8055 | time=12.7s\n",
            "Epoch 038 | train_loss=0.5145 acc=0.7482 | val_loss=0.4807 acc=0.8095 | prec=0.8036 rec=0.8182 spec=0.8009 f1=0.8108 | time=12.8s\n",
            "Epoch 039 | train_loss=0.5017 acc=0.7663 | val_loss=0.4910 acc=0.8095 | prec=0.8063 rec=0.8136 spec=0.8054 f1=0.8100 | time=12.7s\n",
            "Epoch 040 | train_loss=0.4877 acc=0.7924 | val_loss=0.4767 acc=0.8073 | prec=0.8325 rec=0.7682 spec=0.8462 f1=0.7991 | time=12.8s\n",
            "Epoch 041 | train_loss=0.4786 acc=0.7964 | val_loss=0.4749 acc=0.8050 | prec=0.8317 rec=0.7636 spec=0.8462 f1=0.7962 | time=12.8s\n",
            "Epoch 042 | train_loss=0.4739 acc=0.8003 | val_loss=0.4785 acc=0.8209 | prec=0.8249 rec=0.8136 spec=0.8281 f1=0.8192 | time=12.7s\n",
            "Epoch 043 | train_loss=0.4615 acc=0.7947 | val_loss=0.4636 acc=0.8254 | prec=0.8206 rec=0.8318 spec=0.8190 f1=0.8262 | time=12.9s\n",
            "Epoch 044 | train_loss=0.4611 acc=0.8054 | val_loss=0.4663 acc=0.8231 | prec=0.8287 rec=0.8136 spec=0.8326 f1=0.8211 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4657 acc=0.7975 | val_loss=0.4495 acc=0.8186 | prec=0.8097 rec=0.8318 spec=0.8054 f1=0.8206 | time=13.1s\n",
            "Epoch 046 | train_loss=0.4654 acc=0.8003 | val_loss=0.4699 acc=0.8005 | prec=0.8367 rec=0.7455 spec=0.8552 f1=0.7885 | time=12.9s\n",
            "Epoch 047 | train_loss=0.4410 acc=0.8202 | val_loss=0.4507 acc=0.8231 | prec=0.8087 rec=0.8455 spec=0.8009 f1=0.8267 | time=12.9s\n",
            "Epoch 048 | train_loss=0.4524 acc=0.8128 | val_loss=0.4621 acc=0.8141 | prec=0.8382 rec=0.7773 spec=0.8507 f1=0.8066 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4370 acc=0.8123 | val_loss=0.4560 acc=0.8186 | prec=0.8017 rec=0.8455 spec=0.7919 f1=0.8230 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4314 acc=0.8259 | val_loss=0.4491 acc=0.8186 | prec=0.8097 rec=0.8318 spec=0.8054 f1=0.8206 | time=12.8s\n",
            "Epoch 051 | train_loss=0.4266 acc=0.8247 | val_loss=0.4478 acc=0.8254 | prec=0.7918 rec=0.8818 spec=0.7692 f1=0.8344 | time=12.9s\n",
            "Epoch 052 | train_loss=0.4272 acc=0.8276 | val_loss=0.4432 acc=0.8209 | prec=0.8373 rec=0.7955 spec=0.8462 f1=0.8159 | time=12.8s\n",
            "Epoch 053 | train_loss=0.4190 acc=0.8168 | val_loss=0.4342 acc=0.8277 | prec=0.8214 rec=0.8364 spec=0.8190 f1=0.8288 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4170 acc=0.8264 | val_loss=0.4493 acc=0.8163 | prec=0.8357 rec=0.7864 spec=0.8462 f1=0.8103 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4098 acc=0.8338 | val_loss=0.4421 acc=0.8186 | prec=0.8302 rec=0.8000 spec=0.8371 f1=0.8148 | time=12.9s\n",
            "Epoch 056 | train_loss=0.4046 acc=0.8383 | val_loss=0.4264 acc=0.8277 | prec=0.8158 rec=0.8455 spec=0.8100 f1=0.8304 | time=12.9s\n",
            "Epoch 057 | train_loss=0.4090 acc=0.8423 | val_loss=0.4323 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.7s\n",
            "Epoch 058 | train_loss=0.3978 acc=0.8259 | val_loss=0.4251 acc=0.8322 | prec=0.8259 rec=0.8409 spec=0.8235 f1=0.8333 | time=12.7s\n",
            "Epoch 059 | train_loss=0.3877 acc=0.8508 | val_loss=0.4313 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.7s\n",
            "Epoch 060 | train_loss=0.3878 acc=0.8452 | val_loss=0.4280 acc=0.8277 | prec=0.8303 rec=0.8227 spec=0.8326 f1=0.8265 | time=12.7s\n",
            "Epoch 061 | train_loss=0.3855 acc=0.8548 | val_loss=0.4210 acc=0.8254 | prec=0.8235 rec=0.8273 spec=0.8235 f1=0.8254 | time=12.8s\n",
            "Epoch 062 | train_loss=0.3915 acc=0.8469 | val_loss=0.4221 acc=0.8186 | prec=0.8333 rec=0.7955 spec=0.8416 f1=0.8140 | time=12.8s\n",
            "Epoch 063 | train_loss=0.3826 acc=0.8457 | val_loss=0.4153 acc=0.8209 | prec=0.8310 rec=0.8045 spec=0.8371 f1=0.8176 | time=12.7s\n",
            "Epoch 064 | train_loss=0.3774 acc=0.8571 | val_loss=0.4202 acc=0.8277 | prec=0.8303 rec=0.8227 spec=0.8326 f1=0.8265 | time=12.8s\n",
            "Epoch 065 | train_loss=0.3645 acc=0.8610 | val_loss=0.4132 acc=0.8231 | prec=0.8381 rec=0.8000 spec=0.8462 f1=0.8186 | time=12.9s\n",
            "Epoch 066 | train_loss=0.3714 acc=0.8644 | val_loss=0.4120 acc=0.8322 | prec=0.8318 rec=0.8318 spec=0.8326 f1=0.8318 | time=12.8s\n",
            "Epoch 067 | train_loss=0.3617 acc=0.8639 | val_loss=0.4105 acc=0.8367 | prec=0.8333 rec=0.8409 spec=0.8326 f1=0.8371 | time=12.8s\n",
            "Epoch 068 | train_loss=0.3635 acc=0.8616 | val_loss=0.4084 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.6s\n",
            "Epoch 069 | train_loss=0.3624 acc=0.8520 | val_loss=0.4061 acc=0.8231 | prec=0.8287 rec=0.8136 spec=0.8326 f1=0.8211 | time=12.9s\n",
            "Epoch 070 | train_loss=0.3464 acc=0.8627 | val_loss=0.4135 acc=0.8231 | prec=0.8381 rec=0.8000 spec=0.8462 f1=0.8186 | time=12.7s\n",
            "Epoch 071 | train_loss=0.3528 acc=0.8684 | val_loss=0.4042 acc=0.8231 | prec=0.8114 rec=0.8409 spec=0.8054 f1=0.8259 | time=12.8s\n",
            "Epoch 072 | train_loss=0.3484 acc=0.8684 | val_loss=0.3958 acc=0.8390 | prec=0.8253 rec=0.8591 spec=0.8190 f1=0.8419 | time=12.7s\n",
            "Epoch 073 | train_loss=0.3524 acc=0.8576 | val_loss=0.4162 acc=0.8186 | prec=0.8500 rec=0.7727 spec=0.8643 f1=0.8095 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3496 acc=0.8593 | val_loss=0.4310 acc=0.8073 | prec=0.8649 rec=0.7273 spec=0.8869 f1=0.7901 | time=12.8s\n",
            "Epoch 075 | train_loss=0.3308 acc=0.8809 | val_loss=0.4102 acc=0.8277 | prec=0.8529 rec=0.7909 spec=0.8643 f1=0.8208 | time=12.9s\n",
            "Epoch 076 | train_loss=0.3491 acc=0.8667 | val_loss=0.4081 acc=0.8367 | prec=0.8558 rec=0.8091 spec=0.8643 f1=0.8318 | time=12.8s\n",
            "Epoch 077 | train_loss=0.3253 acc=0.8792 | val_loss=0.3953 acc=0.8345 | prec=0.8075 rec=0.8773 spec=0.7919 f1=0.8410 | time=12.8s\n",
            "Epoch 078 | train_loss=0.3447 acc=0.8832 | val_loss=0.3883 acc=0.8390 | prec=0.8197 rec=0.8682 spec=0.8100 f1=0.8433 | time=12.8s\n",
            "Epoch 079 | train_loss=0.3367 acc=0.8701 | val_loss=0.3968 acc=0.8322 | prec=0.8544 rec=0.8000 spec=0.8643 f1=0.8263 | time=12.8s\n",
            "Epoch 080 | train_loss=0.3301 acc=0.8792 | val_loss=0.3913 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=12.7s\n",
            "Epoch 081 | train_loss=0.3254 acc=0.8707 | val_loss=0.3942 acc=0.8277 | prec=0.8364 rec=0.8136 spec=0.8416 f1=0.8249 | time=12.7s\n",
            "Epoch 082 | train_loss=0.3367 acc=0.8849 | val_loss=0.3926 acc=0.8299 | prec=0.8537 rec=0.7955 spec=0.8643 f1=0.8235 | time=12.7s\n",
            "Epoch 083 | train_loss=0.3208 acc=0.9002 | val_loss=0.4192 acc=0.8141 | prec=0.8632 rec=0.7455 spec=0.8824 f1=0.8000 | time=12.8s\n",
            "Epoch 084 | train_loss=0.3302 acc=0.8769 | val_loss=0.3948 acc=0.8345 | prec=0.8551 rec=0.8045 spec=0.8643 f1=0.8290 | time=12.8s\n",
            "Epoch 085 | train_loss=0.3244 acc=0.8775 | val_loss=0.3979 acc=0.8231 | prec=0.8622 rec=0.7682 spec=0.8778 f1=0.8125 | time=12.7s\n",
            "Epoch 086 | train_loss=0.3337 acc=0.8724 | val_loss=0.4097 acc=0.8186 | prec=0.8684 rec=0.7500 spec=0.8869 f1=0.8049 | time=12.8s\n",
            "Epoch 087 | train_loss=0.3193 acc=0.8837 | val_loss=0.4160 acc=0.8118 | prec=0.8703 rec=0.7318 spec=0.8914 f1=0.7951 | time=12.7s\n",
            "Epoch 088 | train_loss=0.3078 acc=0.8792 | val_loss=0.3868 acc=0.8413 | prec=0.8505 rec=0.8273 spec=0.8552 f1=0.8387 | time=12.8s\n",
            "Epoch 089 | train_loss=0.3151 acc=0.8815 | val_loss=0.4255 acc=0.8073 | prec=0.8649 rec=0.7273 spec=0.8869 f1=0.7901 | time=12.8s\n",
            "Epoch 090 | train_loss=0.3189 acc=0.8792 | val_loss=0.3857 acc=0.8299 | prec=0.8502 rec=0.8000 spec=0.8597 f1=0.8244 | time=12.8s\n",
            "Epoch 091 | train_loss=0.2961 acc=0.8939 | val_loss=0.3928 acc=0.8277 | prec=0.8529 rec=0.7909 spec=0.8643 f1=0.8208 | time=12.7s\n",
            "Epoch 092 | train_loss=0.3010 acc=0.8832 | val_loss=0.3851 acc=0.8367 | prec=0.8524 rec=0.8136 spec=0.8597 f1=0.8326 | time=12.8s\n",
            "Epoch 093 | train_loss=0.3184 acc=0.8871 | val_loss=0.3989 acc=0.8186 | prec=0.8646 rec=0.7545 spec=0.8824 f1=0.8058 | time=12.9s\n",
            "Epoch 094 | train_loss=0.3059 acc=0.8888 | val_loss=0.3863 acc=0.8345 | prec=0.8693 rec=0.7864 spec=0.8824 f1=0.8258 | time=12.9s\n",
            "Epoch 095 | train_loss=0.2865 acc=0.9013 | val_loss=0.3764 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=12.7s\n",
            "Epoch 096 | train_loss=0.2907 acc=0.8900 | val_loss=0.3834 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.9s\n",
            "Epoch 097 | train_loss=0.3036 acc=0.8820 | val_loss=0.3748 acc=0.8549 | prec=0.8611 rec=0.8455 spec=0.8643 f1=0.8532 | time=12.7s\n",
            "Epoch 098 | train_loss=0.2844 acc=0.8928 | val_loss=0.3960 acc=0.8231 | prec=0.8777 rec=0.7500 spec=0.8959 f1=0.8088 | time=12.8s\n",
            "Epoch 099 | train_loss=0.2877 acc=0.8922 | val_loss=0.3879 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.8s\n",
            "Epoch 100 | train_loss=0.2993 acc=0.8934 | val_loss=0.3920 acc=0.8209 | prec=0.8691 rec=0.7545 spec=0.8869 f1=0.8078 | time=12.7s\n",
            "Epoch 101 | train_loss=0.2868 acc=0.8928 | val_loss=0.3915 acc=0.8345 | prec=0.8808 rec=0.7727 spec=0.8959 f1=0.8232 | time=12.7s\n",
            "Epoch 102 | train_loss=0.2996 acc=0.8832 | val_loss=0.4106 acc=0.8163 | prec=0.8798 rec=0.7318 spec=0.9005 f1=0.7990 | time=12.8s\n",
            "Epoch 103 | train_loss=0.2947 acc=0.8951 | val_loss=0.3875 acc=0.8254 | prec=0.8705 rec=0.7636 spec=0.8869 f1=0.8136 | time=12.7s\n",
            "Epoch 104 | train_loss=0.2878 acc=0.9070 | val_loss=0.4192 acc=0.8095 | prec=0.8820 rec=0.7136 spec=0.9050 f1=0.7889 | time=12.8s\n",
            "Epoch 105 | train_loss=0.3063 acc=0.8934 | val_loss=0.4016 acc=0.8163 | prec=0.8717 rec=0.7409 spec=0.8914 f1=0.8010 | time=12.8s\n",
            "Epoch 106 | train_loss=0.2981 acc=0.9024 | val_loss=0.3772 acc=0.8458 | prec=0.8619 rec=0.8227 spec=0.8688 f1=0.8419 | time=12.8s\n",
            "Epoch 107 | train_loss=0.2879 acc=0.8871 | val_loss=0.4220 acc=0.7982 | prec=0.8743 rec=0.6955 spec=0.9005 f1=0.7747 | time=12.8s\n",
            "Epoch 108 | train_loss=0.2871 acc=0.8973 | val_loss=0.3871 acc=0.8367 | prec=0.8737 rec=0.7864 spec=0.8869 f1=0.8278 | time=12.8s\n",
            "Epoch 109 | train_loss=0.2810 acc=0.8956 | val_loss=0.3876 acc=0.8299 | prec=0.8718 rec=0.7727 spec=0.8869 f1=0.8193 | time=12.8s\n",
            "Epoch 110 | train_loss=0.2791 acc=0.9013 | val_loss=0.4140 acc=0.8073 | prec=0.8771 rec=0.7136 spec=0.9005 f1=0.7870 | time=12.8s\n",
            "Epoch 111 | train_loss=0.2864 acc=0.8968 | val_loss=0.4230 acc=0.8050 | prec=0.8807 rec=0.7045 spec=0.9050 f1=0.7828 | time=12.7s\n",
            "Epoch 112 | train_loss=0.2826 acc=0.8911 | val_loss=0.3999 acc=0.8186 | prec=0.8804 rec=0.7364 spec=0.9005 f1=0.8020 | time=12.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▇▇▇▇▇▆▁▇▆▅▇▁▆▇▇███████████████████████▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▁▂▁▅▃▆▅▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇██▇▇██████</td></tr><tr><td>recall</td><td>███████▁█▆▅▄▄▅▃▅▅▄▄▆▆▅▆▅▅▅▆▅▆▅▄▄▄▅▆▄▄▄▅▄</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▇▄▁█▆▇█▇▇██▇▇██▇▇█▇██████████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▂▂▂▅▆▆▆▇▇▇▇▇▇▇▇▇▇██▇█▇█▇█████████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▇▆▅▅▅▄▄▄▄▃▃▂▃▂▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▂▁▄▆▆▇▇▇▇▇█▇███▇████▇██▇████▇█▇█▇</td></tr><tr><td>val_loss</td><td>██████▇▆▅▅▄▄▄▃▃▃▂▃▂▂▂▂▂▂▂▁▂▁▂▁▂▁▂▁▁▂▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>112</td></tr><tr><td>f1_score</td><td>0.80198</td></tr><tr><td>precision</td><td>0.88043</td></tr><tr><td>recall</td><td>0.73636</td></tr><tr><td>specificity</td><td>0.90045</td></tr><tr><td>train_acc</td><td>0.89109</td></tr><tr><td>train_loss</td><td>0.28262</td></tr><tr><td>val_acc</td><td>0.81859</td></tr><tr><td>val_loss</td><td>0.39993</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/2bfm4juy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/2bfm4juy</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_212250-2bfm4juy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 21:49:46,543] Trial 0 finished with values: [0.3747793829866818, 0.854875283446712] and parameters: {'lr': 1.9470502508163917e-05, 'wd': 1.4828404303782198e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 1 hyperparams --> lr=7.27e-05, wd=9.33e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_214946-wo9antuq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/wo9antuq' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/wo9antuq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/wo9antuq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 1]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7282 acc=0.5173 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 002 | train_loss=0.7238 acc=0.5167 | val_loss=0.6929 acc=0.5057 | prec=0.5026 rec=0.8682 spec=0.1448 f1=0.6367 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7290 acc=0.5020 | val_loss=0.6928 acc=0.5011 | prec=0.5000 rec=0.9955 spec=0.0090 f1=0.6657 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7209 acc=0.5213 | val_loss=0.6925 acc=0.4966 | prec=0.4971 rec=0.7864 spec=0.2081 f1=0.6092 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7178 acc=0.5162 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7346 acc=0.4980 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 007 | train_loss=0.7282 acc=0.5105 | val_loss=0.6944 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7294 acc=0.4918 | val_loss=0.6914 acc=0.5125 | prec=0.5062 rec=0.9318 spec=0.0950 f1=0.6560 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7290 acc=0.4940 | val_loss=0.6910 acc=0.5147 | prec=0.7143 rec=0.0455 spec=0.9819 f1=0.0855 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7176 acc=0.4963 | val_loss=0.6886 acc=0.5102 | prec=0.5046 rec=0.9909 spec=0.0317 f1=0.6687 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7021 acc=0.5320 | val_loss=0.6833 acc=0.5805 | prec=0.9268 rec=0.1727 spec=0.9864 f1=0.2912 | time=12.7s\n",
            "Epoch 012 | train_loss=0.6921 acc=0.5559 | val_loss=0.6599 acc=0.7120 | prec=0.8252 rec=0.5364 spec=0.8869 f1=0.6501 | time=12.7s\n",
            "Epoch 013 | train_loss=0.6579 acc=0.6109 | val_loss=0.6024 acc=0.7574 | prec=0.7989 rec=0.6864 spec=0.8281 f1=0.7384 | time=12.6s\n",
            "Epoch 014 | train_loss=0.5853 acc=0.6733 | val_loss=0.5218 acc=0.7868 | prec=0.8316 rec=0.7182 spec=0.8552 f1=0.7707 | time=12.8s\n",
            "Epoch 015 | train_loss=0.5580 acc=0.7221 | val_loss=0.5255 acc=0.7642 | prec=0.6986 rec=0.9273 spec=0.6018 f1=0.7969 | time=12.7s\n",
            "Epoch 016 | train_loss=0.5377 acc=0.7226 | val_loss=0.4792 acc=0.8073 | prec=0.7755 rec=0.8636 spec=0.7511 f1=0.8172 | time=12.7s\n",
            "Epoch 017 | train_loss=0.5049 acc=0.7431 | val_loss=0.4877 acc=0.8027 | prec=0.7930 rec=0.8182 spec=0.7873 f1=0.8054 | time=12.8s\n",
            "Epoch 018 | train_loss=0.5159 acc=0.7465 | val_loss=0.4833 acc=0.8095 | prec=0.7615 rec=0.9000 spec=0.7195 f1=0.8250 | time=12.7s\n",
            "Epoch 019 | train_loss=0.4992 acc=0.7646 | val_loss=0.4673 acc=0.8095 | prec=0.7787 rec=0.8636 spec=0.7557 f1=0.8190 | time=12.7s\n",
            "Epoch 020 | train_loss=0.4828 acc=0.7725 | val_loss=0.4538 acc=0.8254 | prec=0.8069 rec=0.8545 spec=0.7964 f1=0.8300 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4575 acc=0.7873 | val_loss=0.4874 acc=0.7823 | prec=0.9026 rec=0.6318 spec=0.9321 f1=0.7433 | time=12.9s\n",
            "Epoch 022 | train_loss=0.4369 acc=0.7998 | val_loss=0.4209 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=12.8s\n",
            "Epoch 023 | train_loss=0.4510 acc=0.8009 | val_loss=0.4153 acc=0.8277 | prec=0.8333 rec=0.8182 spec=0.8371 f1=0.8257 | time=12.7s\n",
            "Epoch 024 | train_loss=0.4210 acc=0.8117 | val_loss=0.4322 acc=0.8345 | prec=0.7816 rec=0.9273 spec=0.7421 f1=0.8482 | time=12.7s\n",
            "Epoch 025 | train_loss=0.4285 acc=0.8202 | val_loss=0.4007 acc=0.8186 | prec=0.8846 rec=0.7318 spec=0.9050 f1=0.8010 | time=12.8s\n",
            "Epoch 026 | train_loss=0.4063 acc=0.8185 | val_loss=0.4448 acc=0.8118 | prec=0.7404 rec=0.9591 spec=0.6652 f1=0.8356 | time=12.7s\n",
            "Epoch 027 | train_loss=0.3947 acc=0.8366 | val_loss=0.3676 acc=0.8413 | prec=0.8505 rec=0.8273 spec=0.8552 f1=0.8387 | time=12.8s\n",
            "Epoch 028 | train_loss=0.3789 acc=0.8372 | val_loss=0.3957 acc=0.8118 | prec=0.8703 rec=0.7318 spec=0.8914 f1=0.7951 | time=12.7s\n",
            "Epoch 029 | train_loss=0.4089 acc=0.8270 | val_loss=0.3860 acc=0.8277 | prec=0.8750 rec=0.7636 spec=0.8914 f1=0.8155 | time=12.8s\n",
            "Epoch 030 | train_loss=0.3457 acc=0.8684 | val_loss=0.3626 acc=0.8390 | prec=0.8433 rec=0.8318 spec=0.8462 f1=0.8375 | time=12.8s\n",
            "Epoch 031 | train_loss=0.3260 acc=0.8678 | val_loss=0.3549 acc=0.8571 | prec=0.8398 rec=0.8818 spec=0.8326 f1=0.8603 | time=12.8s\n",
            "Epoch 032 | train_loss=0.3445 acc=0.8565 | val_loss=0.3393 acc=0.8594 | prec=0.8292 rec=0.9045 spec=0.8145 f1=0.8652 | time=12.8s\n",
            "Epoch 033 | train_loss=0.3129 acc=0.8849 | val_loss=0.3651 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=12.7s\n",
            "Epoch 034 | train_loss=0.3080 acc=0.8633 | val_loss=0.3600 acc=0.8549 | prec=0.8047 rec=0.9364 spec=0.7738 f1=0.8655 | time=12.7s\n",
            "Epoch 035 | train_loss=0.2970 acc=0.8684 | val_loss=0.3632 acc=0.8458 | prec=0.8140 rec=0.8955 spec=0.7964 f1=0.8528 | time=12.9s\n",
            "Epoch 036 | train_loss=0.2842 acc=0.8792 | val_loss=0.3933 acc=0.8345 | prec=0.7692 rec=0.9545 spec=0.7149 f1=0.8519 | time=12.9s\n",
            "Epoch 037 | train_loss=0.2993 acc=0.8758 | val_loss=0.3564 acc=0.8481 | prec=0.7977 rec=0.9318 spec=0.7647 f1=0.8595 | time=12.9s\n",
            "Epoch 038 | train_loss=0.2486 acc=0.9178 | val_loss=0.3350 acc=0.8549 | prec=0.8250 rec=0.9000 spec=0.8100 f1=0.8609 | time=12.8s\n",
            "Epoch 039 | train_loss=0.2519 acc=0.9036 | val_loss=0.3478 acc=0.8549 | prec=0.8250 rec=0.9000 spec=0.8100 f1=0.8609 | time=12.9s\n",
            "Epoch 040 | train_loss=0.2657 acc=0.8905 | val_loss=0.3860 acc=0.8390 | prec=0.7770 rec=0.9500 spec=0.7285 f1=0.8548 | time=12.7s\n",
            "Epoch 041 | train_loss=0.2698 acc=0.8837 | val_loss=0.3693 acc=0.8435 | prec=0.8107 rec=0.8955 spec=0.7919 f1=0.8510 | time=12.7s\n",
            "Epoch 042 | train_loss=0.2508 acc=0.8968 | val_loss=0.3586 acc=0.8458 | prec=0.8016 rec=0.9182 spec=0.7738 f1=0.8559 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2902 acc=0.8956 | val_loss=0.3863 acc=0.8413 | prec=0.7907 rec=0.9273 spec=0.7557 f1=0.8536 | time=12.8s\n",
            "Epoch 044 | train_loss=0.2387 acc=0.9024 | val_loss=0.3811 acc=0.8390 | prec=0.7833 rec=0.9364 spec=0.7421 f1=0.8530 | time=12.9s\n",
            "Epoch 045 | train_loss=0.2274 acc=0.9007 | val_loss=0.3791 acc=0.8322 | prec=0.7808 rec=0.9227 spec=0.7421 f1=0.8458 | time=12.8s\n",
            "Epoch 046 | train_loss=0.2167 acc=0.8973 | val_loss=0.3634 acc=0.8481 | prec=0.8097 rec=0.9091 spec=0.7873 f1=0.8565 | time=12.7s\n",
            "Epoch 047 | train_loss=0.2193 acc=0.9172 | val_loss=0.3600 acc=0.8458 | prec=0.8140 rec=0.8955 spec=0.7964 f1=0.8528 | time=12.9s\n",
            "Epoch 048 | train_loss=0.2367 acc=0.9013 | val_loss=0.4720 acc=0.7800 | prec=0.7113 rec=0.9409 spec=0.6199 f1=0.8102 | time=12.8s\n",
            "Epoch 049 | train_loss=0.2302 acc=0.9053 | val_loss=0.3240 acc=0.8617 | prec=0.8383 rec=0.8955 spec=0.8281 f1=0.8659 | time=12.7s\n",
            "Epoch 050 | train_loss=0.2198 acc=0.9047 | val_loss=0.3316 acc=0.8617 | prec=0.8630 rec=0.8591 spec=0.8643 f1=0.8610 | time=12.8s\n",
            "Epoch 051 | train_loss=0.1914 acc=0.9257 | val_loss=0.3798 acc=0.8435 | prec=0.7984 rec=0.9182 spec=0.7692 f1=0.8541 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2150 acc=0.9229 | val_loss=0.5820 acc=0.7687 | prec=0.6891 rec=0.9773 spec=0.5611 f1=0.8083 | time=12.8s\n",
            "Epoch 053 | train_loss=0.2252 acc=0.9007 | val_loss=0.3655 acc=0.8435 | prec=0.7871 rec=0.9409 spec=0.7466 f1=0.8571 | time=12.8s\n",
            "Epoch 054 | train_loss=0.1905 acc=0.9223 | val_loss=0.3679 acc=0.8367 | prec=0.7891 rec=0.9182 spec=0.7557 f1=0.8487 | time=12.8s\n",
            "Epoch 055 | train_loss=0.1783 acc=0.9178 | val_loss=0.3811 acc=0.8345 | prec=0.7816 rec=0.9273 spec=0.7421 f1=0.8482 | time=12.7s\n",
            "Epoch 056 | train_loss=0.1679 acc=0.9246 | val_loss=0.3468 acc=0.8571 | prec=0.8257 rec=0.9045 spec=0.8100 f1=0.8633 | time=12.8s\n",
            "Epoch 057 | train_loss=0.1551 acc=0.9404 | val_loss=0.3367 acc=0.8571 | prec=0.8178 rec=0.9182 spec=0.7964 f1=0.8651 | time=12.7s\n",
            "Epoch 058 | train_loss=0.1630 acc=0.9280 | val_loss=0.3827 acc=0.8481 | prec=0.8097 rec=0.9091 spec=0.7873 f1=0.8565 | time=12.9s\n",
            "Epoch 059 | train_loss=0.1928 acc=0.9257 | val_loss=0.4118 acc=0.8322 | prec=0.7724 rec=0.9409 spec=0.7240 f1=0.8484 | time=12.7s\n",
            "Epoch 060 | train_loss=0.1677 acc=0.9223 | val_loss=0.3611 acc=0.8662 | prec=0.8455 rec=0.8955 spec=0.8371 f1=0.8698 | time=12.8s\n",
            "Epoch 061 | train_loss=0.1780 acc=0.9280 | val_loss=0.3493 acc=0.8662 | prec=0.8710 rec=0.8591 spec=0.8733 f1=0.8650 | time=12.7s\n",
            "Epoch 062 | train_loss=0.1787 acc=0.9234 | val_loss=0.3201 acc=0.8776 | prec=0.8773 rec=0.8773 spec=0.8778 f1=0.8773 | time=12.8s\n",
            "Epoch 063 | train_loss=0.1699 acc=0.9325 | val_loss=0.3531 acc=0.8594 | prec=0.8160 rec=0.9273 spec=0.7919 f1=0.8681 | time=12.7s\n",
            "Epoch 064 | train_loss=0.1875 acc=0.9217 | val_loss=0.4386 acc=0.8322 | prec=0.7684 rec=0.9500 spec=0.7149 f1=0.8496 | time=12.8s\n",
            "Epoch 065 | train_loss=0.1853 acc=0.9132 | val_loss=0.3322 acc=0.8707 | prec=0.8410 rec=0.9136 spec=0.8281 f1=0.8758 | time=12.7s\n",
            "Epoch 066 | train_loss=0.1556 acc=0.9308 | val_loss=0.3126 acc=0.8934 | prec=0.8879 rec=0.9000 spec=0.8869 f1=0.8939 | time=13.0s\n",
            "Epoch 067 | train_loss=0.1743 acc=0.9172 | val_loss=0.3534 acc=0.8594 | prec=0.8347 rec=0.8955 spec=0.8235 f1=0.8640 | time=12.7s\n",
            "Epoch 068 | train_loss=0.1297 acc=0.9433 | val_loss=0.3253 acc=0.8707 | prec=0.8559 rec=0.8909 spec=0.8507 f1=0.8731 | time=12.8s\n",
            "Epoch 069 | train_loss=0.1387 acc=0.9382 | val_loss=0.3915 acc=0.8526 | prec=0.8063 rec=0.9273 spec=0.7783 f1=0.8626 | time=12.8s\n",
            "Epoch 070 | train_loss=0.1543 acc=0.9331 | val_loss=0.3618 acc=0.8730 | prec=0.8905 rec=0.8500 spec=0.8959 f1=0.8698 | time=12.8s\n",
            "Epoch 071 | train_loss=0.1747 acc=0.9195 | val_loss=0.3352 acc=0.8481 | prec=0.8174 rec=0.8955 spec=0.8009 f1=0.8547 | time=12.8s\n",
            "Epoch 072 | train_loss=0.1469 acc=0.9393 | val_loss=0.3767 acc=0.8503 | prec=0.8182 rec=0.9000 spec=0.8009 f1=0.8571 | time=12.7s\n",
            "Epoch 073 | train_loss=0.1253 acc=0.9421 | val_loss=0.4722 acc=0.8141 | prec=0.7556 rec=0.9273 spec=0.7014 f1=0.8327 | time=12.7s\n",
            "Epoch 074 | train_loss=0.1367 acc=0.9410 | val_loss=0.3857 acc=0.8639 | prec=0.8333 rec=0.9091 spec=0.8190 f1=0.8696 | time=12.8s\n",
            "Epoch 075 | train_loss=0.1469 acc=0.9319 | val_loss=0.3727 acc=0.8594 | prec=0.8238 rec=0.9136 spec=0.8054 f1=0.8664 | time=12.7s\n",
            "Epoch 076 | train_loss=0.1401 acc=0.9297 | val_loss=0.3559 acc=0.8707 | prec=0.9137 rec=0.8182 spec=0.9231 f1=0.8633 | time=12.9s\n",
            "Epoch 077 | train_loss=0.1483 acc=0.9342 | val_loss=0.3481 acc=0.8662 | prec=0.8546 rec=0.8818 spec=0.8507 f1=0.8680 | time=12.9s\n",
            "Epoch 078 | train_loss=0.1422 acc=0.9274 | val_loss=0.3353 acc=0.8776 | prec=0.8739 rec=0.8818 spec=0.8733 f1=0.8778 | time=12.9s\n",
            "Epoch 079 | train_loss=0.1320 acc=0.9325 | val_loss=0.3818 acc=0.8594 | prec=0.8798 rec=0.8318 spec=0.8869 f1=0.8551 | time=12.7s\n",
            "Epoch 080 | train_loss=0.1232 acc=0.9387 | val_loss=0.3633 acc=0.8617 | prec=0.8442 rec=0.8864 spec=0.8371 f1=0.8647 | time=12.7s\n",
            "Epoch 081 | train_loss=0.1122 acc=0.9529 | val_loss=0.4367 acc=0.8549 | prec=0.8120 rec=0.9227 spec=0.7873 f1=0.8638 | time=12.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>f1_score</td><td>▆▆▆▁▁▂▃▇████████████████████████████████</td></tr><tr><td>precision</td><td>▅▅▅▅▁▁▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇▇▇███▇</td></tr><tr><td>recall</td><td>▇█▁▁██▅▆█▇▇▆█▇▆▇▇▇█▇█▇▇▇▇██▇▇▇█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>▁██▂█▇▇▅▆▆▇▇▇▇▆▆▆▇▇▆▆▇▇▆▅▆▇▆▆▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████▇▇████████</td></tr><tr><td>train_loss</td><td>██████▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▅▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇██▇█▇▇▇▇██▇</td></tr><tr><td>val_loss</td><td>███████▇▄▄▄▄▃▃▃▂▂▂▂▂▁▂▂▂▂▁▆▂▁▁▃▁▂▂▁▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>81</td></tr><tr><td>f1_score</td><td>0.86383</td></tr><tr><td>precision</td><td>0.812</td></tr><tr><td>recall</td><td>0.92273</td></tr><tr><td>specificity</td><td>0.78733</td></tr><tr><td>train_acc</td><td>0.95292</td></tr><tr><td>train_loss</td><td>0.1122</td></tr><tr><td>val_acc</td><td>0.85488</td></tr><tr><td>val_loss</td><td>0.43672</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/wo9antuq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/wo9antuq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_214946-wo9antuq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 22:07:03,994] Trial 1 finished with values: [0.3126360561166491, 0.8934240362811792] and parameters: {'lr': 7.26823423077802e-05, 'wd': 9.325987714179456e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 2 hyperparams --> lr=2.74e-05, wd=1.22e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_220703-cfbdia0r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/cfbdia0r' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/cfbdia0r' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/cfbdia0r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 2]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7487 acc=0.5077 | val_loss=0.7014 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 002 | train_loss=0.7362 acc=0.4986 | val_loss=0.6943 acc=0.4989 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7323 acc=0.5218 | val_loss=0.6927 acc=0.5147 | prec=0.5070 rec=0.9864 spec=0.0452 f1=0.6698 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7468 acc=0.4861 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7419 acc=0.4980 | val_loss=0.6941 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7356 acc=0.5020 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7288 acc=0.5026 | val_loss=0.6930 acc=0.5079 | prec=0.5035 rec=0.9773 spec=0.0407 f1=0.6646 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7318 acc=0.4861 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 009 | train_loss=0.7250 acc=0.5009 | val_loss=0.6939 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7355 acc=0.4974 | val_loss=0.6965 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.6s\n",
            "Epoch 011 | train_loss=0.7403 acc=0.4929 | val_loss=0.6935 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7221 acc=0.5009 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 013 | train_loss=0.7192 acc=0.5360 | val_loss=0.6931 acc=0.5057 | prec=0.5082 rec=0.2818 spec=0.7285 f1=0.3626 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7145 acc=0.5213 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 015 | train_loss=0.7215 acc=0.4923 | val_loss=0.6912 acc=0.5624 | prec=0.5480 rec=0.7000 spec=0.4253 f1=0.6148 | time=12.7s\n",
            "Epoch 016 | train_loss=0.7313 acc=0.4946 | val_loss=0.6909 acc=0.5079 | prec=0.8000 rec=0.0182 spec=0.9955 f1=0.0356 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7196 acc=0.4952 | val_loss=0.6882 acc=0.5601 | prec=0.7097 rec=0.2000 spec=0.9186 f1=0.3121 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7210 acc=0.5060 | val_loss=0.6831 acc=0.5556 | prec=0.8158 rec=0.1409 spec=0.9683 f1=0.2403 | time=13.0s\n",
            "Epoch 019 | train_loss=0.7054 acc=0.5088 | val_loss=0.6672 acc=0.6259 | prec=0.7835 rec=0.3455 spec=0.9050 f1=0.4795 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6781 acc=0.5632 | val_loss=0.6095 acc=0.7098 | prec=0.7644 rec=0.6045 spec=0.8145 f1=0.6751 | time=12.8s\n",
            "Epoch 021 | train_loss=0.6554 acc=0.6029 | val_loss=0.6117 acc=0.6417 | prec=0.8370 rec=0.3500 spec=0.9321 f1=0.4936 | time=12.8s\n",
            "Epoch 022 | train_loss=0.5979 acc=0.6670 | val_loss=0.5385 acc=0.7551 | prec=0.7090 rec=0.8636 spec=0.6471 f1=0.7787 | time=12.7s\n",
            "Epoch 023 | train_loss=0.5869 acc=0.6750 | val_loss=0.5141 acc=0.7778 | prec=0.8081 rec=0.7273 spec=0.8281 f1=0.7656 | time=12.7s\n",
            "Epoch 024 | train_loss=0.5535 acc=0.7085 | val_loss=0.5122 acc=0.7823 | prec=0.7672 rec=0.8091 spec=0.7557 f1=0.7876 | time=13.0s\n",
            "Epoch 025 | train_loss=0.5374 acc=0.7340 | val_loss=0.5059 acc=0.7891 | prec=0.8508 rec=0.7000 spec=0.8778 f1=0.7681 | time=12.8s\n",
            "Epoch 026 | train_loss=0.5041 acc=0.7646 | val_loss=0.5058 acc=0.7710 | prec=0.8696 rec=0.6364 spec=0.9050 f1=0.7349 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5054 acc=0.7550 | val_loss=0.4771 acc=0.7959 | prec=0.8095 rec=0.7727 spec=0.8190 f1=0.7907 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5012 acc=0.7538 | val_loss=0.4793 acc=0.7982 | prec=0.8075 rec=0.7818 spec=0.8145 f1=0.7945 | time=12.7s\n",
            "Epoch 029 | train_loss=0.4767 acc=0.7850 | val_loss=0.4676 acc=0.7982 | prec=0.7964 rec=0.8000 spec=0.7964 f1=0.7982 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4564 acc=0.7952 | val_loss=0.4716 acc=0.7868 | prec=0.8580 rec=0.6864 spec=0.8869 f1=0.7626 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4674 acc=0.7941 | val_loss=0.4627 acc=0.8005 | prec=0.8474 rec=0.7318 spec=0.8688 f1=0.7854 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4406 acc=0.7935 | val_loss=0.4589 acc=0.8118 | prec=0.8477 rec=0.7591 spec=0.8643 f1=0.8010 | time=12.7s\n",
            "Epoch 033 | train_loss=0.4305 acc=0.8128 | val_loss=0.4423 acc=0.8209 | prec=0.8279 rec=0.8091 spec=0.8326 f1=0.8184 | time=12.9s\n",
            "Epoch 034 | train_loss=0.4256 acc=0.7975 | val_loss=0.4738 acc=0.7937 | prec=0.7329 rec=0.9227 spec=0.6652 f1=0.8169 | time=12.9s\n",
            "Epoch 035 | train_loss=0.4275 acc=0.8270 | val_loss=0.4568 acc=0.8118 | prec=0.7866 rec=0.8545 spec=0.7692 f1=0.8192 | time=12.8s\n",
            "Epoch 036 | train_loss=0.4086 acc=0.8264 | val_loss=0.4495 acc=0.8095 | prec=0.7810 rec=0.8591 spec=0.7602 f1=0.8182 | time=12.8s\n",
            "Epoch 037 | train_loss=0.3995 acc=0.8355 | val_loss=0.4411 acc=0.8095 | prec=0.8036 rec=0.8182 spec=0.8009 f1=0.8108 | time=12.7s\n",
            "Epoch 038 | train_loss=0.3950 acc=0.8315 | val_loss=0.4387 acc=0.8095 | prec=0.8178 rec=0.7955 spec=0.8235 f1=0.8065 | time=12.8s\n",
            "Epoch 039 | train_loss=0.3872 acc=0.8344 | val_loss=0.4416 acc=0.8050 | prec=0.8418 rec=0.7500 spec=0.8597 f1=0.7933 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3659 acc=0.8514 | val_loss=0.4556 acc=0.8141 | prec=0.7899 rec=0.8545 spec=0.7738 f1=0.8210 | time=12.8s\n",
            "Epoch 041 | train_loss=0.3698 acc=0.8486 | val_loss=0.4406 acc=0.8005 | prec=0.8474 rec=0.7318 spec=0.8688 f1=0.7854 | time=12.7s\n",
            "Epoch 042 | train_loss=0.3762 acc=0.8440 | val_loss=0.4379 acc=0.8005 | prec=0.8587 rec=0.7182 spec=0.8824 f1=0.7822 | time=12.7s\n",
            "Epoch 043 | train_loss=0.3464 acc=0.8491 | val_loss=0.4297 acc=0.8118 | prec=0.8408 rec=0.7682 spec=0.8552 f1=0.8029 | time=12.7s\n",
            "Epoch 044 | train_loss=0.3553 acc=0.8582 | val_loss=0.4116 acc=0.8186 | prec=0.7800 rec=0.8864 spec=0.7511 f1=0.8298 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3571 acc=0.8423 | val_loss=0.4318 acc=0.8027 | prec=0.8715 rec=0.7091 spec=0.8959 f1=0.7820 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3570 acc=0.8650 | val_loss=0.4036 acc=0.8322 | prec=0.8349 rec=0.8273 spec=0.8371 f1=0.8311 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3481 acc=0.8457 | val_loss=0.4397 acc=0.8050 | prec=0.7445 rec=0.9273 spec=0.6833 f1=0.8259 | time=12.6s\n",
            "Epoch 048 | train_loss=0.3439 acc=0.8599 | val_loss=0.4179 acc=0.8095 | prec=0.8696 rec=0.7273 spec=0.8914 f1=0.7921 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3447 acc=0.8656 | val_loss=0.4216 acc=0.8231 | prec=0.8447 rec=0.7909 spec=0.8552 f1=0.8169 | time=12.7s\n",
            "Epoch 050 | train_loss=0.3318 acc=0.8684 | val_loss=0.4231 acc=0.8141 | prec=0.7674 rec=0.9000 spec=0.7285 f1=0.8285 | time=12.8s\n",
            "Epoch 051 | train_loss=0.3130 acc=0.8763 | val_loss=0.3990 acc=0.8277 | prec=0.8462 rec=0.8000 spec=0.8552 f1=0.8224 | time=12.8s\n",
            "Epoch 052 | train_loss=0.3082 acc=0.8763 | val_loss=0.4082 acc=0.8277 | prec=0.7927 rec=0.8864 spec=0.7692 f1=0.8369 | time=12.8s\n",
            "Epoch 053 | train_loss=0.3146 acc=0.8786 | val_loss=0.4023 acc=0.8118 | prec=0.8341 rec=0.7773 spec=0.8462 f1=0.8047 | time=12.7s\n",
            "Epoch 054 | train_loss=0.3100 acc=0.8701 | val_loss=0.4003 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=12.8s\n",
            "Epoch 055 | train_loss=0.2960 acc=0.8780 | val_loss=0.3949 acc=0.8345 | prec=0.8182 rec=0.8591 spec=0.8100 f1=0.8381 | time=12.7s\n",
            "Epoch 056 | train_loss=0.2728 acc=0.9013 | val_loss=0.4146 acc=0.8073 | prec=0.7668 rec=0.8818 spec=0.7330 f1=0.8203 | time=12.8s\n",
            "Epoch 057 | train_loss=0.2925 acc=0.8843 | val_loss=0.4007 acc=0.8299 | prec=0.8537 rec=0.7955 spec=0.8643 f1=0.8235 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2861 acc=0.8843 | val_loss=0.3951 acc=0.8118 | prec=0.8128 rec=0.8091 spec=0.8145 f1=0.8109 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2766 acc=0.8979 | val_loss=0.3825 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=12.7s\n",
            "Epoch 060 | train_loss=0.2845 acc=0.8900 | val_loss=0.4035 acc=0.8209 | prec=0.8543 rec=0.7727 spec=0.8688 f1=0.8115 | time=12.8s\n",
            "Epoch 061 | train_loss=0.2660 acc=0.8945 | val_loss=0.3850 acc=0.8277 | prec=0.8025 rec=0.8682 spec=0.7873 f1=0.8341 | time=12.8s\n",
            "Epoch 062 | train_loss=0.2721 acc=0.8968 | val_loss=0.4095 acc=0.8231 | prec=0.8660 rec=0.7636 spec=0.8824 f1=0.8116 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2944 acc=0.8973 | val_loss=0.3924 acc=0.8231 | prec=0.8480 rec=0.7864 spec=0.8597 f1=0.8160 | time=12.8s\n",
            "Epoch 064 | train_loss=0.2619 acc=0.8996 | val_loss=0.3817 acc=0.8299 | prec=0.8404 rec=0.8136 spec=0.8462 f1=0.8268 | time=12.7s\n",
            "Epoch 065 | train_loss=0.2344 acc=0.9087 | val_loss=0.3938 acc=0.8322 | prec=0.8411 rec=0.8182 spec=0.8462 f1=0.8295 | time=12.8s\n",
            "Epoch 066 | train_loss=0.2503 acc=0.9081 | val_loss=0.3912 acc=0.8186 | prec=0.7966 rec=0.8545 spec=0.7828 f1=0.8246 | time=12.8s\n",
            "Epoch 067 | train_loss=0.2511 acc=0.9002 | val_loss=0.4005 acc=0.8231 | prec=0.8349 rec=0.8045 spec=0.8416 f1=0.8194 | time=12.7s\n",
            "Epoch 068 | train_loss=0.2387 acc=0.9036 | val_loss=0.3819 acc=0.8322 | prec=0.8318 rec=0.8318 spec=0.8326 f1=0.8318 | time=12.9s\n",
            "Epoch 069 | train_loss=0.2415 acc=0.9081 | val_loss=0.3909 acc=0.8209 | prec=0.8473 rec=0.7818 spec=0.8597 f1=0.8132 | time=12.7s\n",
            "Epoch 070 | train_loss=0.2518 acc=0.9019 | val_loss=0.3853 acc=0.8277 | prec=0.8303 rec=0.8227 spec=0.8326 f1=0.8265 | time=12.7s\n",
            "Epoch 071 | train_loss=0.2354 acc=0.9036 | val_loss=0.3943 acc=0.8254 | prec=0.8206 rec=0.8318 spec=0.8190 f1=0.8262 | time=12.7s\n",
            "Epoch 072 | train_loss=0.2296 acc=0.9092 | val_loss=0.3995 acc=0.8209 | prec=0.8473 rec=0.7818 spec=0.8597 f1=0.8132 | time=12.8s\n",
            "Epoch 073 | train_loss=0.2187 acc=0.9149 | val_loss=0.3895 acc=0.8322 | prec=0.8380 rec=0.8227 spec=0.8416 f1=0.8303 | time=12.8s\n",
            "Epoch 074 | train_loss=0.2184 acc=0.9132 | val_loss=0.4099 acc=0.8231 | prec=0.8550 rec=0.7773 spec=0.8688 f1=0.8143 | time=12.7s\n",
            "Epoch 075 | train_loss=0.2270 acc=0.9098 | val_loss=0.3908 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.7s\n",
            "Epoch 076 | train_loss=0.2288 acc=0.9149 | val_loss=0.4237 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.7s\n",
            "Epoch 077 | train_loss=0.2206 acc=0.9138 | val_loss=0.3960 acc=0.8299 | prec=0.8085 rec=0.8636 spec=0.7964 f1=0.8352 | time=12.7s\n",
            "Epoch 078 | train_loss=0.2218 acc=0.9047 | val_loss=0.4197 acc=0.8322 | prec=0.8544 rec=0.8000 spec=0.8643 f1=0.8263 | time=12.8s\n",
            "Epoch 079 | train_loss=0.2306 acc=0.9195 | val_loss=0.3999 acc=0.8254 | prec=0.8017 rec=0.8636 spec=0.7873 f1=0.8315 | time=12.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▇▇▇▇▇▇▄▁▁▄▇▅▇▇████████████████████████</td></tr><tr><td>precision</td><td>▅▅▅▅▅▁▅▇▇██▇████▇███▇███▇▇█████▇███████▇</td></tr><tr><td>recall</td><td>▁████▁▆▁▃▅▇▅▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▆▇▆▇▆▇▇</td></tr><tr><td>specificity</td><td>█▁▁▁▁▁▁██▇█▆▇▇▇▇▆▆▆▇▆▇▇▇▇▆▇▇▆▇▇▆▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▂▁▁▁▁▂▂▁▂▃▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█████████▇▆▅▄▄▄▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▂▁▄▄▇▇▇▇▇██▇▇▇█▇█▇████▇███████████</td></tr><tr><td>val_loss</td><td>███████████▆▄▄▄▄▃▃▃▂▃▂▂▁▂▂▂▁▂▁▁▂▁▁▁▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>f1_score</td><td>0.83151</td></tr><tr><td>precision</td><td>0.80169</td></tr><tr><td>recall</td><td>0.86364</td></tr><tr><td>specificity</td><td>0.78733</td></tr><tr><td>train_acc</td><td>0.91946</td></tr><tr><td>train_loss</td><td>0.23058</td></tr><tr><td>val_acc</td><td>0.8254</td></tr><tr><td>val_loss</td><td>0.39994</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/cfbdia0r' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/cfbdia0r</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_220703-cfbdia0r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 22:23:56,212] Trial 2 finished with values: [0.38170226344040464, 0.8299319727891157] and parameters: {'lr': 2.7364087927664703e-05, 'wd': 1.2157354755723703e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 3 hyperparams --> lr=1.91e-05, wd=2.49e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_222356-p4iexkbz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/p4iexkbz' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/p4iexkbz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/p4iexkbz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 3]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7621 acc=0.5065 | val_loss=0.7063 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7657 acc=0.4935 | val_loss=0.7011 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7493 acc=0.4935 | val_loss=0.6960 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7545 acc=0.5077 | val_loss=0.6962 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7457 acc=0.4923 | val_loss=0.7011 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7391 acc=0.4935 | val_loss=0.7029 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 007 | train_loss=0.7328 acc=0.5014 | val_loss=0.7106 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7177 acc=0.5184 | val_loss=0.7038 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 009 | train_loss=0.7167 acc=0.5122 | val_loss=0.6954 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 010 | train_loss=0.7097 acc=0.5099 | val_loss=0.6949 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 011 | train_loss=0.7123 acc=0.5099 | val_loss=0.6956 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 012 | train_loss=0.7098 acc=0.5009 | val_loss=0.6948 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7187 acc=0.5037 | val_loss=0.6950 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7081 acc=0.4974 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 015 | train_loss=0.7170 acc=0.4923 | val_loss=0.6945 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 016 | train_loss=0.6992 acc=0.5173 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7144 acc=0.4935 | val_loss=0.6934 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 018 | train_loss=0.7085 acc=0.4889 | val_loss=0.6929 acc=0.5170 | prec=0.5096 rec=0.8455 spec=0.1900 f1=0.6359 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7146 acc=0.4804 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 020 | train_loss=0.7079 acc=0.5099 | val_loss=0.6932 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 021 | train_loss=0.7059 acc=0.5201 | val_loss=0.6929 acc=0.4989 | prec=0.4989 rec=0.9955 spec=0.0045 f1=0.6646 | time=12.7s\n",
            "Epoch 022 | train_loss=0.7076 acc=0.5184 | val_loss=0.6926 acc=0.5329 | prec=0.5210 rec=0.7909 spec=0.2760 f1=0.6282 | time=12.7s\n",
            "Epoch 023 | train_loss=0.7032 acc=0.5116 | val_loss=0.6927 acc=0.5011 | prec=0.5000 rec=1.0000 spec=0.0045 f1=0.6667 | time=12.7s\n",
            "Epoch 024 | train_loss=0.7013 acc=0.5230 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7052 acc=0.5071 | val_loss=0.6926 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 026 | train_loss=0.7186 acc=0.4810 | val_loss=0.6931 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 027 | train_loss=0.7129 acc=0.5026 | val_loss=0.6927 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 028 | train_loss=0.7079 acc=0.5031 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.6s\n",
            "Epoch 029 | train_loss=0.7139 acc=0.4918 | val_loss=0.6919 acc=0.5034 | prec=0.5011 rec=0.9955 spec=0.0136 f1=0.6667 | time=12.7s\n",
            "Epoch 030 | train_loss=0.7075 acc=0.5082 | val_loss=0.6923 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 031 | train_loss=0.7096 acc=0.4974 | val_loss=0.6903 acc=0.5692 | prec=0.5904 rec=0.4455 spec=0.6923 f1=0.5078 | time=12.7s\n",
            "Epoch 032 | train_loss=0.7060 acc=0.4969 | val_loss=0.6852 acc=0.5125 | prec=0.5058 rec=0.9909 spec=0.0362 f1=0.6697 | time=12.6s\n",
            "Epoch 033 | train_loss=0.7000 acc=0.5252 | val_loss=0.6775 acc=0.6531 | prec=0.6136 rec=0.8227 spec=0.4842 f1=0.7029 | time=12.8s\n",
            "Epoch 034 | train_loss=0.6791 acc=0.5610 | val_loss=0.6533 acc=0.6871 | prec=0.8060 rec=0.4909 spec=0.8824 f1=0.6102 | time=12.7s\n",
            "Epoch 035 | train_loss=0.6631 acc=0.5876 | val_loss=0.6321 acc=0.7143 | prec=0.7670 rec=0.6136 spec=0.8145 f1=0.6818 | time=12.8s\n",
            "Epoch 036 | train_loss=0.6363 acc=0.6506 | val_loss=0.5752 acc=0.7438 | prec=0.8023 rec=0.6455 spec=0.8416 f1=0.7154 | time=12.8s\n",
            "Epoch 037 | train_loss=0.5842 acc=0.6875 | val_loss=0.5395 acc=0.7551 | prec=0.8333 rec=0.6364 spec=0.8733 f1=0.7216 | time=12.7s\n",
            "Epoch 038 | train_loss=0.5612 acc=0.7266 | val_loss=0.5206 acc=0.7642 | prec=0.8494 rec=0.6409 spec=0.8869 f1=0.7306 | time=12.7s\n",
            "Epoch 039 | train_loss=0.5496 acc=0.7090 | val_loss=0.5023 acc=0.7891 | prec=0.8360 rec=0.7182 spec=0.8597 f1=0.7726 | time=12.8s\n",
            "Epoch 040 | train_loss=0.5229 acc=0.7453 | val_loss=0.4953 acc=0.7846 | prec=0.8342 rec=0.7091 spec=0.8597 f1=0.7666 | time=12.7s\n",
            "Epoch 041 | train_loss=0.5219 acc=0.7408 | val_loss=0.4986 acc=0.7914 | prec=0.8200 rec=0.7455 spec=0.8371 f1=0.7810 | time=12.7s\n",
            "Epoch 042 | train_loss=0.5048 acc=0.7527 | val_loss=0.5065 acc=0.7868 | prec=0.8351 rec=0.7136 spec=0.8597 f1=0.7696 | time=12.7s\n",
            "Epoch 043 | train_loss=0.4988 acc=0.7754 | val_loss=0.5053 acc=0.8118 | prec=0.7708 rec=0.8864 spec=0.7376 f1=0.8245 | time=12.7s\n",
            "Epoch 044 | train_loss=0.4960 acc=0.7725 | val_loss=0.5038 acc=0.8095 | prec=0.8400 rec=0.7636 spec=0.8552 f1=0.8000 | time=12.7s\n",
            "Epoch 045 | train_loss=0.5005 acc=0.7760 | val_loss=0.4863 acc=0.7823 | prec=0.8605 rec=0.6727 spec=0.8914 f1=0.7551 | time=12.8s\n",
            "Epoch 046 | train_loss=0.5192 acc=0.7408 | val_loss=0.4734 acc=0.8141 | prec=0.8382 rec=0.7773 spec=0.8507 f1=0.8066 | time=12.6s\n",
            "Epoch 047 | train_loss=0.5040 acc=0.7731 | val_loss=0.4729 acc=0.8163 | prec=0.8357 rec=0.7864 spec=0.8462 f1=0.8103 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4831 acc=0.7822 | val_loss=0.4794 acc=0.8231 | prec=0.7934 rec=0.8727 spec=0.7738 f1=0.8312 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4982 acc=0.7760 | val_loss=0.4683 acc=0.8118 | prec=0.8186 rec=0.8000 spec=0.8235 f1=0.8092 | time=12.7s\n",
            "Epoch 050 | train_loss=0.4639 acc=0.8071 | val_loss=0.4698 acc=0.8118 | prec=0.8018 rec=0.8273 spec=0.7964 f1=0.8143 | time=12.8s\n",
            "Epoch 051 | train_loss=0.4732 acc=0.7981 | val_loss=0.4566 acc=0.8186 | prec=0.8398 rec=0.7864 spec=0.8507 f1=0.8122 | time=12.7s\n",
            "Epoch 052 | train_loss=0.4706 acc=0.7992 | val_loss=0.4535 acc=0.8209 | prec=0.8190 rec=0.8227 spec=0.8190 f1=0.8209 | time=12.7s\n",
            "Epoch 053 | train_loss=0.4507 acc=0.8111 | val_loss=0.4599 acc=0.8186 | prec=0.8153 rec=0.8227 spec=0.8145 f1=0.8190 | time=12.8s\n",
            "Epoch 054 | train_loss=0.4714 acc=0.7969 | val_loss=0.4540 acc=0.8186 | prec=0.8017 rec=0.8455 spec=0.7919 f1=0.8230 | time=12.7s\n",
            "Epoch 055 | train_loss=0.4448 acc=0.8123 | val_loss=0.4556 acc=0.8209 | prec=0.8310 rec=0.8045 spec=0.8371 f1=0.8176 | time=12.8s\n",
            "Epoch 056 | train_loss=0.4457 acc=0.8117 | val_loss=0.4430 acc=0.8209 | prec=0.8406 rec=0.7909 spec=0.8507 f1=0.8150 | time=12.8s\n",
            "Epoch 057 | train_loss=0.4382 acc=0.8145 | val_loss=0.4551 acc=0.8186 | prec=0.7869 rec=0.8727 spec=0.7647 f1=0.8276 | time=12.7s\n",
            "Epoch 058 | train_loss=0.4425 acc=0.8140 | val_loss=0.4502 acc=0.8209 | prec=0.8106 rec=0.8364 spec=0.8054 f1=0.8233 | time=12.8s\n",
            "Epoch 059 | train_loss=0.4281 acc=0.8202 | val_loss=0.4446 acc=0.8186 | prec=0.8241 rec=0.8091 spec=0.8281 f1=0.8165 | time=12.8s\n",
            "Epoch 060 | train_loss=0.4281 acc=0.8270 | val_loss=0.4516 acc=0.8231 | prec=0.8257 rec=0.8182 spec=0.8281 f1=0.8219 | time=12.7s\n",
            "Epoch 061 | train_loss=0.4458 acc=0.8060 | val_loss=0.4221 acc=0.8186 | prec=0.8271 rec=0.8045 spec=0.8326 f1=0.8157 | time=12.8s\n",
            "Epoch 062 | train_loss=0.4354 acc=0.8202 | val_loss=0.4333 acc=0.8254 | prec=0.8178 rec=0.8364 spec=0.8145 f1=0.8270 | time=12.8s\n",
            "Epoch 063 | train_loss=0.4115 acc=0.8264 | val_loss=0.4340 acc=0.8209 | prec=0.8406 rec=0.7909 spec=0.8507 f1=0.8150 | time=12.9s\n",
            "Epoch 064 | train_loss=0.4111 acc=0.8389 | val_loss=0.4390 acc=0.8118 | prec=0.8586 rec=0.7455 spec=0.8778 f1=0.7981 | time=12.7s\n",
            "Epoch 065 | train_loss=0.3961 acc=0.8355 | val_loss=0.4273 acc=0.8231 | prec=0.8515 rec=0.7818 spec=0.8643 f1=0.8152 | time=12.7s\n",
            "Epoch 066 | train_loss=0.3988 acc=0.8406 | val_loss=0.4211 acc=0.8277 | prec=0.8158 rec=0.8455 spec=0.8100 f1=0.8304 | time=12.9s\n",
            "Epoch 067 | train_loss=0.4067 acc=0.8344 | val_loss=0.4219 acc=0.8254 | prec=0.8235 rec=0.8273 spec=0.8235 f1=0.8254 | time=12.8s\n",
            "Epoch 068 | train_loss=0.3897 acc=0.8480 | val_loss=0.4347 acc=0.8050 | prec=0.8722 rec=0.7136 spec=0.8959 f1=0.7850 | time=12.8s\n",
            "Epoch 069 | train_loss=0.3981 acc=0.8389 | val_loss=0.4190 acc=0.8277 | prec=0.8636 rec=0.7773 spec=0.8778 f1=0.8182 | time=12.7s\n",
            "Epoch 070 | train_loss=0.3873 acc=0.8520 | val_loss=0.4127 acc=0.8254 | prec=0.8235 rec=0.8273 spec=0.8235 f1=0.8254 | time=12.7s\n",
            "Epoch 071 | train_loss=0.3791 acc=0.8395 | val_loss=0.4098 acc=0.8390 | prec=0.8565 rec=0.8136 spec=0.8643 f1=0.8345 | time=12.7s\n",
            "Epoch 072 | train_loss=0.3678 acc=0.8542 | val_loss=0.4279 acc=0.8141 | prec=0.8791 rec=0.7273 spec=0.9005 f1=0.7960 | time=12.7s\n",
            "Epoch 073 | train_loss=0.3868 acc=0.8508 | val_loss=0.4195 acc=0.8186 | prec=0.8684 rec=0.7500 spec=0.8869 f1=0.8049 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3716 acc=0.8469 | val_loss=0.3991 acc=0.8299 | prec=0.8571 rec=0.7909 spec=0.8688 f1=0.8227 | time=12.8s\n",
            "Epoch 075 | train_loss=0.3607 acc=0.8497 | val_loss=0.4093 acc=0.8209 | prec=0.8653 rec=0.7591 spec=0.8824 f1=0.8087 | time=12.9s\n",
            "Epoch 076 | train_loss=0.3591 acc=0.8588 | val_loss=0.4451 acc=0.7891 | prec=0.8896 rec=0.6591 spec=0.9186 f1=0.7572 | time=12.7s\n",
            "Epoch 077 | train_loss=0.3547 acc=0.8531 | val_loss=0.4215 acc=0.8095 | prec=0.8778 rec=0.7182 spec=0.9005 f1=0.7900 | time=12.7s\n",
            "Epoch 078 | train_loss=0.3598 acc=0.8548 | val_loss=0.3945 acc=0.8390 | prec=0.8371 rec=0.8409 spec=0.8371 f1=0.8390 | time=12.8s\n",
            "Epoch 079 | train_loss=0.3475 acc=0.8639 | val_loss=0.4075 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.7s\n",
            "Epoch 080 | train_loss=0.3483 acc=0.8469 | val_loss=0.4111 acc=0.8254 | prec=0.8865 rec=0.7455 spec=0.9050 f1=0.8099 | time=12.7s\n",
            "Epoch 081 | train_loss=0.3484 acc=0.8537 | val_loss=0.3895 acc=0.8413 | prec=0.8472 rec=0.8318 spec=0.8507 f1=0.8394 | time=12.7s\n",
            "Epoch 082 | train_loss=0.3417 acc=0.8559 | val_loss=0.3912 acc=0.8367 | prec=0.8274 rec=0.8500 spec=0.8235 f1=0.8386 | time=12.9s\n",
            "Epoch 083 | train_loss=0.3393 acc=0.8599 | val_loss=0.3890 acc=0.8458 | prec=0.8585 rec=0.8273 spec=0.8643 f1=0.8426 | time=12.8s\n",
            "Epoch 084 | train_loss=0.3408 acc=0.8661 | val_loss=0.3883 acc=0.8345 | prec=0.8326 rec=0.8364 spec=0.8326 f1=0.8345 | time=12.7s\n",
            "Epoch 085 | train_loss=0.3368 acc=0.8678 | val_loss=0.3968 acc=0.8322 | prec=0.8687 rec=0.7818 spec=0.8824 f1=0.8230 | time=12.8s\n",
            "Epoch 086 | train_loss=0.3443 acc=0.8537 | val_loss=0.4066 acc=0.8141 | prec=0.8920 rec=0.7136 spec=0.9140 f1=0.7929 | time=12.9s\n",
            "Epoch 087 | train_loss=0.3343 acc=0.8627 | val_loss=0.3996 acc=0.8254 | prec=0.8950 rec=0.7364 spec=0.9140 f1=0.8080 | time=12.9s\n",
            "Epoch 088 | train_loss=0.3478 acc=0.8565 | val_loss=0.4314 acc=0.7937 | prec=0.8909 rec=0.6682 spec=0.9186 f1=0.7636 | time=12.9s\n",
            "Epoch 089 | train_loss=0.3289 acc=0.8616 | val_loss=0.3899 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.7s\n",
            "Epoch 090 | train_loss=0.3198 acc=0.8798 | val_loss=0.3911 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=12.8s\n",
            "Epoch 091 | train_loss=0.3247 acc=0.8741 | val_loss=0.4010 acc=0.8299 | prec=0.8877 rec=0.7545 spec=0.9050 f1=0.8157 | time=12.8s\n",
            "Epoch 092 | train_loss=0.3263 acc=0.8695 | val_loss=0.4177 acc=0.8118 | prec=0.8960 rec=0.7045 spec=0.9186 f1=0.7888 | time=12.8s\n",
            "Epoch 093 | train_loss=0.3212 acc=0.8712 | val_loss=0.4031 acc=0.8186 | prec=0.8889 rec=0.7273 spec=0.9095 f1=0.8000 | time=12.8s\n",
            "Epoch 094 | train_loss=0.3197 acc=0.8701 | val_loss=0.4044 acc=0.8209 | prec=0.8939 rec=0.7273 spec=0.9140 f1=0.8020 | time=12.7s\n",
            "Epoch 095 | train_loss=0.3170 acc=0.8718 | val_loss=0.3841 acc=0.8322 | prec=0.8650 rec=0.7864 spec=0.8778 f1=0.8238 | time=12.8s\n",
            "Epoch 096 | train_loss=0.3103 acc=0.8775 | val_loss=0.3856 acc=0.8345 | prec=0.8848 rec=0.7682 spec=0.9005 f1=0.8224 | time=12.9s\n",
            "Epoch 097 | train_loss=0.3156 acc=0.8707 | val_loss=0.3956 acc=0.8299 | prec=0.8919 rec=0.7500 spec=0.9095 f1=0.8148 | time=12.8s\n",
            "Epoch 098 | train_loss=0.2999 acc=0.8860 | val_loss=0.3928 acc=0.8322 | prec=0.8883 rec=0.7591 spec=0.9050 f1=0.8186 | time=12.7s\n",
            "Epoch 099 | train_loss=0.3140 acc=0.8792 | val_loss=0.3807 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.7s\n",
            "Epoch 100 | train_loss=0.3157 acc=0.8735 | val_loss=0.3801 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=12.8s\n",
            "Epoch 101 | train_loss=0.2967 acc=0.8735 | val_loss=0.4020 acc=0.8277 | prec=0.8871 rec=0.7500 spec=0.9050 f1=0.8128 | time=12.8s\n",
            "Epoch 102 | train_loss=0.3074 acc=0.8729 | val_loss=0.3798 acc=0.8299 | prec=0.8469 rec=0.8045 spec=0.8552 f1=0.8252 | time=12.7s\n",
            "Epoch 103 | train_loss=0.2903 acc=0.8866 | val_loss=0.3880 acc=0.8322 | prec=0.8842 rec=0.7636 spec=0.9005 f1=0.8195 | time=12.8s\n",
            "Epoch 104 | train_loss=0.3016 acc=0.8843 | val_loss=0.3813 acc=0.8345 | prec=0.8621 rec=0.7955 spec=0.8733 f1=0.8274 | time=12.9s\n",
            "Epoch 105 | train_loss=0.3059 acc=0.8701 | val_loss=0.3815 acc=0.8299 | prec=0.8404 rec=0.8136 spec=0.8462 f1=0.8268 | time=12.7s\n",
            "Epoch 106 | train_loss=0.2976 acc=0.8832 | val_loss=0.3862 acc=0.8322 | prec=0.8802 rec=0.7682 spec=0.8959 f1=0.8204 | time=12.8s\n",
            "Epoch 107 | train_loss=0.2942 acc=0.8780 | val_loss=0.3930 acc=0.8322 | prec=0.8763 rec=0.7727 spec=0.8914 f1=0.8213 | time=12.8s\n",
            "Epoch 108 | train_loss=0.2970 acc=0.8888 | val_loss=0.3845 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=12.9s\n",
            "Epoch 109 | train_loss=0.2884 acc=0.8968 | val_loss=0.4058 acc=0.8209 | prec=0.8939 rec=0.7273 spec=0.9140 f1=0.8020 | time=12.8s\n",
            "Epoch 110 | train_loss=0.2958 acc=0.8911 | val_loss=0.3895 acc=0.8345 | prec=0.8731 rec=0.7818 spec=0.8869 f1=0.8249 | time=12.8s\n",
            "Epoch 111 | train_loss=0.2911 acc=0.8866 | val_loss=0.4059 acc=0.8163 | prec=0.8927 rec=0.7182 spec=0.9140 f1=0.7960 | time=12.7s\n",
            "Epoch 112 | train_loss=0.3051 acc=0.8786 | val_loss=0.3946 acc=0.8254 | prec=0.8865 rec=0.7455 spec=0.9050 f1=0.8099 | time=12.8s\n",
            "Epoch 113 | train_loss=0.3083 acc=0.8752 | val_loss=0.3867 acc=0.8413 | prec=0.8866 rec=0.7818 spec=0.9005 f1=0.8309 | time=12.7s\n",
            "Epoch 114 | train_loss=0.2947 acc=0.8815 | val_loss=0.3868 acc=0.8390 | prec=0.8744 rec=0.7909 spec=0.8869 f1=0.8305 | time=12.7s\n",
            "Epoch 115 | train_loss=0.2992 acc=0.8854 | val_loss=0.3933 acc=0.8345 | prec=0.8973 rec=0.7545 spec=0.9140 f1=0.8198 | time=12.7s\n",
            "Epoch 116 | train_loss=0.2941 acc=0.8905 | val_loss=0.3905 acc=0.8299 | prec=0.8877 rec=0.7545 spec=0.9050 f1=0.8157 | time=12.7s\n",
            "Epoch 117 | train_loss=0.2920 acc=0.8860 | val_loss=0.3839 acc=0.8299 | prec=0.8537 rec=0.7955 spec=0.8643 f1=0.8235 | time=12.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>f1_score</td><td>▁▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇█▇████████████████████</td></tr><tr><td>precision</td><td>▁▁▅▅▅▅▅▅▅▅▅▅▇██▇▇▇▇▇▇▇█▇███▇████████████</td></tr><tr><td>recall</td><td>▁▁███▇█████▄▇▅▅▆▆▇▇▇▇▆▇▆▆▇▆▆▇▆▆▆▆▇▆▆▆▆▆▇</td></tr><tr><td>specificity</td><td>██▁▁▁▁▁▁▁▁▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▁▂▁▂▂▁▁▁▂▃▅▆▅▆▆▇▇▇▇▇▇▇█▇▇██████████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▇▆▅▅▄▄▄▄▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▁▁▄▅▅▇▇▇██████▇███████▇████████</td></tr><tr><td>val_loss</td><td>█████████████▇▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>117</td></tr><tr><td>f1_score</td><td>0.82353</td></tr><tr><td>precision</td><td>0.85366</td></tr><tr><td>recall</td><td>0.79545</td></tr><tr><td>specificity</td><td>0.86425</td></tr><tr><td>train_acc</td><td>0.88599</td></tr><tr><td>train_loss</td><td>0.29197</td></tr><tr><td>val_acc</td><td>0.82993</td></tr><tr><td>val_loss</td><td>0.38394</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/p4iexkbz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/p4iexkbz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_222356-p4iexkbz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 22:48:52,604] Trial 3 finished with values: [0.37982695017542156, 0.8299319727891157] and parameters: {'lr': 1.9064059143578445e-05, 'wd': 2.4906139039397683e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 4 hyperparams --> lr=3.54e-05, wd=4.66e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_224852-c0d93i3b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/c0d93i3b' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/c0d93i3b' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/c0d93i3b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 4]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7611 acc=0.5145 | val_loss=0.7089 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7727 acc=0.4946 | val_loss=0.7055 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7389 acc=0.5094 | val_loss=0.7015 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7406 acc=0.5150 | val_loss=0.6985 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7495 acc=0.5014 | val_loss=0.6994 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7598 acc=0.4878 | val_loss=0.7083 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 007 | train_loss=0.7402 acc=0.4963 | val_loss=0.6982 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7488 acc=0.4963 | val_loss=0.6990 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7377 acc=0.4821 | val_loss=0.6977 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 010 | train_loss=0.7461 acc=0.5003 | val_loss=0.7011 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7478 acc=0.4946 | val_loss=0.6988 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7426 acc=0.4935 | val_loss=0.6964 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 013 | train_loss=0.7373 acc=0.4923 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 014 | train_loss=0.7313 acc=0.4957 | val_loss=0.7021 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7326 acc=0.5094 | val_loss=0.6989 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 016 | train_loss=0.7389 acc=0.4912 | val_loss=0.6946 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7366 acc=0.4923 | val_loss=0.6924 acc=0.5556 | prec=0.7400 rec=0.1682 spec=0.9412 f1=0.2741 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7243 acc=0.5071 | val_loss=0.6919 acc=0.5034 | prec=1.0000 rec=0.0045 spec=1.0000 f1=0.0090 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7366 acc=0.5094 | val_loss=0.6917 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7233 acc=0.5026 | val_loss=0.6902 acc=0.5102 | prec=1.0000 rec=0.0182 spec=1.0000 f1=0.0357 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7247 acc=0.4952 | val_loss=0.6878 acc=0.5329 | prec=0.5169 rec=0.9727 spec=0.0950 f1=0.6751 | time=12.7s\n",
            "Epoch 022 | train_loss=0.7267 acc=0.5014 | val_loss=0.6860 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7131 acc=0.5196 | val_loss=0.6876 acc=0.6145 | prec=0.7016 rec=0.3955 spec=0.8326 f1=0.5058 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7057 acc=0.5218 | val_loss=0.6782 acc=0.5510 | prec=0.8056 rec=0.1318 spec=0.9683 f1=0.2266 | time=12.9s\n",
            "Epoch 025 | train_loss=0.6742 acc=0.5780 | val_loss=0.6282 acc=0.6621 | prec=0.8257 rec=0.4091 spec=0.9140 f1=0.5471 | time=12.7s\n",
            "Epoch 026 | train_loss=0.6237 acc=0.6529 | val_loss=0.5528 acc=0.7619 | prec=0.7291 rec=0.8318 spec=0.6923 f1=0.7771 | time=12.9s\n",
            "Epoch 027 | train_loss=0.5723 acc=0.7113 | val_loss=0.5209 acc=0.7846 | prec=0.7934 rec=0.7682 spec=0.8009 f1=0.7806 | time=12.9s\n",
            "Epoch 028 | train_loss=0.5446 acc=0.7283 | val_loss=0.5044 acc=0.8050 | prec=0.7702 rec=0.8682 spec=0.7421 f1=0.8162 | time=12.7s\n",
            "Epoch 029 | train_loss=0.5144 acc=0.7629 | val_loss=0.4798 acc=0.8027 | prec=0.8308 rec=0.7591 spec=0.8462 f1=0.7933 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4875 acc=0.7777 | val_loss=0.4918 acc=0.7846 | prec=0.8571 rec=0.6818 spec=0.8869 f1=0.7595 | time=12.8s\n",
            "Epoch 031 | train_loss=0.4832 acc=0.7862 | val_loss=0.4576 acc=0.8073 | prec=0.8199 rec=0.7864 spec=0.8281 f1=0.8028 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4664 acc=0.7930 | val_loss=0.4649 acc=0.8118 | prec=0.7645 rec=0.9000 spec=0.7240 f1=0.8267 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4487 acc=0.8026 | val_loss=0.4446 acc=0.8141 | prec=0.8080 rec=0.8227 spec=0.8054 f1=0.8153 | time=12.6s\n",
            "Epoch 034 | train_loss=0.4451 acc=0.7981 | val_loss=0.4282 acc=0.8231 | prec=0.8318 rec=0.8091 spec=0.8371 f1=0.8203 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4304 acc=0.8117 | val_loss=0.4357 acc=0.8050 | prec=0.8418 rec=0.7500 spec=0.8597 f1=0.7933 | time=12.7s\n",
            "Epoch 036 | train_loss=0.4331 acc=0.8015 | val_loss=0.4215 acc=0.8209 | prec=0.8341 rec=0.8000 spec=0.8416 f1=0.8167 | time=12.8s\n",
            "Epoch 037 | train_loss=0.4177 acc=0.8270 | val_loss=0.4217 acc=0.8209 | prec=0.8615 rec=0.7636 spec=0.8778 f1=0.8096 | time=12.7s\n",
            "Epoch 038 | train_loss=0.3857 acc=0.8361 | val_loss=0.4329 acc=0.8027 | prec=0.7509 rec=0.9045 spec=0.7014 f1=0.8206 | time=12.7s\n",
            "Epoch 039 | train_loss=0.3794 acc=0.8327 | val_loss=0.4092 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3714 acc=0.8446 | val_loss=0.4044 acc=0.8163 | prec=0.8390 rec=0.7818 spec=0.8507 f1=0.8094 | time=12.8s\n",
            "Epoch 041 | train_loss=0.3742 acc=0.8503 | val_loss=0.4088 acc=0.8254 | prec=0.8557 rec=0.7818 spec=0.8688 f1=0.8171 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3562 acc=0.8548 | val_loss=0.4076 acc=0.8254 | prec=0.8357 rec=0.8091 spec=0.8416 f1=0.8222 | time=12.7s\n",
            "Epoch 043 | train_loss=0.3340 acc=0.8633 | val_loss=0.4015 acc=0.8209 | prec=0.8052 rec=0.8455 spec=0.7964 f1=0.8248 | time=12.7s\n",
            "Epoch 044 | train_loss=0.3354 acc=0.8627 | val_loss=0.4080 acc=0.8050 | prec=0.7724 rec=0.8636 spec=0.7466 f1=0.8155 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3322 acc=0.8639 | val_loss=0.3998 acc=0.8209 | prec=0.8161 rec=0.8273 spec=0.8145 f1=0.8217 | time=12.8s\n",
            "Epoch 046 | train_loss=0.3460 acc=0.8684 | val_loss=0.4239 acc=0.8209 | prec=0.8852 rec=0.7364 spec=0.9050 f1=0.8040 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3127 acc=0.8798 | val_loss=0.3992 acc=0.8231 | prec=0.8698 rec=0.7591 spec=0.8869 f1=0.8107 | time=12.6s\n",
            "Epoch 048 | train_loss=0.3208 acc=0.8724 | val_loss=0.3877 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=12.9s\n",
            "Epoch 049 | train_loss=0.3145 acc=0.8843 | val_loss=0.3823 acc=0.8390 | prec=0.8433 rec=0.8318 spec=0.8462 f1=0.8375 | time=12.8s\n",
            "Epoch 050 | train_loss=0.3096 acc=0.8815 | val_loss=0.3872 acc=0.8231 | prec=0.8008 rec=0.8591 spec=0.7873 f1=0.8289 | time=12.7s\n",
            "Epoch 051 | train_loss=0.2971 acc=0.8877 | val_loss=0.3863 acc=0.8277 | prec=0.8000 rec=0.8727 spec=0.7828 f1=0.8348 | time=12.8s\n",
            "Epoch 052 | train_loss=0.2891 acc=0.8951 | val_loss=0.4214 acc=0.8299 | prec=0.8877 rec=0.7545 spec=0.9050 f1=0.8157 | time=12.7s\n",
            "Epoch 053 | train_loss=0.2918 acc=0.8746 | val_loss=0.4250 acc=0.8186 | prec=0.9023 rec=0.7136 spec=0.9231 f1=0.7970 | time=12.8s\n",
            "Epoch 054 | train_loss=0.2654 acc=0.9036 | val_loss=0.3830 acc=0.8345 | prec=0.8296 rec=0.8409 spec=0.8281 f1=0.8352 | time=12.8s\n",
            "Epoch 055 | train_loss=0.2522 acc=0.9013 | val_loss=0.3727 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.6s\n",
            "Epoch 056 | train_loss=0.2568 acc=0.9030 | val_loss=0.3860 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=12.9s\n",
            "Epoch 057 | train_loss=0.2460 acc=0.9109 | val_loss=0.3684 acc=0.8345 | prec=0.8451 rec=0.8182 spec=0.8507 f1=0.8314 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2612 acc=0.8990 | val_loss=0.4106 acc=0.8073 | prec=0.7755 rec=0.8636 spec=0.7511 f1=0.8172 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2519 acc=0.9087 | val_loss=0.4180 acc=0.8367 | prec=0.8814 rec=0.7773 spec=0.8959 f1=0.8261 | time=12.7s\n",
            "Epoch 060 | train_loss=0.2966 acc=0.8843 | val_loss=0.4343 acc=0.8254 | prec=0.8994 rec=0.7318 spec=0.9186 f1=0.8070 | time=12.7s\n",
            "Epoch 061 | train_loss=0.2535 acc=0.9007 | val_loss=0.3793 acc=0.8345 | prec=0.8517 rec=0.8091 spec=0.8597 f1=0.8298 | time=12.7s\n",
            "Epoch 062 | train_loss=0.2388 acc=0.9047 | val_loss=0.4014 acc=0.8322 | prec=0.8510 rec=0.8045 spec=0.8597 f1=0.8271 | time=12.9s\n",
            "Epoch 063 | train_loss=0.2398 acc=0.9007 | val_loss=0.4136 acc=0.8367 | prec=0.8895 rec=0.7682 spec=0.9050 f1=0.8244 | time=12.7s\n",
            "Epoch 064 | train_loss=0.2620 acc=0.9047 | val_loss=0.3816 acc=0.8277 | prec=0.8364 rec=0.8136 spec=0.8416 f1=0.8249 | time=12.8s\n",
            "Epoch 065 | train_loss=0.2258 acc=0.9104 | val_loss=0.4065 acc=0.8277 | prec=0.8673 rec=0.7727 spec=0.8824 f1=0.8173 | time=12.8s\n",
            "Epoch 066 | train_loss=0.2230 acc=0.9098 | val_loss=0.3778 acc=0.8299 | prec=0.8537 rec=0.7955 spec=0.8643 f1=0.8235 | time=12.8s\n",
            "Epoch 067 | train_loss=0.2202 acc=0.9155 | val_loss=0.3722 acc=0.8299 | prec=0.8251 rec=0.8364 spec=0.8235 f1=0.8307 | time=12.7s\n",
            "Epoch 068 | train_loss=0.2320 acc=0.9098 | val_loss=0.3952 acc=0.8118 | prec=0.7473 rec=0.9409 spec=0.6833 f1=0.8330 | time=12.8s\n",
            "Epoch 069 | train_loss=0.2189 acc=0.9206 | val_loss=0.3761 acc=0.8277 | prec=0.8429 rec=0.8045 spec=0.8507 f1=0.8233 | time=12.9s\n",
            "Epoch 070 | train_loss=0.2376 acc=0.9144 | val_loss=0.3921 acc=0.8231 | prec=0.8114 rec=0.8409 spec=0.8054 f1=0.8259 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2190 acc=0.9195 | val_loss=0.4356 acc=0.8299 | prec=0.9050 rec=0.7364 spec=0.9231 f1=0.8120 | time=12.8s\n",
            "Epoch 072 | train_loss=0.2107 acc=0.9149 | val_loss=0.4204 acc=0.8299 | prec=0.8796 rec=0.7636 spec=0.8959 f1=0.8175 | time=12.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▃▇▇▇▅▃▆███▇█████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▄█▅▄▆▇▆▇▆▇▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▆▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁█▁▂▄▆▆▇▇▇▇▆▇▆▇▇▆▆▇▇▇▆▇▇▆▇▇█▇▆</td></tr><tr><td>specificity</td><td>████████████▂▁▇▇▇▇▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▆▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▆▆▆▆▇▇▇▇▇▇▇█▇███▇███████</td></tr><tr><td>train_loss</td><td>██████████▇▇▇▇▇▆▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▇▇▇▇▇██▇▇▇██▇███▇██████▇█</td></tr><tr><td>val_loss</td><td>█████████████▆▅▄▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▂▂▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>72</td></tr><tr><td>f1_score</td><td>0.81752</td></tr><tr><td>precision</td><td>0.87958</td></tr><tr><td>recall</td><td>0.76364</td></tr><tr><td>specificity</td><td>0.89593</td></tr><tr><td>train_acc</td><td>0.91492</td></tr><tr><td>train_loss</td><td>0.21074</td></tr><tr><td>val_acc</td><td>0.82993</td></tr><tr><td>val_loss</td><td>0.42038</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/c0d93i3b' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/c0d93i3b</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_224852-c0d93i3b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 23:04:15,369] Trial 4 finished with values: [0.3684142380952835, 0.8344671201814059] and parameters: {'lr': 3.538612648033293e-05, 'wd': 4.664569364296752e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 5 hyperparams --> lr=3.66e-05, wd=2.57e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_230415-upfy0yzq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/upfy0yzq' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/upfy0yzq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/upfy0yzq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 5]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7737 acc=0.4980 | val_loss=0.6980 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7515 acc=0.4991 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 003 | train_loss=0.7418 acc=0.5014 | val_loss=0.6955 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 004 | train_loss=0.7415 acc=0.4986 | val_loss=0.6972 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7281 acc=0.5037 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 006 | train_loss=0.7196 acc=0.4946 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7218 acc=0.5003 | val_loss=0.6953 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7219 acc=0.4821 | val_loss=0.6976 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7215 acc=0.4969 | val_loss=0.6939 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7213 acc=0.4872 | val_loss=0.6984 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7212 acc=0.4969 | val_loss=0.6958 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7146 acc=0.4969 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 013 | train_loss=0.7080 acc=0.5116 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 014 | train_loss=0.7154 acc=0.4827 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 015 | train_loss=0.7120 acc=0.4969 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7053 acc=0.5128 | val_loss=0.6926 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 017 | train_loss=0.7171 acc=0.4957 | val_loss=0.6903 acc=0.5805 | prec=0.7397 rec=0.2455 spec=0.9140 f1=0.3686 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7064 acc=0.5065 | val_loss=0.6876 acc=0.5510 | prec=0.8438 rec=0.1227 spec=0.9774 f1=0.2143 | time=12.7s\n",
            "Epoch 019 | train_loss=0.7053 acc=0.5060 | val_loss=0.6845 acc=0.6984 | prec=0.6747 rec=0.7636 spec=0.6335 f1=0.7164 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7018 acc=0.5326 | val_loss=0.6675 acc=0.5329 | prec=0.8500 rec=0.0773 spec=0.9864 f1=0.1417 | time=12.8s\n",
            "Epoch 021 | train_loss=0.6684 acc=0.5995 | val_loss=0.6065 acc=0.6485 | prec=0.8421 rec=0.3636 spec=0.9321 f1=0.5079 | time=12.8s\n",
            "Epoch 022 | train_loss=0.6156 acc=0.6523 | val_loss=0.5693 acc=0.7959 | prec=0.7826 rec=0.8182 spec=0.7738 f1=0.8000 | time=12.7s\n",
            "Epoch 023 | train_loss=0.5764 acc=0.7294 | val_loss=0.5421 acc=0.7528 | prec=0.8675 rec=0.5955 spec=0.9095 f1=0.7062 | time=12.9s\n",
            "Epoch 024 | train_loss=0.5568 acc=0.7362 | val_loss=0.5251 acc=0.7800 | prec=0.8187 rec=0.7182 spec=0.8416 f1=0.7651 | time=12.8s\n",
            "Epoch 025 | train_loss=0.5506 acc=0.7442 | val_loss=0.5148 acc=0.8073 | prec=0.8000 rec=0.8182 spec=0.7964 f1=0.8090 | time=12.7s\n",
            "Epoch 026 | train_loss=0.5370 acc=0.7572 | val_loss=0.5261 acc=0.7619 | prec=0.8662 rec=0.6182 spec=0.9050 f1=0.7215 | time=12.8s\n",
            "Epoch 027 | train_loss=0.5153 acc=0.7782 | val_loss=0.4922 acc=0.8027 | prec=0.8276 rec=0.7636 spec=0.8416 f1=0.7943 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4941 acc=0.7913 | val_loss=0.5053 acc=0.7959 | prec=0.7407 rec=0.9091 spec=0.6833 f1=0.8163 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4763 acc=0.8134 | val_loss=0.4732 acc=0.8186 | prec=0.8500 rec=0.7727 spec=0.8643 f1=0.8095 | time=12.7s\n",
            "Epoch 030 | train_loss=0.4851 acc=0.7975 | val_loss=0.4676 acc=0.8141 | prec=0.7974 rec=0.8409 spec=0.7873 f1=0.8186 | time=12.7s\n",
            "Epoch 031 | train_loss=0.4476 acc=0.8202 | val_loss=0.4680 acc=0.8254 | prec=0.8421 rec=0.8000 spec=0.8507 f1=0.8205 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4365 acc=0.8225 | val_loss=0.4598 acc=0.8209 | prec=0.8026 rec=0.8500 spec=0.7919 f1=0.8256 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4174 acc=0.8406 | val_loss=0.4514 acc=0.8277 | prec=0.8429 rec=0.8045 spec=0.8507 f1=0.8233 | time=12.7s\n",
            "Epoch 034 | train_loss=0.4081 acc=0.8491 | val_loss=0.4365 acc=0.8299 | prec=0.8281 rec=0.8318 spec=0.8281 f1=0.8299 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4232 acc=0.8327 | val_loss=0.4369 acc=0.8322 | prec=0.8067 rec=0.8727 spec=0.7919 f1=0.8384 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3974 acc=0.8554 | val_loss=0.4282 acc=0.8277 | prec=0.8333 rec=0.8182 spec=0.8371 f1=0.8257 | time=12.7s\n",
            "Epoch 037 | train_loss=0.3719 acc=0.8695 | val_loss=0.4177 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.7s\n",
            "Epoch 038 | train_loss=0.3802 acc=0.8724 | val_loss=0.4551 acc=0.7891 | prec=0.7260 rec=0.9273 spec=0.6516 f1=0.8144 | time=12.7s\n",
            "Epoch 039 | train_loss=0.3570 acc=0.8746 | val_loss=0.4282 acc=0.8254 | prec=0.7761 rec=0.9136 spec=0.7376 f1=0.8392 | time=12.7s\n",
            "Epoch 040 | train_loss=0.3544 acc=0.8627 | val_loss=0.4280 acc=0.8186 | prec=0.8608 rec=0.7591 spec=0.8778 f1=0.8068 | time=12.7s\n",
            "Epoch 041 | train_loss=0.3433 acc=0.8718 | val_loss=0.4111 acc=0.8458 | prec=0.8016 rec=0.9182 spec=0.7738 f1=0.8559 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3381 acc=0.8724 | val_loss=0.3965 acc=0.8503 | prec=0.8263 rec=0.8864 spec=0.8145 f1=0.8553 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3120 acc=0.8900 | val_loss=0.3945 acc=0.8503 | prec=0.8056 rec=0.9227 spec=0.7783 f1=0.8602 | time=12.7s\n",
            "Epoch 044 | train_loss=0.3271 acc=0.8792 | val_loss=0.3843 acc=0.8367 | prec=0.8246 rec=0.8545 spec=0.8190 f1=0.8393 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3163 acc=0.8843 | val_loss=0.4071 acc=0.8322 | prec=0.8578 rec=0.7955 spec=0.8688 f1=0.8255 | time=12.7s\n",
            "Epoch 046 | train_loss=0.3030 acc=0.8917 | val_loss=0.3895 acc=0.8322 | prec=0.8411 rec=0.8182 spec=0.8462 f1=0.8295 | time=12.7s\n",
            "Epoch 047 | train_loss=0.2924 acc=0.9070 | val_loss=0.4071 acc=0.8299 | prec=0.7821 rec=0.9136 spec=0.7466 f1=0.8428 | time=12.8s\n",
            "Epoch 048 | train_loss=0.3006 acc=0.8860 | val_loss=0.4332 acc=0.8163 | prec=0.8639 rec=0.7500 spec=0.8824 f1=0.8029 | time=12.8s\n",
            "Epoch 049 | train_loss=0.3186 acc=0.8849 | val_loss=0.4076 acc=0.8254 | prec=0.8454 rec=0.7955 spec=0.8552 f1=0.8197 | time=12.7s\n",
            "Epoch 050 | train_loss=0.2953 acc=0.8866 | val_loss=0.3968 acc=0.8413 | prec=0.7953 rec=0.9182 spec=0.7647 f1=0.8523 | time=12.8s\n",
            "Epoch 051 | train_loss=0.2799 acc=0.8996 | val_loss=0.3882 acc=0.8390 | prec=0.8225 rec=0.8636 spec=0.8145 f1=0.8426 | time=12.7s\n",
            "Epoch 052 | train_loss=0.2861 acc=0.8979 | val_loss=0.4358 acc=0.8095 | prec=0.7429 rec=0.9455 spec=0.6742 f1=0.8320 | time=12.8s\n",
            "Epoch 053 | train_loss=0.3181 acc=0.8752 | val_loss=0.3952 acc=0.8367 | prec=0.8627 rec=0.8000 spec=0.8733 f1=0.8302 | time=12.8s\n",
            "Epoch 054 | train_loss=0.2608 acc=0.9098 | val_loss=0.3788 acc=0.8390 | prec=0.8041 rec=0.8955 spec=0.7828 f1=0.8473 | time=12.8s\n",
            "Epoch 055 | train_loss=0.2608 acc=0.9024 | val_loss=0.3840 acc=0.8458 | prec=0.8167 rec=0.8909 spec=0.8009 f1=0.8522 | time=12.7s\n",
            "Epoch 056 | train_loss=0.2448 acc=0.9104 | val_loss=0.4238 acc=0.8073 | prec=0.7509 rec=0.9182 spec=0.6968 f1=0.8262 | time=12.8s\n",
            "Epoch 057 | train_loss=0.2482 acc=0.9081 | val_loss=0.3779 acc=0.8435 | prec=0.8268 rec=0.8682 spec=0.8190 f1=0.8470 | time=12.8s\n",
            "Epoch 058 | train_loss=0.2441 acc=0.9166 | val_loss=0.3813 acc=0.8413 | prec=0.8261 rec=0.8636 spec=0.8190 f1=0.8444 | time=12.9s\n",
            "Epoch 059 | train_loss=0.2575 acc=0.9070 | val_loss=0.4348 acc=0.8141 | prec=0.7575 rec=0.9227 spec=0.7059 f1=0.8320 | time=12.7s\n",
            "Epoch 060 | train_loss=0.2430 acc=0.9053 | val_loss=0.3800 acc=0.8367 | prec=0.8246 rec=0.8545 spec=0.8190 f1=0.8393 | time=12.8s\n",
            "Epoch 061 | train_loss=0.2405 acc=0.9081 | val_loss=0.3767 acc=0.8481 | prec=0.8462 rec=0.8500 spec=0.8462 f1=0.8481 | time=12.8s\n",
            "Epoch 062 | train_loss=0.2354 acc=0.9115 | val_loss=0.3891 acc=0.8345 | prec=0.7905 rec=0.9091 spec=0.7602 f1=0.8457 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2385 acc=0.9081 | val_loss=0.3945 acc=0.8277 | prec=0.7835 rec=0.9045 spec=0.7511 f1=0.8397 | time=12.7s\n",
            "Epoch 064 | train_loss=0.2361 acc=0.9087 | val_loss=0.3972 acc=0.8367 | prec=0.7960 rec=0.9045 spec=0.7692 f1=0.8468 | time=12.7s\n",
            "Epoch 065 | train_loss=0.2189 acc=0.9161 | val_loss=0.3745 acc=0.8367 | prec=0.8083 rec=0.8818 spec=0.7919 f1=0.8435 | time=12.7s\n",
            "Epoch 066 | train_loss=0.2430 acc=0.9149 | val_loss=0.3766 acc=0.8345 | prec=0.8238 rec=0.8500 spec=0.8190 f1=0.8367 | time=12.7s\n",
            "Epoch 067 | train_loss=0.2206 acc=0.9246 | val_loss=0.4034 acc=0.8367 | prec=0.8558 rec=0.8091 spec=0.8643 f1=0.8318 | time=12.7s\n",
            "Epoch 068 | train_loss=0.2172 acc=0.9195 | val_loss=0.5073 acc=0.7664 | prec=0.6944 rec=0.9500 spec=0.5837 f1=0.8023 | time=12.7s\n",
            "Epoch 069 | train_loss=0.2061 acc=0.9251 | val_loss=0.3920 acc=0.8390 | prec=0.8016 rec=0.9000 spec=0.7783 f1=0.8480 | time=12.8s\n",
            "Epoch 070 | train_loss=0.2304 acc=0.9217 | val_loss=0.4113 acc=0.8299 | prec=0.8643 rec=0.7818 spec=0.8778 f1=0.8210 | time=12.9s\n",
            "Epoch 071 | train_loss=0.2158 acc=0.9229 | val_loss=0.3900 acc=0.8413 | prec=0.8074 rec=0.8955 spec=0.7873 f1=0.8491 | time=12.8s\n",
            "Epoch 072 | train_loss=0.2229 acc=0.9081 | val_loss=0.3884 acc=0.8413 | prec=0.8641 rec=0.8091 spec=0.8733 f1=0.8357 | time=12.7s\n",
            "Epoch 073 | train_loss=0.2254 acc=0.9183 | val_loss=0.3806 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=12.9s\n",
            "Epoch 074 | train_loss=0.2102 acc=0.9189 | val_loss=0.4253 acc=0.8118 | prec=0.7585 rec=0.9136 spec=0.7104 f1=0.8289 | time=12.7s\n",
            "Epoch 075 | train_loss=0.2148 acc=0.9121 | val_loss=0.4065 acc=0.8254 | prec=0.7871 rec=0.8909 spec=0.7602 f1=0.8358 | time=12.8s\n",
            "Epoch 076 | train_loss=0.2078 acc=0.9251 | val_loss=0.3916 acc=0.8299 | prec=0.7984 rec=0.8818 spec=0.7783 f1=0.8380 | time=12.6s\n",
            "Epoch 077 | train_loss=0.2134 acc=0.9280 | val_loss=0.3986 acc=0.8254 | prec=0.8178 rec=0.8364 spec=0.8145 f1=0.8270 | time=12.8s\n",
            "Epoch 078 | train_loss=0.1798 acc=0.9427 | val_loss=0.4009 acc=0.8345 | prec=0.8585 rec=0.8000 spec=0.8688 f1=0.8282 | time=12.7s\n",
            "Epoch 079 | train_loss=0.1863 acc=0.9234 | val_loss=0.3809 acc=0.8345 | prec=0.8075 rec=0.8773 spec=0.7919 f1=0.8410 | time=12.7s\n",
            "Epoch 080 | train_loss=0.1821 acc=0.9359 | val_loss=0.4023 acc=0.8345 | prec=0.8050 rec=0.8818 spec=0.7873 f1=0.8416 | time=12.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▄▅▇▇████████████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▇█▆█▇▇██▇█████▇███▇█▇█▇██▇▇▇██▇█▇▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▃▇▄▆▆▇█▇▇▇▇█████▇█▇██▇██▇██▇▇█▇▇█</td></tr><tr><td>specificity</td><td>█████████▇▂▇▄▆▅▆▄▅▅▅▂▅▄▅▅▆▅▆▄▅▅▄▅▁▄▆▅▃▄▄</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▃▅▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█████████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▅▅▄▅▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▃▅▄▇▆▇▇▇▇▇█▇██▇███▇██▇█▇████████</td></tr><tr><td>val_loss</td><td>██████████▅▅▄▄▄▄▃▃▂▃▁▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>80</td></tr><tr><td>f1_score</td><td>0.84165</td></tr><tr><td>precision</td><td>0.80498</td></tr><tr><td>recall</td><td>0.88182</td></tr><tr><td>specificity</td><td>0.78733</td></tr><tr><td>train_acc</td><td>0.9359</td></tr><tr><td>train_loss</td><td>0.18211</td></tr><tr><td>val_acc</td><td>0.83447</td></tr><tr><td>val_loss</td><td>0.40228</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/upfy0yzq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/upfy0yzq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_230415-upfy0yzq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 23:21:19,294] Trial 5 finished with values: [0.3744664192199707, 0.8367346938775511] and parameters: {'lr': 3.660841642579726e-05, 'wd': 2.5660490984694296e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 6 hyperparams --> lr=7.52e-05, wd=5.63e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_232119-uzinhn8x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/uzinhn8x' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/uzinhn8x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/uzinhn8x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 6]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7535 acc=0.5014 | val_loss=0.7012 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7547 acc=0.5162 | val_loss=0.7337 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7502 acc=0.5031 | val_loss=0.7266 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 004 | train_loss=0.7411 acc=0.5037 | val_loss=0.7204 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7272 acc=0.5003 | val_loss=0.7035 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 006 | train_loss=0.7359 acc=0.4912 | val_loss=0.7069 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7327 acc=0.5094 | val_loss=0.6928 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 008 | train_loss=0.7325 acc=0.5009 | val_loss=0.6956 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 009 | train_loss=0.7229 acc=0.4991 | val_loss=0.6962 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7343 acc=0.4940 | val_loss=0.6833 acc=0.5714 | prec=0.5394 rec=0.9636 spec=0.1810 f1=0.6917 | time=12.7s\n",
            "Epoch 011 | train_loss=0.7104 acc=0.5167 | val_loss=0.6803 acc=0.6190 | prec=0.8023 rec=0.3136 spec=0.9231 f1=0.4510 | time=12.9s\n",
            "Epoch 012 | train_loss=0.6958 acc=0.5366 | val_loss=0.6294 acc=0.7188 | prec=0.7892 rec=0.5955 spec=0.8416 f1=0.6788 | time=12.8s\n",
            "Epoch 013 | train_loss=0.6500 acc=0.6177 | val_loss=0.5791 acc=0.7302 | prec=0.8633 rec=0.5455 spec=0.9140 f1=0.6685 | time=12.8s\n",
            "Epoch 014 | train_loss=0.6091 acc=0.6574 | val_loss=0.5602 acc=0.7710 | prec=0.8742 rec=0.6318 spec=0.9095 f1=0.7335 | time=12.9s\n",
            "Epoch 015 | train_loss=0.5919 acc=0.6858 | val_loss=0.5290 acc=0.7959 | prec=0.8385 rec=0.7318 spec=0.8597 f1=0.7816 | time=12.7s\n",
            "Epoch 016 | train_loss=0.5745 acc=0.6886 | val_loss=0.5272 acc=0.7823 | prec=0.8563 rec=0.6773 spec=0.8869 f1=0.7563 | time=12.8s\n",
            "Epoch 017 | train_loss=0.5528 acc=0.7147 | val_loss=0.5020 acc=0.7937 | prec=0.8486 rec=0.7136 spec=0.8733 f1=0.7753 | time=12.9s\n",
            "Epoch 018 | train_loss=0.5201 acc=0.7431 | val_loss=0.4887 acc=0.7868 | prec=0.8580 rec=0.6864 spec=0.8869 f1=0.7626 | time=12.8s\n",
            "Epoch 019 | train_loss=0.4943 acc=0.7765 | val_loss=0.4500 acc=0.8118 | prec=0.8442 rec=0.7636 spec=0.8597 f1=0.8019 | time=12.8s\n",
            "Epoch 020 | train_loss=0.4699 acc=0.7884 | val_loss=0.4289 acc=0.8186 | prec=0.8333 rec=0.7955 spec=0.8416 f1=0.8140 | time=12.9s\n",
            "Epoch 021 | train_loss=0.4421 acc=0.8100 | val_loss=0.4870 acc=0.7664 | prec=0.8980 rec=0.6000 spec=0.9321 f1=0.7193 | time=12.7s\n",
            "Epoch 022 | train_loss=0.4213 acc=0.8225 | val_loss=0.4399 acc=0.7982 | prec=0.8786 rec=0.6909 spec=0.9050 f1=0.7735 | time=12.8s\n",
            "Epoch 023 | train_loss=0.4025 acc=0.8134 | val_loss=0.4138 acc=0.8073 | prec=0.8462 rec=0.7500 spec=0.8643 f1=0.7952 | time=12.7s\n",
            "Epoch 024 | train_loss=0.4056 acc=0.8253 | val_loss=0.4135 acc=0.8299 | prec=0.8796 rec=0.7636 spec=0.8959 f1=0.8175 | time=12.7s\n",
            "Epoch 025 | train_loss=0.3692 acc=0.8378 | val_loss=0.3853 acc=0.8413 | prec=0.8233 rec=0.8682 spec=0.8145 f1=0.8451 | time=12.9s\n",
            "Epoch 026 | train_loss=0.3394 acc=0.8486 | val_loss=0.4357 acc=0.8118 | prec=0.9053 rec=0.6955 spec=0.9276 f1=0.7866 | time=12.8s\n",
            "Epoch 027 | train_loss=0.3486 acc=0.8548 | val_loss=0.3667 acc=0.8481 | prec=0.8525 rec=0.8409 spec=0.8552 f1=0.8467 | time=12.7s\n",
            "Epoch 028 | train_loss=0.3289 acc=0.8690 | val_loss=0.3701 acc=0.8503 | prec=0.8208 rec=0.8955 spec=0.8054 f1=0.8565 | time=12.8s\n",
            "Epoch 029 | train_loss=0.2936 acc=0.8928 | val_loss=0.3631 acc=0.8435 | prec=0.8268 rec=0.8682 spec=0.8190 f1=0.8470 | time=12.8s\n",
            "Epoch 030 | train_loss=0.2939 acc=0.8837 | val_loss=0.3706 acc=0.8390 | prec=0.8117 rec=0.8818 spec=0.7964 f1=0.8453 | time=13.0s\n",
            "Epoch 031 | train_loss=0.2872 acc=0.8934 | val_loss=0.5325 acc=0.7324 | prec=0.6594 rec=0.9591 spec=0.5068 f1=0.7815 | time=12.7s\n",
            "Epoch 032 | train_loss=0.3476 acc=0.8480 | val_loss=0.3473 acc=0.8549 | prec=0.8482 rec=0.8636 spec=0.8462 f1=0.8559 | time=12.8s\n",
            "Epoch 033 | train_loss=0.2732 acc=0.8894 | val_loss=0.3566 acc=0.8413 | prec=0.8049 rec=0.9000 spec=0.7828 f1=0.8498 | time=12.7s\n",
            "Epoch 034 | train_loss=0.3005 acc=0.8809 | val_loss=0.3837 acc=0.8209 | prec=0.7621 rec=0.9318 spec=0.7104 f1=0.8384 | time=12.7s\n",
            "Epoch 035 | train_loss=0.2718 acc=0.8905 | val_loss=0.3819 acc=0.8503 | prec=0.9010 rec=0.7864 spec=0.9140 f1=0.8398 | time=12.8s\n",
            "Epoch 036 | train_loss=0.2339 acc=0.8996 | val_loss=0.4496 acc=0.8050 | prec=0.7343 rec=0.9545 spec=0.6561 f1=0.8300 | time=12.8s\n",
            "Epoch 037 | train_loss=0.2481 acc=0.9109 | val_loss=0.3387 acc=0.8481 | prec=0.8201 rec=0.8909 spec=0.8054 f1=0.8540 | time=12.8s\n",
            "Epoch 038 | train_loss=0.2463 acc=0.8917 | val_loss=0.4286 acc=0.8005 | prec=0.7340 rec=0.9409 spec=0.6606 f1=0.8247 | time=12.9s\n",
            "Epoch 039 | train_loss=0.2438 acc=0.9087 | val_loss=0.3550 acc=0.8413 | prec=0.8099 rec=0.8909 spec=0.7919 f1=0.8485 | time=12.7s\n",
            "Epoch 040 | train_loss=0.2176 acc=0.9138 | val_loss=0.3592 acc=0.8549 | prec=0.8305 rec=0.8909 spec=0.8190 f1=0.8596 | time=12.8s\n",
            "Epoch 041 | train_loss=0.2499 acc=0.8894 | val_loss=0.3466 acc=0.8526 | prec=0.8384 rec=0.8727 spec=0.8326 f1=0.8552 | time=12.8s\n",
            "Epoch 042 | train_loss=0.2294 acc=0.8996 | val_loss=0.3785 acc=0.8345 | prec=0.7795 rec=0.9318 spec=0.7376 f1=0.8489 | time=12.8s\n",
            "Epoch 043 | train_loss=0.2058 acc=0.9087 | val_loss=0.3655 acc=0.8549 | prec=0.8824 rec=0.8182 spec=0.8914 f1=0.8491 | time=12.8s\n",
            "Epoch 044 | train_loss=0.1871 acc=0.9234 | val_loss=0.4138 acc=0.8322 | prec=0.7786 rec=0.9273 spec=0.7376 f1=0.8465 | time=12.9s\n",
            "Epoch 045 | train_loss=0.1779 acc=0.9370 | val_loss=0.3597 acc=0.8571 | prec=0.8584 rec=0.8545 spec=0.8597 f1=0.8565 | time=12.7s\n",
            "Epoch 046 | train_loss=0.1873 acc=0.9263 | val_loss=0.3784 acc=0.8413 | prec=0.8178 rec=0.8773 spec=0.8054 f1=0.8465 | time=12.7s\n",
            "Epoch 047 | train_loss=0.1688 acc=0.9325 | val_loss=0.3617 acc=0.8549 | prec=0.8714 rec=0.8318 spec=0.8778 f1=0.8512 | time=12.7s\n",
            "Epoch 048 | train_loss=0.1674 acc=0.9285 | val_loss=0.4022 acc=0.8299 | prec=0.7866 rec=0.9045 spec=0.7557 f1=0.8414 | time=12.8s\n",
            "Epoch 049 | train_loss=0.1593 acc=0.9359 | val_loss=0.4136 acc=0.8345 | prec=0.8808 rec=0.7727 spec=0.8959 f1=0.8232 | time=12.8s\n",
            "Epoch 050 | train_loss=0.1818 acc=0.9138 | val_loss=0.3717 acc=0.8503 | prec=0.8632 rec=0.8318 spec=0.8688 f1=0.8472 | time=12.8s\n",
            "Epoch 051 | train_loss=0.1549 acc=0.9257 | val_loss=0.3735 acc=0.8594 | prec=0.8435 rec=0.8818 spec=0.8371 f1=0.8622 | time=12.8s\n",
            "Epoch 052 | train_loss=0.1437 acc=0.9421 | val_loss=0.4238 acc=0.8299 | prec=0.7757 rec=0.9273 spec=0.7330 f1=0.8447 | time=12.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▇▅▇▇▇▇▇▇▇▇▇██████▇███████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▅▇▇█████████▇█▇▇▇▆▇▇█▇▇▇▇█▇▇▇█▇█▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁█▃▅▅▆▆▆▆▇▅▆▆▇▇▇█▇▇███▇█▇▇▇▇█▇▇█▇▇█</td></tr><tr><td>specificity</td><td>████████▁▇▇▇▇▇▇▇▇▇▇▆▇▆▆▆▄▆▇▅▆▅▇▆▇▆▇▇▆▇▇▆</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▃▄▄▄▄▅▆▆▆▆▆▇▇▇▇▇▇▇▇█▇█▇▇▇█████</td></tr><tr><td>train_loss</td><td>█████████▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▂▃▅▆▇▇▇▇▆▇▇██████▆█▇█▇███████████</td></tr><tr><td>val_loss</td><td>▇███▇▇▇▇▇▇▅▅▄▄▄▃▃▄▃▂▂▁▂▁▂▂▂▃▁▃▁▁▂▁▂▂▁▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>52</td></tr><tr><td>f1_score</td><td>0.84472</td></tr><tr><td>precision</td><td>0.77567</td></tr><tr><td>recall</td><td>0.92727</td></tr><tr><td>specificity</td><td>0.73303</td></tr><tr><td>train_acc</td><td>0.94214</td></tr><tr><td>train_loss</td><td>0.14367</td></tr><tr><td>val_acc</td><td>0.82993</td></tr><tr><td>val_loss</td><td>0.42384</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/uzinhn8x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/uzinhn8x</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_232119-uzinhn8x/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 23:32:26,859] Trial 6 finished with values: [0.33871942332812716, 0.8480725623582767] and parameters: {'lr': 7.516871749773753e-05, 'wd': 5.634459360274128e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 7 hyperparams --> lr=2.43e-05, wd=3.82e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_233226-gi9hwke1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/gi9hwke1' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/gi9hwke1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/gi9hwke1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 7]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7472 acc=0.4861 | val_loss=0.6952 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7210 acc=0.4997 | val_loss=0.6961 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.4s\n",
            "Epoch 003 | train_loss=0.7226 acc=0.5111 | val_loss=0.6955 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.5s\n",
            "Epoch 004 | train_loss=0.7151 acc=0.5145 | val_loss=0.6958 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.6s\n",
            "Epoch 005 | train_loss=0.7196 acc=0.5077 | val_loss=0.6951 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 006 | train_loss=0.7205 acc=0.4895 | val_loss=0.6953 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 007 | train_loss=0.7123 acc=0.4986 | val_loss=0.6953 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 008 | train_loss=0.7236 acc=0.5003 | val_loss=0.6952 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.6s\n",
            "Epoch 009 | train_loss=0.7121 acc=0.5026 | val_loss=0.6944 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.7s\n",
            "Epoch 010 | train_loss=0.7186 acc=0.4991 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.8s\n",
            "Epoch 011 | train_loss=0.7185 acc=0.4872 | val_loss=0.6951 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.9s\n",
            "Epoch 012 | train_loss=0.7049 acc=0.4986 | val_loss=0.6953 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=13.5s\n",
            "Epoch 013 | train_loss=0.7099 acc=0.4952 | val_loss=0.6943 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 014 | train_loss=0.6964 acc=0.5179 | val_loss=0.6937 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7136 acc=0.5003 | val_loss=0.6976 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 016 | train_loss=0.7089 acc=0.4895 | val_loss=0.6957 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 017 | train_loss=0.7127 acc=0.4997 | val_loss=0.6940 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 018 | train_loss=0.7095 acc=0.5043 | val_loss=0.6953 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 019 | train_loss=0.7053 acc=0.5224 | val_loss=0.6951 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 020 | train_loss=0.6956 acc=0.5377 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7053 acc=0.5031 | val_loss=0.6981 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 022 | train_loss=0.7050 acc=0.5014 | val_loss=0.6958 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.8s\n",
            "Epoch 023 | train_loss=0.7055 acc=0.5105 | val_loss=0.6924 acc=0.5238 | prec=0.5118 rec=0.9864 spec=0.0633 f1=0.6739 | time=12.7s\n",
            "Epoch 024 | train_loss=0.7016 acc=0.5020 | val_loss=0.6936 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 025 | train_loss=0.7016 acc=0.5264 | val_loss=0.6924 acc=0.4989 | prec=0.4989 rec=0.9955 spec=0.0045 f1=0.6646 | time=12.8s\n",
            "Epoch 026 | train_loss=0.6987 acc=0.5241 | val_loss=0.6875 acc=0.5578 | prec=0.5546 rec=0.5773 spec=0.5385 f1=0.5657 | time=12.8s\n",
            "Epoch 027 | train_loss=0.6886 acc=0.5389 | val_loss=0.6760 acc=0.5986 | prec=0.5749 rec=0.7500 spec=0.4480 f1=0.6509 | time=12.9s\n",
            "Epoch 028 | train_loss=0.6775 acc=0.5820 | val_loss=0.6482 acc=0.6848 | prec=0.6473 rec=0.8091 spec=0.5611 f1=0.7192 | time=12.8s\n",
            "Epoch 029 | train_loss=0.6330 acc=0.6551 | val_loss=0.6601 acc=0.5488 | prec=0.5251 rec=1.0000 spec=0.0995 f1=0.6886 | time=12.9s\n",
            "Epoch 030 | train_loss=0.6095 acc=0.6699 | val_loss=0.5480 acc=0.7460 | prec=0.8214 rec=0.6273 spec=0.8643 f1=0.7113 | time=12.7s\n",
            "Epoch 031 | train_loss=0.5650 acc=0.7136 | val_loss=0.5481 acc=0.7868 | prec=0.7423 rec=0.8773 spec=0.6968 f1=0.8042 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5484 acc=0.7300 | val_loss=0.5288 acc=0.7914 | prec=0.7712 rec=0.8273 spec=0.7557 f1=0.7982 | time=12.7s\n",
            "Epoch 033 | train_loss=0.5493 acc=0.7283 | val_loss=0.5162 acc=0.7868 | prec=0.7520 rec=0.8545 spec=0.7195 f1=0.8000 | time=12.7s\n",
            "Epoch 034 | train_loss=0.5409 acc=0.7453 | val_loss=0.4995 acc=0.7868 | prec=0.8539 rec=0.6909 spec=0.8824 f1=0.7638 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5236 acc=0.7425 | val_loss=0.4800 acc=0.8005 | prec=0.8438 rec=0.7364 spec=0.8643 f1=0.7864 | time=12.8s\n",
            "Epoch 036 | train_loss=0.5170 acc=0.7669 | val_loss=0.4793 acc=0.7959 | prec=0.8316 rec=0.7409 spec=0.8507 f1=0.7837 | time=12.8s\n",
            "Epoch 037 | train_loss=0.4949 acc=0.7805 | val_loss=0.4724 acc=0.8050 | prec=0.8221 rec=0.7773 spec=0.8326 f1=0.7991 | time=12.8s\n",
            "Epoch 038 | train_loss=0.4755 acc=0.7930 | val_loss=0.4672 acc=0.8118 | prec=0.8246 rec=0.7909 spec=0.8326 f1=0.8074 | time=12.8s\n",
            "Epoch 039 | train_loss=0.4697 acc=0.7930 | val_loss=0.4550 acc=0.8050 | prec=0.8190 rec=0.7818 spec=0.8281 f1=0.8000 | time=12.8s\n",
            "Epoch 040 | train_loss=0.4892 acc=0.8003 | val_loss=0.4428 acc=0.8095 | prec=0.8178 rec=0.7955 spec=0.8235 f1=0.8065 | time=12.7s\n",
            "Epoch 041 | train_loss=0.4899 acc=0.7708 | val_loss=0.4672 acc=0.8118 | prec=0.8072 rec=0.8182 spec=0.8054 f1=0.8126 | time=12.8s\n",
            "Epoch 042 | train_loss=0.4612 acc=0.7998 | val_loss=0.4424 acc=0.8095 | prec=0.8238 rec=0.7864 spec=0.8326 f1=0.8047 | time=12.9s\n",
            "Epoch 043 | train_loss=0.4473 acc=0.8111 | val_loss=0.4572 acc=0.8209 | prec=0.8000 rec=0.8545 spec=0.7873 f1=0.8264 | time=12.7s\n",
            "Epoch 044 | train_loss=0.4521 acc=0.8066 | val_loss=0.4365 acc=0.8073 | prec=0.8027 rec=0.8136 spec=0.8009 f1=0.8081 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4540 acc=0.7947 | val_loss=0.4694 acc=0.8095 | prec=0.7615 rec=0.9000 spec=0.7195 f1=0.8250 | time=12.8s\n",
            "Epoch 046 | train_loss=0.4268 acc=0.8287 | val_loss=0.4338 acc=0.8231 | prec=0.8170 rec=0.8318 spec=0.8145 f1=0.8243 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4251 acc=0.8230 | val_loss=0.4261 acc=0.8186 | prec=0.8365 rec=0.7909 spec=0.8462 f1=0.8131 | time=12.9s\n",
            "Epoch 048 | train_loss=0.4158 acc=0.8208 | val_loss=0.4414 acc=0.8118 | prec=0.7842 rec=0.8591 spec=0.7647 f1=0.8200 | time=12.8s\n",
            "Epoch 049 | train_loss=0.4212 acc=0.8321 | val_loss=0.4269 acc=0.8095 | prec=0.8036 rec=0.8182 spec=0.8009 f1=0.8108 | time=12.7s\n",
            "Epoch 050 | train_loss=0.4233 acc=0.8242 | val_loss=0.4424 acc=0.8163 | prec=0.8840 rec=0.7273 spec=0.9050 f1=0.7980 | time=12.7s\n",
            "Epoch 051 | train_loss=0.4275 acc=0.8225 | val_loss=0.4236 acc=0.8186 | prec=0.8333 rec=0.7955 spec=0.8416 f1=0.8140 | time=12.9s\n",
            "Epoch 052 | train_loss=0.4044 acc=0.8400 | val_loss=0.4267 acc=0.8141 | prec=0.8108 rec=0.8182 spec=0.8100 f1=0.8145 | time=12.9s\n",
            "Epoch 053 | train_loss=0.4057 acc=0.8327 | val_loss=0.4085 acc=0.8231 | prec=0.8142 rec=0.8364 spec=0.8100 f1=0.8251 | time=12.8s\n",
            "Epoch 054 | train_loss=0.3964 acc=0.8366 | val_loss=0.4222 acc=0.8277 | prec=0.8750 rec=0.7636 spec=0.8914 f1=0.8155 | time=12.8s\n",
            "Epoch 055 | train_loss=0.4010 acc=0.8395 | val_loss=0.4111 acc=0.8186 | prec=0.7941 rec=0.8591 spec=0.7783 f1=0.8253 | time=12.8s\n",
            "Epoch 056 | train_loss=0.3960 acc=0.8429 | val_loss=0.4077 acc=0.8367 | prec=0.8663 rec=0.7955 spec=0.8778 f1=0.8294 | time=12.8s\n",
            "Epoch 057 | train_loss=0.3787 acc=0.8559 | val_loss=0.4085 acc=0.8367 | prec=0.8592 rec=0.8045 spec=0.8688 f1=0.8310 | time=12.9s\n",
            "Epoch 058 | train_loss=0.3771 acc=0.8593 | val_loss=0.4092 acc=0.8254 | prec=0.8178 rec=0.8364 spec=0.8145 f1=0.8270 | time=12.7s\n",
            "Epoch 059 | train_loss=0.3798 acc=0.8463 | val_loss=0.4110 acc=0.8254 | prec=0.8743 rec=0.7591 spec=0.8914 f1=0.8127 | time=12.7s\n",
            "Epoch 060 | train_loss=0.3669 acc=0.8525 | val_loss=0.4002 acc=0.8390 | prec=0.8706 rec=0.7955 spec=0.8824 f1=0.8314 | time=12.6s\n",
            "Epoch 061 | train_loss=0.3672 acc=0.8605 | val_loss=0.4008 acc=0.8322 | prec=0.8443 rec=0.8136 spec=0.8507 f1=0.8287 | time=12.8s\n",
            "Epoch 062 | train_loss=0.3519 acc=0.8690 | val_loss=0.4278 acc=0.8163 | prec=0.8798 rec=0.7318 spec=0.9005 f1=0.7990 | time=12.7s\n",
            "Epoch 063 | train_loss=0.3542 acc=0.8798 | val_loss=0.3859 acc=0.8458 | prec=0.8585 rec=0.8273 spec=0.8643 f1=0.8426 | time=12.8s\n",
            "Epoch 064 | train_loss=0.3592 acc=0.8610 | val_loss=0.4121 acc=0.8231 | prec=0.8777 rec=0.7500 spec=0.8959 f1=0.8088 | time=12.8s\n",
            "Epoch 065 | train_loss=0.3521 acc=0.8633 | val_loss=0.3887 acc=0.8367 | prec=0.8592 rec=0.8045 spec=0.8688 f1=0.8310 | time=12.7s\n",
            "Epoch 066 | train_loss=0.3501 acc=0.8752 | val_loss=0.3928 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=12.8s\n",
            "Epoch 067 | train_loss=0.3550 acc=0.8610 | val_loss=0.3970 acc=0.8299 | prec=0.7959 rec=0.8864 spec=0.7738 f1=0.8387 | time=12.8s\n",
            "Epoch 068 | train_loss=0.3479 acc=0.8622 | val_loss=0.3877 acc=0.8367 | prec=0.8627 rec=0.8000 spec=0.8733 f1=0.8302 | time=12.7s\n",
            "Epoch 069 | train_loss=0.3580 acc=0.8633 | val_loss=0.3864 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=12.8s\n",
            "Epoch 070 | train_loss=0.3460 acc=0.8746 | val_loss=0.4318 acc=0.8073 | prec=0.8902 rec=0.7000 spec=0.9140 f1=0.7837 | time=12.8s\n",
            "Epoch 071 | train_loss=0.3227 acc=0.8746 | val_loss=0.3761 acc=0.8413 | prec=0.8538 rec=0.8227 spec=0.8597 f1=0.8380 | time=12.8s\n",
            "Epoch 072 | train_loss=0.3344 acc=0.8661 | val_loss=0.3710 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.8s\n",
            "Epoch 073 | train_loss=0.3329 acc=0.8820 | val_loss=0.3630 acc=0.8503 | prec=0.8500 rec=0.8500 spec=0.8507 f1=0.8500 | time=12.8s\n",
            "Epoch 074 | train_loss=0.3347 acc=0.8763 | val_loss=0.3698 acc=0.8435 | prec=0.8545 rec=0.8273 spec=0.8597 f1=0.8406 | time=12.9s\n",
            "Epoch 075 | train_loss=0.3184 acc=0.8843 | val_loss=0.3711 acc=0.8435 | prec=0.8647 rec=0.8136 spec=0.8733 f1=0.8384 | time=12.8s\n",
            "Epoch 076 | train_loss=0.3100 acc=0.8786 | val_loss=0.3902 acc=0.8390 | prec=0.8744 rec=0.7909 spec=0.8869 f1=0.8305 | time=12.9s\n",
            "Epoch 077 | train_loss=0.3291 acc=0.8809 | val_loss=0.3690 acc=0.8390 | prec=0.8565 rec=0.8136 spec=0.8643 f1=0.8345 | time=12.9s\n",
            "Epoch 078 | train_loss=0.3256 acc=0.8605 | val_loss=0.3688 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=12.9s\n",
            "Epoch 079 | train_loss=0.3172 acc=0.8780 | val_loss=0.3523 acc=0.8481 | prec=0.8525 rec=0.8409 spec=0.8552 f1=0.8467 | time=12.9s\n",
            "Epoch 080 | train_loss=0.3051 acc=0.8871 | val_loss=0.3765 acc=0.8367 | prec=0.8663 rec=0.7955 spec=0.8778 f1=0.8294 | time=12.7s\n",
            "Epoch 081 | train_loss=0.3046 acc=0.8832 | val_loss=0.3656 acc=0.8367 | prec=0.8558 rec=0.8091 spec=0.8643 f1=0.8318 | time=12.8s\n",
            "Epoch 082 | train_loss=0.3063 acc=0.8780 | val_loss=0.3514 acc=0.8526 | prec=0.8326 rec=0.8818 spec=0.8235 f1=0.8565 | time=12.7s\n",
            "Epoch 083 | train_loss=0.3097 acc=0.8752 | val_loss=0.3663 acc=0.8390 | prec=0.8599 rec=0.8091 spec=0.8688 f1=0.8337 | time=12.8s\n",
            "Epoch 084 | train_loss=0.3216 acc=0.8809 | val_loss=0.3761 acc=0.8367 | prec=0.8627 rec=0.8000 spec=0.8733 f1=0.8302 | time=12.9s\n",
            "Epoch 085 | train_loss=0.2826 acc=0.9047 | val_loss=0.3518 acc=0.8549 | prec=0.8391 rec=0.8773 spec=0.8326 f1=0.8578 | time=12.7s\n",
            "Epoch 086 | train_loss=0.2944 acc=0.8849 | val_loss=0.3505 acc=0.8481 | prec=0.8592 rec=0.8318 spec=0.8643 f1=0.8453 | time=12.8s\n",
            "Epoch 087 | train_loss=0.2888 acc=0.8922 | val_loss=0.3726 acc=0.8435 | prec=0.8832 rec=0.7909 spec=0.8959 f1=0.8345 | time=12.7s\n",
            "Epoch 088 | train_loss=0.2991 acc=0.8871 | val_loss=0.3552 acc=0.8481 | prec=0.8558 rec=0.8364 spec=0.8597 f1=0.8460 | time=12.7s\n",
            "Epoch 089 | train_loss=0.2723 acc=0.9036 | val_loss=0.3582 acc=0.8458 | prec=0.8551 rec=0.8318 spec=0.8597 f1=0.8433 | time=12.9s\n",
            "Epoch 090 | train_loss=0.3032 acc=0.8905 | val_loss=0.3591 acc=0.8435 | prec=0.8612 rec=0.8182 spec=0.8688 f1=0.8392 | time=12.7s\n",
            "Epoch 091 | train_loss=0.2979 acc=0.8792 | val_loss=0.3496 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.7s\n",
            "Epoch 092 | train_loss=0.2813 acc=0.8917 | val_loss=0.3563 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.7s\n",
            "Epoch 093 | train_loss=0.2912 acc=0.8951 | val_loss=0.3461 acc=0.8503 | prec=0.8532 rec=0.8455 spec=0.8552 f1=0.8493 | time=12.8s\n",
            "Epoch 094 | train_loss=0.2722 acc=0.9024 | val_loss=0.3529 acc=0.8435 | prec=0.8578 rec=0.8227 spec=0.8643 f1=0.8399 | time=12.9s\n",
            "Epoch 095 | train_loss=0.2865 acc=0.8877 | val_loss=0.3491 acc=0.8435 | prec=0.8479 rec=0.8364 spec=0.8507 f1=0.8421 | time=12.8s\n",
            "Epoch 096 | train_loss=0.2819 acc=0.8985 | val_loss=0.3441 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.8s\n",
            "Epoch 097 | train_loss=0.2834 acc=0.8911 | val_loss=0.3469 acc=0.8435 | prec=0.8386 rec=0.8500 spec=0.8371 f1=0.8442 | time=12.8s\n",
            "Epoch 098 | train_loss=0.2697 acc=0.8917 | val_loss=0.3515 acc=0.8481 | prec=0.8558 rec=0.8364 spec=0.8597 f1=0.8460 | time=12.8s\n",
            "Epoch 099 | train_loss=0.2714 acc=0.8905 | val_loss=0.3497 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.7s\n",
            "Epoch 100 | train_loss=0.2923 acc=0.9002 | val_loss=0.3552 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.8s\n",
            "Epoch 101 | train_loss=0.2654 acc=0.9024 | val_loss=0.3494 acc=0.8458 | prec=0.8486 rec=0.8409 spec=0.8507 f1=0.8447 | time=12.8s\n",
            "Epoch 102 | train_loss=0.2640 acc=0.9030 | val_loss=0.3526 acc=0.8413 | prec=0.8505 rec=0.8273 spec=0.8552 f1=0.8387 | time=12.8s\n",
            "Epoch 103 | train_loss=0.2661 acc=0.9036 | val_loss=0.3503 acc=0.8481 | prec=0.8592 rec=0.8318 spec=0.8643 f1=0.8453 | time=12.8s\n",
            "Epoch 104 | train_loss=0.2624 acc=0.8973 | val_loss=0.3465 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=12.9s\n",
            "Epoch 105 | train_loss=0.2641 acc=0.9019 | val_loss=0.3439 acc=0.8390 | prec=0.8371 rec=0.8409 spec=0.8371 f1=0.8390 | time=12.8s\n",
            "Epoch 106 | train_loss=0.2682 acc=0.9087 | val_loss=0.3463 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=12.8s\n",
            "Epoch 107 | train_loss=0.2608 acc=0.9019 | val_loss=0.3446 acc=0.8458 | prec=0.8304 rec=0.8682 spec=0.8235 f1=0.8489 | time=12.8s\n",
            "Epoch 108 | train_loss=0.2572 acc=0.9013 | val_loss=0.3498 acc=0.8390 | prec=0.8433 rec=0.8318 spec=0.8462 f1=0.8375 | time=12.8s\n",
            "Epoch 109 | train_loss=0.2630 acc=0.9019 | val_loss=0.3538 acc=0.8390 | prec=0.8498 rec=0.8227 spec=0.8552 f1=0.8360 | time=12.8s\n",
            "Epoch 110 | train_loss=0.2590 acc=0.8973 | val_loss=0.3397 acc=0.8367 | prec=0.8333 rec=0.8409 spec=0.8326 f1=0.8371 | time=13.0s\n",
            "Epoch 111 | train_loss=0.2642 acc=0.9041 | val_loss=0.3417 acc=0.8390 | prec=0.8371 rec=0.8409 spec=0.8371 f1=0.8390 | time=12.8s\n",
            "Epoch 112 | train_loss=0.2558 acc=0.9104 | val_loss=0.3521 acc=0.8390 | prec=0.8565 rec=0.8136 spec=0.8643 f1=0.8345 | time=12.8s\n",
            "Epoch 113 | train_loss=0.2520 acc=0.9047 | val_loss=0.3483 acc=0.8390 | prec=0.8402 rec=0.8364 spec=0.8416 f1=0.8383 | time=12.9s\n",
            "Epoch 114 | train_loss=0.2470 acc=0.9047 | val_loss=0.3462 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=12.7s\n",
            "Epoch 115 | train_loss=0.2442 acc=0.9030 | val_loss=0.3498 acc=0.8413 | prec=0.8409 rec=0.8409 spec=0.8416 f1=0.8409 | time=12.7s\n",
            "Epoch 116 | train_loss=0.2426 acc=0.9132 | val_loss=0.3494 acc=0.8345 | prec=0.8387 rec=0.8273 spec=0.8416 f1=0.8330 | time=12.7s\n",
            "Epoch 117 | train_loss=0.2628 acc=0.9115 | val_loss=0.3529 acc=0.8299 | prec=0.8372 rec=0.8182 spec=0.8416 f1=0.8276 | time=12.9s\n",
            "Epoch 118 | train_loss=0.2861 acc=0.8815 | val_loss=0.3498 acc=0.8322 | prec=0.8380 rec=0.8227 spec=0.8416 f1=0.8303 | time=12.7s\n",
            "Epoch 119 | train_loss=0.2547 acc=0.8905 | val_loss=0.3456 acc=0.8345 | prec=0.8326 rec=0.8364 spec=0.8326 f1=0.8345 | time=12.7s\n",
            "Epoch 120 | train_loss=0.2571 acc=0.9058 | val_loss=0.3514 acc=0.8345 | prec=0.8451 rec=0.8182 spec=0.8507 f1=0.8314 | time=12.8s\n",
            "Epoch 121 | train_loss=0.2479 acc=0.9007 | val_loss=0.3478 acc=0.8458 | prec=0.8423 rec=0.8500 spec=0.8416 f1=0.8462 | time=12.7s\n",
            "Epoch 122 | train_loss=0.2696 acc=0.9058 | val_loss=0.3643 acc=0.8413 | prec=0.8606 rec=0.8136 spec=0.8688 f1=0.8364 | time=12.7s\n",
            "Epoch 123 | train_loss=0.2548 acc=0.9092 | val_loss=0.3510 acc=0.8322 | prec=0.8349 rec=0.8273 spec=0.8371 f1=0.8311 | time=12.8s\n",
            "Epoch 124 | train_loss=0.2445 acc=0.9041 | val_loss=0.3527 acc=0.8367 | prec=0.8491 rec=0.8182 spec=0.8552 f1=0.8333 | time=12.8s\n",
            "Epoch 125 | train_loss=0.2559 acc=0.8934 | val_loss=0.3469 acc=0.8413 | prec=0.8319 rec=0.8545 spec=0.8281 f1=0.8430 | time=12.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▃▆▆▆▇▆▆▆▇▇▆▇▇▇▇███▇▇█▇██▇▇▇▇▇▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▆█▇▇▇▆▇██▇█████▇██▇██▇▇▇█▇█▇▇▇▇</td></tr><tr><td>recall</td><td>████████▂▄▅▁▃▅▄▃▄▃▅▃▁▄▄▃▅▃▄▄▄▅▄▄▄▅▄▄▄▄▄▅</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▅▄▂▇▇▇▇▇██████▇█████▇███▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▂▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█████████████▇██</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▆▅▅▅▄▄▃▄▃▃▃▃▃▂▂▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▃▅▆▇▇▇▇▇▇▇█▇█▇███▇████████████████</td></tr><tr><td>val_loss</td><td>███████▅▄▃▃▃▃▃▃▂▂▂▂▂▁▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>125</td></tr><tr><td>f1_score</td><td>0.84305</td></tr><tr><td>precision</td><td>0.83186</td></tr><tr><td>recall</td><td>0.85455</td></tr><tr><td>specificity</td><td>0.82805</td></tr><tr><td>train_acc</td><td>0.89336</td></tr><tr><td>train_loss</td><td>0.25594</td></tr><tr><td>val_acc</td><td>0.84127</td></tr><tr><td>val_loss</td><td>0.34694</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/gi9hwke1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/gi9hwke1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_233226-gi9hwke1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-07 23:59:18,255] Trial 7 finished with values: [0.33969579743487494, 0.8367346938775511] and parameters: {'lr': 2.4341654897015225e-05, 'wd': 3.8228868493102075e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 8 hyperparams --> lr=1.86e-05, wd=3.88e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250507_235918-odhku913</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/odhku913' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/odhku913' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/odhku913</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 8]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.8093 acc=0.5048 | val_loss=0.7232 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7969 acc=0.5003 | val_loss=0.7139 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 003 | train_loss=0.7840 acc=0.4957 | val_loss=0.7069 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7798 acc=0.4963 | val_loss=0.7046 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 005 | train_loss=0.7653 acc=0.5162 | val_loss=0.7017 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7561 acc=0.4952 | val_loss=0.6948 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.7288 acc=0.5122 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7327 acc=0.4997 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.7239 acc=0.5026 | val_loss=0.6938 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 010 | train_loss=0.7336 acc=0.4799 | val_loss=0.6934 acc=0.4921 | prec=0.4916 rec=0.5318 spec=0.4525 f1=0.5109 | time=12.8s\n",
            "Epoch 011 | train_loss=0.7224 acc=0.5054 | val_loss=0.6941 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7312 acc=0.4895 | val_loss=0.6932 acc=0.4989 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.9s\n",
            "Epoch 013 | train_loss=0.7218 acc=0.5071 | val_loss=0.6931 acc=0.4989 | prec=0.3333 rec=0.0045 spec=0.9910 f1=0.0090 | time=12.9s\n",
            "Epoch 014 | train_loss=0.7244 acc=0.4980 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 015 | train_loss=0.7222 acc=0.4952 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 016 | train_loss=0.7123 acc=0.5184 | val_loss=0.6932 acc=0.5125 | prec=0.5080 rec=0.7182 spec=0.3077 f1=0.5951 | time=12.8s\n",
            "Epoch 017 | train_loss=0.7229 acc=0.4952 | val_loss=0.6942 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 018 | train_loss=0.7213 acc=0.4980 | val_loss=0.6932 acc=0.4966 | prec=0.4977 rec=0.9955 spec=0.0000 f1=0.6636 | time=12.7s\n",
            "Epoch 019 | train_loss=0.7143 acc=0.5020 | val_loss=0.6933 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 020 | train_loss=0.7099 acc=0.5156 | val_loss=0.6931 acc=0.4785 | prec=0.4857 rec=0.7727 spec=0.1855 f1=0.5965 | time=12.9s\n",
            "Epoch 021 | train_loss=0.7190 acc=0.4974 | val_loss=0.6927 acc=0.5079 | prec=0.5050 rec=0.6955 spec=0.3213 f1=0.5851 | time=12.7s\n",
            "Epoch 022 | train_loss=0.7127 acc=0.5128 | val_loss=0.6929 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 023 | train_loss=0.7168 acc=0.4787 | val_loss=0.6921 acc=0.5306 | prec=0.5359 rec=0.4409 spec=0.6199 f1=0.4838 | time=12.8s\n",
            "Epoch 024 | train_loss=0.7236 acc=0.4861 | val_loss=0.6912 acc=0.5329 | prec=0.8889 rec=0.0727 spec=0.9910 f1=0.1345 | time=12.8s\n",
            "Epoch 025 | train_loss=0.7079 acc=0.5020 | val_loss=0.6904 acc=0.5329 | prec=0.8182 rec=0.0818 spec=0.9819 f1=0.1488 | time=12.9s\n",
            "Epoch 026 | train_loss=0.7111 acc=0.5315 | val_loss=0.6884 acc=0.5102 | prec=1.0000 rec=0.0182 spec=1.0000 f1=0.0357 | time=12.8s\n",
            "Epoch 027 | train_loss=0.7009 acc=0.5224 | val_loss=0.6693 acc=0.5669 | prec=0.8919 rec=0.1500 spec=0.9819 f1=0.2568 | time=13.0s\n",
            "Epoch 028 | train_loss=0.6500 acc=0.6166 | val_loss=0.6009 acc=0.7302 | prec=0.8389 rec=0.5682 spec=0.8914 f1=0.6775 | time=12.8s\n",
            "Epoch 029 | train_loss=0.6272 acc=0.6472 | val_loss=0.5595 acc=0.7551 | prec=0.8500 rec=0.6182 spec=0.8914 f1=0.7158 | time=12.7s\n",
            "Epoch 030 | train_loss=0.5925 acc=0.6988 | val_loss=0.5320 acc=0.7732 | prec=0.7804 rec=0.7591 spec=0.7873 f1=0.7696 | time=12.9s\n",
            "Epoch 031 | train_loss=0.5741 acc=0.7028 | val_loss=0.5244 acc=0.7823 | prec=0.8370 rec=0.7000 spec=0.8643 f1=0.7624 | time=12.8s\n",
            "Epoch 032 | train_loss=0.5424 acc=0.7277 | val_loss=0.5139 acc=0.7868 | prec=0.8462 rec=0.7000 spec=0.8733 f1=0.7662 | time=12.9s\n",
            "Epoch 033 | train_loss=0.5479 acc=0.7311 | val_loss=0.4833 acc=0.7891 | prec=0.8223 rec=0.7364 spec=0.8416 f1=0.7770 | time=12.7s\n",
            "Epoch 034 | train_loss=0.5164 acc=0.7499 | val_loss=0.4942 acc=0.7937 | prec=0.8241 rec=0.7455 spec=0.8416 f1=0.7828 | time=12.8s\n",
            "Epoch 035 | train_loss=0.5166 acc=0.7499 | val_loss=0.4993 acc=0.7914 | prec=0.8441 rec=0.7136 spec=0.8688 f1=0.7734 | time=12.9s\n",
            "Epoch 036 | train_loss=0.5067 acc=0.7652 | val_loss=0.4921 acc=0.8027 | prec=0.8213 rec=0.7727 spec=0.8326 f1=0.7963 | time=12.9s\n",
            "Epoch 037 | train_loss=0.5117 acc=0.7623 | val_loss=0.4738 acc=0.8095 | prec=0.8269 rec=0.7818 spec=0.8371 f1=0.8037 | time=12.6s\n",
            "Epoch 038 | train_loss=0.4990 acc=0.7657 | val_loss=0.4873 acc=0.8095 | prec=0.8119 rec=0.8045 spec=0.8145 f1=0.8082 | time=12.8s\n",
            "Epoch 039 | train_loss=0.4990 acc=0.7686 | val_loss=0.4769 acc=0.8095 | prec=0.8469 rec=0.7545 spec=0.8643 f1=0.7981 | time=12.7s\n",
            "Epoch 040 | train_loss=0.4736 acc=0.7811 | val_loss=0.4776 acc=0.8050 | prec=0.8490 rec=0.7409 spec=0.8688 f1=0.7913 | time=12.8s\n",
            "Epoch 041 | train_loss=0.4740 acc=0.7873 | val_loss=0.4770 acc=0.8027 | prec=0.8595 rec=0.7227 spec=0.8824 f1=0.7852 | time=12.8s\n",
            "Epoch 042 | train_loss=0.4582 acc=0.7992 | val_loss=0.4614 acc=0.8073 | prec=0.8462 rec=0.7500 spec=0.8643 f1=0.7952 | time=12.8s\n",
            "Epoch 043 | train_loss=0.4591 acc=0.8083 | val_loss=0.4699 acc=0.8073 | prec=0.8534 rec=0.7409 spec=0.8733 f1=0.7932 | time=12.8s\n",
            "Epoch 044 | train_loss=0.4537 acc=0.8009 | val_loss=0.4728 acc=0.7937 | prec=0.8564 rec=0.7045 spec=0.8824 f1=0.7731 | time=12.8s\n",
            "Epoch 045 | train_loss=0.4489 acc=0.8020 | val_loss=0.4610 acc=0.8073 | prec=0.8571 rec=0.7364 spec=0.8778 f1=0.7922 | time=12.8s\n",
            "Epoch 046 | train_loss=0.4544 acc=0.8043 | val_loss=0.4504 acc=0.8163 | prec=0.8357 rec=0.7864 spec=0.8462 f1=0.8103 | time=12.8s\n",
            "Epoch 047 | train_loss=0.4432 acc=0.8009 | val_loss=0.4458 acc=0.8209 | prec=0.8219 rec=0.8182 spec=0.8235 f1=0.8200 | time=12.8s\n",
            "Epoch 048 | train_loss=0.4408 acc=0.8032 | val_loss=0.4370 acc=0.8231 | prec=0.8381 rec=0.8000 spec=0.8462 f1=0.8186 | time=12.7s\n",
            "Epoch 049 | train_loss=0.4240 acc=0.8174 | val_loss=0.4525 acc=0.8095 | prec=0.8696 rec=0.7273 spec=0.8914 f1=0.7921 | time=12.8s\n",
            "Epoch 050 | train_loss=0.4277 acc=0.8088 | val_loss=0.4315 acc=0.8186 | prec=0.8431 rec=0.7818 spec=0.8552 f1=0.8113 | time=12.7s\n",
            "Epoch 051 | train_loss=0.4241 acc=0.8287 | val_loss=0.4767 acc=0.7937 | prec=0.8817 rec=0.6773 spec=0.9095 f1=0.7661 | time=12.8s\n",
            "Epoch 052 | train_loss=0.4051 acc=0.8332 | val_loss=0.4372 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=12.7s\n",
            "Epoch 053 | train_loss=0.4067 acc=0.8298 | val_loss=0.4259 acc=0.8345 | prec=0.8551 rec=0.8045 spec=0.8643 f1=0.8290 | time=12.9s\n",
            "Epoch 054 | train_loss=0.3927 acc=0.8480 | val_loss=0.4181 acc=0.8367 | prec=0.8304 rec=0.8455 spec=0.8281 f1=0.8378 | time=12.8s\n",
            "Epoch 055 | train_loss=0.3908 acc=0.8406 | val_loss=0.4276 acc=0.8299 | prec=0.8718 rec=0.7727 spec=0.8869 f1=0.8193 | time=12.8s\n",
            "Epoch 056 | train_loss=0.3949 acc=0.8349 | val_loss=0.4133 acc=0.8413 | prec=0.8713 rec=0.8000 spec=0.8824 f1=0.8341 | time=12.8s\n",
            "Epoch 057 | train_loss=0.3822 acc=0.8412 | val_loss=0.4334 acc=0.8209 | prec=0.8770 rec=0.7455 spec=0.8959 f1=0.8059 | time=12.7s\n",
            "Epoch 058 | train_loss=0.3792 acc=0.8400 | val_loss=0.4183 acc=0.8163 | prec=0.7814 rec=0.8773 spec=0.7557 f1=0.8266 | time=12.8s\n",
            "Epoch 059 | train_loss=0.3833 acc=0.8389 | val_loss=0.4291 acc=0.8231 | prec=0.8817 rec=0.7455 spec=0.9005 f1=0.8079 | time=12.7s\n",
            "Epoch 060 | train_loss=0.4093 acc=0.8281 | val_loss=0.4065 acc=0.8277 | prec=0.8051 rec=0.8636 spec=0.7919 f1=0.8333 | time=12.8s\n",
            "Epoch 061 | train_loss=0.3693 acc=0.8423 | val_loss=0.3979 acc=0.8435 | prec=0.8416 rec=0.8455 spec=0.8416 f1=0.8435 | time=12.7s\n",
            "Epoch 062 | train_loss=0.3521 acc=0.8644 | val_loss=0.4146 acc=0.8254 | prec=0.8824 rec=0.7500 spec=0.9005 f1=0.8108 | time=12.7s\n",
            "Epoch 063 | train_loss=0.3683 acc=0.8559 | val_loss=0.4405 acc=0.8073 | prec=0.8857 rec=0.7045 spec=0.9095 f1=0.7848 | time=12.7s\n",
            "Epoch 064 | train_loss=0.3633 acc=0.8537 | val_loss=0.4123 acc=0.8186 | prec=0.8889 rec=0.7273 spec=0.9095 f1=0.8000 | time=12.7s\n",
            "Epoch 065 | train_loss=0.3600 acc=0.8582 | val_loss=0.3792 acc=0.8481 | prec=0.8626 rec=0.8273 spec=0.8688 f1=0.8445 | time=12.8s\n",
            "Epoch 066 | train_loss=0.3689 acc=0.8542 | val_loss=0.3923 acc=0.8435 | prec=0.8794 rec=0.7955 spec=0.8914 f1=0.8353 | time=12.7s\n",
            "Epoch 067 | train_loss=0.3542 acc=0.8486 | val_loss=0.3830 acc=0.8503 | prec=0.8812 rec=0.8091 spec=0.8914 f1=0.8436 | time=12.8s\n",
            "Epoch 068 | train_loss=0.3434 acc=0.8610 | val_loss=0.3787 acc=0.8549 | prec=0.8750 rec=0.8273 spec=0.8824 f1=0.8505 | time=12.9s\n",
            "Epoch 069 | train_loss=0.3236 acc=0.8815 | val_loss=0.3798 acc=0.8549 | prec=0.8786 rec=0.8227 spec=0.8869 f1=0.8498 | time=12.7s\n",
            "Epoch 070 | train_loss=0.3305 acc=0.8815 | val_loss=0.3833 acc=0.8413 | prec=0.8827 rec=0.7864 spec=0.8959 f1=0.8317 | time=12.8s\n",
            "Epoch 071 | train_loss=0.3423 acc=0.8684 | val_loss=0.4224 acc=0.8141 | prec=0.8833 rec=0.7227 spec=0.9050 f1=0.7950 | time=12.8s\n",
            "Epoch 072 | train_loss=0.3112 acc=0.8911 | val_loss=0.3859 acc=0.8345 | prec=0.8769 rec=0.7773 spec=0.8914 f1=0.8241 | time=12.9s\n",
            "Epoch 073 | train_loss=0.3206 acc=0.8712 | val_loss=0.3663 acc=0.8549 | prec=0.8750 rec=0.8273 spec=0.8824 f1=0.8505 | time=12.7s\n",
            "Epoch 074 | train_loss=0.3125 acc=0.8712 | val_loss=0.3725 acc=0.8458 | prec=0.8762 rec=0.8045 spec=0.8869 f1=0.8389 | time=12.7s\n",
            "Epoch 075 | train_loss=0.3089 acc=0.8832 | val_loss=0.3874 acc=0.8367 | prec=0.8776 rec=0.7818 spec=0.8914 f1=0.8269 | time=12.8s\n",
            "Epoch 076 | train_loss=0.3210 acc=0.8661 | val_loss=0.3947 acc=0.8322 | prec=0.8883 rec=0.7591 spec=0.9050 f1=0.8186 | time=12.8s\n",
            "Epoch 077 | train_loss=0.3029 acc=0.8945 | val_loss=0.3895 acc=0.8345 | prec=0.8808 rec=0.7727 spec=0.8959 f1=0.8232 | time=12.9s\n",
            "Epoch 078 | train_loss=0.2924 acc=0.8928 | val_loss=0.3609 acc=0.8503 | prec=0.8702 rec=0.8227 spec=0.8778 f1=0.8458 | time=12.8s\n",
            "Epoch 079 | train_loss=0.3360 acc=0.8769 | val_loss=0.3813 acc=0.8322 | prec=0.8687 rec=0.7818 spec=0.8824 f1=0.8230 | time=12.7s\n",
            "Epoch 080 | train_loss=0.2998 acc=0.8973 | val_loss=0.3800 acc=0.8367 | prec=0.8663 rec=0.7955 spec=0.8778 f1=0.8294 | time=12.8s\n",
            "Epoch 081 | train_loss=0.3287 acc=0.8826 | val_loss=0.3926 acc=0.8254 | prec=0.8743 rec=0.7591 spec=0.8914 f1=0.8127 | time=12.8s\n",
            "Epoch 082 | train_loss=0.2905 acc=0.8877 | val_loss=0.3569 acc=0.8503 | prec=0.8598 rec=0.8364 spec=0.8643 f1=0.8479 | time=12.8s\n",
            "Epoch 083 | train_loss=0.2829 acc=0.9024 | val_loss=0.3565 acc=0.8458 | prec=0.8519 rec=0.8364 spec=0.8552 f1=0.8440 | time=12.8s\n",
            "Epoch 084 | train_loss=0.2953 acc=0.8763 | val_loss=0.3709 acc=0.8322 | prec=0.8614 rec=0.7909 spec=0.8733 f1=0.8246 | time=12.7s\n",
            "Epoch 085 | train_loss=0.2836 acc=0.8968 | val_loss=0.3767 acc=0.8367 | prec=0.8663 rec=0.7955 spec=0.8778 f1=0.8294 | time=12.8s\n",
            "Epoch 086 | train_loss=0.2893 acc=0.8871 | val_loss=0.3701 acc=0.8299 | prec=0.8643 rec=0.7818 spec=0.8778 f1=0.8210 | time=12.7s\n",
            "Epoch 087 | train_loss=0.2845 acc=0.8985 | val_loss=0.3825 acc=0.8367 | prec=0.8776 rec=0.7818 spec=0.8914 f1=0.8269 | time=12.8s\n",
            "Epoch 088 | train_loss=0.2841 acc=0.8815 | val_loss=0.3922 acc=0.8277 | prec=0.8750 rec=0.7636 spec=0.8914 f1=0.8155 | time=12.8s\n",
            "Epoch 089 | train_loss=0.2811 acc=0.8956 | val_loss=0.3804 acc=0.8277 | prec=0.8600 rec=0.7818 spec=0.8733 f1=0.8190 | time=12.7s\n",
            "Epoch 090 | train_loss=0.2788 acc=0.8951 | val_loss=0.3645 acc=0.8367 | prec=0.8524 rec=0.8136 spec=0.8597 f1=0.8326 | time=12.7s\n",
            "Epoch 091 | train_loss=0.2938 acc=0.8803 | val_loss=0.3637 acc=0.8390 | prec=0.8565 rec=0.8136 spec=0.8643 f1=0.8345 | time=12.8s\n",
            "Epoch 092 | train_loss=0.2668 acc=0.9115 | val_loss=0.4008 acc=0.8254 | prec=0.8783 rec=0.7545 spec=0.8959 f1=0.8117 | time=12.8s\n",
            "Epoch 093 | train_loss=0.2687 acc=0.8962 | val_loss=0.3707 acc=0.8345 | prec=0.8657 rec=0.7909 spec=0.8778 f1=0.8266 | time=12.8s\n",
            "Epoch 094 | train_loss=0.2769 acc=0.8939 | val_loss=0.4166 acc=0.8141 | prec=0.8791 rec=0.7273 spec=0.9005 f1=0.7960 | time=12.8s\n",
            "Epoch 095 | train_loss=0.2653 acc=0.8962 | val_loss=0.3761 acc=0.8322 | prec=0.8578 rec=0.7955 spec=0.8688 f1=0.8255 | time=12.8s\n",
            "Epoch 096 | train_loss=0.2744 acc=0.8951 | val_loss=0.3960 acc=0.8163 | prec=0.8601 rec=0.7545 spec=0.8778 f1=0.8039 | time=12.8s\n",
            "Epoch 097 | train_loss=0.2777 acc=0.9007 | val_loss=0.3944 acc=0.8277 | prec=0.8636 rec=0.7773 spec=0.8778 f1=0.8182 | time=12.8s\n",
            "Epoch 098 | train_loss=0.2730 acc=0.8917 | val_loss=0.3909 acc=0.8390 | prec=0.8634 rec=0.8045 spec=0.8733 f1=0.8329 | time=12.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▆▆▆▅▂▂▁▃▇█████▇█████▇████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▅▅███▇▇▇▇██▇███▇██████████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▅▁▁▁▁▇▅▂▁▆▇▇▇▇▇█▇▇▆▇███▇▇██▇▇▇▇▇▇▇▇</td></tr><tr><td>specificity</td><td>████▂██▁▁█▇▆▆▇▆▇▇▇▇▇▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▁▁▁▁▂▂▂▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇███▇███████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▆▅▄▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▂▁▃▆▆▇▇▇▇▇▇▇▇███▇█▇█████████████</td></tr><tr><td>val_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▇▆▄▄▃▃▃▃▃▂▂▂▂▂▂▁▂▁▂▁▁▁▁▂▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>98</td></tr><tr><td>f1_score</td><td>0.83294</td></tr><tr><td>precision</td><td>0.86341</td></tr><tr><td>recall</td><td>0.80455</td></tr><tr><td>specificity</td><td>0.8733</td></tr><tr><td>train_acc</td><td>0.89166</td></tr><tr><td>train_loss</td><td>0.27301</td></tr><tr><td>val_acc</td><td>0.839</td></tr><tr><td>val_loss</td><td>0.39088</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/odhku913' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/odhku913</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250507_235918-odhku913/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 00:20:14,798] Trial 8 finished with values: [0.3564561243568148, 0.8458049886621315] and parameters: {'lr': 1.8635838587368996e-05, 'wd': 3.883467417457848e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 9 hyperparams --> lr=3.22e-05, wd=6.65e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_002014-ncf2ru78</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/ncf2ru78' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/ncf2ru78' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/ncf2ru78</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 9]  TRAIN n=1763 (AD=881, CN=882) | VAL n=441 (AD=221, CN=220)\n",
            "Epoch 001 | train_loss=0.7172 acc=0.5071 | val_loss=0.6949 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 002 | train_loss=0.7255 acc=0.4884 | val_loss=0.6936 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.9s\n",
            "Epoch 003 | train_loss=0.7231 acc=0.4918 | val_loss=0.6937 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 004 | train_loss=0.7081 acc=0.5071 | val_loss=0.6928 acc=0.4989 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.7s\n",
            "Epoch 005 | train_loss=0.7113 acc=0.4827 | val_loss=0.6934 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 006 | train_loss=0.7056 acc=0.5116 | val_loss=0.6935 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 007 | train_loss=0.6986 acc=0.5082 | val_loss=0.6940 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.7s\n",
            "Epoch 008 | train_loss=0.7039 acc=0.5054 | val_loss=0.6930 acc=0.4989 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.8s\n",
            "Epoch 009 | train_loss=0.6965 acc=0.5184 | val_loss=0.6931 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.6s\n",
            "Epoch 010 | train_loss=0.7085 acc=0.5009 | val_loss=0.6927 acc=0.5465 | prec=0.5424 rec=0.5818 spec=0.5113 f1=0.5614 | time=13.0s\n",
            "Epoch 011 | train_loss=0.7057 acc=0.5111 | val_loss=0.6927 acc=0.5215 | prec=0.5556 rec=0.2045 spec=0.8371 f1=0.2990 | time=12.8s\n",
            "Epoch 012 | train_loss=0.7077 acc=0.5048 | val_loss=0.6928 acc=0.4989 | prec=0.0000 rec=0.0000 spec=0.9955 f1=0.0000 | time=12.6s\n",
            "Epoch 013 | train_loss=0.7030 acc=0.5213 | val_loss=0.6925 acc=0.5306 | prec=0.6585 rec=0.1227 spec=0.9367 f1=0.2069 | time=12.8s\n",
            "Epoch 014 | train_loss=0.7019 acc=0.5116 | val_loss=0.6930 acc=0.5011 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=12.8s\n",
            "Epoch 015 | train_loss=0.7031 acc=0.5009 | val_loss=0.6916 acc=0.5329 | prec=0.5178 rec=0.9273 spec=0.1403 f1=0.6645 | time=12.7s\n",
            "Epoch 016 | train_loss=0.7043 acc=0.4997 | val_loss=0.6906 acc=0.4989 | prec=0.4989 rec=1.0000 spec=0.0000 f1=0.6657 | time=12.7s\n",
            "Epoch 017 | train_loss=0.6984 acc=0.5037 | val_loss=0.6897 acc=0.5057 | prec=0.5023 rec=0.9864 spec=0.0271 f1=0.6656 | time=12.8s\n",
            "Epoch 018 | train_loss=0.7046 acc=0.5071 | val_loss=0.6872 acc=0.5896 | prec=0.6560 rec=0.3727 spec=0.8054 f1=0.4754 | time=12.8s\n",
            "Epoch 019 | train_loss=0.6842 acc=0.5604 | val_loss=0.6626 acc=0.6236 | prec=0.5767 rec=0.9227 spec=0.3258 f1=0.7098 | time=12.7s\n",
            "Epoch 020 | train_loss=0.6672 acc=0.5995 | val_loss=0.6275 acc=0.7166 | prec=0.7189 rec=0.7091 spec=0.7240 f1=0.7140 | time=12.7s\n",
            "Epoch 021 | train_loss=0.6169 acc=0.6710 | val_loss=0.5441 acc=0.7687 | prec=0.8073 rec=0.7045 spec=0.8326 f1=0.7524 | time=12.8s\n",
            "Epoch 022 | train_loss=0.5797 acc=0.7124 | val_loss=0.5294 acc=0.7755 | prec=0.7689 rec=0.7864 spec=0.7647 f1=0.7775 | time=12.8s\n",
            "Epoch 023 | train_loss=0.5676 acc=0.7391 | val_loss=0.5144 acc=0.7959 | prec=0.7902 rec=0.8045 spec=0.7873 f1=0.7973 | time=12.8s\n",
            "Epoch 024 | train_loss=0.5287 acc=0.7516 | val_loss=0.5152 acc=0.8027 | prec=0.7608 rec=0.8818 spec=0.7240 f1=0.8168 | time=13.0s\n",
            "Epoch 025 | train_loss=0.5193 acc=0.7674 | val_loss=0.5292 acc=0.7846 | prec=0.7163 rec=0.9409 spec=0.6290 f1=0.8134 | time=12.8s\n",
            "Epoch 026 | train_loss=0.5034 acc=0.7811 | val_loss=0.4987 acc=0.8141 | prec=0.8026 rec=0.8318 spec=0.7964 f1=0.8170 | time=12.9s\n",
            "Epoch 027 | train_loss=0.4960 acc=0.7907 | val_loss=0.5010 acc=0.8118 | prec=0.7625 rec=0.9045 spec=0.7195 f1=0.8274 | time=12.8s\n",
            "Epoch 028 | train_loss=0.4745 acc=0.8049 | val_loss=0.4813 acc=0.8095 | prec=0.8148 rec=0.8000 spec=0.8190 f1=0.8073 | time=12.8s\n",
            "Epoch 029 | train_loss=0.4680 acc=0.8157 | val_loss=0.4891 acc=0.7982 | prec=0.8394 rec=0.7364 spec=0.8597 f1=0.7845 | time=12.8s\n",
            "Epoch 030 | train_loss=0.4511 acc=0.8230 | val_loss=0.4699 acc=0.8163 | prec=0.7837 rec=0.8727 spec=0.7602 f1=0.8258 | time=12.9s\n",
            "Epoch 031 | train_loss=0.4379 acc=0.8412 | val_loss=0.4595 acc=0.8118 | prec=0.7940 rec=0.8409 spec=0.7828 f1=0.8168 | time=12.8s\n",
            "Epoch 032 | train_loss=0.4469 acc=0.8196 | val_loss=0.4653 acc=0.8141 | prec=0.8255 rec=0.7955 spec=0.8326 f1=0.8102 | time=12.8s\n",
            "Epoch 033 | train_loss=0.4311 acc=0.8383 | val_loss=0.4598 acc=0.8163 | prec=0.8174 rec=0.8136 spec=0.8190 f1=0.8155 | time=12.7s\n",
            "Epoch 034 | train_loss=0.4266 acc=0.8349 | val_loss=0.4493 acc=0.8209 | prec=0.8341 rec=0.8000 spec=0.8416 f1=0.8167 | time=12.8s\n",
            "Epoch 035 | train_loss=0.4063 acc=0.8616 | val_loss=0.4515 acc=0.8163 | prec=0.7983 rec=0.8455 spec=0.7873 f1=0.8212 | time=12.8s\n",
            "Epoch 036 | train_loss=0.3913 acc=0.8616 | val_loss=0.4665 acc=0.7937 | prec=0.8644 rec=0.6955 spec=0.8914 f1=0.7708 | time=12.9s\n",
            "Epoch 037 | train_loss=0.3876 acc=0.8503 | val_loss=0.4498 acc=0.8095 | prec=0.7720 rec=0.8773 spec=0.7421 f1=0.8213 | time=12.8s\n",
            "Epoch 038 | train_loss=0.3813 acc=0.8690 | val_loss=0.4378 acc=0.8163 | prec=0.7791 rec=0.8818 spec=0.7511 f1=0.8273 | time=12.9s\n",
            "Epoch 039 | train_loss=0.3679 acc=0.8900 | val_loss=0.4424 acc=0.8050 | prec=0.7617 rec=0.8864 spec=0.7240 f1=0.8193 | time=12.8s\n",
            "Epoch 040 | train_loss=0.3569 acc=0.8837 | val_loss=0.4474 acc=0.8095 | prec=0.7615 rec=0.9000 spec=0.7195 f1=0.8250 | time=12.7s\n",
            "Epoch 041 | train_loss=0.3487 acc=0.8934 | val_loss=0.4161 acc=0.8277 | prec=0.8564 rec=0.7864 spec=0.8688 f1=0.8199 | time=12.8s\n",
            "Epoch 042 | train_loss=0.3499 acc=0.8775 | val_loss=0.4213 acc=0.8322 | prec=0.8510 rec=0.8045 spec=0.8597 f1=0.8271 | time=12.8s\n",
            "Epoch 043 | train_loss=0.3367 acc=0.8803 | val_loss=0.4286 acc=0.8141 | prec=0.8255 rec=0.7955 spec=0.8326 f1=0.8102 | time=12.8s\n",
            "Epoch 044 | train_loss=0.3412 acc=0.8837 | val_loss=0.4644 acc=0.7868 | prec=0.7203 rec=0.9364 spec=0.6380 f1=0.8142 | time=12.8s\n",
            "Epoch 045 | train_loss=0.3426 acc=0.8673 | val_loss=0.4164 acc=0.8118 | prec=0.7940 rec=0.8409 spec=0.7828 f1=0.8168 | time=12.7s\n",
            "Epoch 046 | train_loss=0.3545 acc=0.8701 | val_loss=0.4133 acc=0.8209 | prec=0.8133 rec=0.8318 spec=0.8100 f1=0.8225 | time=12.8s\n",
            "Epoch 047 | train_loss=0.3241 acc=0.8928 | val_loss=0.4210 acc=0.8141 | prec=0.8485 rec=0.7636 spec=0.8643 f1=0.8038 | time=12.9s\n",
            "Epoch 048 | train_loss=0.3305 acc=0.8832 | val_loss=0.4258 acc=0.8118 | prec=0.8341 rec=0.7773 spec=0.8462 f1=0.8047 | time=13.0s\n",
            "Epoch 049 | train_loss=0.3111 acc=0.8917 | val_loss=0.4182 acc=0.8073 | prec=0.8140 rec=0.7955 spec=0.8190 f1=0.8046 | time=12.7s\n",
            "Epoch 050 | train_loss=0.3022 acc=0.8945 | val_loss=0.4182 acc=0.7982 | prec=0.8227 rec=0.7591 spec=0.8371 f1=0.7896 | time=12.7s\n",
            "Epoch 051 | train_loss=0.3058 acc=0.9030 | val_loss=0.4230 acc=0.8050 | prec=0.8490 rec=0.7409 spec=0.8688 f1=0.7913 | time=12.8s\n",
            "Epoch 052 | train_loss=0.3027 acc=0.8962 | val_loss=0.4123 acc=0.8073 | prec=0.8111 rec=0.8000 spec=0.8145 f1=0.8055 | time=12.7s\n",
            "Epoch 053 | train_loss=0.2939 acc=0.9002 | val_loss=0.4199 acc=0.8141 | prec=0.8710 rec=0.7364 spec=0.8914 f1=0.7980 | time=12.8s\n",
            "Epoch 054 | train_loss=0.2696 acc=0.9075 | val_loss=0.4018 acc=0.8163 | prec=0.8062 rec=0.8318 spec=0.8009 f1=0.8188 | time=12.8s\n",
            "Epoch 055 | train_loss=0.2629 acc=0.9178 | val_loss=0.4087 acc=0.8095 | prec=0.8091 rec=0.8091 spec=0.8100 f1=0.8091 | time=12.8s\n",
            "Epoch 056 | train_loss=0.2575 acc=0.9126 | val_loss=0.3996 acc=0.8095 | prec=0.8148 rec=0.8000 spec=0.8190 f1=0.8073 | time=12.7s\n",
            "Epoch 057 | train_loss=0.2562 acc=0.9087 | val_loss=0.4056 acc=0.8118 | prec=0.8216 rec=0.7955 spec=0.8281 f1=0.8083 | time=12.7s\n",
            "Epoch 058 | train_loss=0.2393 acc=0.9195 | val_loss=0.4121 acc=0.8050 | prec=0.8045 rec=0.8045 spec=0.8054 f1=0.8045 | time=12.8s\n",
            "Epoch 059 | train_loss=0.2721 acc=0.9109 | val_loss=0.4006 acc=0.8231 | prec=0.8198 rec=0.8273 spec=0.8190 f1=0.8235 | time=12.8s\n",
            "Epoch 060 | train_loss=0.2543 acc=0.9257 | val_loss=0.4193 acc=0.8073 | prec=0.8325 rec=0.7682 spec=0.8462 f1=0.7991 | time=12.6s\n",
            "Epoch 061 | train_loss=0.2494 acc=0.9104 | val_loss=0.4438 acc=0.8141 | prec=0.8594 rec=0.7500 spec=0.8778 f1=0.8010 | time=12.8s\n",
            "Epoch 062 | train_loss=0.2462 acc=0.8996 | val_loss=0.4114 acc=0.8050 | prec=0.7839 rec=0.8409 spec=0.7692 f1=0.8114 | time=12.8s\n",
            "Epoch 063 | train_loss=0.2378 acc=0.9178 | val_loss=0.4258 acc=0.8186 | prec=0.8763 rec=0.7409 spec=0.8959 f1=0.8030 | time=12.7s\n",
            "Epoch 064 | train_loss=0.2261 acc=0.9251 | val_loss=0.4146 acc=0.8027 | prec=0.7956 rec=0.8136 spec=0.7919 f1=0.8045 | time=12.7s\n",
            "Epoch 065 | train_loss=0.2339 acc=0.9138 | val_loss=0.4231 acc=0.7868 | prec=0.7763 rec=0.8045 spec=0.7692 f1=0.7902 | time=12.7s\n",
            "Epoch 066 | train_loss=0.2176 acc=0.9195 | val_loss=0.4056 acc=0.8141 | prec=0.8350 rec=0.7818 spec=0.8462 f1=0.8075 | time=12.8s\n",
            "Epoch 067 | train_loss=0.2136 acc=0.9280 | val_loss=0.4375 acc=0.8141 | prec=0.8710 rec=0.7364 spec=0.8914 f1=0.7980 | time=13.0s\n",
            "Epoch 068 | train_loss=0.2149 acc=0.9183 | val_loss=0.4185 acc=0.8027 | prec=0.7879 rec=0.8273 spec=0.7783 f1=0.8071 | time=12.9s\n",
            "Epoch 069 | train_loss=0.2103 acc=0.9353 | val_loss=0.4277 acc=0.8141 | prec=0.8194 rec=0.8045 spec=0.8235 f1=0.8119 | time=12.7s\n",
            "Epoch 070 | train_loss=0.2040 acc=0.9240 | val_loss=0.4464 acc=0.8095 | prec=0.8656 rec=0.7318 spec=0.8869 f1=0.7931 | time=12.8s\n",
            "Epoch 071 | train_loss=0.2182 acc=0.9246 | val_loss=0.4349 acc=0.8095 | prec=0.8063 rec=0.8136 spec=0.8054 f1=0.8100 | time=12.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▃▁▇▇▅▇▇▇██████████████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▅▅▁▆▁▅▆▆▇▇▇▇██▇▇█▇▇█████▇▇▇█▇██▇█▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▂▁▂█▄▇▆▆▇█▇▇▇▇▇▇▇▆▇▇▇▇▇▆▆▆▇▆▇▆▆▇▆▇</td></tr><tr><td>specificity</td><td>██████▅▇██▁▁▃▆▇▇▆▇▇▆▇▇▆▇▆▇▇▇▇▇▇▇▇▇▆▇▇▆▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▂▁▁▁▂▃▄▅▅▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>███████████▇▇▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▂▁▁▂▃▆▇▇▇▇██████▇████▇▇██▇█▇█▇██▇█</td></tr><tr><td>val_loss</td><td>███████████▆▄▄▄▃▂▃▂▂▂▂▂▃▁▁▂▁▁▁▁▁▁▁▁▁▁▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>71</td></tr><tr><td>f1_score</td><td>0.80995</td></tr><tr><td>precision</td><td>0.80631</td></tr><tr><td>recall</td><td>0.81364</td></tr><tr><td>specificity</td><td>0.80543</td></tr><tr><td>train_acc</td><td>0.92456</td></tr><tr><td>train_loss</td><td>0.21824</td></tr><tr><td>val_acc</td><td>0.80952</td></tr><tr><td>val_loss</td><td>0.43492</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/ncf2ru78' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3/runs/ncf2ru78</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_002014-ncf2ru78/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 00:35:25,210] Trial 9 finished with values: [0.39963088078158243, 0.8095238095238095] and parameters: {'lr': 3.219382221806734e-05, 'wd': 6.646893444723269e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Selected Trial #1 ===\n",
            " val_loss=0.3126, val_acc=0.8934, params={'lr': 7.26823423077802e-05, 'wd': 9.325987714179456e-05}, best_epoch=66, ckpt=ckpts/trial1_optimal_AD_CN_1.pth\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=272  A(D)=146, C(N)=126\n",
            " Accuracy=0.8493 Sensitivity=0.8651 Specificity=0.8356 F1=0.8417\n",
            "\n",
            "-- test_cross -- total=626  A(D)=319, C(N)=307\n",
            " Accuracy=0.6901 Sensitivity=0.7231 Specificity=0.6583 F1=0.6959\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### === Final Evaluation on Test Sets (Full Train) ===\n",
        "- test_within -- total=272  A(D)=146, C(N)=126\n",
        " Accuracy=0.8272 Sensitivity=0.9206 Specificity=0.7466 F1=0.8315\n",
        "\n",
        "- test_cross -- total=626  A(D)=319, C(N)=307\n",
        " Accuracy=0.6534 Sensitivity=0.8306 Specificity=0.4828 F1=0.7015\n",
        "\n",
        " === Final Evaluation on Test Sets ===\n",
        "- test_within -- total=272  A(D)=146, C(N)=126\n",
        " Loss=0.3395 Acc=0.8676 Sens=0.8571 Spec=0.8767 F1=0.8571\n",
        "\n",
        "- test_cross -- total=626  A(D)=319, C(N)=307\n",
        " Loss=0.6494 Acc=0.6757 Sens=0.6612 Spec=0.6897 F1=0.6667\n",
        "\n",
        "#### === Final Evaluation on Test Sets (without Full Train + use the best trial instead of early stopping epoch model)===\n",
        "- test_within -- total=272  A(D)=146, C(N)=126\n",
        " Accuracy=0.8493 Sensitivity=0.8651 Specificity=0.8356 F1=0.8417\n",
        "\n",
        "- test_cross -- total=626  A(D)=319, C(N)=307\n",
        " Accuracy=0.6901 Sensitivity=0.7231 Specificity=0.6583 F1=0.6959"
      ],
      "metadata": {
        "id": "iOdXTZC_BJeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_with_val_then_full.py\n",
        "import os, json, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── 1) Config & reproducibility ───────────────────────────────\n",
        "SEED        = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE   = 32\n",
        "NUM_WORKERS  = 4\n",
        "MAX_EPOCHS   = 150     # max for search\n",
        "ES_PATIENCE  = 15     # early-stop patience\n",
        "PCT_START    = 0.2\n",
        "\n",
        "# hyperparameters you’ve tuned\n",
        "LR           = 7.268e-5\n",
        "WD           = 9.326e-5\n",
        "\n",
        "# ─── 2) Dataset wrapper ────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── 3) Load & prepare metadata ───────────────────────────────\n",
        "with open(Path(DATA_DIR)/LABEL_FILE,'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "# filter A vs F\n",
        "class0, class1 = 'A','C'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "train_meta       = [d for d in train_meta       if d['label'] in label_map]\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in label_map]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in label_map]\n",
        "\n",
        "# balance train by down-sampling\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "k     = min(len(data0), len(data1))\n",
        "balanced_meta = random.sample(data0, k) + random.sample(data1, k)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# convert to ints\n",
        "for d in balanced_meta:       d['label'] = label_map[d['label']]\n",
        "for d in test_within_meta:    d['label'] = label_map[d['label']]\n",
        "for d in test_cross_meta:     d['label'] = label_map[d['label']]\n",
        "\n",
        "print(f\"\\n[Balanced TRAIN] total={len(balanced_meta)} (each class={k})\")\n",
        "\n",
        "# build full balanced dataset\n",
        "full_raw    = EEGDataset(DATA_DIR, balanced_meta)\n",
        "full_ds     = BinaryEEGDataset(full_raw, balanced_meta)\n",
        "labels_full = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── 4) Split train/val for epoch search ──────────────────────\n",
        "idx            = np.arange(len(full_ds))\n",
        "tr_idx, va_idx = train_test_split(\n",
        "    idx, test_size=0.2, stratify=labels_full, random_state=SEED\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    Subset(full_ds, tr_idx), batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  num_workers=NUM_WORKERS\n",
        ")\n",
        "val_loader   = DataLoader(\n",
        "    Subset(full_ds, va_idx), batch_size=BATCH_SIZE,\n",
        "    shuffle=False, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# ─── 5) Model builder & eval fn ───────────────────────────────\n",
        "def build_model():\n",
        "    input_len = full_ds[0][0].shape[-1]\n",
        "    return EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "def evaluate(loader, model, criterion):\n",
        "    model.eval()\n",
        "    loss_sum = correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y    = X.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model(X)\n",
        "            loss_sum += criterion(logits,y).item()*y.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "# ─── 6) Phase 1: find best_epoch with early stopping ──────────\n",
        "best_val_loss = float('inf')\n",
        "best_epoch    = 0\n",
        "es_count      = 0\n",
        "\n",
        "model     = build_model()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR, epochs=MAX_EPOCHS,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=PCT_START, anneal_strategy='cos',\n",
        "    cycle_momentum=False\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\nSearching best epoch using validation set...\")\n",
        "for epoch in range(1, MAX_EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    # ── train ──\n",
        "    model.train()\n",
        "    tr_loss_sum = tr_correct = tr_total = 0\n",
        "    for X,y in train_loader:\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss   = criterion(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tr_loss_sum  += loss.item()*y.size(0)\n",
        "        preds        = logits.argmax(1)\n",
        "        tr_correct  += (preds==y).sum().item()\n",
        "        tr_total    += y.size(0)\n",
        "\n",
        "    train_loss = tr_loss_sum / tr_total\n",
        "    train_acc  = tr_correct / tr_total\n",
        "\n",
        "    # ── validate ──\n",
        "    val_loss, val_acc = evaluate(val_loader, model, criterion)\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
        "        f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | \"\n",
        "        f\"time={elapsed:.1f}s\"\n",
        "    )\n",
        "\n",
        "    # check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch    = epoch\n",
        "        es_count      = 0\n",
        "    else:\n",
        "        es_count += 1\n",
        "        if es_count >= ES_PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {ES_PATIENCE} epochs)\")\n",
        "            break\n",
        "\n",
        "print(f\"\\n>> Best epoch by validation: {best_epoch}\")\n",
        "\n",
        "# ─── 7) Phase 2: retrain on full balanced set ─────────────────\n",
        "print(f\"\\nRetraining from scratch on FULL set for {best_epoch} epochs...\")\n",
        "model2     = build_model()\n",
        "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler2 = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer2, max_lr=LR,\n",
        "    epochs=best_epoch,\n",
        "    steps_per_epoch=len(DataLoader(full_ds, batch_size=BATCH_SIZE)),\n",
        "    pct_start=PCT_START, anneal_strategy='cos',\n",
        "    cycle_momentum=False\n",
        ")\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, best_epoch+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss_sum = tr_correct = tr_total = 0\n",
        "    model2.train()\n",
        "    for X,y in DataLoader(full_ds, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=NUM_WORKERS):\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer2.zero_grad()\n",
        "        logits = model2(X)\n",
        "        loss   = criterion2(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        scheduler2.step()\n",
        "\n",
        "        tr_loss_sum  += loss.item()*y.size(0)\n",
        "        preds        = logits.argmax(1)\n",
        "        tr_correct  += (preds==y).sum().item()\n",
        "        tr_total    += y.size(0)\n",
        "\n",
        "    avg_loss = tr_loss_sum / tr_total\n",
        "    acc      = tr_correct / tr_total\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={avg_loss:.4f} acc={acc:.4f} time={time.time()-t0:.1f}s\")\n",
        "\n",
        "# ─── 8) Final Evaluation on Test Sets ─────────────────────────\n",
        "def full_evaluate(metas, loader):\n",
        "    model2.eval()\n",
        "    loss_sum = correct = total = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model2(X)\n",
        "            loss_sum += criterion2(logits,y).item()*y.size(0)\n",
        "            preds    = logits.argmax(1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(all_preds)\n",
        "    labs  = np.concatenate(all_labels)\n",
        "    tn,fp,fn,tp = confusion_matrix(labs,preds,labels=[0,1]).ravel()\n",
        "    return {\n",
        "        \"loss\": loss_sum/total,\n",
        "        \"acc\": correct/total,\n",
        "        \"sensitivity\": recall_score(labs,preds,zero_division=0),\n",
        "        \"specificity\": tn/(tn+fp) if (tn+fp)>0 else 0.0,\n",
        "        \"f1\": f1_score(labs,preds,zero_division=0)\n",
        "    }\n",
        "\n",
        "print(\"\\n=== Final Evaluation on Test Sets ===\")\n",
        "for name, metas in [(\"test_within\", test_within_meta), (\"test_cross\", test_cross_meta)]:\n",
        "    loader = DataLoader(BinaryEEGDataset(EEGDataset(DATA_DIR, metas), metas),\n",
        "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    res = full_evaluate(metas, loader)\n",
        "    n0 = sum(1 for d in metas if d['label']==0)\n",
        "    n1 = sum(1 for d in metas if d['label']==1)\n",
        "    print(f\"-- {name} -- total={len(metas)}  A(D)={n0}, C(N)={n1}\")\n",
        "    print(f\" Loss={res['loss']:.4f} Acc={res['acc']:.4f} \"\n",
        "          f\"Sens={res['sensitivity']:.4f} Spec={res['specificity']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9g2ck83TVOCi",
        "outputId": "4698eec5-f872-454d-e9ba-1286e745c9fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Balanced TRAIN] total=2204 (each class=1102)\n",
            "\n",
            "Searching best epoch using validation set...\n",
            "Epoch 01 | train_loss=0.7453 | train_acc=0.5173 | val_loss=0.6950 | val_acc=0.5011 | time=315.8s\n",
            "Epoch 02 | train_loss=0.7388 | train_acc=0.5003 | val_loss=0.6943 | val_acc=0.4943 | time=13.1s\n",
            "Epoch 03 | train_loss=0.7470 | train_acc=0.4810 | val_loss=0.6940 | val_acc=0.4943 | time=13.1s\n",
            "Epoch 04 | train_loss=0.7419 | train_acc=0.4861 | val_loss=0.6940 | val_acc=0.4989 | time=13.0s\n",
            "Epoch 05 | train_loss=0.7316 | train_acc=0.5014 | val_loss=0.6934 | val_acc=0.5102 | time=13.1s\n",
            "Epoch 06 | train_loss=0.7283 | train_acc=0.4844 | val_loss=0.6938 | val_acc=0.5011 | time=12.8s\n",
            "Epoch 07 | train_loss=0.7295 | train_acc=0.4940 | val_loss=0.6971 | val_acc=0.5011 | time=12.9s\n",
            "Epoch 08 | train_loss=0.7210 | train_acc=0.5071 | val_loss=0.6945 | val_acc=0.5011 | time=13.1s\n",
            "Epoch 09 | train_loss=0.7240 | train_acc=0.4991 | val_loss=0.6936 | val_acc=0.5057 | time=13.0s\n",
            "Epoch 10 | train_loss=0.7255 | train_acc=0.4901 | val_loss=0.6930 | val_acc=0.4898 | time=12.9s\n",
            "Epoch 11 | train_loss=0.7284 | train_acc=0.4793 | val_loss=0.6925 | val_acc=0.5170 | time=12.8s\n",
            "Epoch 12 | train_loss=0.7170 | train_acc=0.5077 | val_loss=0.6921 | val_acc=0.5329 | time=12.9s\n",
            "Epoch 13 | train_loss=0.7119 | train_acc=0.5190 | val_loss=0.6911 | val_acc=0.5442 | time=12.9s\n",
            "Epoch 14 | train_loss=0.7147 | train_acc=0.5111 | val_loss=0.6923 | val_acc=0.4989 | time=12.8s\n",
            "Epoch 15 | train_loss=0.7222 | train_acc=0.4929 | val_loss=0.6850 | val_acc=0.5170 | time=12.8s\n",
            "Epoch 16 | train_loss=0.7200 | train_acc=0.5122 | val_loss=0.6847 | val_acc=0.5941 | time=12.8s\n",
            "Epoch 17 | train_loss=0.6844 | train_acc=0.5621 | val_loss=0.6441 | val_acc=0.7166 | time=12.9s\n",
            "Epoch 18 | train_loss=0.6226 | train_acc=0.6495 | val_loss=0.5535 | val_acc=0.7483 | time=12.9s\n",
            "Epoch 19 | train_loss=0.5399 | train_acc=0.7340 | val_loss=0.4982 | val_acc=0.7778 | time=12.9s\n",
            "Epoch 20 | train_loss=0.5137 | train_acc=0.7578 | val_loss=0.4866 | val_acc=0.7937 | time=13.0s\n",
            "Epoch 21 | train_loss=0.5092 | train_acc=0.7725 | val_loss=0.4726 | val_acc=0.7982 | time=12.8s\n",
            "Epoch 22 | train_loss=0.4983 | train_acc=0.7737 | val_loss=0.5060 | val_acc=0.7664 | time=12.9s\n",
            "Epoch 23 | train_loss=0.4526 | train_acc=0.8003 | val_loss=0.4405 | val_acc=0.7982 | time=12.9s\n",
            "Epoch 24 | train_loss=0.4424 | train_acc=0.7975 | val_loss=0.4499 | val_acc=0.8027 | time=12.8s\n",
            "Epoch 25 | train_loss=0.4252 | train_acc=0.8230 | val_loss=0.5344 | val_acc=0.7392 | time=12.7s\n",
            "Epoch 26 | train_loss=0.4056 | train_acc=0.8315 | val_loss=0.4300 | val_acc=0.8118 | time=12.9s\n",
            "Epoch 27 | train_loss=0.3818 | train_acc=0.8429 | val_loss=0.5670 | val_acc=0.7143 | time=12.8s\n",
            "Epoch 28 | train_loss=0.3890 | train_acc=0.8366 | val_loss=0.4530 | val_acc=0.7914 | time=13.0s\n",
            "Epoch 29 | train_loss=0.3563 | train_acc=0.8457 | val_loss=0.4515 | val_acc=0.7937 | time=13.0s\n",
            "Epoch 30 | train_loss=0.3294 | train_acc=0.8605 | val_loss=0.4361 | val_acc=0.8209 | time=12.9s\n",
            "Epoch 31 | train_loss=0.3180 | train_acc=0.8724 | val_loss=0.4763 | val_acc=0.7710 | time=12.8s\n",
            "Epoch 32 | train_loss=0.3186 | train_acc=0.8571 | val_loss=0.3863 | val_acc=0.8277 | time=12.9s\n",
            "Epoch 33 | train_loss=0.3088 | train_acc=0.8775 | val_loss=0.4068 | val_acc=0.8277 | time=12.8s\n",
            "Epoch 34 | train_loss=0.2920 | train_acc=0.8803 | val_loss=0.3745 | val_acc=0.8413 | time=12.8s\n",
            "Epoch 35 | train_loss=0.2828 | train_acc=0.8849 | val_loss=0.3773 | val_acc=0.8390 | time=12.9s\n",
            "Epoch 36 | train_loss=0.2513 | train_acc=0.9007 | val_loss=0.3940 | val_acc=0.8186 | time=12.9s\n",
            "Epoch 37 | train_loss=0.2432 | train_acc=0.9070 | val_loss=0.4043 | val_acc=0.8209 | time=12.9s\n",
            "Epoch 38 | train_loss=0.2388 | train_acc=0.9064 | val_loss=0.3802 | val_acc=0.8322 | time=13.0s\n",
            "Epoch 39 | train_loss=0.2472 | train_acc=0.8962 | val_loss=0.5429 | val_acc=0.7528 | time=13.1s\n",
            "Epoch 40 | train_loss=0.2272 | train_acc=0.9126 | val_loss=0.4204 | val_acc=0.8050 | time=13.0s\n",
            "Epoch 41 | train_loss=0.2171 | train_acc=0.9132 | val_loss=0.4549 | val_acc=0.8027 | time=13.0s\n",
            "Epoch 42 | train_loss=0.2284 | train_acc=0.9075 | val_loss=0.4646 | val_acc=0.7982 | time=12.8s\n",
            "Epoch 43 | train_loss=0.2182 | train_acc=0.9183 | val_loss=0.4023 | val_acc=0.8299 | time=12.9s\n",
            "Epoch 44 | train_loss=0.2059 | train_acc=0.9229 | val_loss=0.4415 | val_acc=0.8050 | time=13.0s\n",
            "Epoch 45 | train_loss=0.2296 | train_acc=0.9087 | val_loss=0.4543 | val_acc=0.7891 | time=12.8s\n",
            "Epoch 46 | train_loss=0.2251 | train_acc=0.9036 | val_loss=0.4280 | val_acc=0.8118 | time=12.9s\n",
            "Epoch 47 | train_loss=0.1917 | train_acc=0.9212 | val_loss=0.4160 | val_acc=0.8299 | time=12.9s\n",
            "Epoch 48 | train_loss=0.1766 | train_acc=0.9342 | val_loss=0.4391 | val_acc=0.8118 | time=12.9s\n",
            "Epoch 49 | train_loss=0.2122 | train_acc=0.9070 | val_loss=0.4456 | val_acc=0.8050 | time=12.9s\n",
            "\n",
            "Early stopping at epoch 49 (no improvement for 15 epochs)\n",
            "\n",
            ">> Best epoch by validation: 34\n",
            "\n",
            "Retraining from scratch on FULL set for 34 epochs...\n",
            "Epoch 01 | train_loss=0.7299 acc=0.5045 time=12.7s\n",
            "Epoch 02 | train_loss=0.7263 acc=0.4909 time=12.7s\n",
            "Epoch 03 | train_loss=0.7250 acc=0.5054 time=12.7s\n",
            "Epoch 04 | train_loss=0.7183 acc=0.4955 time=12.7s\n",
            "Epoch 05 | train_loss=0.6948 acc=0.5363 time=12.7s\n",
            "Epoch 06 | train_loss=0.5996 acc=0.6701 time=12.7s\n",
            "Epoch 07 | train_loss=0.5335 acc=0.7355 time=12.7s\n",
            "Epoch 08 | train_loss=0.5042 acc=0.7604 time=12.8s\n",
            "Epoch 09 | train_loss=0.4525 acc=0.8013 time=12.7s\n",
            "Epoch 10 | train_loss=0.4338 acc=0.8026 time=12.7s\n",
            "Epoch 11 | train_loss=0.4111 acc=0.8226 time=12.7s\n",
            "Epoch 12 | train_loss=0.3941 acc=0.8240 time=12.7s\n",
            "Epoch 13 | train_loss=0.3659 acc=0.8435 time=12.8s\n",
            "Epoch 14 | train_loss=0.3533 acc=0.8525 time=12.8s\n",
            "Epoch 15 | train_loss=0.3413 acc=0.8512 time=12.7s\n",
            "Epoch 16 | train_loss=0.3154 acc=0.8684 time=12.7s\n",
            "Epoch 17 | train_loss=0.3054 acc=0.8789 time=12.7s\n",
            "Epoch 18 | train_loss=0.2956 acc=0.8779 time=12.7s\n",
            "Epoch 19 | train_loss=0.2834 acc=0.8829 time=12.8s\n",
            "Epoch 20 | train_loss=0.2891 acc=0.8721 time=12.7s\n",
            "Epoch 21 | train_loss=0.2568 acc=0.9102 time=12.7s\n",
            "Epoch 22 | train_loss=0.2705 acc=0.8907 time=12.7s\n",
            "Epoch 23 | train_loss=0.2561 acc=0.8979 time=12.8s\n",
            "Epoch 24 | train_loss=0.2492 acc=0.9006 time=12.7s\n",
            "Epoch 25 | train_loss=0.2285 acc=0.9192 time=12.7s\n",
            "Epoch 26 | train_loss=0.2303 acc=0.9079 time=12.7s\n",
            "Epoch 27 | train_loss=0.2277 acc=0.9183 time=12.7s\n",
            "Epoch 28 | train_loss=0.2392 acc=0.9070 time=12.7s\n",
            "Epoch 29 | train_loss=0.2297 acc=0.9152 time=12.7s\n",
            "Epoch 30 | train_loss=0.2249 acc=0.9133 time=12.7s\n",
            "Epoch 31 | train_loss=0.2162 acc=0.9183 time=12.7s\n",
            "Epoch 32 | train_loss=0.2224 acc=0.9138 time=12.7s\n",
            "Epoch 33 | train_loss=0.2104 acc=0.9265 time=12.8s\n",
            "Epoch 34 | train_loss=0.2213 acc=0.9174 time=12.7s\n",
            "\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=272  A(D)=146, C(N)=126\n",
            " Loss=0.3395 Acc=0.8676 Sens=0.8571 Spec=0.8767 F1=0.8571\n",
            "\n",
            "-- test_cross -- total=626  A(D)=319, C(N)=307\n",
            " Loss=0.6494 Acc=0.6757 Sens=0.6612 Spec=0.6897 F1=0.6667\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AD vs FTD (Best Optuna Search)"
      ],
      "metadata": {
        "id": "1t0XN6APlhyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "PCT_START   = 0.2  # fixed\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & count splits ──────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "class0, class1 = 'A','F'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "# AD vs FTD Filtering\n",
        "train_meta       = [d for d in train_meta if d['label'] in (class0, class1)]\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "def count_labels(meta_list):\n",
        "    cnt0 = cnt1 = 0\n",
        "    for d in meta_list:\n",
        "        lbl = d['label']\n",
        "        if isinstance(lbl, str):\n",
        "            if lbl == class0: cnt0 += 1\n",
        "            elif lbl == class1: cnt1 += 1\n",
        "        else:\n",
        "            if lbl == 0: cnt0 += 1\n",
        "            elif lbl == 1: cnt1 += 1\n",
        "    return cnt0, cnt1\n",
        "\n",
        "n_tr0, n_tr1 = count_labels(train_meta)\n",
        "n_tw0, n_tw1 = count_labels(test_within_meta)\n",
        "n_tc0, n_tc1 = count_labels(test_cross_meta)\n",
        "\n",
        "print(f\"--> Data counts before balancing:\")\n",
        "print(f\"    TRAIN         total={len(train_meta)}  A(D)={n_tr0}, F(TD)={n_tr1}\")\n",
        "print(f\"    TEST_WITHIN   total={len(test_within_meta)}  A(D)={n_tw0}, F(TD)={n_tw1}\")\n",
        "print(f\"    TEST_CROSS    total={len(test_cross_meta)}  A(D)={n_tc0}, F(TD)={n_tc1}\\n\")\n",
        "\n",
        "# ─── Balance train set ───────────────────────────────────────────\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "balanced_meta = [copy.deepcopy(d) for d in balanced_meta]\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# ─── After Balancing ───────────────────────────────────────────\n",
        "bal_AD, bal_FTD = count_labels(balanced_meta)\n",
        "print(f\"[BALANCED TRAIN] total={len(balanced_meta)}  AD={bal_AD}, FTD={bal_FTD}\\n\")\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset_train = BinaryEEGDataset(raw_ds_train, balanced_meta)\n",
        "labels_train  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── Optuna Objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # 1) sample hyperparameters\n",
        "    lr = trial.suggest_float('lr', 5e-5, 5e-4, log=True)\n",
        "    wd = trial.suggest_float('wd', 5e-5, 5e-4, log=True)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-AD-FTD-test-within-cross-1',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # train/validation split\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2,\n",
        "        stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "\n",
        "    tr_AD = np.sum(labels_train[tr_idx] == 0)\n",
        "    tr_FTD = np.sum(labels_train[tr_idx] == 1)\n",
        "    va_AD = np.sum(labels_train[va_idx] == 0)\n",
        "    va_FTD = np.sum(labels_train[va_idx] == 1)\n",
        "    print(f\"[Trial {trial.number}]  \"\n",
        "          f\"TRAIN n={len(tr_idx)} AD={tr_AD}, FTD={tr_FTD}) | \"\n",
        "          f\"VAL n={len(va_idx)} AD={va_AD}, FTD={va_FTD}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset_train, tr_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset_train, va_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # model & optimizer & scheduler & loss\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc  = 0.0\n",
        "    best_state= None\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # ── train ──\n",
        "        model.train()\n",
        "        train_loss_sum = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight_decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = wd * (cur_lr / lr)\n",
        "            train_loss_sum += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            train_correct += (preds == y).sum().item()\n",
        "            train_total   += y.size(0)\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "        train_acc  = train_correct / train_total\n",
        "\n",
        "        # ── validate ──\n",
        "        model.eval()\n",
        "        vloss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss / len(val_loader)\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        val_acc = (preds == labs).sum() / labs.size\n",
        "\n",
        "        # ── metrics ──\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "        spec = tn / (tn+fp) if (tn+fp)>0 else 0.0\n",
        "        prec = precision_score(labs, preds, zero_division=0)\n",
        "        rec  = recall_score(labs, preds, zero_division=0)\n",
        "        f1   = f1_score(labs, preds, zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={prec:.4f} rec={rec:.4f} spec={spec:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':       epoch,\n",
        "            'train_loss':  train_loss,\n",
        "            'train_acc':   train_acc,\n",
        "            'val_loss':    val_loss,\n",
        "            'val_acc':     val_acc,\n",
        "            'specificity': spec,\n",
        "            'precision':   prec,\n",
        "            'recall':      rec,\n",
        "            'f1_score':    f1\n",
        "        })\n",
        "\n",
        "        # ── Early Stopping (val_loss 기준) ──\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc  = val_acc\n",
        "            es_count      = 0\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            os.makedirs('ckpts', exist_ok=True)\n",
        "            ckpt_path = f'ckpts/trial{trial.number}_optimal_AD_FTD_1.pth'\n",
        "            torch.save(best_state, ckpt_path)\n",
        "            trial.set_user_attr('ckpt_path', ckpt_path)\n",
        "            trial.set_user_attr('best_epoch', epoch)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, train_loader, val_loader, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 다중목적 리턴\n",
        "    return best_val_loss, best_val_acc\n",
        "\n",
        "# ─── Run Optuna Study ────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=[\"minimize\", \"maximize\"],\n",
        "        study_name=\"eeg_multiobj_AD_FTD_1\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Pareto front 중 val_acc 가 가장 높은 trial 선택\n",
        "    best       = max(study.best_trials, key=lambda t: t.values[1])\n",
        "    bv_loss, bv_acc = best.values\n",
        "    best_epoch = best.user_attrs[\"best_epoch\"]\n",
        "    ckpt_path  = best.user_attrs[\"ckpt_path\"]\n",
        "\n",
        "    print(f\"\\n=== Selected Trial #{best.number} ===\")\n",
        "    print(\n",
        "        f\" val_loss={bv_loss:.4f}, val_acc={bv_acc:.4f}, \"\n",
        "        f\"params={best.params}, best_epoch={best_epoch}, ckpt={ckpt_path}\"\n",
        "    )\n",
        "\n",
        "    # ─── Search‑model 로드 ───────────────────────────────────────\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    search_model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    search_model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    search_model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── Evaluation 함수 ─────────────────────────────────────────\n",
        "    def evaluate(model, metas, criterion):\n",
        "        metas_copy = copy.deepcopy(metas)\n",
        "        for d in metas_copy:\n",
        "            if isinstance(d[\"label\"], str):\n",
        "                d[\"label\"] = label_map[d[\"label\"]]\n",
        "\n",
        "        ds = BinaryEEGDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal += y.size(0)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0, 1]).ravel()\n",
        "        return {\n",
        "            \"loss\":        vloss / len(loader),\n",
        "            \"acc\":         vcorrect / vtotal,\n",
        "            \"sensitivity\": recall_score(labs, preds, zero_division=0),\n",
        "            \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
        "            \"f1\":          f1_score(labs, preds, zero_division=0),\n",
        "        }\n",
        "\n",
        "    # ─── Final Evaluation on Test Sets ────────────────────────────\n",
        "    print(\"=== Final Evaluation on Test Sets ===\")\n",
        "    for name, metas in [\n",
        "        (\"test_within\", test_within_meta),\n",
        "        (\"test_cross\",  test_cross_meta),\n",
        "    ]:\n",
        "        res = evaluate(search_model, metas, criterion)\n",
        "        n0, n1 = count_labels(metas)\n",
        "        print(f\"-- {name} -- total={len(metas)}  A(D)={n0}, F(TD)={n1}\")\n",
        "        print(\n",
        "            f\" Accuracy={res['acc']:.4f} \"\n",
        "            f\"Sensitivity={res['sensitivity']:.4f} \"\n",
        "            f\"Specificity={res['specificity']:.4f} \"\n",
        "            f\"F1={res['f1']:.4f}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4X0U2feVn8oQ",
        "outputId": "f18236f0-1d86-46f5-942e-36660b78f8fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 06:55:58,296] A new study created in memory with name: eeg_multiobj_AD_FTD_1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Data counts before balancing:\n",
            "    TRAIN         total=2117  A(D)=1388, F(TD)=729\n",
            "    TEST_WITHIN   total=218  A(D)=146, F(TD)=72\n",
            "    TEST_CROSS    total=566  A(D)=319, F(TD)=247\n",
            "\n",
            "[BALANCED TRAIN] total=1458  AD=729, FTD=729\n",
            "\n",
            "\n",
            "--- Trial 0 hyperparams --> lr=3.94e-05, wd=2.07e-04\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_065558-tjem8kn0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/tjem8kn0' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/tjem8kn0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/tjem8kn0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 0]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7508 acc=0.4880 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=11.3s\n",
            "Epoch 002 | train_loss=0.7425 acc=0.5026 | val_loss=0.6922 acc=0.4932 | prec=0.4965 rec=0.9795 spec=0.0068 f1=0.6590 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7524 acc=0.4983 | val_loss=0.6923 acc=0.5068 | prec=0.5035 rec=0.9863 spec=0.0274 f1=0.6667 | time=9.2s\n",
            "Epoch 004 | train_loss=0.7578 acc=0.4931 | val_loss=0.6935 acc=0.5068 | prec=0.5185 rec=0.1918 spec=0.8219 f1=0.2800 | time=9.6s\n",
            "Epoch 005 | train_loss=0.7265 acc=0.5111 | val_loss=0.6920 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.4s\n",
            "Epoch 006 | train_loss=0.7434 acc=0.4871 | val_loss=0.6921 acc=0.5103 | prec=0.5052 rec=0.9932 spec=0.0274 f1=0.6697 | time=9.2s\n",
            "Epoch 007 | train_loss=0.7350 acc=0.5103 | val_loss=0.6913 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7238 acc=0.5206 | val_loss=0.6950 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 009 | train_loss=0.7327 acc=0.5060 | val_loss=0.6920 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 010 | train_loss=0.7198 acc=0.5223 | val_loss=0.6920 acc=0.5034 | prec=0.5017 rec=1.0000 spec=0.0068 f1=0.6682 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7303 acc=0.5077 | val_loss=0.6920 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 012 | train_loss=0.7285 acc=0.4897 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7328 acc=0.4940 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7223 acc=0.5051 | val_loss=0.6930 acc=0.5342 | prec=0.5431 rec=0.4315 spec=0.6370 f1=0.4809 | time=9.1s\n",
            "Epoch 015 | train_loss=0.7254 acc=0.4906 | val_loss=0.6966 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 016 | train_loss=0.7128 acc=0.5051 | val_loss=0.6921 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 017 | train_loss=0.7148 acc=0.5292 | val_loss=0.6938 acc=0.5240 | prec=0.5897 rec=0.1575 spec=0.8904 f1=0.2486 | time=9.2s\n",
            "Epoch 018 | train_loss=0.7309 acc=0.4794 | val_loss=0.6917 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 019 | train_loss=0.7275 acc=0.4931 | val_loss=0.6965 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 020 | train_loss=0.7149 acc=0.5223 | val_loss=0.6925 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 021 | train_loss=0.7205 acc=0.5077 | val_loss=0.6960 acc=0.5205 | prec=0.5263 rec=0.4110 spec=0.6301 f1=0.4615 | time=9.1s\n",
            "Epoch 022 | train_loss=0.7141 acc=0.5197 | val_loss=0.7061 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>███▄█████████▆▁█▄█▁█▆▁</td></tr><tr><td>precision</td><td>▇▇▇▇▇▇▇▇▇▇▇▇▇▇▁▇█▇▁▇▇▁</td></tr><tr><td>recall</td><td>███▂█████████▄▁█▂█▁█▄▁</td></tr><tr><td>specificity</td><td>▁▁▁▇▁▁▁▁▁▁▁▁▁▅█▁▇▁█▁▅█</td></tr><tr><td>train_acc</td><td>▂▄▄▃▅▂▅▇▅▇▅▂▃▅▃▅█▁▃▇▅▇</td></tr><tr><td>train_loss</td><td>▇▆▇█▃▆▄▃▄▂▄▃▄▂▃▁▁▄▃▁▂▁</td></tr><tr><td>val_acc</td><td>▂▁▃▃▂▄▂▂▂▃▂▂▂█▂▂▆▂▂▂▆▂</td></tr><tr><td>val_loss</td><td>▁▁▁▂▁▁▁▃▁▁▁▂▂▂▃▁▂▁▃▂▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>f1_score</td><td>0</td></tr><tr><td>precision</td><td>0</td></tr><tr><td>recall</td><td>0</td></tr><tr><td>specificity</td><td>0.99315</td></tr><tr><td>train_acc</td><td>0.51973</td></tr><tr><td>train_loss</td><td>0.71405</td></tr><tr><td>val_acc</td><td>0.49658</td></tr><tr><td>val_loss</td><td>0.70612</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/tjem8kn0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/tjem8kn0</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_065558-tjem8kn0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 06:59:29,217] Trial 0 finished with values: [0.6913403928279876, 0.5] and parameters: {'lr': 3.940075081438162e-05, 'wd': 0.00020672032789101642}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 1 hyperparams --> lr=3.61e-05, wd=3.58e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_065929-254bc5b1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/254bc5b1' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/254bc5b1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/254bc5b1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 1]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7803 acc=0.4880 | val_loss=0.7212 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7531 acc=0.4974 | val_loss=0.7272 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7448 acc=0.4906 | val_loss=0.7211 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 004 | train_loss=0.7547 acc=0.4931 | val_loss=0.7205 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.4s\n",
            "Epoch 005 | train_loss=0.7397 acc=0.5026 | val_loss=0.7163 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.7s\n",
            "Epoch 006 | train_loss=0.7268 acc=0.4957 | val_loss=0.7076 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 007 | train_loss=0.7234 acc=0.5051 | val_loss=0.7110 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7308 acc=0.4966 | val_loss=0.6957 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7341 acc=0.4803 | val_loss=0.7007 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7386 acc=0.4923 | val_loss=0.7011 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7296 acc=0.4889 | val_loss=0.6993 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 012 | train_loss=0.7247 acc=0.4914 | val_loss=0.6976 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 013 | train_loss=0.7190 acc=0.4863 | val_loss=0.6971 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 014 | train_loss=0.7185 acc=0.4983 | val_loss=0.6953 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.3s\n",
            "Epoch 015 | train_loss=0.7160 acc=0.5051 | val_loss=0.6917 acc=0.5034 | prec=0.5017 rec=0.9863 spec=0.0205 f1=0.6651 | time=9.1s\n",
            "Epoch 016 | train_loss=0.7100 acc=0.5369 | val_loss=0.6942 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 017 | train_loss=0.7208 acc=0.4897 | val_loss=0.7045 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 018 | train_loss=0.7127 acc=0.5086 | val_loss=0.7008 acc=0.4966 | prec=0.4839 rec=0.1027 spec=0.8904 f1=0.1695 | time=9.0s\n",
            "Epoch 019 | train_loss=0.7106 acc=0.5232 | val_loss=0.6958 acc=0.4829 | prec=0.4825 rec=0.4726 spec=0.4932 f1=0.4775 | time=9.1s\n",
            "Epoch 020 | train_loss=0.7054 acc=0.5137 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 021 | train_loss=0.6978 acc=0.5180 | val_loss=0.6991 acc=0.5034 | prec=0.5017 rec=0.9863 spec=0.0205 f1=0.6651 | time=9.2s\n",
            "Epoch 022 | train_loss=0.6956 acc=0.5352 | val_loss=0.7024 acc=0.4658 | prec=0.4390 rec=0.2466 spec=0.6849 f1=0.3158 | time=9.1s\n",
            "Epoch 023 | train_loss=0.6886 acc=0.5515 | val_loss=0.7104 acc=0.4760 | prec=0.4426 rec=0.1849 spec=0.7671 f1=0.2609 | time=9.0s\n",
            "Epoch 024 | train_loss=0.6907 acc=0.5360 | val_loss=0.7026 acc=0.5137 | prec=0.5095 rec=0.7329 spec=0.2945 f1=0.6011 | time=9.2s\n",
            "Epoch 025 | train_loss=0.6786 acc=0.5695 | val_loss=0.7098 acc=0.5000 | prec=0.5000 rec=0.2123 spec=0.7877 f1=0.2981 | time=9.1s\n",
            "Epoch 026 | train_loss=0.6888 acc=0.5317 | val_loss=0.6978 acc=0.5068 | prec=0.5104 rec=0.3356 spec=0.6781 f1=0.4050 | time=9.0s\n",
            "Epoch 027 | train_loss=0.6768 acc=0.5763 | val_loss=0.7074 acc=0.5068 | prec=0.5088 rec=0.3973 spec=0.6164 f1=0.4462 | time=8.9s\n",
            "Epoch 028 | train_loss=0.6809 acc=0.5566 | val_loss=0.7025 acc=0.5308 | prec=0.5276 rec=0.5890 spec=0.4726 f1=0.5566 | time=9.0s\n",
            "Epoch 029 | train_loss=0.6530 acc=0.6029 | val_loss=0.7330 acc=0.5171 | prec=0.5362 rec=0.2534 spec=0.7808 f1=0.3442 | time=9.0s\n",
            "Epoch 030 | train_loss=0.6701 acc=0.5926 | val_loss=0.7238 acc=0.5137 | prec=0.5526 rec=0.1438 spec=0.8836 f1=0.2283 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▃▆██▄▄▇▄▅▆▇▅▃</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▇▇▁▇▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▂▄██▃▂▆▂▃▄▅▃▂</td></tr><tr><td>specificity</td><td>██████████████▁▁█▇▄▁▁▆▆▃▇▆▅▄▆▇</td></tr><tr><td>train_acc</td><td>▁▂▂▂▂▂▂▂▁▂▁▂▁▂▂▄▂▃▃▃▃▄▅▄▆▄▆▅█▇</td></tr><tr><td>train_loss</td><td>█▇▆▇▆▅▅▅▅▆▅▅▅▅▄▄▅▄▄▄▃▃▃▃▂▃▂▃▁▂</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▃▅▅▁▂▆▅▅▅█▇▆</td></tr><tr><td>val_loss</td><td>▆▇▆▆▅▄▄▂▃▃▂▂▂▂▁▁▃▃▂▁▂▃▄▃▄▂▄▃█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>f1_score</td><td>0.22826</td></tr><tr><td>precision</td><td>0.55263</td></tr><tr><td>recall</td><td>0.14384</td></tr><tr><td>specificity</td><td>0.88356</td></tr><tr><td>train_acc</td><td>0.59262</td></tr><tr><td>train_loss</td><td>0.67006</td></tr><tr><td>val_acc</td><td>0.5137</td></tr><tr><td>val_loss</td><td>0.72377</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/254bc5b1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/254bc5b1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_065929-254bc5b1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:04:04,620] Trial 1 finished with values: [0.6917383909225464, 0.5034246575342466] and parameters: {'lr': 3.612614983994956e-05, 'wd': 3.580441493057465e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 2 hyperparams --> lr=1.19e-03, wd=6.80e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_070404-80e8wqiq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/80e8wqiq' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/80e8wqiq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/80e8wqiq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 2]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7282 acc=0.5000 | val_loss=0.6962 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7228 acc=0.4974 | val_loss=0.6990 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7185 acc=0.5154 | val_loss=0.6990 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7272 acc=0.4837 | val_loss=0.6987 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7083 acc=0.5343 | val_loss=0.6944 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7208 acc=0.4966 | val_loss=0.6971 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7026 acc=0.5232 | val_loss=0.7120 acc=0.4623 | prec=0.4764 rec=0.7603 spec=0.1644 f1=0.5858 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7029 acc=0.5352 | val_loss=0.7167 acc=0.4795 | prec=0.4833 rec=0.5959 spec=0.3630 f1=0.5337 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7025 acc=0.5377 | val_loss=0.7021 acc=0.4932 | prec=0.4881 rec=0.2808 spec=0.7055 f1=0.3565 | time=9.0s\n",
            "Epoch 010 | train_loss=0.6992 acc=0.5480 | val_loss=0.7264 acc=0.5034 | prec=0.5021 rec=0.8014 spec=0.2055 f1=0.6174 | time=9.0s\n",
            "Epoch 011 | train_loss=0.6842 acc=0.5592 | val_loss=0.7199 acc=0.5342 | prec=0.5472 rec=0.3973 spec=0.6712 f1=0.4603 | time=9.2s\n",
            "Epoch 012 | train_loss=0.6414 acc=0.6235 | val_loss=0.6777 acc=0.5822 | prec=0.6333 rec=0.3904 spec=0.7740 f1=0.4831 | time=9.2s\n",
            "Epoch 013 | train_loss=0.6079 acc=0.6630 | val_loss=0.6397 acc=0.5993 | prec=0.6107 rec=0.5479 spec=0.6507 f1=0.5776 | time=9.1s\n",
            "Epoch 014 | train_loss=0.5678 acc=0.7178 | val_loss=0.6332 acc=0.6541 | prec=0.6471 rec=0.6781 spec=0.6301 f1=0.6622 | time=9.0s\n",
            "Epoch 015 | train_loss=0.5367 acc=0.7470 | val_loss=0.6212 acc=0.5959 | prec=0.5654 rec=0.8288 spec=0.3630 f1=0.6722 | time=9.1s\n",
            "Epoch 016 | train_loss=0.4974 acc=0.7642 | val_loss=0.6210 acc=0.6610 | prec=0.6146 rec=0.8630 spec=0.4589 f1=0.7179 | time=9.2s\n",
            "Epoch 017 | train_loss=0.4589 acc=0.7907 | val_loss=0.6398 acc=0.6473 | prec=0.7529 rec=0.4384 spec=0.8562 f1=0.5541 | time=9.1s\n",
            "Epoch 018 | train_loss=0.4421 acc=0.8053 | val_loss=0.6795 acc=0.6199 | prec=0.5926 rec=0.7671 spec=0.4726 f1=0.6687 | time=9.1s\n",
            "Epoch 019 | train_loss=0.3908 acc=0.8353 | val_loss=0.6435 acc=0.6644 | prec=0.7000 rec=0.5753 spec=0.7534 f1=0.6316 | time=9.2s\n",
            "Epoch 020 | train_loss=0.3527 acc=0.8499 | val_loss=0.6786 acc=0.6747 | prec=0.7073 rec=0.5959 spec=0.7534 f1=0.6468 | time=9.1s\n",
            "Epoch 021 | train_loss=0.3714 acc=0.8216 | val_loss=0.5946 acc=0.6370 | prec=0.6538 rec=0.5822 spec=0.6918 f1=0.6159 | time=9.1s\n",
            "Epoch 022 | train_loss=0.3213 acc=0.8516 | val_loss=0.9601 acc=0.5993 | prec=0.6706 rec=0.3904 spec=0.8082 f1=0.4935 | time=9.1s\n",
            "Epoch 023 | train_loss=0.2676 acc=0.8825 | val_loss=0.8139 acc=0.6438 | prec=0.6721 rec=0.5616 spec=0.7260 f1=0.6119 | time=9.2s\n",
            "Epoch 024 | train_loss=0.2730 acc=0.8731 | val_loss=0.9292 acc=0.6370 | prec=0.6538 rec=0.5822 spec=0.6918 f1=0.6159 | time=9.1s\n",
            "Epoch 025 | train_loss=0.2334 acc=0.8937 | val_loss=0.9715 acc=0.5651 | prec=0.5364 rec=0.9589 spec=0.1712 f1=0.6880 | time=9.0s\n",
            "Epoch 026 | train_loss=0.2553 acc=0.8859 | val_loss=0.8992 acc=0.6164 | prec=0.6269 rec=0.5753 spec=0.6575 f1=0.6000 | time=9.1s\n",
            "Epoch 027 | train_loss=0.2053 acc=0.9022 | val_loss=1.0541 acc=0.6027 | prec=0.6000 rec=0.6164 spec=0.5890 f1=0.6081 | time=9.2s\n",
            "Epoch 028 | train_loss=0.1683 acc=0.9168 | val_loss=1.3936 acc=0.6027 | prec=0.5781 rec=0.7603 spec=0.4452 f1=0.6568 | time=9.1s\n",
            "Epoch 029 | train_loss=0.1233 acc=0.9451 | val_loss=1.4495 acc=0.5959 | prec=0.6148 rec=0.5137 spec=0.6781 f1=0.5597 | time=9.2s\n",
            "Epoch 030 | train_loss=0.1465 acc=0.9314 | val_loss=1.6652 acc=0.5651 | prec=0.5419 rec=0.8425 spec=0.2877 f1=0.6595 | time=9.1s\n",
            "Epoch 031 | train_loss=0.1533 acc=0.9262 | val_loss=1.4806 acc=0.5890 | prec=0.5774 rec=0.6644 spec=0.5137 f1=0.6178 | time=9.0s\n",
            "Epoch 032 | train_loss=0.1211 acc=0.9322 | val_loss=1.4086 acc=0.5925 | prec=0.5828 rec=0.6507 spec=0.5342 f1=0.6149 | time=9.1s\n",
            "Epoch 033 | train_loss=0.1195 acc=0.9425 | val_loss=1.3792 acc=0.6096 | prec=0.6081 rec=0.6164 spec=0.6027 f1=0.6122 | time=9.1s\n",
            "Epoch 034 | train_loss=0.1119 acc=0.9451 | val_loss=1.4996 acc=0.6096 | prec=0.6053 rec=0.6301 spec=0.5890 f1=0.6174 | time=9.1s\n",
            "Epoch 035 | train_loss=0.1285 acc=0.9314 | val_loss=1.4359 acc=0.5993 | prec=0.5948 rec=0.6233 spec=0.5753 f1=0.6087 | time=9.0s\n",
            "Epoch 036 | train_loss=0.1484 acc=0.9237 | val_loss=1.6744 acc=0.6027 | prec=0.6190 rec=0.5342 spec=0.6712 f1=0.5735 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▇▆▄▇▅▆▇▇██▆█▇▇▇▆▇▇█▇▇▇▆▇▇▇▇▇▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▅▅▆▆▆▇▇▇▆▇█▇██▇▇▇▇▆▇▇▆▇▆▆▆▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▇▅▃▇▄▄▅▆▇▇▄▇▅▅▅▄▅▅█▅▅▇▅▇▆▆▅▆▆▅</td></tr><tr><td>specificity</td><td>██████▁▃▆▁▅▆▅▅▃▃▇▄▆▆▅▆▆▅▁▅▅▃▅▂▄▄▅▅▄▅</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▁▂▂▂▂▂▃▄▅▅▅▆▆▆▇▆▇▇▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>███████████▇▇▆▆▅▅▅▄▄▄▃▃▃▂▃▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▁▂▂▂▃▅▆▇▅█▇▆██▇▆▇▇▄▆▆▆▅▄▅▅▆▆▆▆</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▂▁▂▁▃▂▃▃▃▄▆▇█▇▆▆▇▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>f1_score</td><td>0.57353</td></tr><tr><td>precision</td><td>0.61905</td></tr><tr><td>recall</td><td>0.53425</td></tr><tr><td>specificity</td><td>0.67123</td></tr><tr><td>train_acc</td><td>0.92367</td></tr><tr><td>train_loss</td><td>0.14841</td></tr><tr><td>val_acc</td><td>0.60274</td></tr><tr><td>val_loss</td><td>1.67442</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/80e8wqiq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/80e8wqiq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_070404-80e8wqiq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:09:34,583] Trial 2 finished with values: [0.5946097016334534, 0.636986301369863] and parameters: {'lr': 0.0011887155314264707, 'wd': 6.801448765418015e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 3 hyperparams --> lr=3.26e-04, wd=1.15e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_070934-olzlrj3k</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/olzlrj3k' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/olzlrj3k' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/olzlrj3k</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 3]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7729 acc=0.4906 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7403 acc=0.4811 | val_loss=0.6927 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.4s\n",
            "Epoch 003 | train_loss=0.7316 acc=0.5043 | val_loss=0.6923 acc=0.5034 | prec=0.5023 rec=0.7603 spec=0.2466 f1=0.6049 | time=9.3s\n",
            "Epoch 004 | train_loss=0.7312 acc=0.4974 | val_loss=0.6931 acc=0.4897 | prec=0.4938 rec=0.8151 spec=0.1644 f1=0.6150 | time=9.4s\n",
            "Epoch 005 | train_loss=0.7147 acc=0.5043 | val_loss=0.6934 acc=0.4589 | prec=0.4651 rec=0.5479 spec=0.3699 f1=0.5031 | time=9.6s\n",
            "Epoch 006 | train_loss=0.7195 acc=0.5043 | val_loss=0.6917 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.6s\n",
            "Epoch 007 | train_loss=0.7050 acc=0.5343 | val_loss=0.6929 acc=0.4863 | prec=0.4928 rec=0.9384 spec=0.0342 f1=0.6462 | time=9.4s\n",
            "Epoch 008 | train_loss=0.7078 acc=0.5223 | val_loss=0.6934 acc=0.4726 | prec=0.4836 rec=0.8082 spec=0.1370 f1=0.6051 | time=9.6s\n",
            "Epoch 009 | train_loss=0.7181 acc=0.5120 | val_loss=0.6939 acc=0.4795 | prec=0.4886 rec=0.8836 spec=0.0753 f1=0.6293 | time=9.5s\n",
            "Epoch 010 | train_loss=0.6966 acc=0.5283 | val_loss=0.6974 acc=0.4897 | prec=0.4942 rec=0.8699 spec=0.1096 f1=0.6303 | time=10.0s\n",
            "Epoch 011 | train_loss=0.7040 acc=0.5172 | val_loss=0.7045 acc=0.4966 | prec=0.4972 rec=0.6096 spec=0.3836 f1=0.5477 | time=10.1s\n",
            "Epoch 012 | train_loss=0.7041 acc=0.5386 | val_loss=0.7117 acc=0.4897 | prec=0.4835 rec=0.3014 spec=0.6781 f1=0.3713 | time=10.0s\n",
            "Epoch 013 | train_loss=0.6773 acc=0.5755 | val_loss=0.7274 acc=0.4726 | prec=0.4444 rec=0.2192 spec=0.7260 f1=0.2936 | time=10.0s\n",
            "Epoch 014 | train_loss=0.6813 acc=0.5823 | val_loss=0.7113 acc=0.4760 | prec=0.4798 rec=0.5685 spec=0.3836 f1=0.5204 | time=10.1s\n",
            "Epoch 015 | train_loss=0.6584 acc=0.6218 | val_loss=0.7119 acc=0.5034 | prec=0.5032 rec=0.5411 spec=0.4658 f1=0.5215 | time=9.9s\n",
            "Epoch 016 | train_loss=0.6397 acc=0.6329 | val_loss=0.6677 acc=0.5616 | prec=0.5489 rec=0.6918 spec=0.4315 f1=0.6121 | time=10.1s\n",
            "Epoch 017 | train_loss=0.6060 acc=0.6552 | val_loss=0.6886 acc=0.5616 | prec=0.5441 rec=0.7603 spec=0.3630 f1=0.6343 | time=10.0s\n",
            "Epoch 018 | train_loss=0.5645 acc=0.6947 | val_loss=0.6444 acc=0.6199 | prec=0.5799 rec=0.8699 spec=0.3699 f1=0.6959 | time=10.0s\n",
            "Epoch 019 | train_loss=0.5220 acc=0.7479 | val_loss=0.6654 acc=0.6233 | prec=0.6385 rec=0.5685 spec=0.6781 f1=0.6014 | time=9.7s\n",
            "Epoch 020 | train_loss=0.4611 acc=0.8002 | val_loss=0.7115 acc=0.6164 | prec=0.6700 rec=0.4589 spec=0.7740 f1=0.5447 | time=9.1s\n",
            "Epoch 021 | train_loss=0.4437 acc=0.8130 | val_loss=0.6529 acc=0.6027 | prec=0.5701 rec=0.8356 spec=0.3699 f1=0.6778 | time=9.2s\n",
            "Epoch 022 | train_loss=0.4024 acc=0.8268 | val_loss=0.6865 acc=0.6370 | prec=0.6389 rec=0.6301 spec=0.6438 f1=0.6345 | time=9.0s\n",
            "Epoch 023 | train_loss=0.3785 acc=0.8293 | val_loss=0.7130 acc=0.6473 | prec=0.6525 rec=0.6301 spec=0.6644 f1=0.6411 | time=9.1s\n",
            "Epoch 024 | train_loss=0.3258 acc=0.8619 | val_loss=0.7682 acc=0.6233 | prec=0.6385 rec=0.5685 spec=0.6781 f1=0.6014 | time=9.1s\n",
            "Epoch 025 | train_loss=0.3077 acc=0.8782 | val_loss=0.8788 acc=0.6301 | prec=0.6939 rec=0.4658 spec=0.7945 f1=0.5574 | time=9.1s\n",
            "Epoch 026 | train_loss=0.2864 acc=0.8808 | val_loss=0.7913 acc=0.6301 | prec=0.6377 rec=0.6027 spec=0.6575 f1=0.6197 | time=9.1s\n",
            "Epoch 027 | train_loss=0.2621 acc=0.8962 | val_loss=0.8542 acc=0.6096 | prec=0.6176 rec=0.5753 spec=0.6438 f1=0.5957 | time=9.1s\n",
            "Epoch 028 | train_loss=0.2383 acc=0.9039 | val_loss=0.9352 acc=0.5890 | prec=0.5756 rec=0.6781 spec=0.5000 f1=0.6226 | time=9.2s\n",
            "Epoch 029 | train_loss=0.2356 acc=0.8988 | val_loss=0.9042 acc=0.5925 | prec=0.5678 rec=0.7740 spec=0.4110 f1=0.6551 | time=8.9s\n",
            "Epoch 030 | train_loss=0.2618 acc=0.8911 | val_loss=0.9649 acc=0.5753 | prec=0.5687 rec=0.6233 spec=0.5274 f1=0.5948 | time=9.2s\n",
            "Epoch 031 | train_loss=0.2166 acc=0.9057 | val_loss=0.9879 acc=0.5753 | prec=0.5705 rec=0.6096 spec=0.5411 f1=0.5894 | time=9.0s\n",
            "Epoch 032 | train_loss=0.2089 acc=0.9151 | val_loss=1.0017 acc=0.5753 | prec=0.5632 rec=0.6712 spec=0.4795 f1=0.6125 | time=9.0s\n",
            "Epoch 033 | train_loss=0.1885 acc=0.9211 | val_loss=0.9008 acc=0.6301 | prec=0.6338 rec=0.6164 spec=0.6438 f1=0.6250 | time=9.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▇▇▆▇▅▇▇▆▇▇▅▂▁▅▅▇▇█▆▅█▇▇▆▆▇▆▇▇▆▆▇▇</td></tr><tr><td>precision</td><td>▃▃▃▂▂▃▂▂▂▂▂▂▁▂▃▄▄▅▆▇▅▆▇▆█▆▆▅▄▄▅▄▆</td></tr><tr><td>recall</td><td>██▆▆▄█▇▆▇▇▅▂▁▄▄▅▆▇▄▃▇▅▅▄▃▄▄▅▆▅▅▅▅</td></tr><tr><td>specificity</td><td>▁▁▃▂▄▁▁▂▂▂▄▇▇▄▅▅▄▄▇█▄▇▇▇█▇▇▅▅▆▆▅▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▂▂▁▂▂▂▃▃▃▃▄▄▅▆▆▆▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▇▇▇▇▇▇▇▆▆▆▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▃▃▃▂▁▃▂▂▂▂▂▂▂▂▃▅▅▇▇▇▆██▇▇▇▇▆▆▅▅▅▇</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▁▂▁▁▂▁▂▂▃▆▄▅▇▆▇██▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>f1_score</td><td>0.625</td></tr><tr><td>precision</td><td>0.6338</td></tr><tr><td>recall</td><td>0.61644</td></tr><tr><td>specificity</td><td>0.64384</td></tr><tr><td>train_acc</td><td>0.9211</td></tr><tr><td>train_loss</td><td>0.18854</td></tr><tr><td>val_acc</td><td>0.63014</td></tr><tr><td>val_loss</td><td>0.90079</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/olzlrj3k' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/olzlrj3k</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_070934-olzlrj3k/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:14:49,155] Trial 3 finished with values: [0.644352850317955, 0.6198630136986302] and parameters: {'lr': 0.0003258375713962658, 'wd': 1.1497395311377755e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 4 hyperparams --> lr=8.72e-05, wd=3.99e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_071449-47so6vvw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/47so6vvw' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/47so6vvw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/47so6vvw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 4]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7329 acc=0.5051 | val_loss=0.6954 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 002 | train_loss=0.7417 acc=0.4974 | val_loss=0.6968 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7417 acc=0.4906 | val_loss=0.6999 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7218 acc=0.5154 | val_loss=0.6959 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7282 acc=0.5000 | val_loss=0.6984 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7109 acc=0.5266 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 007 | train_loss=0.7026 acc=0.5240 | val_loss=0.6917 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7253 acc=0.4906 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7060 acc=0.5120 | val_loss=0.6926 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7102 acc=0.4949 | val_loss=0.6918 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7171 acc=0.4777 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 012 | train_loss=0.7199 acc=0.5051 | val_loss=0.6928 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7204 acc=0.5000 | val_loss=0.6925 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 014 | train_loss=0.7189 acc=0.4743 | val_loss=0.6942 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 015 | train_loss=0.7245 acc=0.4906 | val_loss=0.6926 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 016 | train_loss=0.7150 acc=0.5000 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 017 | train_loss=0.7011 acc=0.5249 | val_loss=0.6929 acc=0.5000 | prec=0.5000 rec=0.9932 spec=0.0068 f1=0.6651 | time=9.1s\n",
            "Epoch 018 | train_loss=0.7085 acc=0.5120 | val_loss=0.6948 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 019 | train_loss=0.7160 acc=0.4897 | val_loss=0.6932 acc=0.5137 | prec=0.5073 rec=0.9521 spec=0.0753 f1=0.6619 | time=9.0s\n",
            "Epoch 020 | train_loss=0.7156 acc=0.4906 | val_loss=0.6934 acc=0.4829 | prec=0.4888 rec=0.7466 spec=0.2192 f1=0.5908 | time=9.1s\n",
            "Epoch 021 | train_loss=0.7080 acc=0.5309 | val_loss=0.6930 acc=0.5000 | prec=0.5000 rec=0.9863 spec=0.0137 f1=0.6636 | time=9.2s\n",
            "Epoch 022 | train_loss=0.7119 acc=0.5137 | val_loss=0.6935 acc=0.4897 | prec=0.4934 rec=0.7740 spec=0.2055 f1=0.6027 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>█████████████████▁█▇█▇</td></tr><tr><td>precision</td><td>█████████████████▁████</td></tr><tr><td>recall</td><td>█████████████████▁█▆█▆</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▂▃▁▂</td></tr><tr><td>train_acc</td><td>▅▄▃▆▄▇▇▃▆▄▁▅▄▁▃▄▇▆▃▃█▆</td></tr><tr><td>train_loss</td><td>▆██▅▆▃▁▅▂▃▄▄▄▄▅▃▁▂▄▃▂▃</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅█▁▅▃</td></tr><tr><td>val_loss</td><td>▄▅█▅▇▂▁▁▂▁▂▂▂▃▂▂▂▄▂▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>f1_score</td><td>0.60267</td></tr><tr><td>precision</td><td>0.49345</td></tr><tr><td>recall</td><td>0.77397</td></tr><tr><td>specificity</td><td>0.20548</td></tr><tr><td>train_acc</td><td>0.51372</td></tr><tr><td>train_loss</td><td>0.71186</td></tr><tr><td>val_acc</td><td>0.48973</td></tr><tr><td>val_loss</td><td>0.69352</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/47so6vvw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/47so6vvw</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_071449-47so6vvw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:18:11,222] Trial 4 finished with values: [0.6917131602764129, 0.5] and parameters: {'lr': 8.718683454505486e-05, 'wd': 0.00039905975052944567}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 5 hyperparams --> lr=4.43e-05, wd=5.34e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_071811-mohfkj54</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/mohfkj54' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/mohfkj54' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/mohfkj54</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 5]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7408 acc=0.5009 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7427 acc=0.4828 | val_loss=0.6935 acc=0.5137 | prec=0.7000 rec=0.0479 spec=0.9795 f1=0.0897 | time=9.2s\n",
            "Epoch 003 | train_loss=0.7297 acc=0.5137 | val_loss=0.6934 acc=0.4897 | prec=0.4571 rec=0.1096 spec=0.8699 f1=0.1768 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7306 acc=0.4949 | val_loss=0.6926 acc=0.5068 | prec=0.5037 rec=0.9315 spec=0.0822 f1=0.6538 | time=9.0s\n",
            "Epoch 005 | train_loss=0.7358 acc=0.4854 | val_loss=0.6936 acc=0.4932 | prec=0.4688 rec=0.1027 spec=0.8836 f1=0.1685 | time=9.2s\n",
            "Epoch 006 | train_loss=0.7345 acc=0.4863 | val_loss=0.6929 acc=0.5445 | prec=0.5289 rec=0.8151 spec=0.2740 f1=0.6415 | time=9.0s\n",
            "Epoch 007 | train_loss=0.7187 acc=0.4991 | val_loss=0.6926 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7226 acc=0.4777 | val_loss=0.6925 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7268 acc=0.4923 | val_loss=0.6929 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7313 acc=0.4623 | val_loss=0.6936 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7303 acc=0.4828 | val_loss=0.6937 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7225 acc=0.4889 | val_loss=0.6932 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 013 | train_loss=0.7050 acc=0.5369 | val_loss=0.6950 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7195 acc=0.5086 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 015 | train_loss=0.7311 acc=0.4786 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 016 | train_loss=0.7036 acc=0.5300 | val_loss=0.6933 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>f1_score</td><td>█▁▂█▂███████████</td></tr><tr><td>precision</td><td>▂█▁▂▁▃▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>recall</td><td>█▁▁▇▁▇██████████</td></tr><tr><td>specificity</td><td>▁█▇▂▇▃▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_acc</td><td>▅▃▆▄▃▃▄▂▄▁▃▃█▅▃▇</td></tr><tr><td>train_loss</td><td>██▆▆▇▇▄▄▅▆▆▄▁▄▆▁</td></tr><tr><td>val_acc</td><td>▂▄▁▃▁█▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>val_loss</td><td>▁▄▄▂▄▃▂▂▃▄▅▄█▂▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>f1_score</td><td>0.66667</td></tr><tr><td>precision</td><td>0.5</td></tr><tr><td>recall</td><td>1</td></tr><tr><td>specificity</td><td>0</td></tr><tr><td>train_acc</td><td>0.53002</td></tr><tr><td>train_loss</td><td>0.70361</td></tr><tr><td>val_acc</td><td>0.5</td></tr><tr><td>val_loss</td><td>0.6933</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/mohfkj54' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/mohfkj54</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_071811-mohfkj54/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:20:38,869] Trial 5 finished with values: [0.6921621084213256, 0.5] and parameters: {'lr': 4.428865607812821e-05, 'wd': 0.0005339267579949896}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 6 hyperparams --> lr=1.53e-04, wd=1.79e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_072038-s9ve5lhs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/s9ve5lhs' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/s9ve5lhs' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/s9ve5lhs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 6]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7475 acc=0.5017 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7146 acc=0.5446 | val_loss=0.6928 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7285 acc=0.4897 | val_loss=0.7015 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7058 acc=0.5137 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7129 acc=0.4949 | val_loss=0.6941 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7130 acc=0.5103 | val_loss=0.6944 acc=0.4932 | prec=0.3750 rec=0.0205 spec=0.9658 f1=0.0390 | time=9.0s\n",
            "Epoch 007 | train_loss=0.7149 acc=0.5000 | val_loss=0.6933 acc=0.5205 | prec=0.5536 rec=0.2123 spec=0.8288 f1=0.3069 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7061 acc=0.4786 | val_loss=0.6954 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7047 acc=0.5086 | val_loss=0.6910 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7127 acc=0.5129 | val_loss=0.6909 acc=0.5068 | prec=0.5034 rec=1.0000 spec=0.0137 f1=0.6697 | time=9.2s\n",
            "Epoch 011 | train_loss=0.7057 acc=0.5283 | val_loss=0.6921 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 012 | train_loss=0.7013 acc=0.5180 | val_loss=0.6932 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7075 acc=0.5034 | val_loss=0.6928 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7024 acc=0.5163 | val_loss=0.6921 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.4s\n",
            "Epoch 015 | train_loss=0.7026 acc=0.5197 | val_loss=0.6920 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 016 | train_loss=0.6981 acc=0.5129 | val_loss=0.6920 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 017 | train_loss=0.6932 acc=0.5292 | val_loss=0.6932 acc=0.5000 | prec=0.5000 rec=0.9932 spec=0.0068 f1=0.6651 | time=9.1s\n",
            "Epoch 018 | train_loss=0.7050 acc=0.5026 | val_loss=0.6928 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=9.1s\n",
            "Epoch 019 | train_loss=0.6931 acc=0.5172 | val_loss=0.7001 acc=0.4692 | prec=0.4767 rec=0.6301 spec=0.3082 f1=0.5428 | time=9.0s\n",
            "Epoch 020 | train_loss=0.6904 acc=0.5420 | val_loss=0.6971 acc=0.4932 | prec=0.4911 rec=0.3767 spec=0.6096 f1=0.4264 | time=9.1s\n",
            "Epoch 021 | train_loss=0.6831 acc=0.5635 | val_loss=0.6995 acc=0.4795 | prec=0.4766 rec=0.4178 spec=0.5411 f1=0.4453 | time=9.1s\n",
            "Epoch 022 | train_loss=0.6803 acc=0.5626 | val_loss=0.7205 acc=0.5308 | prec=0.5542 rec=0.3151 spec=0.7466 f1=0.4017 | time=9.1s\n",
            "Epoch 023 | train_loss=0.6591 acc=0.5909 | val_loss=0.6835 acc=0.5582 | prec=0.6076 rec=0.3288 spec=0.7877 f1=0.4267 | time=9.1s\n",
            "Epoch 024 | train_loss=0.6653 acc=0.5961 | val_loss=0.6730 acc=0.5685 | prec=0.5781 rec=0.5068 spec=0.6301 f1=0.5401 | time=9.2s\n",
            "Epoch 025 | train_loss=0.6301 acc=0.6381 | val_loss=0.6402 acc=0.5993 | prec=0.5890 rec=0.6575 spec=0.5411 f1=0.6214 | time=9.0s\n",
            "Epoch 026 | train_loss=0.5938 acc=0.6913 | val_loss=0.6244 acc=0.5959 | prec=0.5700 rec=0.7808 spec=0.4110 f1=0.6590 | time=9.1s\n",
            "Epoch 027 | train_loss=0.5542 acc=0.7238 | val_loss=0.6167 acc=0.6096 | prec=0.5833 rec=0.7671 spec=0.4521 f1=0.6627 | time=9.0s\n",
            "Epoch 028 | train_loss=0.5181 acc=0.7393 | val_loss=0.5913 acc=0.6336 | prec=0.6102 rec=0.7397 spec=0.5274 f1=0.6687 | time=9.0s\n",
            "Epoch 029 | train_loss=0.4776 acc=0.7796 | val_loss=0.5694 acc=0.6610 | prec=0.6822 rec=0.6027 spec=0.7192 f1=0.6400 | time=9.1s\n",
            "Epoch 030 | train_loss=0.4541 acc=0.7804 | val_loss=0.6045 acc=0.6438 | prec=0.6061 rec=0.8219 spec=0.4658 f1=0.6977 | time=9.1s\n",
            "Epoch 031 | train_loss=0.4171 acc=0.8199 | val_loss=0.5758 acc=0.6575 | prec=0.6353 rec=0.7397 spec=0.5753 f1=0.6835 | time=9.2s\n",
            "Epoch 032 | train_loss=0.3800 acc=0.8456 | val_loss=0.5786 acc=0.6952 | prec=0.7021 rec=0.6781 spec=0.7123 f1=0.6899 | time=9.0s\n",
            "Epoch 033 | train_loss=0.3581 acc=0.8568 | val_loss=0.5859 acc=0.6712 | prec=0.7155 rec=0.5685 spec=0.7740 f1=0.6336 | time=9.1s\n",
            "Epoch 034 | train_loss=0.3211 acc=0.8705 | val_loss=0.6020 acc=0.6610 | prec=0.6298 rec=0.7808 spec=0.5411 f1=0.6972 | time=9.0s\n",
            "Epoch 035 | train_loss=0.3033 acc=0.8739 | val_loss=0.6257 acc=0.6644 | prec=0.6463 rec=0.7260 spec=0.6027 f1=0.6839 | time=9.0s\n",
            "Epoch 036 | train_loss=0.2935 acc=0.8834 | val_loss=0.6762 acc=0.6781 | prec=0.7063 rec=0.6096 spec=0.7466 f1=0.6544 | time=9.1s\n",
            "Epoch 037 | train_loss=0.2834 acc=0.8748 | val_loss=0.6380 acc=0.6815 | prec=0.6803 rec=0.6849 spec=0.6781 f1=0.6826 | time=9.0s\n",
            "Epoch 038 | train_loss=0.2716 acc=0.8825 | val_loss=0.6594 acc=0.6712 | prec=0.6543 rec=0.7260 spec=0.6164 f1=0.6883 | time=9.1s\n",
            "Epoch 039 | train_loss=0.2296 acc=0.9099 | val_loss=0.6808 acc=0.6747 | prec=0.6584 rec=0.7260 spec=0.6233 f1=0.6906 | time=9.0s\n",
            "Epoch 040 | train_loss=0.2463 acc=0.9074 | val_loss=0.7187 acc=0.6541 | prec=0.6364 rec=0.7192 spec=0.5890 f1=0.6752 | time=9.0s\n",
            "Epoch 041 | train_loss=0.2196 acc=0.9134 | val_loss=0.7177 acc=0.6507 | prec=0.6358 rec=0.7055 spec=0.5959 f1=0.6688 | time=9.1s\n",
            "Epoch 042 | train_loss=0.2132 acc=0.9082 | val_loss=0.7289 acc=0.6575 | prec=0.6554 rec=0.6644 spec=0.6507 f1=0.6599 | time=9.0s\n",
            "Epoch 043 | train_loss=0.1987 acc=0.9237 | val_loss=0.7309 acc=0.6644 | prec=0.6739 rec=0.6370 spec=0.6918 f1=0.6549 | time=9.0s\n",
            "Epoch 044 | train_loss=0.1904 acc=0.9271 | val_loss=0.7478 acc=0.6747 | prec=0.6917 rec=0.6301 spec=0.7192 f1=0.6595 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>██▁█▁▁▄▁█████████▆▅▅▅▆▇███▇█████████████</td></tr><tr><td>precision</td><td>▆▆▁▆▁▅▆▁▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇█▇▇█▇▇██▇█▇▇▇█</td></tr><tr><td>recall</td><td>██▁█▁▁▂▁█████████▅▄▄▃▅▆▆▆▆▅▇▆▆▆▆▅▆▆▆▆▆▆▅</td></tr><tr><td>specificity</td><td>▁▁█▁██▇█▁▁▁▁▁▁▁▁▁▃▅▅▇▅▅▄▄▅▆▄▅▆▅▅▆▆▅▅▅▅▆▆</td></tr><tr><td>train_acc</td><td>▁▂▁▂▁▁▁▁▁▂▂▁▂▂▂▂▁▂▂▂▃▃▃▄▅▅▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>███▇███▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▅▄▄▃▃▂▂▂▂▁▂▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▁▄▄▅▅▅▆▇▆▇█▇▇▇█▇▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▅▅▄▃▃▂▁▂▁▁▂▃▅▄▅▅▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>44</td></tr><tr><td>f1_score</td><td>0.6595</td></tr><tr><td>precision</td><td>0.69173</td></tr><tr><td>recall</td><td>0.63014</td></tr><tr><td>specificity</td><td>0.71918</td></tr><tr><td>train_acc</td><td>0.9271</td></tr><tr><td>train_loss</td><td>0.19043</td></tr><tr><td>val_acc</td><td>0.67466</td></tr><tr><td>val_loss</td><td>0.74781</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/s9ve5lhs' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/s9ve5lhs</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_072038-s9ve5lhs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:27:20,167] Trial 6 finished with values: [0.5694409310817719, 0.660958904109589] and parameters: {'lr': 0.00015285834289101232, 'wd': 0.00017917460394166492}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 7 hyperparams --> lr=9.18e-03, wd=1.75e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_072720-kli1ayul</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/kli1ayul' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/kli1ayul' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/kli1ayul</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 7]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7254 acc=0.5017 | val_loss=0.6929 acc=0.4932 | prec=0.4965 rec=0.9658 spec=0.0205 f1=0.6558 | time=9.3s\n",
            "Epoch 002 | train_loss=0.7098 acc=0.5034 | val_loss=0.6944 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7331 acc=0.5069 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7038 acc=0.5257 | val_loss=0.6914 acc=0.5342 | prec=0.5225 rec=0.7945 spec=0.2740 f1=0.6304 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7084 acc=0.4923 | val_loss=0.7042 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 006 | train_loss=0.7100 acc=0.4871 | val_loss=0.6971 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.6994 acc=0.5103 | val_loss=0.6914 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7009 acc=0.5129 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 009 | train_loss=0.6918 acc=0.5326 | val_loss=0.8229 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 010 | train_loss=0.6999 acc=0.5111 | val_loss=0.6883 acc=0.5719 | prec=0.6000 rec=0.4315 spec=0.7123 f1=0.5020 | time=9.1s\n",
            "Epoch 011 | train_loss=0.6772 acc=0.5695 | val_loss=0.6689 acc=0.6404 | prec=0.6158 rec=0.7466 spec=0.5342 f1=0.6749 | time=9.0s\n",
            "Epoch 012 | train_loss=0.6569 acc=0.5892 | val_loss=0.6946 acc=0.5034 | prec=0.5017 rec=0.9932 spec=0.0137 f1=0.6667 | time=9.2s\n",
            "Epoch 013 | train_loss=0.6987 acc=0.5026 | val_loss=0.6947 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=9.1s\n",
            "Epoch 014 | train_loss=0.6904 acc=0.5326 | val_loss=0.7123 acc=0.5068 | prec=0.5068 rec=0.5137 spec=0.5000 f1=0.5102 | time=9.1s\n",
            "Epoch 015 | train_loss=0.6714 acc=0.5566 | val_loss=0.6972 acc=0.5651 | prec=0.5748 rec=0.5000 spec=0.6301 f1=0.5348 | time=9.3s\n",
            "Epoch 016 | train_loss=0.6472 acc=0.6029 | val_loss=0.6908 acc=0.5582 | prec=0.8696 rec=0.1370 spec=0.9795 f1=0.2367 | time=9.2s\n",
            "Epoch 017 | train_loss=0.6276 acc=0.6321 | val_loss=0.7399 acc=0.5205 | prec=0.5106 rec=0.9863 spec=0.0548 f1=0.6729 | time=9.2s\n",
            "Epoch 018 | train_loss=0.6028 acc=0.6715 | val_loss=1.9364 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 019 | train_loss=0.5961 acc=0.6638 | val_loss=0.8693 acc=0.5137 | prec=0.5070 rec=0.9932 spec=0.0342 f1=0.6713 | time=9.2s\n",
            "Epoch 020 | train_loss=0.5890 acc=0.6981 | val_loss=0.7022 acc=0.6233 | prec=0.6552 rec=0.5205 spec=0.7260 f1=0.5802 | time=9.2s\n",
            "Epoch 021 | train_loss=0.6171 acc=0.6492 | val_loss=0.7060 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 022 | train_loss=0.5817 acc=0.6904 | val_loss=0.7553 acc=0.5993 | prec=0.8718 rec=0.2329 spec=0.9658 f1=0.3676 | time=9.3s\n",
            "Epoch 023 | train_loss=0.5139 acc=0.7419 | val_loss=1.3464 acc=0.5171 | prec=0.7778 rec=0.0479 spec=0.9863 f1=0.0903 | time=9.1s\n",
            "Epoch 024 | train_loss=0.5226 acc=0.7384 | val_loss=0.6777 acc=0.5890 | prec=0.5890 rec=0.5890 spec=0.5890 f1=0.5890 | time=9.3s\n",
            "Epoch 025 | train_loss=0.4909 acc=0.7350 | val_loss=0.8328 acc=0.5582 | prec=0.8400 rec=0.1438 spec=0.9726 f1=0.2456 | time=9.2s\n",
            "Epoch 026 | train_loss=0.4818 acc=0.7719 | val_loss=0.8066 acc=0.6164 | prec=0.8400 rec=0.2877 spec=0.9452 f1=0.4286 | time=9.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>f1_score</td><td>████▁▁███▆███▆▇▃███▇█▅▂▇▄▅</td></tr><tr><td>precision</td><td>▅▅▅▅▁▁▅▅▅▆▆▅▅▅▆█▅▅▅▆▅█▇▆██</td></tr><tr><td>recall</td><td>███▇▁▁███▄▆██▅▅▂███▅█▃▁▅▂▃</td></tr><tr><td>specificity</td><td>▁▁▁▃██▁▁▁▆▅▁▁▅▅█▁▁▁▆▁██▅██</td></tr><tr><td>train_acc</td><td>▁▁▁▂▁▁▂▂▂▂▃▄▁▂▃▄▅▆▅▆▅▆▇▇▇█</td></tr><tr><td>train_loss</td><td>█▇█▇▇▇▇▇▇▇▆▆▇▇▆▆▅▄▄▄▅▄▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▃▁▁▁▁▁▅█▁▁▂▄▄▂▁▂▇▁▆▂▆▄▇</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁█▂▁▁▁▅▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>f1_score</td><td>0.42857</td></tr><tr><td>precision</td><td>0.84</td></tr><tr><td>recall</td><td>0.28767</td></tr><tr><td>specificity</td><td>0.94521</td></tr><tr><td>train_acc</td><td>0.77187</td></tr><tr><td>train_loss</td><td>0.48175</td></tr><tr><td>val_acc</td><td>0.61644</td></tr><tr><td>val_loss</td><td>0.80665</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/kli1ayul' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/kli1ayul</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_072720-kli1ayul/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:31:20,607] Trial 7 finished with values: [0.6688831150531769, 0.6404109589041096] and parameters: {'lr': 0.009177968480590444, 'wd': 1.7462020448863173e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 8 hyperparams --> lr=1.07e-05, wd=6.87e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_073120-ldcw4c41</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/ldcw4c41' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/ldcw4c41' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/ldcw4c41</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 8]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7317 acc=0.4931 | val_loss=0.7075 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7246 acc=0.5017 | val_loss=0.7041 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 003 | train_loss=0.7236 acc=0.5034 | val_loss=0.7040 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7338 acc=0.4974 | val_loss=0.7019 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 005 | train_loss=0.7212 acc=0.5163 | val_loss=0.7006 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7227 acc=0.5060 | val_loss=0.7023 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7188 acc=0.5094 | val_loss=0.7035 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7190 acc=0.4983 | val_loss=0.7060 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7244 acc=0.4897 | val_loss=0.7059 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7299 acc=0.4691 | val_loss=0.7000 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7091 acc=0.5051 | val_loss=0.7013 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 012 | train_loss=0.7023 acc=0.5334 | val_loss=0.7020 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 013 | train_loss=0.7152 acc=0.4940 | val_loss=0.6995 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7217 acc=0.4691 | val_loss=0.7001 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 015 | train_loss=0.7140 acc=0.5051 | val_loss=0.6954 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.0s\n",
            "Epoch 016 | train_loss=0.7270 acc=0.4794 | val_loss=0.6923 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=9.0s\n",
            "Epoch 017 | train_loss=0.7172 acc=0.4949 | val_loss=0.6917 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 018 | train_loss=0.7077 acc=0.5120 | val_loss=0.6940 acc=0.4966 | prec=0.4667 rec=0.0479 spec=0.9452 f1=0.0870 | time=9.1s\n",
            "Epoch 019 | train_loss=0.7093 acc=0.5043 | val_loss=0.6947 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 020 | train_loss=0.7054 acc=0.5094 | val_loss=0.6967 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 021 | train_loss=0.7107 acc=0.4931 | val_loss=0.6941 acc=0.4589 | prec=0.4167 rec=0.2055 spec=0.7123 f1=0.2752 | time=9.1s\n",
            "Epoch 022 | train_loss=0.7001 acc=0.5086 | val_loss=0.6950 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.1s\n",
            "Epoch 023 | train_loss=0.7057 acc=0.5051 | val_loss=0.6947 acc=0.4863 | prec=0.2500 rec=0.0137 spec=0.9589 f1=0.0260 | time=9.0s\n",
            "Epoch 024 | train_loss=0.7061 acc=0.5034 | val_loss=0.6953 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 025 | train_loss=0.7025 acc=0.5180 | val_loss=0.6941 acc=0.5000 | prec=0.5000 rec=0.0068 spec=0.9932 f1=0.0135 | time=9.1s\n",
            "Epoch 026 | train_loss=0.7082 acc=0.4949 | val_loss=0.6953 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 027 | train_loss=0.7047 acc=0.5172 | val_loss=0.6951 acc=0.4932 | prec=0.0000 rec=0.0000 spec=0.9863 f1=0.0000 | time=8.9s\n",
            "Epoch 028 | train_loss=0.7044 acc=0.5051 | val_loss=0.6936 acc=0.5240 | prec=0.5154 rec=0.8014 spec=0.2466 f1=0.6273 | time=9.1s\n",
            "Epoch 029 | train_loss=0.7076 acc=0.5043 | val_loss=0.6946 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.1s\n",
            "Epoch 030 | train_loss=0.7080 acc=0.4828 | val_loss=0.6941 acc=0.5034 | prec=0.5041 rec=0.4178 spec=0.5890 f1=0.4569 | time=9.1s\n",
            "Epoch 031 | train_loss=0.6961 acc=0.5292 | val_loss=0.6937 acc=0.4932 | prec=0.4955 rec=0.7534 spec=0.2329 f1=0.5978 | time=9.1s\n",
            "Epoch 032 | train_loss=0.7066 acc=0.4914 | val_loss=0.6938 acc=0.4726 | prec=0.4818 rec=0.7260 spec=0.2192 f1=0.5792 | time=8.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▂▁▁▄▁▁▁▁▁▁█▁▆▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▇▁▁▇▁▄▁█▁▁█▁███</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁██▁▁▁▂▁▁▁▁▁▁▇▁▄▆▆</td></tr><tr><td>specificity</td><td>███████████████▁▁███▆██████▃█▅▃▃</td></tr><tr><td>train_acc</td><td>▄▅▅▄▆▅▅▄▃▁▅█▄▁▅▂▄▆▅▅▄▅▅▅▆▄▆▅▅▂█▃</td></tr><tr><td>train_loss</td><td>█▆▆█▆▆▅▅▆▇▃▂▅▆▄▇▅▃▃▃▄▂▃▃▂▃▃▃▃▃▁▃</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▄▅▅▅▅█▅▆▅▂</td></tr><tr><td>val_loss</td><td>█▆▆▆▅▆▆▇▇▅▅▆▄▅▃▁▁▂▂▃▂▂▂▃▂▃▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>f1_score</td><td>0.57923</td></tr><tr><td>precision</td><td>0.48182</td></tr><tr><td>recall</td><td>0.72603</td></tr><tr><td>specificity</td><td>0.21918</td></tr><tr><td>train_acc</td><td>0.49142</td></tr><tr><td>train_loss</td><td>0.70658</td></tr><tr><td>val_acc</td><td>0.4726</td></tr><tr><td>val_loss</td><td>0.69378</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/ldcw4c41' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/ldcw4c41</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_073120-ldcw4c41/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:36:12,988] Trial 8 finished with values: [0.6917372405529022, 0.5] and parameters: {'lr': 1.0679005508681456e-05, 'wd': 0.0006872347333110481}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 9 hyperparams --> lr=4.27e-03, wd=3.81e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_073612-9n4t1v4o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/9n4t1v4o' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/9n4t1v4o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/9n4t1v4o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 9]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7149 acc=0.4880 | val_loss=0.6934 acc=0.4658 | prec=0.4324 rec=0.2192 spec=0.7123 f1=0.2909 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7022 acc=0.4949 | val_loss=0.7016 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.6993 acc=0.5043 | val_loss=0.6962 acc=0.4966 | prec=0.4951 rec=0.3493 spec=0.6438 f1=0.4096 | time=9.1s\n",
            "Epoch 004 | train_loss=0.6913 acc=0.5206 | val_loss=0.6992 acc=0.5137 | prec=0.5455 rec=0.1644 spec=0.8630 f1=0.2526 | time=9.0s\n",
            "Epoch 005 | train_loss=0.6766 acc=0.5763 | val_loss=0.6924 acc=0.5445 | prec=0.5684 rec=0.3699 spec=0.7192 f1=0.4481 | time=9.1s\n",
            "Epoch 006 | train_loss=0.6467 acc=0.6235 | val_loss=0.6862 acc=0.5479 | prec=0.5603 rec=0.4452 spec=0.6507 f1=0.4962 | time=9.2s\n",
            "Epoch 007 | train_loss=0.6179 acc=0.6655 | val_loss=0.6401 acc=0.5788 | prec=0.5550 rec=0.7945 spec=0.3630 f1=0.6535 | time=9.2s\n",
            "Epoch 008 | train_loss=0.5803 acc=0.6895 | val_loss=0.6437 acc=0.6096 | prec=0.8200 rec=0.2808 spec=0.9384 f1=0.4184 | time=9.1s\n",
            "Epoch 009 | train_loss=0.5514 acc=0.7041 | val_loss=0.6644 acc=0.6370 | prec=0.7703 rec=0.3904 spec=0.8836 f1=0.5182 | time=9.1s\n",
            "Epoch 010 | train_loss=0.4818 acc=0.7684 | val_loss=0.8633 acc=0.5411 | prec=0.5214 rec=1.0000 spec=0.0822 f1=0.6854 | time=9.1s\n",
            "Epoch 011 | train_loss=0.4795 acc=0.7547 | val_loss=0.6739 acc=0.6233 | prec=0.6552 rec=0.5205 spec=0.7260 f1=0.5802 | time=9.0s\n",
            "Epoch 012 | train_loss=0.4736 acc=0.7830 | val_loss=0.7276 acc=0.5479 | prec=0.5263 rec=0.9589 spec=0.1370 f1=0.6796 | time=9.1s\n",
            "Epoch 013 | train_loss=0.4706 acc=0.7659 | val_loss=0.7912 acc=0.6336 | prec=0.6102 rec=0.7397 spec=0.5274 f1=0.6687 | time=9.0s\n",
            "Epoch 014 | train_loss=0.4769 acc=0.7633 | val_loss=0.7712 acc=0.6130 | prec=0.6260 rec=0.5616 spec=0.6644 f1=0.5921 | time=9.2s\n",
            "Epoch 015 | train_loss=0.4457 acc=0.7873 | val_loss=0.9351 acc=0.5582 | prec=0.8400 rec=0.1438 spec=0.9726 f1=0.2456 | time=9.1s\n",
            "Epoch 016 | train_loss=0.3976 acc=0.8233 | val_loss=1.1264 acc=0.5856 | prec=0.8205 rec=0.2192 spec=0.9521 f1=0.3459 | time=9.0s\n",
            "Epoch 017 | train_loss=0.4022 acc=0.8182 | val_loss=0.9035 acc=0.5856 | prec=0.6984 rec=0.3014 spec=0.8699 f1=0.4211 | time=9.1s\n",
            "Epoch 018 | train_loss=0.3646 acc=0.8370 | val_loss=1.2600 acc=0.5753 | prec=0.5407 rec=1.0000 spec=0.1507 f1=0.7019 | time=9.0s\n",
            "Epoch 019 | train_loss=0.3301 acc=0.8448 | val_loss=0.7410 acc=0.5753 | prec=0.6100 rec=0.4178 spec=0.7329 f1=0.4959 | time=9.1s\n",
            "Epoch 020 | train_loss=0.3427 acc=0.8328 | val_loss=0.6331 acc=0.6575 | prec=0.6438 rec=0.7055 spec=0.6096 f1=0.6732 | time=9.1s\n",
            "Epoch 021 | train_loss=0.3114 acc=0.8576 | val_loss=1.0031 acc=0.6096 | prec=0.5708 rec=0.8836 spec=0.3356 f1=0.6935 | time=9.2s\n",
            "Epoch 022 | train_loss=0.3113 acc=0.8525 | val_loss=1.6447 acc=0.5034 | prec=0.6667 rec=0.0137 spec=0.9932 f1=0.0268 | time=9.1s\n",
            "Epoch 023 | train_loss=0.3090 acc=0.8705 | val_loss=0.7775 acc=0.6027 | prec=0.7206 rec=0.3356 spec=0.8699 f1=0.4579 | time=9.1s\n",
            "Epoch 024 | train_loss=0.2769 acc=0.8585 | val_loss=1.4553 acc=0.5719 | prec=0.6082 rec=0.4041 spec=0.7397 f1=0.4856 | time=9.3s\n",
            "Epoch 025 | train_loss=0.2453 acc=0.8937 | val_loss=1.4068 acc=0.6062 | prec=0.5728 rec=0.8356 spec=0.3767 f1=0.6797 | time=9.2s\n",
            "Epoch 026 | train_loss=0.2769 acc=0.8756 | val_loss=1.9455 acc=0.5205 | prec=0.5105 rec=1.0000 spec=0.0411 f1=0.6759 | time=9.2s\n",
            "Epoch 027 | train_loss=0.2213 acc=0.9039 | val_loss=2.1434 acc=0.5582 | prec=0.5326 rec=0.9521 spec=0.1644 f1=0.6830 | time=9.0s\n",
            "Epoch 028 | train_loss=0.2262 acc=0.8945 | val_loss=1.5479 acc=0.6027 | prec=0.6562 rec=0.4315 spec=0.7740 f1=0.5207 | time=9.2s\n",
            "Epoch 029 | train_loss=0.2120 acc=0.9014 | val_loss=1.7037 acc=0.5788 | prec=0.6456 rec=0.3493 spec=0.8082 f1=0.4533 | time=9.3s\n",
            "Epoch 030 | train_loss=0.1697 acc=0.9117 | val_loss=1.4444 acc=0.5274 | prec=0.6429 rec=0.1233 spec=0.9315 f1=0.2069 | time=9.1s\n",
            "Epoch 031 | train_loss=0.2033 acc=0.8834 | val_loss=2.5486 acc=0.5342 | prec=0.5188 rec=0.9452 spec=0.1233 f1=0.6699 | time=9.3s\n",
            "Epoch 032 | train_loss=0.1849 acc=0.9117 | val_loss=1.5793 acc=0.6233 | prec=0.6475 rec=0.5411 spec=0.7055 f1=0.5896 | time=9.1s\n",
            "Epoch 033 | train_loss=0.2973 acc=0.8636 | val_loss=0.7062 acc=0.6507 | prec=0.6183 rec=0.7877 spec=0.5137 f1=0.6928 | time=9.3s\n",
            "Epoch 034 | train_loss=0.2877 acc=0.8499 | val_loss=2.7352 acc=0.5616 | prec=0.6406 rec=0.2808 spec=0.8425 f1=0.3905 | time=9.2s\n",
            "Epoch 035 | train_loss=0.2563 acc=0.8636 | val_loss=1.8360 acc=0.6199 | prec=0.5822 rec=0.8493 spec=0.3904 f1=0.6908 | time=9.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▄▁▅▄▅▆█▅▆█▇██▇▃▄▅█▆██▁▆▆███▆▆▃█▇█▅█</td></tr><tr><td>precision</td><td>▅▁▅▆▆▆▆█▇▅▆▅▆▆██▇▆▆▆▆▇▇▆▆▅▅▆▆▆▅▆▆▆▆</td></tr><tr><td>recall</td><td>▃▁▃▂▄▄▇▃▄█▅█▆▅▂▃▃█▄▆▇▁▃▄▇██▄▃▂█▅▇▃▇</td></tr><tr><td>specificity</td><td>▆█▅▇▆▅▃█▇▁▆▂▅▆██▇▂▆▅▃█▇▆▃▁▂▆▇█▂▆▄▇▄</td></tr><tr><td>train_acc</td><td>▁▁▁▂▂▃▄▄▅▆▅▆▆▆▆▇▆▇▇▇▇▇▇▇█▇██████▇▇▇</td></tr><tr><td>train_loss</td><td>█████▇▇▆▆▅▅▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▃▃▂</td></tr><tr><td>val_acc</td><td>▁▂▂▃▄▄▅▆▇▄▇▄▇▆▄▅▅▅▅█▆▂▆▅▆▃▄▆▅▃▃▇█▅▇</td></tr><tr><td>val_loss</td><td>▁▁▁▁▁▁▁▁▁▂▁▁▂▁▂▃▂▃▁▁▂▄▁▄▄▅▆▄▅▄▇▄▁█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>f1_score</td><td>0.69081</td></tr><tr><td>precision</td><td>0.58216</td></tr><tr><td>recall</td><td>0.84932</td></tr><tr><td>specificity</td><td>0.39041</td></tr><tr><td>train_acc</td><td>0.86364</td></tr><tr><td>train_loss</td><td>0.25632</td></tr><tr><td>val_acc</td><td>0.61986</td></tr><tr><td>val_loss</td><td>1.83603</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/9n4t1v4o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1/runs/9n4t1v4o</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_073612-9n4t1v4o/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:41:35,178] Trial 9 finished with values: [0.6331340909004212, 0.6575342465753424] and parameters: {'lr': 0.0042651260745797835, 'wd': 3.8141822461242195e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Selected Trial #6 ===\n",
            " val_loss=0.5694, val_acc=0.6610, params={'lr': 0.00015285834289101232, 'wd': 0.00017917460394166492}, best_epoch=29, ckpt=ckpts/trial6_optimal_AD_FTD_1.pth\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=218  A(D)=146, F(TD)=72\n",
            " Accuracy=0.6927 Sensitivity=0.5833 Specificity=0.7466 F1=0.5563\n",
            "\n",
            "-- test_cross -- total=566  A(D)=319, F(TD)=247\n",
            " Accuracy=0.5883 Sensitivity=0.3725 Specificity=0.7555 F1=0.4412\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "PCT_START   = 0.2  # fixed\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & count splits ──────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "class0, class1 = 'A','F'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "# AD vs FTD Filtering\n",
        "train_meta       = [d for d in train_meta if d['label'] in (class0, class1)]\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "def count_labels(meta_list):\n",
        "    cnt0 = cnt1 = 0\n",
        "    for d in meta_list:\n",
        "        lbl = d['label']\n",
        "        if isinstance(lbl, str):\n",
        "            if lbl == class0: cnt0 += 1\n",
        "            elif lbl == class1: cnt1 += 1\n",
        "        else:\n",
        "            if lbl == 0: cnt0 += 1\n",
        "            elif lbl == 1: cnt1 += 1\n",
        "    return cnt0, cnt1\n",
        "\n",
        "n_tr0, n_tr1 = count_labels(train_meta)\n",
        "n_tw0, n_tw1 = count_labels(test_within_meta)\n",
        "n_tc0, n_tc1 = count_labels(test_cross_meta)\n",
        "\n",
        "print(f\"--> Data counts before balancing:\")\n",
        "print(f\"    TRAIN         total={len(train_meta)}  A(D)={n_tr0}, F(TD)={n_tr1}\")\n",
        "print(f\"    TEST_WITHIN   total={len(test_within_meta)}  A(D)={n_tw0}, F(TD)={n_tw1}\")\n",
        "print(f\"    TEST_CROSS    total={len(test_cross_meta)}  A(D)={n_tc0}, F(TD)={n_tc1}\\n\")\n",
        "\n",
        "# ─── Balance train set ───────────────────────────────────────────\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "balanced_meta = [copy.deepcopy(d) for d in balanced_meta]\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# ─── After Balancing ───────────────────────────────────────────\n",
        "bal_AD, bal_FTD = count_labels(balanced_meta)\n",
        "print(f\"[BALANCED TRAIN] total={len(balanced_meta)}  AD={bal_AD}, FTD={bal_FTD}\\n\")\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset_train = BinaryEEGDataset(raw_ds_train, balanced_meta)\n",
        "labels_train  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── Optuna Objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # 1) sample hyperparameters\n",
        "    lr = trial.suggest_float('lr', 5e-5, 5e-4, log=True)\n",
        "    wd = trial.suggest_float('wd', 5e-5, 5e-4, log=True)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-AD-FTD-test-within-cross-2',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # train/validation split\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2,\n",
        "        stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "\n",
        "    tr_AD = np.sum(labels_train[tr_idx] == 0)\n",
        "    tr_FTD = np.sum(labels_train[tr_idx] == 1)\n",
        "    va_AD = np.sum(labels_train[va_idx] == 0)\n",
        "    va_FTD = np.sum(labels_train[va_idx] == 1)\n",
        "    print(f\"[Trial {trial.number}]  \"\n",
        "          f\"TRAIN n={len(tr_idx)} AD={tr_AD}, FTD={tr_FTD}) | \"\n",
        "          f\"VAL n={len(va_idx)} AD={va_AD}, FTD={va_FTD}\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset_train, tr_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset_train, va_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # model & optimizer & scheduler & loss\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc  = 0.0\n",
        "    best_state= None\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # ── train ──\n",
        "        model.train()\n",
        "        train_loss_sum = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight_decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = wd * (cur_lr / lr)\n",
        "            train_loss_sum += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            train_correct += (preds == y).sum().item()\n",
        "            train_total   += y.size(0)\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "        train_acc  = train_correct / train_total\n",
        "\n",
        "        # ── validate ──\n",
        "        model.eval()\n",
        "        vloss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss / len(val_loader)\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        val_acc = (preds == labs).sum() / labs.size\n",
        "\n",
        "        # ── metrics ──\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "        spec = tn / (tn+fp) if (tn+fp)>0 else 0.0\n",
        "        prec = precision_score(labs, preds, zero_division=0)\n",
        "        rec  = recall_score(labs, preds, zero_division=0)\n",
        "        f1   = f1_score(labs, preds, zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={prec:.4f} rec={rec:.4f} spec={spec:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':       epoch,\n",
        "            'train_loss':  train_loss,\n",
        "            'train_acc':   train_acc,\n",
        "            'val_loss':    val_loss,\n",
        "            'val_acc':     val_acc,\n",
        "            'specificity': spec,\n",
        "            'precision':   prec,\n",
        "            'recall':      rec,\n",
        "            'f1_score':    f1\n",
        "        })\n",
        "\n",
        "        # ── Early Stopping (val_loss 기준) ──\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc  = val_acc\n",
        "            es_count      = 0\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            os.makedirs('ckpts', exist_ok=True)\n",
        "            ckpt_path = f'ckpts/trial{trial.number}_optimal_AD_FTD_2.pth'\n",
        "            torch.save(best_state, ckpt_path)\n",
        "            trial.set_user_attr('ckpt_path', ckpt_path)\n",
        "            trial.set_user_attr('best_epoch', epoch)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, train_loader, val_loader, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 다중목적 리턴\n",
        "    return best_val_loss, best_val_acc\n",
        "\n",
        "# ─── Run Optuna Study ────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=[\"minimize\", \"maximize\"],\n",
        "        study_name=\"eeg_multiobj_AD_FTD_2\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Pareto front 중 val_acc 가 가장 높은 trial 선택\n",
        "    best       = max(study.best_trials, key=lambda t: t.values[1])\n",
        "    bv_loss, bv_acc = best.values\n",
        "    best_epoch = best.user_attrs[\"best_epoch\"]\n",
        "    ckpt_path  = best.user_attrs[\"ckpt_path\"]\n",
        "\n",
        "    print(f\"\\n=== Selected Trial #{best.number} ===\")\n",
        "    print(\n",
        "        f\" val_loss={bv_loss:.4f}, val_acc={bv_acc:.4f}, \"\n",
        "        f\"params={best.params}, best_epoch={best_epoch}, ckpt={ckpt_path}\"\n",
        "    )\n",
        "\n",
        "    # ─── Search‑model 로드 ───────────────────────────────────────\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    search_model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    search_model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    search_model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── Evaluation 함수 ─────────────────────────────────────────\n",
        "    def evaluate(model, metas, criterion):\n",
        "        metas_copy = copy.deepcopy(metas)\n",
        "        for d in metas_copy:\n",
        "            if isinstance(d[\"label\"], str):\n",
        "                d[\"label\"] = label_map[d[\"label\"]]\n",
        "\n",
        "        ds = BinaryEEGDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal += y.size(0)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0, 1]).ravel()\n",
        "        return {\n",
        "            \"loss\":        vloss / len(loader),\n",
        "            \"acc\":         vcorrect / vtotal,\n",
        "            \"sensitivity\": recall_score(labs, preds, zero_division=0),\n",
        "            \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
        "            \"f1\":          f1_score(labs, preds, zero_division=0),\n",
        "        }\n",
        "\n",
        "    # ─── Final Evaluation on Test Sets ────────────────────────────\n",
        "    print(\"=== Final Evaluation on Test Sets ===\")\n",
        "    for name, metas in [\n",
        "        (\"test_within\", test_within_meta),\n",
        "        (\"test_cross\",  test_cross_meta),\n",
        "    ]:\n",
        "        res = evaluate(search_model, metas, criterion)\n",
        "        n0, n1 = count_labels(metas)\n",
        "        print(f\"-- {name} -- total={len(metas)}  A(D)={n0}, F(TD)={n1}\")\n",
        "        print(\n",
        "            f\" Accuracy={res['acc']:.4f} \"\n",
        "            f\"Sensitivity={res['sensitivity']:.4f} \"\n",
        "            f\"Specificity={res['specificity']:.4f} \"\n",
        "            f\"F1={res['f1']:.4f}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bj8KUWA3_6sy",
        "outputId": "376851f7-6e5b-4d92-e8f4-ce3df29d5973"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:25:56,052] A new study created in memory with name: eeg_multiobj_AD_FTD_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Data counts before balancing:\n",
            "    TRAIN         total=2117  A(D)=1388, F(TD)=729\n",
            "    TEST_WITHIN   total=218  A(D)=146, F(TD)=72\n",
            "    TEST_CROSS    total=566  A(D)=319, F(TD)=247\n",
            "\n",
            "[BALANCED TRAIN] total=1458  AD=729, FTD=729\n",
            "\n",
            "\n",
            "--- Trial 0 hyperparams --> lr=1.28e-04, wd=1.39e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_082556-ahmqowfr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/ahmqowfr' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/ahmqowfr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/ahmqowfr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 0]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7494 acc=0.4906 | val_loss=0.6921 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7342 acc=0.5146 | val_loss=0.6940 acc=0.4966 | prec=0.4615 rec=0.0411 spec=0.9521 f1=0.0755 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7469 acc=0.4940 | val_loss=0.6940 acc=0.5103 | prec=0.5306 rec=0.1781 spec=0.8425 f1=0.2667 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7486 acc=0.5069 | val_loss=0.6953 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 005 | train_loss=0.7198 acc=0.5232 | val_loss=0.6986 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7352 acc=0.4906 | val_loss=0.6918 acc=0.5000 | prec=0.5000 rec=0.9589 spec=0.0411 f1=0.6573 | time=8.9s\n",
            "Epoch 007 | train_loss=0.7276 acc=0.4991 | val_loss=0.6924 acc=0.5205 | prec=0.5160 rec=0.6644 spec=0.3767 f1=0.5808 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7187 acc=0.5154 | val_loss=0.6916 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 009 | train_loss=0.7227 acc=0.5137 | val_loss=0.6919 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 010 | train_loss=0.7133 acc=0.5137 | val_loss=0.6936 acc=0.4966 | prec=0.4889 rec=0.1507 spec=0.8425 f1=0.2304 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7244 acc=0.5051 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7189 acc=0.5163 | val_loss=0.6921 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7271 acc=0.4949 | val_loss=0.6925 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 014 | train_loss=0.7131 acc=0.5129 | val_loss=0.6947 acc=0.5205 | prec=0.6000 rec=0.1233 spec=0.9178 f1=0.2045 | time=8.9s\n",
            "Epoch 015 | train_loss=0.7226 acc=0.4923 | val_loss=0.6931 acc=0.5034 | prec=0.5030 rec=0.5753 spec=0.4315 f1=0.5367 | time=9.1s\n",
            "Epoch 016 | train_loss=0.6936 acc=0.5412 | val_loss=0.7035 acc=0.5205 | prec=0.5250 rec=0.4315 spec=0.6096 f1=0.4737 | time=8.9s\n",
            "Epoch 017 | train_loss=0.7039 acc=0.5360 | val_loss=0.7033 acc=0.5137 | prec=0.5078 rec=0.8904 spec=0.1370 f1=0.6468 | time=9.0s\n",
            "Epoch 018 | train_loss=0.7128 acc=0.5343 | val_loss=0.7102 acc=0.4897 | prec=0.4892 rec=0.4658 spec=0.5137 f1=0.4772 | time=9.0s\n",
            "Epoch 019 | train_loss=0.6927 acc=0.5540 | val_loss=0.7044 acc=0.5068 | prec=0.5079 rec=0.4384 spec=0.5753 f1=0.4706 | time=9.1s\n",
            "Epoch 020 | train_loss=0.6807 acc=0.5746 | val_loss=0.7264 acc=0.5205 | prec=0.5300 rec=0.3630 spec=0.6781 f1=0.4309 | time=9.1s\n",
            "Epoch 021 | train_loss=0.6679 acc=0.5943 | val_loss=0.7167 acc=0.5205 | prec=0.5205 rec=0.5205 spec=0.5205 f1=0.5205 | time=8.9s\n",
            "Epoch 022 | train_loss=0.6634 acc=0.6106 | val_loss=0.7229 acc=0.5479 | prec=0.5432 rec=0.6027 spec=0.4932 f1=0.5714 | time=9.1s\n",
            "Epoch 023 | train_loss=0.6529 acc=0.6081 | val_loss=0.6822 acc=0.5616 | prec=0.5517 rec=0.6575 spec=0.4658 f1=0.6000 | time=8.9s\n",
            "Epoch 024 | train_loss=0.6422 acc=0.6372 | val_loss=0.7179 acc=0.5719 | prec=0.5802 rec=0.5205 spec=0.6233 f1=0.5487 | time=8.9s\n",
            "Epoch 025 | train_loss=0.6199 acc=0.6527 | val_loss=0.7090 acc=0.5822 | prec=0.6000 rec=0.4932 spec=0.6712 f1=0.5414 | time=9.0s\n",
            "Epoch 026 | train_loss=0.5901 acc=0.6998 | val_loss=0.7010 acc=0.6062 | prec=0.6372 rec=0.4932 spec=0.7192 f1=0.5560 | time=8.9s\n",
            "Epoch 027 | train_loss=0.5393 acc=0.7470 | val_loss=0.6587 acc=0.6130 | prec=0.5902 rec=0.7397 spec=0.4863 f1=0.6565 | time=9.1s\n",
            "Epoch 028 | train_loss=0.5284 acc=0.7444 | val_loss=0.6545 acc=0.6164 | prec=0.6250 rec=0.5822 spec=0.6507 f1=0.6028 | time=9.1s\n",
            "Epoch 029 | train_loss=0.5109 acc=0.7633 | val_loss=0.6889 acc=0.6199 | prec=0.6577 rec=0.5000 spec=0.7397 f1=0.5681 | time=9.0s\n",
            "Epoch 030 | train_loss=0.4680 acc=0.7942 | val_loss=0.6441 acc=0.6301 | prec=0.6397 rec=0.5959 spec=0.6644 f1=0.6170 | time=9.1s\n",
            "Epoch 031 | train_loss=0.4490 acc=0.8027 | val_loss=0.6460 acc=0.6404 | prec=0.6614 rec=0.5753 spec=0.7055 f1=0.6154 | time=9.0s\n",
            "Epoch 032 | train_loss=0.4390 acc=0.7950 | val_loss=0.6430 acc=0.6336 | prec=0.6291 rec=0.6507 spec=0.6164 f1=0.6397 | time=9.0s\n",
            "Epoch 033 | train_loss=0.3891 acc=0.8431 | val_loss=0.6145 acc=0.6610 | prec=0.6621 rec=0.6575 spec=0.6644 f1=0.6598 | time=9.0s\n",
            "Epoch 034 | train_loss=0.3534 acc=0.8705 | val_loss=0.6657 acc=0.6404 | prec=0.6358 rec=0.6575 spec=0.6233 f1=0.6465 | time=9.0s\n",
            "Epoch 035 | train_loss=0.3370 acc=0.8533 | val_loss=0.6461 acc=0.6507 | prec=0.6358 rec=0.7055 spec=0.5959 f1=0.6688 | time=9.0s\n",
            "Epoch 036 | train_loss=0.3250 acc=0.8714 | val_loss=0.7277 acc=0.6404 | prec=0.6096 rec=0.7808 spec=0.5000 f1=0.6847 | time=9.0s\n",
            "Epoch 037 | train_loss=0.3061 acc=0.8756 | val_loss=0.6802 acc=0.6473 | prec=0.6319 rec=0.7055 spec=0.5890 f1=0.6667 | time=9.0s\n",
            "Epoch 038 | train_loss=0.2951 acc=0.8799 | val_loss=0.7273 acc=0.6575 | prec=0.6186 rec=0.8219 spec=0.4932 f1=0.7059 | time=9.0s\n",
            "Epoch 039 | train_loss=0.2848 acc=0.8885 | val_loss=0.7260 acc=0.6507 | prec=0.6429 rec=0.6781 spec=0.6233 f1=0.6600 | time=9.0s\n",
            "Epoch 040 | train_loss=0.2552 acc=0.9031 | val_loss=0.7147 acc=0.6575 | prec=0.6402 rec=0.7192 spec=0.5959 f1=0.6774 | time=9.0s\n",
            "Epoch 041 | train_loss=0.2288 acc=0.9160 | val_loss=0.7676 acc=0.6507 | prec=0.6375 rec=0.6986 spec=0.6027 f1=0.6667 | time=8.9s\n",
            "Epoch 042 | train_loss=0.2413 acc=0.9117 | val_loss=0.8039 acc=0.6473 | prec=0.6215 rec=0.7534 spec=0.5411 f1=0.6811 | time=9.1s\n",
            "Epoch 043 | train_loss=0.2134 acc=0.9202 | val_loss=0.7822 acc=0.6336 | prec=0.6021 rec=0.7877 spec=0.4795 f1=0.6825 | time=9.1s\n",
            "Epoch 044 | train_loss=0.1960 acc=0.9271 | val_loss=0.8013 acc=0.6575 | prec=0.6643 rec=0.6370 spec=0.6781 f1=0.6503 | time=8.9s\n",
            "Epoch 045 | train_loss=0.1825 acc=0.9340 | val_loss=0.8235 acc=0.6438 | prec=0.6544 rec=0.6096 spec=0.6781 f1=0.6312 | time=9.0s\n",
            "Epoch 046 | train_loss=0.1834 acc=0.9322 | val_loss=0.9098 acc=0.6062 | prec=0.6148 rec=0.5685 spec=0.6438 f1=0.5907 | time=9.1s\n",
            "Epoch 047 | train_loss=0.1893 acc=0.9082 | val_loss=0.9250 acc=0.6233 | prec=0.6475 rec=0.5411 spec=0.7055 f1=0.5896 | time=9.0s\n",
            "Epoch 048 | train_loss=0.1764 acc=0.9340 | val_loss=0.8980 acc=0.6130 | prec=0.5922 rec=0.7260 spec=0.5000 f1=0.6523 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>█▂▄▁▁▇██▃██▃▆▆▇▆▅▆▇▇▆▇█▇▇▇▇█▇███████▇▇▇▇</td></tr><tr><td>precision</td><td>▆▆▇▁▁▆▆▆▆▆▆▇▆▇▆▆▇▆▇▇▇█▇████████████▇██▇▇</td></tr><tr><td>recall</td><td>█▁▂▁▁▆██▂██▂▅▄▇▄▄▅▅▆▄▄▆▅▅▅▆▆▆▆▆▇▆▆▆▇▅▅▅▆</td></tr><tr><td>specificity</td><td>▁█▇██▄▁▁▇▁▁▇▄▅▂▅▆▅▄▄▆▆▄▆▆▆▅▆▅▅▅▄▅▅▅▄▆▆▆▅</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▅▅▅▆▆▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▇▇▇▇▇▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▁▁▂▁▁▁▁▁▂▁▂▂▁▂▂▃▄▅▆▆▆▆▇▇█▇█▇████▇█▇▆▆</td></tr><tr><td>val_loss</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▄▃▃▃▂▂▃▂▂▁▂▂▃▄▄▃▅▅▅▆██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>48</td></tr><tr><td>f1_score</td><td>0.65231</td></tr><tr><td>precision</td><td>0.59218</td></tr><tr><td>recall</td><td>0.72603</td></tr><tr><td>specificity</td><td>0.5</td></tr><tr><td>train_acc</td><td>0.93396</td></tr><tr><td>train_loss</td><td>0.17638</td></tr><tr><td>val_acc</td><td>0.61301</td></tr><tr><td>val_loss</td><td>0.89804</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/ahmqowfr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/ahmqowfr</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_082556-ahmqowfr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:33:10,956] Trial 0 finished with values: [0.6145085096359253, 0.660958904109589] and parameters: {'lr': 0.00012798057921434346, 'wd': 0.0001388702054424885}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 1 hyperparams --> lr=2.90e-04, wd=6.69e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_083310-g85xn6y1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/g85xn6y1' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/g85xn6y1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/g85xn6y1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 1]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7486 acc=0.4828 | val_loss=0.7052 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7368 acc=0.4940 | val_loss=0.7023 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7074 acc=0.5300 | val_loss=0.7101 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7174 acc=0.5129 | val_loss=0.6992 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 005 | train_loss=0.7245 acc=0.4914 | val_loss=0.6955 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7206 acc=0.5051 | val_loss=0.7058 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7146 acc=0.5060 | val_loss=0.7008 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7110 acc=0.5137 | val_loss=0.6950 acc=0.4966 | prec=0.4828 rec=0.0959 spec=0.8973 f1=0.1600 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7160 acc=0.5137 | val_loss=0.7017 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7073 acc=0.5146 | val_loss=0.6981 acc=0.5068 | prec=0.5060 rec=0.5822 spec=0.4315 f1=0.5414 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7013 acc=0.5420 | val_loss=0.7039 acc=0.5034 | prec=0.5116 rec=0.1507 spec=0.8562 f1=0.2328 | time=8.9s\n",
            "Epoch 012 | train_loss=0.7164 acc=0.5343 | val_loss=0.6995 acc=0.4829 | prec=0.4889 rec=0.7534 spec=0.2123 f1=0.5930 | time=9.1s\n",
            "Epoch 013 | train_loss=0.7002 acc=0.5437 | val_loss=0.7135 acc=0.4863 | prec=0.4722 rec=0.2329 spec=0.7397 f1=0.3119 | time=9.1s\n",
            "Epoch 014 | train_loss=0.6950 acc=0.5600 | val_loss=0.7085 acc=0.4726 | prec=0.4733 rec=0.4863 spec=0.4589 f1=0.4797 | time=9.0s\n",
            "Epoch 015 | train_loss=0.6900 acc=0.5532 | val_loss=0.7068 acc=0.5342 | prec=0.5625 rec=0.3082 spec=0.7603 f1=0.3982 | time=9.1s\n",
            "Epoch 016 | train_loss=0.6615 acc=0.6132 | val_loss=0.6812 acc=0.5685 | prec=0.5893 rec=0.4521 spec=0.6849 f1=0.5116 | time=9.1s\n",
            "Epoch 017 | train_loss=0.6421 acc=0.6244 | val_loss=0.6615 acc=0.5993 | prec=0.5901 rec=0.6507 spec=0.5479 f1=0.6189 | time=9.1s\n",
            "Epoch 018 | train_loss=0.6298 acc=0.6561 | val_loss=0.6633 acc=0.6027 | prec=0.5987 rec=0.6233 spec=0.5822 f1=0.6107 | time=9.0s\n",
            "Epoch 019 | train_loss=0.5875 acc=0.6955 | val_loss=0.6721 acc=0.5616 | prec=0.5479 rec=0.7055 spec=0.4178 f1=0.6168 | time=9.1s\n",
            "Epoch 020 | train_loss=0.5469 acc=0.7324 | val_loss=0.6216 acc=0.6233 | prec=0.5874 rec=0.8288 spec=0.4178 f1=0.6875 | time=9.2s\n",
            "Epoch 021 | train_loss=0.5156 acc=0.7607 | val_loss=0.7139 acc=0.6062 | prec=0.6962 rec=0.3767 spec=0.8356 f1=0.4889 | time=9.0s\n",
            "Epoch 022 | train_loss=0.4881 acc=0.7753 | val_loss=0.6077 acc=0.6473 | prec=0.6039 rec=0.8562 spec=0.4384 f1=0.7082 | time=9.1s\n",
            "Epoch 023 | train_loss=0.4532 acc=0.8027 | val_loss=0.6316 acc=0.6301 | prec=0.6092 rec=0.7260 spec=0.5342 f1=0.6625 | time=9.0s\n",
            "Epoch 024 | train_loss=0.4036 acc=0.8148 | val_loss=0.6865 acc=0.6644 | prec=0.7609 rec=0.4795 spec=0.8493 f1=0.5882 | time=9.1s\n",
            "Epoch 025 | train_loss=0.3693 acc=0.8388 | val_loss=0.7139 acc=0.6336 | prec=0.7191 rec=0.4384 spec=0.8288 f1=0.5447 | time=9.1s\n",
            "Epoch 026 | train_loss=0.3203 acc=0.8671 | val_loss=0.6384 acc=0.6575 | prec=0.6186 rec=0.8219 spec=0.4932 f1=0.7059 | time=9.1s\n",
            "Epoch 027 | train_loss=0.3367 acc=0.8508 | val_loss=0.7572 acc=0.6370 | prec=0.7174 rec=0.4521 spec=0.8219 f1=0.5546 | time=9.0s\n",
            "Epoch 028 | train_loss=0.2981 acc=0.8782 | val_loss=0.6708 acc=0.6575 | prec=0.6264 rec=0.7808 spec=0.5342 f1=0.6951 | time=9.0s\n",
            "Epoch 029 | train_loss=0.2497 acc=0.8997 | val_loss=0.8936 acc=0.6027 | prec=0.6744 rec=0.3973 spec=0.8082 f1=0.5000 | time=9.1s\n",
            "Epoch 030 | train_loss=0.2303 acc=0.8971 | val_loss=0.8580 acc=0.6267 | prec=0.5949 rec=0.7945 spec=0.4589 f1=0.6804 | time=9.1s\n",
            "Epoch 031 | train_loss=0.2065 acc=0.9142 | val_loss=0.9099 acc=0.6301 | prec=0.5872 rec=0.8767 spec=0.3836 f1=0.7033 | time=9.0s\n",
            "Epoch 032 | train_loss=0.2026 acc=0.9220 | val_loss=0.9951 acc=0.5925 | prec=0.6627 rec=0.3767 spec=0.8082 f1=0.4803 | time=9.2s\n",
            "Epoch 033 | train_loss=0.1864 acc=0.9228 | val_loss=0.8538 acc=0.6575 | prec=0.6691 rec=0.6233 spec=0.6918 f1=0.6454 | time=9.0s\n",
            "Epoch 034 | train_loss=0.1960 acc=0.9108 | val_loss=0.8927 acc=0.6473 | prec=0.6693 rec=0.5822 spec=0.7123 f1=0.6227 | time=9.1s\n",
            "Epoch 035 | train_loss=0.1920 acc=0.9211 | val_loss=0.8993 acc=0.6404 | prec=0.6475 rec=0.6164 spec=0.6644 f1=0.6316 | time=9.0s\n",
            "Epoch 036 | train_loss=0.1687 acc=0.9237 | val_loss=0.9045 acc=0.6507 | prec=0.6594 rec=0.6233 spec=0.6781 f1=0.6408 | time=9.1s\n",
            "Epoch 037 | train_loss=0.1373 acc=0.9451 | val_loss=1.0722 acc=0.6130 | prec=0.6279 rec=0.5548 spec=0.6712 f1=0.5891 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▃▁▆▃▇▄▆▅▆▇▇▇█▆██▇▆█▆█▆██▆▇▇▇▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▅▁▆▆▅▅▅▆▆▆▇▆▆▇▇▇██▇█▇▇▆▆▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▂▁▆▂▇▃▅▃▅▆▆▇█▄█▇▅▅█▅▇▄▇█▄▆▆▆▆▅</td></tr><tr><td>specificity</td><td>███████▇█▃▇▁▆▃▆▅▄▄▃▃▇▃▄▇▆▃▆▄▆▃▃▆▅▅▅▅▅</td></tr><tr><td>train_acc</td><td>▁▁▂▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▄▅▅▅▆▆▆▇▇▇▇▇███▇███</td></tr><tr><td>train_loss</td><td>██████████▇█▇▇▇▇▇▇▆▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▂▂▂▂▁▂▁▃▄▆▆▄▆▆▇▇█▇█▇█▆▇▇▅█▇▇▇▆</td></tr><tr><td>val_loss</td><td>▂▂▃▂▂▂▂▂▂▂▂▂▃▃▂▂▂▂▂▁▃▁▁▂▃▁▃▂▅▅▆▇▅▅▅▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>f1_score</td><td>0.58909</td></tr><tr><td>precision</td><td>0.62791</td></tr><tr><td>recall</td><td>0.55479</td></tr><tr><td>specificity</td><td>0.67123</td></tr><tr><td>train_acc</td><td>0.94511</td></tr><tr><td>train_loss</td><td>0.13729</td></tr><tr><td>val_acc</td><td>0.61301</td></tr><tr><td>val_loss</td><td>1.07224</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/g85xn6y1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/g85xn6y1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_083310-g85xn6y1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:38:48,538] Trial 1 finished with values: [0.6076550751924514, 0.6472602739726028] and parameters: {'lr': 0.0002897227844496736, 'wd': 6.692800512958711e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 2 hyperparams --> lr=6.66e-05, wd=2.09e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_083848-5ejdt1lo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/5ejdt1lo' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/5ejdt1lo' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/5ejdt1lo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 2]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7261 acc=0.4974 | val_loss=0.7092 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 002 | train_loss=0.7367 acc=0.4726 | val_loss=0.7063 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7129 acc=0.5189 | val_loss=0.7104 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 004 | train_loss=0.7242 acc=0.5154 | val_loss=0.6996 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7256 acc=0.4974 | val_loss=0.7005 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7290 acc=0.4803 | val_loss=0.6987 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7271 acc=0.5009 | val_loss=0.6976 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7204 acc=0.5069 | val_loss=0.6972 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 009 | train_loss=0.7234 acc=0.4923 | val_loss=0.6992 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7213 acc=0.4700 | val_loss=0.6977 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7256 acc=0.4820 | val_loss=0.6975 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 012 | train_loss=0.7140 acc=0.5017 | val_loss=0.7035 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 013 | train_loss=0.7081 acc=0.5197 | val_loss=0.7051 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7104 acc=0.4991 | val_loss=0.7034 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 015 | train_loss=0.7010 acc=0.5189 | val_loss=0.6936 acc=0.4658 | prec=0.4432 rec=0.2671 spec=0.6644 f1=0.3333 | time=9.1s\n",
            "Epoch 016 | train_loss=0.7110 acc=0.5111 | val_loss=0.7068 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 017 | train_loss=0.7102 acc=0.5043 | val_loss=0.6942 acc=0.4932 | prec=0.3750 rec=0.0205 spec=0.9658 f1=0.0390 | time=9.1s\n",
            "Epoch 018 | train_loss=0.7154 acc=0.4983 | val_loss=0.6952 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 019 | train_loss=0.7069 acc=0.5086 | val_loss=0.6977 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 020 | train_loss=0.7098 acc=0.5172 | val_loss=0.6948 acc=0.5103 | prec=0.5652 rec=0.0890 spec=0.9315 f1=0.1538 | time=9.0s\n",
            "Epoch 021 | train_loss=0.6976 acc=0.5343 | val_loss=0.6986 acc=0.4966 | prec=0.4000 rec=0.0137 spec=0.9795 f1=0.0265 | time=9.1s\n",
            "Epoch 022 | train_loss=0.6964 acc=0.5086 | val_loss=0.6975 acc=0.5068 | prec=0.5385 rec=0.0959 spec=0.9178 f1=0.1628 | time=9.0s\n",
            "Epoch 023 | train_loss=0.6910 acc=0.5326 | val_loss=0.6982 acc=0.4829 | prec=0.4627 rec=0.2123 spec=0.7534 f1=0.2911 | time=9.1s\n",
            "Epoch 024 | train_loss=0.6876 acc=0.5557 | val_loss=0.7049 acc=0.4760 | prec=0.4660 rec=0.3288 spec=0.6233 f1=0.3855 | time=9.1s\n",
            "Epoch 025 | train_loss=0.6885 acc=0.5463 | val_loss=0.7140 acc=0.4829 | prec=0.4638 rec=0.2192 spec=0.7466 f1=0.2977 | time=9.0s\n",
            "Epoch 026 | train_loss=0.6740 acc=0.5755 | val_loss=0.7135 acc=0.4760 | prec=0.4646 rec=0.3151 spec=0.6370 f1=0.3755 | time=9.2s\n",
            "Epoch 027 | train_loss=0.6741 acc=0.5909 | val_loss=0.7055 acc=0.5240 | prec=0.5333 rec=0.3836 spec=0.6644 f1=0.4462 | time=9.0s\n",
            "Epoch 028 | train_loss=0.6778 acc=0.5943 | val_loss=0.7047 acc=0.5171 | prec=0.5111 rec=0.7877 spec=0.2466 f1=0.6199 | time=9.0s\n",
            "Epoch 029 | train_loss=0.6669 acc=0.6038 | val_loss=0.7081 acc=0.5137 | prec=0.5159 rec=0.4452 spec=0.5822 f1=0.4779 | time=9.1s\n",
            "Epoch 030 | train_loss=0.6536 acc=0.6158 | val_loss=0.6976 acc=0.5000 | prec=0.5000 rec=0.6438 spec=0.3562 f1=0.5629 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁▁▃▁▃▄▅▄▅▆█▆▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▆▁▆▁▁█▆█▇▇▇▇█▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▃▁▁▁▁▂▁▂▃▄▃▄▄█▅▇</td></tr><tr><td>specificity</td><td>██████████████▅████▇█▇▆▅▆▅▅▁▄▂</td></tr><tr><td>train_acc</td><td>▂▁▃▃▂▁▂▃▂▁▂▃▃▂▃▃▃▂▃▃▄▃▄▅▅▆▇▇▇█</td></tr><tr><td>train_loss</td><td>▇█▆▇▇▇▇▇▇▇▇▆▆▆▅▆▆▆▅▆▅▅▄▄▄▃▃▃▂▁</td></tr><tr><td>val_acc</td><td>▅▅▅▅▅▅▅▅▅▅▅▅▅▅▁▅▄▅▅▆▅▆▃▂▃▂█▇▇▅</td></tr><tr><td>val_loss</td><td>▆▅▇▃▃▃▂▂▃▂▂▄▅▄▁▆▁▂▂▁▃▂▃▅██▅▅▆▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>f1_score</td><td>0.56287</td></tr><tr><td>precision</td><td>0.5</td></tr><tr><td>recall</td><td>0.64384</td></tr><tr><td>specificity</td><td>0.35616</td></tr><tr><td>train_acc</td><td>0.61578</td></tr><tr><td>train_loss</td><td>0.65359</td></tr><tr><td>val_acc</td><td>0.5</td></tr><tr><td>val_loss</td><td>0.69764</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/5ejdt1lo' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/5ejdt1lo</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_083848-5ejdt1lo/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:43:21,863] Trial 2 finished with values: [0.6935628175735473, 0.4657534246575342] and parameters: {'lr': 6.655172064451736e-05, 'wd': 0.00020860026263992234}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 3 hyperparams --> lr=4.47e-04, wd=7.77e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_084321-8xgzz7mn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/8xgzz7mn' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/8xgzz7mn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/8xgzz7mn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 3]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7502 acc=0.4871 | val_loss=0.6921 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7395 acc=0.4974 | val_loss=0.6970 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7365 acc=0.4949 | val_loss=0.6929 acc=0.5034 | prec=0.6667 rec=0.0137 spec=0.9932 f1=0.0268 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7177 acc=0.5086 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 005 | train_loss=0.7095 acc=0.5274 | val_loss=0.6979 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7402 acc=0.4794 | val_loss=0.6953 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 007 | train_loss=0.7177 acc=0.5146 | val_loss=0.6996 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7145 acc=0.5111 | val_loss=0.6944 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7386 acc=0.4786 | val_loss=0.6973 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7186 acc=0.5129 | val_loss=0.6991 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7243 acc=0.5026 | val_loss=0.6947 acc=0.4623 | prec=0.4671 rec=0.5342 spec=0.3904 f1=0.4984 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7153 acc=0.5009 | val_loss=0.6957 acc=0.4897 | prec=0.4831 rec=0.2945 spec=0.6849 f1=0.3660 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7116 acc=0.5386 | val_loss=0.7116 acc=0.4555 | prec=0.4301 rec=0.2740 spec=0.6370 f1=0.3347 | time=8.9s\n",
            "Epoch 014 | train_loss=0.6948 acc=0.5780 | val_loss=0.6992 acc=0.5342 | prec=0.5694 rec=0.2808 spec=0.7877 f1=0.3761 | time=9.0s\n",
            "Epoch 015 | train_loss=0.6688 acc=0.5892 | val_loss=0.7294 acc=0.5514 | prec=0.5524 rec=0.5411 spec=0.5616 f1=0.5467 | time=9.0s\n",
            "Epoch 016 | train_loss=0.6532 acc=0.6072 | val_loss=0.6605 acc=0.6130 | prec=0.6204 rec=0.5822 spec=0.6438 f1=0.6007 | time=9.0s\n",
            "Epoch 017 | train_loss=0.6080 acc=0.6647 | val_loss=0.6990 acc=0.5651 | prec=0.6044 rec=0.3767 spec=0.7534 f1=0.4641 | time=9.0s\n",
            "Epoch 018 | train_loss=0.5538 acc=0.7204 | val_loss=0.6446 acc=0.6062 | prec=0.6084 rec=0.5959 spec=0.6164 f1=0.6021 | time=9.0s\n",
            "Epoch 019 | train_loss=0.5319 acc=0.7410 | val_loss=0.6579 acc=0.6301 | prec=0.6827 rec=0.4863 spec=0.7740 f1=0.5680 | time=9.0s\n",
            "Epoch 020 | train_loss=0.4833 acc=0.7616 | val_loss=0.7147 acc=0.6027 | prec=0.5962 rec=0.6370 spec=0.5685 f1=0.6159 | time=9.1s\n",
            "Epoch 021 | train_loss=0.4425 acc=0.7830 | val_loss=0.7754 acc=0.5993 | prec=0.5633 rec=0.8836 spec=0.3151 f1=0.6880 | time=8.9s\n",
            "Epoch 022 | train_loss=0.3966 acc=0.8405 | val_loss=0.6572 acc=0.6267 | prec=0.6082 rec=0.7123 spec=0.5411 f1=0.6562 | time=9.0s\n",
            "Epoch 023 | train_loss=0.3698 acc=0.8465 | val_loss=0.9237 acc=0.5822 | prec=0.5522 rec=0.8699 spec=0.2945 f1=0.6755 | time=9.0s\n",
            "Epoch 024 | train_loss=0.3662 acc=0.8456 | val_loss=0.7760 acc=0.6301 | prec=0.6234 rec=0.6575 spec=0.6027 f1=0.6400 | time=9.1s\n",
            "Epoch 025 | train_loss=0.2945 acc=0.8911 | val_loss=0.9201 acc=0.6062 | prec=0.6505 rec=0.4589 spec=0.7534 f1=0.5382 | time=9.1s\n",
            "Epoch 026 | train_loss=0.2779 acc=0.8859 | val_loss=0.9253 acc=0.6164 | prec=0.6545 rec=0.4932 spec=0.7397 f1=0.5625 | time=9.0s\n",
            "Epoch 027 | train_loss=0.2868 acc=0.8765 | val_loss=0.8738 acc=0.6027 | prec=0.5915 rec=0.6644 spec=0.5411 f1=0.6258 | time=9.0s\n",
            "Epoch 028 | train_loss=0.2324 acc=0.8997 | val_loss=0.9985 acc=0.6027 | prec=0.5773 rec=0.7671 spec=0.4384 f1=0.6588 | time=9.0s\n",
            "Epoch 029 | train_loss=0.2251 acc=0.9057 | val_loss=0.9324 acc=0.6062 | prec=0.5917 rec=0.6849 spec=0.5274 f1=0.6349 | time=9.1s\n",
            "Epoch 030 | train_loss=0.1866 acc=0.9160 | val_loss=0.9960 acc=0.5993 | prec=0.5868 rec=0.6712 spec=0.5274 f1=0.6262 | time=9.0s\n",
            "Epoch 031 | train_loss=0.1968 acc=0.9142 | val_loss=1.0614 acc=0.6027 | prec=0.5781 rec=0.7603 spec=0.4452 f1=0.6568 | time=9.0s\n",
            "Epoch 032 | train_loss=0.1774 acc=0.9245 | val_loss=1.1618 acc=0.5890 | prec=0.5739 rec=0.6918 spec=0.4863 f1=0.6273 | time=9.0s\n",
            "Epoch 033 | train_loss=0.1351 acc=0.9443 | val_loss=1.0647 acc=0.6096 | prec=0.6067 rec=0.6233 spec=0.5959 f1=0.6149 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>█▁▁█▁█▁▁▁▁▆▅▄▅▇▇▆▇▇▇████▆▇▇█▇▇█▇▇</td></tr><tr><td>precision</td><td>▆▁█▆▁▆▁▁▁▁▆▆▅▇▇▇▇▇█▇▇▇▇▇██▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>█▁▁█▁█▁▁▁▁▅▃▃▃▅▅▄▅▄▅▇▆▇▆▄▄▆▆▆▆▆▆▅</td></tr><tr><td>specificity</td><td>▁██▁█▁████▄▆▅▇▅▆▆▅▆▅▃▅▃▅▆▆▅▄▅▅▄▄▅</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▁▂▁▁▂▁▁▂▂▃▃▄▅▅▅▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▇▆▆▆▅▄▄▄▄▃▃▃▂▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▃▃▃▃▃▃▃▃▃▃▁▂▁▄▅▇▅▇█▇▇█▆█▇▇▇▇▇▇▇▆▇</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▂▃▁▅▃▅▅▄▆▅▆▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>f1_score</td><td>0.61486</td></tr><tr><td>precision</td><td>0.60667</td></tr><tr><td>recall</td><td>0.62329</td></tr><tr><td>specificity</td><td>0.59589</td></tr><tr><td>train_acc</td><td>0.94425</td></tr><tr><td>train_loss</td><td>0.1351</td></tr><tr><td>val_acc</td><td>0.60959</td></tr><tr><td>val_loss</td><td>1.06474</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/8xgzz7mn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/8xgzz7mn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_084321-8xgzz7mn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:48:21,893] Trial 3 finished with values: [0.6445517301559448, 0.6061643835616438] and parameters: {'lr': 0.00044651327822145363, 'wd': 7.77294990973984e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 4 hyperparams --> lr=1.33e-04, wd=8.46e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_084821-yw46osom</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/yw46osom' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/yw46osom' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/yw46osom</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 4]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7560 acc=0.5111 | val_loss=0.7015 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7355 acc=0.4837 | val_loss=0.6957 acc=0.5000 | prec=0.5000 rec=0.0205 spec=0.9795 f1=0.0395 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7141 acc=0.5489 | val_loss=0.6967 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7431 acc=0.4777 | val_loss=0.6970 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 005 | train_loss=0.7273 acc=0.4931 | val_loss=0.6929 acc=0.4966 | prec=0.4981 rec=0.9041 spec=0.0890 f1=0.6423 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7120 acc=0.5283 | val_loss=0.6943 acc=0.5171 | prec=0.5309 rec=0.2945 spec=0.7397 f1=0.3789 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7159 acc=0.5189 | val_loss=0.7010 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7093 acc=0.5223 | val_loss=0.6966 acc=0.5377 | prec=0.6667 rec=0.1507 spec=0.9247 f1=0.2458 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7175 acc=0.5146 | val_loss=0.6950 acc=0.4966 | prec=0.4972 rec=0.6164 spec=0.3767 f1=0.5505 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7165 acc=0.5163 | val_loss=0.6962 acc=0.5000 | prec=0.5000 rec=0.5890 spec=0.4110 f1=0.5409 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7174 acc=0.5197 | val_loss=0.7005 acc=0.4829 | prec=0.4797 rec=0.4041 spec=0.5616 f1=0.4387 | time=8.9s\n",
            "Epoch 012 | train_loss=0.7065 acc=0.5369 | val_loss=0.7208 acc=0.4658 | prec=0.4713 rec=0.5616 spec=0.3699 f1=0.5125 | time=9.0s\n",
            "Epoch 013 | train_loss=0.6985 acc=0.5557 | val_loss=0.7111 acc=0.4829 | prec=0.4828 rec=0.4795 spec=0.4863 f1=0.4811 | time=9.1s\n",
            "Epoch 014 | train_loss=0.6844 acc=0.5592 | val_loss=0.7176 acc=0.4932 | prec=0.4915 rec=0.3973 spec=0.5890 f1=0.4394 | time=8.9s\n",
            "Epoch 015 | train_loss=0.7006 acc=0.5652 | val_loss=0.7311 acc=0.5000 | prec=0.5000 rec=0.0068 spec=0.9932 f1=0.0135 | time=9.0s\n",
            "Epoch 016 | train_loss=0.7066 acc=0.5197 | val_loss=0.7156 acc=0.4863 | prec=0.4815 rec=0.3562 spec=0.6164 f1=0.4094 | time=9.1s\n",
            "Epoch 017 | train_loss=0.6937 acc=0.5549 | val_loss=0.7073 acc=0.4760 | prec=0.4824 rec=0.6575 spec=0.2945 f1=0.5565 | time=8.9s\n",
            "Epoch 018 | train_loss=0.6769 acc=0.5883 | val_loss=0.7127 acc=0.4966 | prec=0.4957 rec=0.3973 spec=0.5959 f1=0.4411 | time=9.1s\n",
            "Epoch 019 | train_loss=0.6880 acc=0.5635 | val_loss=0.7152 acc=0.4829 | prec=0.4820 rec=0.4589 spec=0.5068 f1=0.4702 | time=9.0s\n",
            "Epoch 020 | train_loss=0.6848 acc=0.5755 | val_loss=0.7191 acc=0.5616 | prec=0.5818 rec=0.4384 spec=0.6849 f1=0.5000 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁█▅▁▄▇▇▆▇▆▆▁▅▇▆▆▆</td></tr><tr><td>precision</td><td>▁▆▁▁▆▇▁█▆▆▆▆▆▆▆▆▆▆▆▇</td></tr><tr><td>recall</td><td>▁▁▁▁█▃▁▂▆▆▄▅▅▄▁▄▆▄▅▄</td></tr><tr><td>specificity</td><td>████▁▆█▇▃▃▅▃▄▅█▅▃▅▄▆</td></tr><tr><td>train_acc</td><td>▃▁▆▁▂▄▄▄▃▃▄▅▆▆▇▄▆█▆▇</td></tr><tr><td>train_loss</td><td>█▆▄▇▅▄▄▄▅▅▅▄▃▂▃▄▂▁▂▂</td></tr><tr><td>val_acc</td><td>▃▃▃▃▃▅▃▆▃▃▂▁▂▃▃▂▂▃▂█</td></tr><tr><td>val_loss</td><td>▃▂▂▂▁▁▂▂▁▂▂▆▄▆█▅▄▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>f1_score</td><td>0.5</td></tr><tr><td>precision</td><td>0.58182</td></tr><tr><td>recall</td><td>0.43836</td></tr><tr><td>specificity</td><td>0.68493</td></tr><tr><td>train_acc</td><td>0.57547</td></tr><tr><td>train_loss</td><td>0.68482</td></tr><tr><td>val_acc</td><td>0.56164</td></tr><tr><td>val_loss</td><td>0.71908</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/yw46osom' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/yw46osom</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_084821-yw46osom/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:51:24,401] Trial 4 finished with values: [0.6928520023822784, 0.4965753424657534] and parameters: {'lr': 0.0001328731021190919, 'wd': 8.460063789758898e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 5 hyperparams --> lr=2.52e-04, wd=1.36e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_085124-t2yxisq7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/t2yxisq7' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/t2yxisq7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/t2yxisq7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 5]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7954 acc=0.4906 | val_loss=0.7073 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7348 acc=0.4906 | val_loss=0.7086 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7643 acc=0.4605 | val_loss=0.7016 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 004 | train_loss=0.7443 acc=0.4846 | val_loss=0.6947 acc=0.4966 | prec=0.3333 rec=0.0068 spec=0.9863 f1=0.0134 | time=9.0s\n",
            "Epoch 005 | train_loss=0.7317 acc=0.4974 | val_loss=0.6960 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7315 acc=0.5111 | val_loss=0.6954 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 007 | train_loss=0.7227 acc=0.5086 | val_loss=0.6975 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7254 acc=0.5094 | val_loss=0.6928 acc=0.4760 | prec=0.4844 rec=0.7466 spec=0.2055 f1=0.5876 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7257 acc=0.5086 | val_loss=0.6936 acc=0.5171 | prec=0.5185 rec=0.4795 spec=0.5548 f1=0.4982 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7030 acc=0.5497 | val_loss=0.7112 acc=0.5137 | prec=0.5141 rec=0.5000 spec=0.5274 f1=0.5069 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7213 acc=0.5300 | val_loss=0.6967 acc=0.4966 | prec=0.4667 rec=0.0479 spec=0.9452 f1=0.0870 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7173 acc=0.5352 | val_loss=0.7216 acc=0.5103 | prec=0.5138 rec=0.3836 spec=0.6370 f1=0.4392 | time=9.0s\n",
            "Epoch 013 | train_loss=0.6902 acc=0.5660 | val_loss=0.7128 acc=0.4966 | prec=0.4898 rec=0.1644 spec=0.8288 f1=0.2462 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7058 acc=0.5403 | val_loss=0.7101 acc=0.4795 | prec=0.4667 rec=0.2877 spec=0.6712 f1=0.3559 | time=9.1s\n",
            "Epoch 015 | train_loss=0.6887 acc=0.5600 | val_loss=0.7224 acc=0.5103 | prec=0.5254 rec=0.2123 spec=0.8082 f1=0.3024 | time=9.1s\n",
            "Epoch 016 | train_loss=0.6729 acc=0.6038 | val_loss=0.7449 acc=0.5137 | prec=0.5095 rec=0.7329 spec=0.2945 f1=0.6011 | time=9.1s\n",
            "Epoch 017 | train_loss=0.6733 acc=0.5909 | val_loss=0.7117 acc=0.5308 | prec=0.5243 rec=0.6644 spec=0.3973 f1=0.5861 | time=9.0s\n",
            "Epoch 018 | train_loss=0.6184 acc=0.6449 | val_loss=0.6983 acc=0.5822 | prec=0.5612 rec=0.7534 spec=0.4110 f1=0.6433 | time=9.1s\n",
            "Epoch 019 | train_loss=0.6345 acc=0.6424 | val_loss=0.6940 acc=0.5890 | prec=0.5756 rec=0.6781 spec=0.5000 f1=0.6226 | time=9.1s\n",
            "Epoch 020 | train_loss=0.5760 acc=0.7084 | val_loss=0.6553 acc=0.5993 | prec=0.5729 rec=0.7808 spec=0.4178 f1=0.6609 | time=9.0s\n",
            "Epoch 021 | train_loss=0.5487 acc=0.7341 | val_loss=0.7948 acc=0.5445 | prec=0.5261 rec=0.8973 spec=0.1918 f1=0.6633 | time=9.0s\n",
            "Epoch 022 | train_loss=0.5145 acc=0.7573 | val_loss=0.5941 acc=0.6849 | prec=0.7045 rec=0.6370 spec=0.7329 f1=0.6691 | time=9.0s\n",
            "Epoch 023 | train_loss=0.4903 acc=0.7787 | val_loss=0.6747 acc=0.5959 | prec=0.5833 rec=0.6712 spec=0.5205 f1=0.6242 | time=9.0s\n",
            "Epoch 024 | train_loss=0.4461 acc=0.7933 | val_loss=0.7262 acc=0.5856 | prec=0.5676 rec=0.7192 spec=0.4521 f1=0.6344 | time=9.0s\n",
            "Epoch 025 | train_loss=0.4171 acc=0.8216 | val_loss=0.7009 acc=0.6473 | prec=0.6138 rec=0.7945 spec=0.5000 f1=0.6925 | time=8.9s\n",
            "Epoch 026 | train_loss=0.3924 acc=0.8190 | val_loss=0.7613 acc=0.6199 | prec=0.5845 rec=0.8288 spec=0.4110 f1=0.6856 | time=9.0s\n",
            "Epoch 027 | train_loss=0.3576 acc=0.8499 | val_loss=0.6610 acc=0.6336 | prec=0.6242 rec=0.6712 spec=0.5959 f1=0.6469 | time=9.0s\n",
            "Epoch 028 | train_loss=0.3406 acc=0.8525 | val_loss=0.7492 acc=0.6473 | prec=0.6720 rec=0.5753 spec=0.7192 f1=0.6199 | time=9.0s\n",
            "Epoch 029 | train_loss=0.3450 acc=0.8362 | val_loss=0.7620 acc=0.6096 | prec=0.5825 rec=0.7740 spec=0.4452 f1=0.6647 | time=9.1s\n",
            "Epoch 030 | train_loss=0.3320 acc=0.8559 | val_loss=0.8053 acc=0.6164 | prec=0.5955 rec=0.7260 spec=0.5068 f1=0.6543 | time=9.0s\n",
            "Epoch 031 | train_loss=0.2915 acc=0.8696 | val_loss=0.7706 acc=0.6267 | prec=0.6209 rec=0.6507 spec=0.6027 f1=0.6355 | time=9.0s\n",
            "Epoch 032 | train_loss=0.2723 acc=0.8928 | val_loss=0.8255 acc=0.6164 | prec=0.6118 rec=0.6370 spec=0.5959 f1=0.6242 | time=9.1s\n",
            "Epoch 033 | train_loss=0.2402 acc=0.9039 | val_loss=0.8205 acc=0.6130 | prec=0.6435 rec=0.5068 spec=0.7192 f1=0.5670 | time=8.9s\n",
            "Epoch 034 | train_loss=0.2229 acc=0.9057 | val_loss=0.8417 acc=0.6096 | prec=0.6111 rec=0.6027 spec=0.6164 f1=0.6069 | time=9.0s\n",
            "Epoch 035 | train_loss=0.1999 acc=0.9288 | val_loss=0.8641 acc=0.6301 | prec=0.6203 rec=0.6712 spec=0.5890 f1=0.6447 | time=9.0s\n",
            "Epoch 036 | train_loss=0.1953 acc=0.9245 | val_loss=0.9116 acc=0.6336 | prec=0.6291 rec=0.6507 spec=0.6164 f1=0.6397 | time=9.1s\n",
            "Epoch 037 | train_loss=0.1749 acc=0.9254 | val_loss=1.0047 acc=0.5993 | prec=0.6465 rec=0.4384 spec=0.7603 f1=0.5224 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▇▆▆▂▅▃▅▄▇▇█▇███▇▇███▇██▇▇▇▇█▇▆</td></tr><tr><td>precision</td><td>▁▁▁▄▁▁▁▆▆▆▆▆▆▆▆▆▆▇▇▇▆█▇▇▇▇▇█▇▇▇▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▇▅▅▁▄▂▃▃▇▆▇▆▇█▆▆▇▇▇▆▅▇▇▆▆▅▆▆▆▄</td></tr><tr><td>specificity</td><td>███████▁▄▄█▅▇▅▆▂▃▃▄▃▁▆▄▃▄▃▄▆▃▄▅▄▆▅▄▅▆</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▂▂▂▂▂▂▂▃▂▂▃▃▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▃▅▅▅▃█▅▅▇▆▆▇▅▆▆▆▆▅▆▆▅</td></tr><tr><td>val_loss</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▃▃▃▂▄▁▂▃▃▄▂▄▄▅▄▅▅▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>f1_score</td><td>0.52245</td></tr><tr><td>precision</td><td>0.64646</td></tr><tr><td>recall</td><td>0.43836</td></tr><tr><td>specificity</td><td>0.76027</td></tr><tr><td>train_acc</td><td>0.92539</td></tr><tr><td>train_loss</td><td>0.1749</td></tr><tr><td>val_acc</td><td>0.59932</td></tr><tr><td>val_loss</td><td>1.00465</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/t2yxisq7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/t2yxisq7</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_085124-t2yxisq7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 08:57:00,450] Trial 5 finished with values: [0.5940559208393097, 0.684931506849315] and parameters: {'lr': 0.0002519980361581342, 'wd': 0.0001360355853136191}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 6 hyperparams --> lr=2.50e-04, wd=2.30e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_085700-kjy8fm6h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/kjy8fm6h' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/kjy8fm6h' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/kjy8fm6h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 6]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7245 acc=0.5000 | val_loss=0.6966 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7094 acc=0.5103 | val_loss=0.6959 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7143 acc=0.5172 | val_loss=0.6938 acc=0.4932 | prec=0.4643 rec=0.0890 spec=0.8973 f1=0.1494 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7162 acc=0.5223 | val_loss=0.6988 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 005 | train_loss=0.7195 acc=0.5077 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7277 acc=0.4906 | val_loss=0.6947 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7192 acc=0.4863 | val_loss=0.6919 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7117 acc=0.5094 | val_loss=0.6920 acc=0.4932 | prec=0.4965 rec=0.9795 spec=0.0068 f1=0.6590 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7163 acc=0.5051 | val_loss=0.6919 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7229 acc=0.4880 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 011 | train_loss=0.7207 acc=0.4983 | val_loss=0.6966 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7092 acc=0.5309 | val_loss=0.6968 acc=0.5240 | prec=0.5185 rec=0.6712 spec=0.3767 f1=0.5851 | time=9.1s\n",
            "Epoch 013 | train_loss=0.6979 acc=0.5463 | val_loss=0.7067 acc=0.5205 | prec=0.5183 rec=0.5822 spec=0.4589 f1=0.5484 | time=9.1s\n",
            "Epoch 014 | train_loss=0.6952 acc=0.5360 | val_loss=0.6942 acc=0.5034 | prec=0.5031 rec=0.5548 spec=0.4521 f1=0.5277 | time=9.1s\n",
            "Epoch 015 | train_loss=0.6897 acc=0.5455 | val_loss=0.7060 acc=0.5137 | prec=0.5370 rec=0.1986 spec=0.8288 f1=0.2900 | time=9.0s\n",
            "Epoch 016 | train_loss=0.6788 acc=0.5660 | val_loss=0.7026 acc=0.5377 | prec=0.5364 rec=0.5548 spec=0.5205 f1=0.5455 | time=9.0s\n",
            "Epoch 017 | train_loss=0.6654 acc=0.5883 | val_loss=0.6678 acc=0.5959 | prec=0.6458 rec=0.4247 spec=0.7671 f1=0.5124 | time=9.0s\n",
            "Epoch 018 | train_loss=0.6447 acc=0.6021 | val_loss=0.6472 acc=0.6199 | prec=0.7869 rec=0.3288 spec=0.9110 f1=0.4638 | time=9.0s\n",
            "Epoch 019 | train_loss=0.6184 acc=0.6698 | val_loss=0.6402 acc=0.6233 | prec=0.7812 rec=0.3425 spec=0.9041 f1=0.4762 | time=9.0s\n",
            "Epoch 020 | train_loss=0.5842 acc=0.6930 | val_loss=0.6347 acc=0.6199 | prec=0.7059 rec=0.4110 spec=0.8288 f1=0.5195 | time=9.1s\n",
            "Epoch 021 | train_loss=0.5436 acc=0.7187 | val_loss=0.6546 acc=0.6301 | prec=0.6727 rec=0.5068 spec=0.7534 f1=0.5781 | time=9.0s\n",
            "Epoch 022 | train_loss=0.5051 acc=0.7684 | val_loss=0.6372 acc=0.6233 | prec=0.6034 rec=0.7192 spec=0.5274 f1=0.6562 | time=9.0s\n",
            "Epoch 023 | train_loss=0.4901 acc=0.7702 | val_loss=0.6527 acc=0.6370 | prec=0.6299 rec=0.6644 spec=0.6096 f1=0.6467 | time=9.1s\n",
            "Epoch 024 | train_loss=0.4335 acc=0.8148 | val_loss=0.6897 acc=0.6473 | prec=0.6807 rec=0.5548 spec=0.7397 f1=0.6113 | time=9.0s\n",
            "Epoch 025 | train_loss=0.4165 acc=0.8182 | val_loss=0.7701 acc=0.6267 | prec=0.7342 rec=0.3973 spec=0.8562 f1=0.5156 | time=8.9s\n",
            "Epoch 026 | train_loss=0.3732 acc=0.8482 | val_loss=0.7016 acc=0.6267 | prec=0.6209 rec=0.6507 spec=0.6027 f1=0.6355 | time=9.0s\n",
            "Epoch 027 | train_loss=0.3339 acc=0.8714 | val_loss=0.7397 acc=0.5856 | prec=0.5796 rec=0.6233 spec=0.5479 f1=0.6007 | time=9.0s\n",
            "Epoch 028 | train_loss=0.3159 acc=0.8894 | val_loss=0.7466 acc=0.6062 | prec=0.6202 rec=0.5479 spec=0.6644 f1=0.5818 | time=9.0s\n",
            "Epoch 029 | train_loss=0.2870 acc=0.8859 | val_loss=0.7531 acc=0.6336 | prec=0.6154 rec=0.7123 spec=0.5548 f1=0.6603 | time=9.0s\n",
            "Epoch 030 | train_loss=0.2525 acc=0.9014 | val_loss=0.8698 acc=0.5890 | prec=0.5722 rec=0.7055 spec=0.4726 f1=0.6319 | time=9.1s\n",
            "Epoch 031 | train_loss=0.2339 acc=0.9125 | val_loss=0.8882 acc=0.5925 | prec=0.5839 rec=0.6438 spec=0.5411 f1=0.6124 | time=9.0s\n",
            "Epoch 032 | train_loss=0.2367 acc=0.9074 | val_loss=0.9224 acc=0.5788 | prec=0.5891 rec=0.5205 spec=0.6370 f1=0.5527 | time=8.9s\n",
            "Epoch 033 | train_loss=0.2446 acc=0.8928 | val_loss=0.8430 acc=0.6507 | prec=0.6294 rec=0.7329 spec=0.5685 f1=0.6772 | time=9.1s\n",
            "Epoch 034 | train_loss=0.2323 acc=0.8971 | val_loss=0.9085 acc=0.6164 | prec=0.6118 rec=0.6370 spec=0.5959 f1=0.6242 | time=9.0s\n",
            "Epoch 035 | train_loss=0.1994 acc=0.9194 | val_loss=0.9772 acc=0.6164 | prec=0.6349 rec=0.5479 spec=0.6849 f1=0.5882 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▃▁█▁████▁▇▇▆▄▇▆▆▆▆▇██▇▆█▇▇██▇▇█▇▇</td></tr><tr><td>precision</td><td>▁▁▅▁▅▁▅▅▅▅▁▆▆▅▆▆▇██▇▇▆▇▇█▇▆▇▆▆▆▆▇▆▇</td></tr><tr><td>recall</td><td>▁▁▂▁█▁████▁▆▅▅▂▅▄▃▃▄▅▆▆▅▄▆▅▅▆▆▆▅▆▅▅</td></tr><tr><td>specificity</td><td>██▇█▁█▁▁▁▁█▄▄▄▇▅▆▇▇▇▆▅▅▆▇▅▅▆▅▄▅▅▅▅▆</td></tr><tr><td>train_acc</td><td>▁▁▁▂▁▁▁▁▁▁▁▂▂▂▂▂▃▃▄▄▅▆▆▆▆▇▇█▇██████</td></tr><tr><td>train_loss</td><td>██████████████▇▇▇▇▇▆▆▅▅▄▄▃▃▃▂▂▁▁▂▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▃▆▇▇▇▇▇▇█▇▇▅▆▇▅▅▅█▆▆</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▄▂▃▃▃▆▆▇▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>f1_score</td><td>0.58824</td></tr><tr><td>precision</td><td>0.63492</td></tr><tr><td>recall</td><td>0.54795</td></tr><tr><td>specificity</td><td>0.68493</td></tr><tr><td>train_acc</td><td>0.91938</td></tr><tr><td>train_loss</td><td>0.19943</td></tr><tr><td>val_acc</td><td>0.61644</td></tr><tr><td>val_loss</td><td>0.9772</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/kjy8fm6h' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/kjy8fm6h</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_085700-kjy8fm6h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 09:02:19,289] Trial 6 finished with values: [0.6346890270709992, 0.6198630136986302] and parameters: {'lr': 0.0002496598808699953, 'wd': 0.0002301426167280013}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 7 hyperparams --> lr=7.70e-05, wd=4.40e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_090219-thipxjll</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/thipxjll' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/thipxjll' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/thipxjll</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 7]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7397 acc=0.4991 | val_loss=0.6935 acc=0.5000 | prec=0.5000 rec=0.0137 spec=0.9863 f1=0.0267 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7288 acc=0.5051 | val_loss=0.6906 acc=0.5103 | prec=0.5053 rec=0.9863 spec=0.0342 f1=0.6682 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7399 acc=0.4966 | val_loss=0.6940 acc=0.4932 | prec=0.3333 rec=0.0137 spec=0.9726 f1=0.0263 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7231 acc=0.5060 | val_loss=0.6980 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.3s\n",
            "Epoch 005 | train_loss=0.7291 acc=0.4889 | val_loss=0.6912 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.3s\n",
            "Epoch 006 | train_loss=0.7300 acc=0.5009 | val_loss=0.6915 acc=0.5068 | prec=0.5051 rec=0.6849 spec=0.3288 f1=0.5814 | time=9.2s\n",
            "Epoch 007 | train_loss=0.7323 acc=0.5077 | val_loss=0.6922 acc=0.4966 | prec=0.4983 rec=0.9863 spec=0.0068 f1=0.6621 | time=9.6s\n",
            "Epoch 008 | train_loss=0.7212 acc=0.4991 | val_loss=0.6921 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=9.4s\n",
            "Epoch 009 | train_loss=0.7229 acc=0.5034 | val_loss=0.6933 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.9s\n",
            "Epoch 010 | train_loss=0.7247 acc=0.5120 | val_loss=0.6963 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=10.0s\n",
            "Epoch 011 | train_loss=0.7144 acc=0.5197 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=10.0s\n",
            "Epoch 012 | train_loss=0.7361 acc=0.4786 | val_loss=0.7001 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=10.0s\n",
            "Epoch 013 | train_loss=0.7359 acc=0.4640 | val_loss=0.6920 acc=0.5103 | prec=0.5055 rec=0.9384 spec=0.0822 f1=0.6571 | time=9.9s\n",
            "Epoch 014 | train_loss=0.7244 acc=0.4931 | val_loss=0.6986 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=10.0s\n",
            "Epoch 015 | train_loss=0.7166 acc=0.5077 | val_loss=0.6912 acc=0.5411 | prec=0.5435 rec=0.5137 spec=0.5685 f1=0.5282 | time=9.5s\n",
            "Epoch 016 | train_loss=0.7207 acc=0.5034 | val_loss=0.6907 acc=0.5479 | prec=0.5299 rec=0.8493 spec=0.2466 f1=0.6526 | time=9.1s\n",
            "Epoch 017 | train_loss=0.7057 acc=0.5257 | val_loss=0.6932 acc=0.5103 | prec=0.8000 rec=0.0274 spec=0.9932 f1=0.0530 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▅▆▆▇▇██</td></tr><tr><td>f1_score</td><td>▁█▁▁█▇███▁█▁██▇█▂</td></tr><tr><td>precision</td><td>▅▅▄▁▅▅▅▅▅▁▅▁▅▅▆▆█</td></tr><tr><td>recall</td><td>▁█▁▁█▆███▁█▁██▅▇▁</td></tr><tr><td>specificity</td><td>█▁██▁▃▁▁▁█▁█▂▁▅▃█</td></tr><tr><td>train_acc</td><td>▅▆▅▆▄▅▆▅▅▆▇▃▁▄▆▅█</td></tr><tr><td>train_loss</td><td>█▆█▅▆▆▆▄▅▅▃▇▇▅▃▄▁</td></tr><tr><td>val_acc</td><td>▂▃▁▂▂▃▁▁▂▂▂▂▃▂▇█▃</td></tr><tr><td>val_loss</td><td>▃▁▃▆▁▂▂▂▃▅▂█▂▇▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>17</td></tr><tr><td>f1_score</td><td>0.05298</td></tr><tr><td>precision</td><td>0.8</td></tr><tr><td>recall</td><td>0.0274</td></tr><tr><td>specificity</td><td>0.99315</td></tr><tr><td>train_acc</td><td>0.52573</td></tr><tr><td>train_loss</td><td>0.70575</td></tr><tr><td>val_acc</td><td>0.51027</td></tr><tr><td>val_loss</td><td>0.69318</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/thipxjll' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/thipxjll</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_090219-thipxjll/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 09:05:02,935] Trial 7 finished with values: [0.6906349241733551, 0.5102739726027398] and parameters: {'lr': 7.697385245912632e-05, 'wd': 0.0004402082852603951}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 8 hyperparams --> lr=9.76e-05, wd=3.64e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_090502-q559rbnf</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/q559rbnf' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/q559rbnf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/q559rbnf</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 8]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7623 acc=0.5043 | val_loss=0.7104 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 002 | train_loss=0.7224 acc=0.5137 | val_loss=0.7019 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7137 acc=0.5043 | val_loss=0.6993 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7062 acc=0.5077 | val_loss=0.7001 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7118 acc=0.4820 | val_loss=0.6995 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7101 acc=0.4794 | val_loss=0.6995 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7057 acc=0.5206 | val_loss=0.6999 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 008 | train_loss=0.7012 acc=0.4949 | val_loss=0.6987 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7046 acc=0.4897 | val_loss=0.6954 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7076 acc=0.4889 | val_loss=0.6957 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7013 acc=0.5137 | val_loss=0.6958 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7089 acc=0.4897 | val_loss=0.6935 acc=0.4658 | prec=0.2727 rec=0.0411 spec=0.8904 f1=0.0714 | time=9.2s\n",
            "Epoch 013 | train_loss=0.7037 acc=0.5086 | val_loss=0.6967 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 014 | train_loss=0.6987 acc=0.5094 | val_loss=0.6950 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.0s\n",
            "Epoch 015 | train_loss=0.7013 acc=0.5069 | val_loss=0.7019 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 016 | train_loss=0.6992 acc=0.5163 | val_loss=0.6917 acc=0.4966 | prec=0.4979 rec=0.7945 spec=0.1986 f1=0.6121 | time=9.0s\n",
            "Epoch 017 | train_loss=0.6984 acc=0.5026 | val_loss=0.6943 acc=0.5171 | prec=0.5175 rec=0.5068 spec=0.5274 f1=0.5121 | time=9.0s\n",
            "Epoch 018 | train_loss=0.6957 acc=0.5360 | val_loss=0.6979 acc=0.4795 | prec=0.4712 rec=0.3356 spec=0.6233 f1=0.3920 | time=9.1s\n",
            "Epoch 019 | train_loss=0.6978 acc=0.4957 | val_loss=0.7016 acc=0.5205 | prec=0.5231 rec=0.4658 spec=0.5753 f1=0.4928 | time=9.0s\n",
            "Epoch 020 | train_loss=0.6877 acc=0.5497 | val_loss=0.7056 acc=0.4897 | prec=0.4904 rec=0.5274 spec=0.4521 f1=0.5083 | time=9.1s\n",
            "Epoch 021 | train_loss=0.6891 acc=0.5412 | val_loss=0.7067 acc=0.5034 | prec=0.5042 rec=0.4110 spec=0.5959 f1=0.4528 | time=8.9s\n",
            "Epoch 022 | train_loss=0.6897 acc=0.5257 | val_loss=0.7080 acc=0.4897 | prec=0.4882 rec=0.4247 spec=0.5548 f1=0.4542 | time=9.1s\n",
            "Epoch 023 | train_loss=0.6935 acc=0.5317 | val_loss=0.7315 acc=0.5000 | prec=0.5000 rec=0.4110 spec=0.5890 f1=0.4511 | time=9.0s\n",
            "Epoch 024 | train_loss=0.6775 acc=0.5643 | val_loss=0.7274 acc=0.4692 | prec=0.4720 rec=0.5205 spec=0.4178 f1=0.4951 | time=8.9s\n",
            "Epoch 025 | train_loss=0.6693 acc=0.5720 | val_loss=0.7361 acc=0.4966 | prec=0.4966 rec=0.4932 spec=0.5000 f1=0.4948 | time=9.1s\n",
            "Epoch 026 | train_loss=0.6688 acc=0.5995 | val_loss=0.7349 acc=0.4692 | prec=0.4646 rec=0.4041 spec=0.5342 f1=0.4322 | time=9.0s\n",
            "Epoch 027 | train_loss=0.6663 acc=0.5815 | val_loss=0.7578 acc=0.4829 | prec=0.4818 rec=0.4521 spec=0.5137 f1=0.4664 | time=8.9s\n",
            "Epoch 028 | train_loss=0.6603 acc=0.6244 | val_loss=0.7297 acc=0.4555 | prec=0.4611 rec=0.5274 spec=0.3836 f1=0.4920 | time=9.0s\n",
            "Epoch 029 | train_loss=0.6486 acc=0.6123 | val_loss=0.7546 acc=0.4726 | prec=0.4722 rec=0.4658 spec=0.4795 f1=0.4690 | time=9.1s\n",
            "Epoch 030 | train_loss=0.6455 acc=0.6252 | val_loss=0.7314 acc=0.4829 | prec=0.4863 rec=0.6096 spec=0.3562 f1=0.5410 | time=9.0s\n",
            "Epoch 031 | train_loss=0.6286 acc=0.6364 | val_loss=0.7851 acc=0.4897 | prec=0.4872 rec=0.3904 spec=0.5890 f1=0.4335 | time=8.9s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁█▇▅▇▇▆▆▆▇▇▆▆▇▆▇▆</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▁▁▁██▇█████▇█▇▇▇▇██</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁█▅▄▅▆▅▅▅▆▅▅▅▆▅▆▄</td></tr><tr><td>specificity</td><td>███████████▇███▁▄▅▄▃▄▄▄▃▄▄▄▃▃▂▄</td></tr><tr><td>train_acc</td><td>▂▃▂▂▁▁▃▂▁▁▃▁▂▂▂▃▂▄▂▄▄▃▃▅▅▆▆▇▇██</td></tr><tr><td>train_loss</td><td>█▆▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▁</td></tr><tr><td>val_acc</td><td>▆▆▆▆▆▆▆▆▆▆▆▂▆▅▆▅█▄█▅▆▅▆▂▅▂▄▁▃▄▅</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▂▂▂▂▄▄▄▄▆▄▆▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>31</td></tr><tr><td>f1_score</td><td>0.43346</td></tr><tr><td>precision</td><td>0.48718</td></tr><tr><td>recall</td><td>0.39041</td></tr><tr><td>specificity</td><td>0.58904</td></tr><tr><td>train_acc</td><td>0.63636</td></tr><tr><td>train_loss</td><td>0.62857</td></tr><tr><td>val_acc</td><td>0.48973</td></tr><tr><td>val_loss</td><td>0.78506</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/q559rbnf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/q559rbnf</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_090502-q559rbnf/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 09:09:45,440] Trial 8 finished with values: [0.6916638433933258, 0.4965753424657534] and parameters: {'lr': 9.763117914154554e-05, 'wd': 0.0003640519520785867}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 9 hyperparams --> lr=3.81e-04, wd=3.50e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_090945-awn96ezz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/awn96ezz' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/awn96ezz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/awn96ezz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 9]  TRAIN n=1166 AD=583, FTD=583) | VAL n=292 AD=146, FTD=146\n",
            "Epoch 001 | train_loss=0.7272 acc=0.5163 | val_loss=0.6940 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7242 acc=0.5017 | val_loss=0.7042 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7143 acc=0.5172 | val_loss=0.6920 acc=0.5068 | prec=0.5037 rec=0.9247 spec=0.0890 f1=0.6522 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7341 acc=0.4683 | val_loss=0.6952 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 005 | train_loss=0.7201 acc=0.5120 | val_loss=0.6968 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7155 acc=0.5017 | val_loss=0.6936 acc=0.4418 | prec=0.4605 rec=0.6781 spec=0.2055 f1=0.5485 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7257 acc=0.4863 | val_loss=0.6930 acc=0.4897 | prec=0.4933 rec=0.7534 spec=0.2260 f1=0.5962 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7070 acc=0.5232 | val_loss=0.7040 acc=0.4966 | prec=0.0000 rec=0.0000 spec=0.9932 f1=0.0000 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7028 acc=0.5077 | val_loss=0.6992 acc=0.4932 | prec=0.4964 rec=0.9452 spec=0.0411 f1=0.6509 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7178 acc=0.5111 | val_loss=0.6979 acc=0.5000 | prec=0.5000 rec=0.7945 spec=0.2055 f1=0.6138 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7148 acc=0.5386 | val_loss=0.7028 acc=0.5034 | prec=0.5030 rec=0.5685 spec=0.4384 f1=0.5338 | time=9.0s\n",
            "Epoch 012 | train_loss=0.6984 acc=0.5583 | val_loss=0.7052 acc=0.4486 | prec=0.4634 rec=0.6507 spec=0.2466 f1=0.5413 | time=9.0s\n",
            "Epoch 013 | train_loss=0.6982 acc=0.5515 | val_loss=0.7059 acc=0.5103 | prec=0.5091 rec=0.5753 spec=0.4452 f1=0.5402 | time=9.0s\n",
            "Epoch 014 | train_loss=0.6923 acc=0.5352 | val_loss=0.7161 acc=0.4623 | prec=0.4675 rec=0.5411 spec=0.3836 f1=0.5016 | time=9.0s\n",
            "Epoch 015 | train_loss=0.6868 acc=0.5523 | val_loss=0.7035 acc=0.5171 | prec=0.5163 rec=0.5411 spec=0.4932 f1=0.5284 | time=9.1s\n",
            "Epoch 016 | train_loss=0.6873 acc=0.5472 | val_loss=0.7144 acc=0.4795 | prec=0.4844 rec=0.6370 spec=0.3219 f1=0.5503 | time=8.9s\n",
            "Epoch 017 | train_loss=0.6688 acc=0.5763 | val_loss=0.7324 acc=0.4726 | prec=0.4792 rec=0.6301 spec=0.3151 f1=0.5444 | time=9.0s\n",
            "Epoch 018 | train_loss=0.6659 acc=0.5883 | val_loss=0.7733 acc=0.4692 | prec=0.4737 rec=0.5548 spec=0.3836 f1=0.5110 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▆▆▆▇▇██</td></tr><tr><td>f1_score</td><td>█▁█▁▁▇▇▁█▇▇▇▇▆▇▇▇▆</td></tr><tr><td>precision</td><td>█▁█▁▁▇█▁███▇█▇██▇▇</td></tr><tr><td>recall</td><td>█▁▇▁▁▆▆▁█▇▅▆▅▅▅▅▅▅</td></tr><tr><td>specificity</td><td>▁█▂██▂▃█▁▂▄▃▄▄▄▃▃▄</td></tr><tr><td>train_acc</td><td>▄▃▄▁▄▃▂▄▃▄▅▆▆▅▆▆▇█</td></tr><tr><td>train_loss</td><td>▇▇▆█▇▆▇▅▅▆▆▄▄▄▃▃▁▁</td></tr><tr><td>val_acc</td><td>▆▆▇▆▆▁▅▆▆▆▇▂▇▃█▄▄▄</td></tr><tr><td>val_loss</td><td>▁▂▁▁▁▁▁▂▂▂▂▂▂▃▂▃▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>18</td></tr><tr><td>f1_score</td><td>0.51104</td></tr><tr><td>precision</td><td>0.47368</td></tr><tr><td>recall</td><td>0.55479</td></tr><tr><td>specificity</td><td>0.38356</td></tr><tr><td>train_acc</td><td>0.58834</td></tr><tr><td>train_loss</td><td>0.6659</td></tr><tr><td>val_acc</td><td>0.46918</td></tr><tr><td>val_loss</td><td>0.77328</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/awn96ezz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2/runs/awn96ezz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-AD-FTD-test-within-cross-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_090945-awn96ezz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 09:12:30,098] Trial 9 finished with values: [0.6919521510601043, 0.5068493150684932] and parameters: {'lr': 0.0003811633779098951, 'wd': 0.00035001663370376116}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Selected Trial #5 ===\n",
            " val_loss=0.5941, val_acc=0.6849, params={'lr': 0.0002519980361581342, 'wd': 0.0001360355853136191}, best_epoch=22, ckpt=ckpts/trial5_optimal_AD_FTD_2.pth\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=218  A(D)=146, F(TD)=72\n",
            " Accuracy=0.6835 Sensitivity=0.6111 Specificity=0.7192 F1=0.5605\n",
            "\n",
            "-- test_cross -- total=566  A(D)=319, F(TD)=247\n",
            " Accuracy=0.6343 Sensitivity=0.4251 Specificity=0.7962 F1=0.5036\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AD vs FTD (Full Train test)"
      ],
      "metadata": {
        "id": "A4DzxReusZOl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_with_val_then_full.py\n",
        "import os, json, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── 1) Config & reproducibility ───────────────────────────────\n",
        "SEED        = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE   = 32\n",
        "NUM_WORKERS  = 4\n",
        "MAX_EPOCHS   = 100     # max for search\n",
        "ES_PATIENCE  = 10     # early-stop patience\n",
        "PCT_START    = 0.2\n",
        "\n",
        "# hyperparameters you’ve tuned\n",
        "LR           = 0.00025\n",
        "WD           = 0.00013\n",
        "\n",
        "# ─── 2) Dataset wrapper ────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── 3) Load & prepare metadata ───────────────────────────────\n",
        "with open(Path(DATA_DIR)/LABEL_FILE,'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "# filter A vs F\n",
        "class0, class1 = 'A','F'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "train_meta       = [d for d in train_meta       if d['label'] in label_map]\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in label_map]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in label_map]\n",
        "\n",
        "# balance train by down-sampling\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "k     = min(len(data0), len(data1))\n",
        "balanced_meta = random.sample(data0, k) + random.sample(data1, k)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# convert to ints\n",
        "for d in balanced_meta:       d['label'] = label_map[d['label']]\n",
        "for d in test_within_meta:    d['label'] = label_map[d['label']]\n",
        "for d in test_cross_meta:     d['label'] = label_map[d['label']]\n",
        "\n",
        "print(f\"\\n[Balanced TRAIN] total={len(balanced_meta)} (each class={k})\")\n",
        "\n",
        "# build full balanced dataset\n",
        "full_raw    = EEGDataset(DATA_DIR, balanced_meta)\n",
        "full_ds     = BinaryEEGDataset(full_raw, balanced_meta)\n",
        "labels_full = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── 4) Split train/val for epoch search ──────────────────────\n",
        "idx            = np.arange(len(full_ds))\n",
        "tr_idx, va_idx = train_test_split(\n",
        "    idx, test_size=0.2, stratify=labels_full, random_state=SEED\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    Subset(full_ds, tr_idx), batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  num_workers=NUM_WORKERS\n",
        ")\n",
        "val_loader   = DataLoader(\n",
        "    Subset(full_ds, va_idx), batch_size=BATCH_SIZE,\n",
        "    shuffle=False, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# ─── 5) Model builder & eval fn ───────────────────────────────\n",
        "def build_model():\n",
        "    input_len = full_ds[0][0].shape[-1]\n",
        "    return EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "def evaluate(loader, model, criterion):\n",
        "    model.eval()\n",
        "    loss_sum = correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y    = X.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model(X)\n",
        "            loss_sum += criterion(logits,y).item()*y.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "# ─── 6) Phase 1: find best_epoch with early stopping ──────────\n",
        "best_val_loss = float('inf')\n",
        "best_epoch    = 0\n",
        "es_count      = 0\n",
        "\n",
        "model     = build_model()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR, epochs=MAX_EPOCHS,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=PCT_START, anneal_strategy='cos',\n",
        "    cycle_momentum=False\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\nSearching best epoch using validation set...\")\n",
        "for epoch in range(1, MAX_EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    # ── train ──\n",
        "    model.train()\n",
        "    tr_loss_sum = tr_correct = tr_total = 0\n",
        "    for X,y in train_loader:\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss   = criterion(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tr_loss_sum  += loss.item()*y.size(0)\n",
        "        preds        = logits.argmax(1)\n",
        "        tr_correct  += (preds==y).sum().item()\n",
        "        tr_total    += y.size(0)\n",
        "\n",
        "    train_loss = tr_loss_sum / tr_total\n",
        "    train_acc  = tr_correct / tr_total\n",
        "\n",
        "    # ── validate ──\n",
        "    val_loss, val_acc = evaluate(val_loader, model, criterion)\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
        "        f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | \"\n",
        "        f\"time={elapsed:.1f}s\"\n",
        "    )\n",
        "\n",
        "    # check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch    = epoch\n",
        "        es_count      = 0\n",
        "    else:\n",
        "        es_count += 1\n",
        "        if es_count >= ES_PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {ES_PATIENCE} epochs)\")\n",
        "            break\n",
        "\n",
        "print(f\"\\n>> Best epoch by validation: {best_epoch}\")\n",
        "\n",
        "# ─── 7) Phase 2: retrain on full balanced set ─────────────────\n",
        "print(f\"\\nRetraining from scratch on FULL set for {best_epoch} epochs...\")\n",
        "model2     = build_model()\n",
        "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler2 = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer2, max_lr=LR,\n",
        "    epochs=best_epoch,\n",
        "    steps_per_epoch=len(DataLoader(full_ds, batch_size=BATCH_SIZE)),\n",
        "    pct_start=PCT_START, anneal_strategy='cos',\n",
        "    cycle_momentum=False\n",
        ")\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, best_epoch+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss_sum = tr_correct = tr_total = 0\n",
        "    model2.train()\n",
        "    for X,y in DataLoader(full_ds, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=NUM_WORKERS):\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer2.zero_grad()\n",
        "        logits = model2(X)\n",
        "        loss   = criterion2(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        scheduler2.step()\n",
        "\n",
        "        tr_loss_sum  += loss.item()*y.size(0)\n",
        "        preds        = logits.argmax(1)\n",
        "        tr_correct  += (preds==y).sum().item()\n",
        "        tr_total    += y.size(0)\n",
        "\n",
        "    avg_loss = tr_loss_sum / tr_total\n",
        "    acc      = tr_correct / tr_total\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={avg_loss:.4f} acc={acc:.4f} time={time.time()-t0:.1f}s\")\n",
        "\n",
        "# ─── 8) Final Evaluation on Test Sets ─────────────────────────\n",
        "def full_evaluate(metas, loader):\n",
        "    model2.eval()\n",
        "    loss_sum = correct = total = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model2(X)\n",
        "            loss_sum += criterion2(logits,y).item()*y.size(0)\n",
        "            preds    = logits.argmax(1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(all_preds)\n",
        "    labs  = np.concatenate(all_labels)\n",
        "    tn,fp,fn,tp = confusion_matrix(labs,preds,labels=[0,1]).ravel()\n",
        "    return {\n",
        "        \"loss\": loss_sum/total,\n",
        "        \"acc\": correct/total,\n",
        "        \"sensitivity\": recall_score(labs,preds,zero_division=0),\n",
        "        \"specificity\": tn/(tn+fp) if (tn+fp)>0 else 0.0,\n",
        "        \"f1\": f1_score(labs,preds,zero_division=0)\n",
        "    }\n",
        "\n",
        "print(\"\\n=== Final Evaluation on Test Sets ===\")\n",
        "for name, metas in [(\"test_within\", test_within_meta), (\"test_cross\", test_cross_meta)]:\n",
        "    loader = DataLoader(BinaryEEGDataset(EEGDataset(DATA_DIR, metas), metas),\n",
        "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    res = full_evaluate(metas, loader)\n",
        "    n0 = sum(1 for d in metas if d['label']==0)\n",
        "    n1 = sum(1 for d in metas if d['label']==1)\n",
        "    print(f\"-- {name} -- total={len(metas)}  A(D)={n0}, F(TD)={n1}\")\n",
        "    print(f\" Loss={res['loss']:.4f} Acc={res['acc']:.4f} \"\n",
        "          f\"Sens={res['sensitivity']:.4f} Spec={res['specificity']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cC5sJHmulhKs",
        "outputId": "77204a47-c92b-40e6-8b8b-3ebf0445e63f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "[Balanced TRAIN] total=1458 (each class=729)\n",
            "\n",
            "Searching best epoch using validation set...\n",
            "Epoch 01 | train_loss=0.7511 | train_acc=0.4889 | val_loss=0.6940 | val_acc=0.4966 | time=326.5s\n",
            "Epoch 02 | train_loss=0.7302 | train_acc=0.5094 | val_loss=0.6928 | val_acc=0.5274 | time=9.2s\n",
            "Epoch 03 | train_loss=0.7425 | train_acc=0.5009 | val_loss=0.6932 | val_acc=0.4966 | time=8.9s\n",
            "Epoch 04 | train_loss=0.7458 | train_acc=0.4923 | val_loss=0.6935 | val_acc=0.5377 | time=9.0s\n",
            "Epoch 05 | train_loss=0.7196 | train_acc=0.5163 | val_loss=0.6939 | val_acc=0.4897 | time=8.9s\n",
            "Epoch 06 | train_loss=0.7295 | train_acc=0.4991 | val_loss=0.6935 | val_acc=0.5240 | time=8.9s\n",
            "Epoch 07 | train_loss=0.7184 | train_acc=0.5051 | val_loss=0.6931 | val_acc=0.5000 | time=9.0s\n",
            "Epoch 08 | train_loss=0.7190 | train_acc=0.5172 | val_loss=0.7064 | val_acc=0.5000 | time=8.9s\n",
            "Epoch 09 | train_loss=0.7235 | train_acc=0.5163 | val_loss=0.6938 | val_acc=0.5000 | time=9.0s\n",
            "Epoch 10 | train_loss=0.7126 | train_acc=0.5223 | val_loss=0.6929 | val_acc=0.5000 | time=8.9s\n",
            "Epoch 11 | train_loss=0.7146 | train_acc=0.5206 | val_loss=0.6942 | val_acc=0.5068 | time=8.9s\n",
            "Epoch 12 | train_loss=0.7103 | train_acc=0.5137 | val_loss=0.6973 | val_acc=0.5000 | time=9.1s\n",
            "Epoch 13 | train_loss=0.7127 | train_acc=0.5197 | val_loss=0.7014 | val_acc=0.4863 | time=9.0s\n",
            "Epoch 14 | train_loss=0.7036 | train_acc=0.5540 | val_loss=0.7070 | val_acc=0.4932 | time=9.0s\n",
            "Epoch 15 | train_loss=0.7082 | train_acc=0.5412 | val_loss=0.7292 | val_acc=0.4795 | time=8.9s\n",
            "Epoch 16 | train_loss=0.6889 | train_acc=0.5523 | val_loss=0.7130 | val_acc=0.4726 | time=9.0s\n",
            "Epoch 17 | train_loss=0.6889 | train_acc=0.5609 | val_loss=0.7149 | val_acc=0.4726 | time=8.9s\n",
            "\n",
            "Early stopping at epoch 17 (no improvement for 15 epochs)\n",
            "\n",
            ">> Best epoch by validation: 2\n",
            "\n",
            "Retraining from scratch on FULL set for 2 epochs...\n",
            "Epoch 01 | train_loss=0.7183 acc=0.4938 time=8.7s\n",
            "Epoch 02 | train_loss=0.7097 acc=0.4986 time=8.6s\n",
            "\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=218  A(D)=146, F(TD)=72\n",
            " Loss=0.6931 Acc=0.5092 Sens=0.7917 Spec=0.3699 F1=0.5158\n",
            "\n",
            "-- test_cross -- total=566  A(D)=319, F(TD)=247\n",
            " Loss=0.6935 Acc=0.4841 Sens=0.5992 Spec=0.3950 F1=0.5034\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multiclassification Task (AD vs FTD vs CN)\n"
      ],
      "metadata": {
        "id": "OyyuW8iRXSMQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "PCT_START   = 0.2  # for OneCycleLR\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class EEGMultiDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load metadata & split ───────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_data       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "class_names = ['A','C','F']        # AD, CN, FTD\n",
        "label_map   = {'A':0,'C':1,'F':2}\n",
        "\n",
        "# ─── Manual Class Balancing ──────────────────────────────────────\n",
        "train_data_A = [d for d in train_data if d['label']=='A']\n",
        "train_data_C = [d for d in train_data if d['label']=='C']\n",
        "train_data_F = [d for d in train_data if d['label']=='F']\n",
        "\n",
        "min_samples = min(\n",
        "    (len(train_data_A)+len(train_data_C)) / 2,\n",
        "    (len(train_data_A)+len(train_data_F)) / 2,\n",
        "    (len(train_data_C)+len(train_data_F)) / 2\n",
        ")\n",
        "\n",
        "a_index = int(min(min_samples, len(train_data_A)))\n",
        "c_index = int(min(min_samples, len(train_data_C)))\n",
        "f_index = int(min(min_samples, len(train_data_F)))\n",
        "\n",
        "balanced_train_data = (\n",
        "    random.sample(train_data_A, a_index) +\n",
        "    random.sample(train_data_C, c_index) +\n",
        "    random.sample(train_data_F, f_index)\n",
        ")\n",
        "\n",
        "print(f'Before Balancing  A={len(train_data_A)}, C={len(train_data_C)}, F={len(train_data_F)}')\n",
        "print(f'After  Balancing  A={a_index}, C={c_index}, F={f_index}  Total={len(balanced_train_data)}')\n",
        "\n",
        "for d in balanced_train_data:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_train_data)\n",
        "dataset_train = EEGMultiDataset(raw_ds_train, balanced_train_data)\n",
        "labels_train  = np.array([d['label'] for d in balanced_train_data])\n",
        "\n",
        "# ─── Utility: count labels ───────────────────────────────────────\n",
        "def count_labels(meta_list):\n",
        "    counts = {0:0, 1:0, 2:0}\n",
        "    for d in meta_list:\n",
        "        counts[d['label']] += 1\n",
        "    return counts\n",
        "\n",
        "# ─── Optuna objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    wd = trial.suggest_float('wd', 1e-5, 1e-2, log=True)\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-multiclass-AD_CN_FTD',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr,'wd':wd,'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2, stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "\n",
        "    tr_counts = count_labels([balanced_train_data[i] for i in tr_idx])\n",
        "    va_counts = count_labels([balanced_train_data[i] for i in va_idx])\n",
        "    print(f\"[Trial {trial.number}] TRAIN n={len(tr_idx)} A={tr_counts[0]}, C={tr_counts[1]}, F={tr_counts[2]} | \"\n",
        "          f\"VAL n={len(va_idx)} A={va_counts[0]}, C={va_counts[1]}, F={va_counts[2]}\")\n",
        "\n",
        "    train_loader = DataLoader(Subset(dataset_train, tr_idx),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    val_loader   = DataLoader(Subset(dataset_train, va_idx),\n",
        "                              batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_state    = None\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "        train_loss = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss   += loss.item()\n",
        "            preds        = logits.argmax(1)\n",
        "            train_correct+= (preds==y).sum().item()\n",
        "            train_total  += y.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc   = train_correct / train_total\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                val_loss += criterion(logits, y).item()\n",
        "                all_preds.append(logits.argmax(1).cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "\n",
        "        val_acc = accuracy_score(labs, preds)\n",
        "        prec    = precision_score(labs, preds, average='macro', zero_division=0)\n",
        "        rec     = recall_score(labs, preds, average='macro', zero_division=0)\n",
        "        f1      = f1_score(labs, preds, average='macro', zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | prec={prec:.4f} rec={rec:.4f} f1={f1:.4f} | \"\n",
        "              f\"time={elapsed:.1f}s\")\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':      epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc':  train_acc,\n",
        "            'val_loss':   val_loss,\n",
        "            'val_acc':    val_acc,\n",
        "            'precision':  prec,\n",
        "            'recall':     rec,\n",
        "            'f1_score':   f1\n",
        "        })\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "            es_count      = 0\n",
        "            os.makedirs('ckpts', exist_ok=True)\n",
        "            ckpt_path = f'ckpts/trial{trial.number}_best.pth'\n",
        "            torch.save(best_state, ckpt_path)\n",
        "            trial.set_user_attr('ckpt_path', ckpt_path)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, scheduler, train_loader, val_loader\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    return best_val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna & Summarize ─────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=[\"minimize\",\"maximize\"],\n",
        "        study_name=\"eeg_multiobj_AD_CN_FTD\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    print(\"\\nAll trials: hyperparams → (val_loss, val_acc)\")\n",
        "    for t in study.trials:\n",
        "        print(f\"  Trial {t.number}: lr={t.params['lr']:.2e}, wd={t.params['wd']:.2e} → ({t.values[0]:.4f}, {t.values[1]:.4f})\")\n",
        "\n",
        "    best_trial = max(study.best_trials, key=lambda t: t.values[1])\n",
        "    bv_loss, bv_acc = best_trial.values\n",
        "    ckpt_path       = best_trial.user_attrs[\"ckpt_path\"]\n",
        "\n",
        "    print(f\"\\n=== Selected Trial #{best_trial.number} ===\")\n",
        "    print(f\" val_loss={bv_loss:.4f}, val_acc={bv_acc:.4f}\")\n",
        "    print(f\" Best hyperparameters: {best_trial.params}\")\n",
        "\n",
        "    # ─── Load Best Model & Evaluate on Test Sets ────────────────\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=3\n",
        "    ).to(DEVICE)\n",
        "    model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    model.eval()\n",
        "\n",
        "    def evaluate(model, metas):\n",
        "        metas_copy = []\n",
        "        for d in metas:\n",
        "            new_d = d.copy()\n",
        "            if isinstance(new_d['label'], str):\n",
        "                new_d['label'] = label_map[new_d['label']]\n",
        "            metas_copy.append(new_d)\n",
        "\n",
        "        ds     = EEGMultiDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                all_preds.append(logits.argmax(1).cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        metrics = {\n",
        "            \"acc\":       accuracy_score(labs, preds),\n",
        "            \"precision\": precision_score(labs, preds, average='macro', zero_division=0),\n",
        "            \"recall\":    recall_score(labs, preds, average='macro', zero_division=0),\n",
        "            \"f1\":        f1_score(labs, preds, average='macro', zero_division=0),\n",
        "        }\n",
        "        counts = count_labels(metas_copy)\n",
        "        return metrics, counts\n",
        "\n",
        "    print(\"\\n=== Final Evaluation on Test Sets ===\")\n",
        "    for name, metas in [(\"test_within\", test_within_meta), (\"test_cross\", test_cross_meta)]:\n",
        "        res, counts = evaluate(model, metas)\n",
        "        print(f\"-- {name} -- total={len(metas)}  A={counts[0]}, C={counts[1]}, F={counts[2]}\")\n",
        "        print(f\" Acc={res['acc']:.4f}  Prec={res['precision']:.4f}  Rec={res['recall']:.4f}  F1={res['f1']:.4f}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "STbPSyrXn8qf",
        "outputId": "e05aa1ce-bde9-42d3-9cb2-6a9f3815aebf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 09:30:55,938] A new study created in memory with name: eeg_multiobj_AD_CN_FTD_2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Balancing  A=1388, C=1102, F=729\n",
            "After  Balancing  A=915, C=915, F=729  Total=2559\n",
            "\n",
            "--- Trial 0 hyperparams --> lr=2.79e-05, wd=1.73e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_093055-kpkmau9p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/kpkmau9p' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/kpkmau9p' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/kpkmau9p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 0] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1654 acc=0.3298 | val_loss=1.0988 acc=0.3223 | prec=0.5475 rec=0.3356 f1=0.2636 | time=15.1s\n",
            "Epoch 002 | train_loss=1.1677 acc=0.3214 | val_loss=1.0977 acc=0.3359 | prec=0.3300 rec=0.3216 f1=0.2778 | time=14.9s\n",
            "Epoch 003 | train_loss=1.1708 acc=0.3317 | val_loss=1.0979 acc=0.3750 | prec=0.3872 rec=0.3862 f1=0.3714 | time=14.9s\n",
            "Epoch 004 | train_loss=1.1718 acc=0.3258 | val_loss=1.0981 acc=0.3477 | prec=0.3828 rec=0.3616 f1=0.3080 | time=14.6s\n",
            "Epoch 005 | train_loss=1.1818 acc=0.3004 | val_loss=1.0997 acc=0.2852 | prec=0.1950 rec=0.3319 f1=0.1575 | time=14.5s\n",
            "Epoch 006 | train_loss=1.1516 acc=0.3459 | val_loss=1.0978 acc=0.3672 | prec=0.3587 rec=0.3558 f1=0.3473 | time=14.6s\n",
            "Epoch 007 | train_loss=1.1506 acc=0.3405 | val_loss=1.0984 acc=0.3555 | prec=0.2875 rec=0.3555 f1=0.2830 | time=14.4s\n",
            "Epoch 008 | train_loss=1.1389 acc=0.3439 | val_loss=1.0980 acc=0.3223 | prec=0.2964 rec=0.3098 f1=0.2397 | time=14.6s\n",
            "Epoch 009 | train_loss=1.1607 acc=0.3229 | val_loss=1.0931 acc=0.3555 | prec=0.2353 rec=0.3315 f1=0.2128 | time=14.6s\n",
            "Epoch 010 | train_loss=1.1512 acc=0.3341 | val_loss=1.0944 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 011 | train_loss=1.1549 acc=0.3170 | val_loss=1.0956 acc=0.3613 | prec=0.2536 rec=0.3370 f1=0.1839 | time=14.6s\n",
            "Epoch 012 | train_loss=1.1299 acc=0.3385 | val_loss=1.0939 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 013 | train_loss=1.1390 acc=0.3390 | val_loss=1.0930 acc=0.3691 | prec=0.2461 rec=0.3443 f1=0.2686 | time=14.6s\n",
            "Epoch 014 | train_loss=1.1466 acc=0.3244 | val_loss=1.0931 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 015 | train_loss=1.1418 acc=0.3278 | val_loss=1.0937 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 016 | train_loss=1.1377 acc=0.3444 | val_loss=1.0943 acc=0.3555 | prec=0.1187 rec=0.3315 f1=0.1748 | time=14.6s\n",
            "Epoch 017 | train_loss=1.1081 acc=0.3698 | val_loss=1.0941 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 018 | train_loss=1.1356 acc=0.3312 | val_loss=1.0968 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 019 | train_loss=1.1313 acc=0.3483 | val_loss=1.0960 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 020 | train_loss=1.1328 acc=0.3268 | val_loss=1.0978 acc=0.3496 | prec=0.1651 rec=0.3265 f1=0.1768 | time=14.6s\n",
            "Epoch 021 | train_loss=1.1155 acc=0.3542 | val_loss=1.0908 acc=0.3672 | prec=0.2713 rec=0.3424 f1=0.2063 | time=14.6s\n",
            "Epoch 022 | train_loss=1.1169 acc=0.3473 | val_loss=1.0874 acc=0.4004 | prec=0.2767 rec=0.3734 f1=0.2946 | time=14.6s\n",
            "Epoch 023 | train_loss=1.1072 acc=0.3649 | val_loss=1.0756 acc=0.4336 | prec=0.2969 rec=0.4044 f1=0.3243 | time=14.7s\n",
            "Epoch 024 | train_loss=1.0959 acc=0.3898 | val_loss=1.1016 acc=0.3594 | prec=0.4527 rec=0.3356 f1=0.1803 | time=14.9s\n",
            "Epoch 025 | train_loss=1.0752 acc=0.4167 | val_loss=1.0515 acc=0.5078 | prec=0.5023 rec=0.5045 f1=0.4796 | time=14.6s\n",
            "Epoch 026 | train_loss=1.0230 acc=0.4817 | val_loss=0.9926 acc=0.5156 | prec=0.5462 rec=0.4846 f1=0.4252 | time=14.5s\n",
            "Epoch 027 | train_loss=0.9929 acc=0.5027 | val_loss=0.9643 acc=0.5371 | prec=0.5800 rec=0.5037 f1=0.4358 | time=14.9s\n",
            "Epoch 028 | train_loss=0.9890 acc=0.5139 | val_loss=0.9663 acc=0.5371 | prec=0.6933 rec=0.5046 f1=0.4403 | time=14.6s\n",
            "Epoch 029 | train_loss=0.9437 acc=0.5618 | val_loss=0.9312 acc=0.5664 | prec=0.5702 rec=0.5333 f1=0.4774 | time=14.5s\n",
            "Epoch 030 | train_loss=0.9456 acc=0.5442 | val_loss=0.9378 acc=0.5508 | prec=0.5614 rec=0.5229 f1=0.4855 | time=14.7s\n",
            "Epoch 031 | train_loss=0.9236 acc=0.5589 | val_loss=0.9241 acc=0.5469 | prec=0.5628 rec=0.5193 f1=0.4823 | time=14.8s\n",
            "Epoch 032 | train_loss=0.9116 acc=0.5804 | val_loss=0.9220 acc=0.5664 | prec=0.5671 rec=0.5490 f1=0.5366 | time=14.7s\n",
            "Epoch 033 | train_loss=0.8755 acc=0.5906 | val_loss=0.8951 acc=0.5684 | prec=0.5576 rec=0.5388 f1=0.5007 | time=14.4s\n",
            "Epoch 034 | train_loss=0.8654 acc=0.6111 | val_loss=0.8902 acc=0.5781 | prec=0.5750 rec=0.5521 f1=0.5246 | time=14.6s\n",
            "Epoch 035 | train_loss=0.8496 acc=0.6165 | val_loss=0.8738 acc=0.5879 | prec=0.5849 rec=0.5621 f1=0.5375 | time=14.6s\n",
            "Epoch 036 | train_loss=0.8547 acc=0.6028 | val_loss=0.8670 acc=0.6016 | prec=0.5858 rec=0.5790 f1=0.5647 | time=14.6s\n",
            "Epoch 037 | train_loss=0.8376 acc=0.6248 | val_loss=0.8708 acc=0.6133 | prec=0.6109 rec=0.5992 f1=0.5943 | time=14.7s\n",
            "Epoch 038 | train_loss=0.8281 acc=0.6317 | val_loss=0.8340 acc=0.6230 | prec=0.6352 rec=0.5958 f1=0.5723 | time=14.8s\n",
            "Epoch 039 | train_loss=0.8075 acc=0.6385 | val_loss=0.8303 acc=0.6445 | prec=0.6447 rec=0.6302 f1=0.6302 | time=14.6s\n",
            "Epoch 040 | train_loss=0.7970 acc=0.6590 | val_loss=0.8320 acc=0.6309 | prec=0.6306 rec=0.6156 f1=0.6116 | time=14.5s\n",
            "Epoch 041 | train_loss=0.7843 acc=0.6698 | val_loss=0.8110 acc=0.6504 | prec=0.6612 rec=0.6287 f1=0.6203 | time=14.6s\n",
            "Epoch 042 | train_loss=0.7845 acc=0.6698 | val_loss=0.8178 acc=0.6465 | prec=0.6654 rec=0.6278 f1=0.6211 | time=14.6s\n",
            "Epoch 043 | train_loss=0.7723 acc=0.6644 | val_loss=0.8023 acc=0.6484 | prec=0.6532 rec=0.6352 f1=0.6363 | time=14.6s\n",
            "Epoch 044 | train_loss=0.7575 acc=0.6878 | val_loss=0.7945 acc=0.6660 | prec=0.6588 rec=0.6521 f1=0.6502 | time=14.5s\n",
            "Epoch 045 | train_loss=0.7522 acc=0.6742 | val_loss=0.7872 acc=0.6562 | prec=0.6622 rec=0.6397 f1=0.6383 | time=14.7s\n",
            "Epoch 046 | train_loss=0.7463 acc=0.6878 | val_loss=0.7700 acc=0.6699 | prec=0.6655 rec=0.6566 f1=0.6569 | time=14.8s\n",
            "Epoch 047 | train_loss=0.7294 acc=0.7074 | val_loss=0.7811 acc=0.6562 | prec=0.6723 rec=0.6369 f1=0.6326 | time=14.5s\n",
            "Epoch 048 | train_loss=0.7306 acc=0.7040 | val_loss=0.7601 acc=0.6719 | prec=0.6778 rec=0.6543 f1=0.6519 | time=14.6s\n",
            "Epoch 049 | train_loss=0.7123 acc=0.7059 | val_loss=0.7603 acc=0.6777 | prec=0.6794 rec=0.6611 f1=0.6591 | time=14.8s\n",
            "Epoch 050 | train_loss=0.6975 acc=0.7211 | val_loss=0.7527 acc=0.6855 | prec=0.6891 rec=0.6693 f1=0.6678 | time=14.5s\n",
            "Epoch 051 | train_loss=0.6762 acc=0.7386 | val_loss=0.7572 acc=0.6758 | prec=0.6751 rec=0.6649 f1=0.6663 | time=14.7s\n",
            "Epoch 052 | train_loss=0.6842 acc=0.7347 | val_loss=0.7467 acc=0.6953 | prec=0.6916 rec=0.6868 f1=0.6878 | time=14.7s\n",
            "Epoch 053 | train_loss=0.6737 acc=0.7250 | val_loss=0.7425 acc=0.6934 | prec=0.6932 rec=0.6789 f1=0.6779 | time=14.8s\n",
            "Epoch 054 | train_loss=0.6689 acc=0.7347 | val_loss=0.7627 acc=0.6973 | prec=0.7003 rec=0.6877 f1=0.6866 | time=14.5s\n",
            "Epoch 055 | train_loss=0.6631 acc=0.7220 | val_loss=0.7398 acc=0.6836 | prec=0.6981 rec=0.6680 f1=0.6692 | time=14.4s\n",
            "Epoch 056 | train_loss=0.6511 acc=0.7406 | val_loss=0.7363 acc=0.6816 | prec=0.6882 rec=0.6671 f1=0.6681 | time=14.6s\n",
            "Epoch 057 | train_loss=0.6339 acc=0.7636 | val_loss=0.7304 acc=0.7031 | prec=0.6958 rec=0.6936 f1=0.6930 | time=14.5s\n",
            "Epoch 058 | train_loss=0.6349 acc=0.7509 | val_loss=0.7288 acc=0.6855 | prec=0.6876 rec=0.6717 f1=0.6724 | time=14.5s\n",
            "Epoch 059 | train_loss=0.6215 acc=0.7616 | val_loss=0.7513 acc=0.6875 | prec=0.7043 rec=0.6679 f1=0.6646 | time=14.6s\n",
            "Epoch 060 | train_loss=0.6223 acc=0.7709 | val_loss=0.7325 acc=0.6895 | prec=0.6932 rec=0.6758 f1=0.6768 | time=14.7s\n",
            "Epoch 061 | train_loss=0.6165 acc=0.7567 | val_loss=0.7220 acc=0.7109 | prec=0.7060 rec=0.7032 f1=0.7039 | time=14.6s\n",
            "Epoch 062 | train_loss=0.6110 acc=0.7709 | val_loss=0.7098 acc=0.7031 | prec=0.7029 rec=0.6941 f1=0.6961 | time=14.4s\n",
            "Epoch 063 | train_loss=0.5993 acc=0.7733 | val_loss=0.7192 acc=0.7129 | prec=0.7075 rec=0.7069 f1=0.7061 | time=14.5s\n",
            "Epoch 064 | train_loss=0.6023 acc=0.7772 | val_loss=0.7077 acc=0.7207 | prec=0.7169 rec=0.7183 f1=0.7173 | time=14.6s\n",
            "Epoch 065 | train_loss=0.5817 acc=0.7782 | val_loss=0.7037 acc=0.7070 | prec=0.7064 rec=0.6968 f1=0.6982 | time=14.6s\n",
            "Epoch 066 | train_loss=0.6012 acc=0.7733 | val_loss=0.7124 acc=0.7090 | prec=0.7036 rec=0.7000 f1=0.7002 | time=14.8s\n",
            "Epoch 067 | train_loss=0.5720 acc=0.7826 | val_loss=0.7243 acc=0.7070 | prec=0.7187 rec=0.6898 f1=0.6889 | time=14.8s\n",
            "Epoch 068 | train_loss=0.5783 acc=0.7816 | val_loss=0.6959 acc=0.7227 | prec=0.7190 rec=0.7206 f1=0.7187 | time=14.7s\n",
            "Epoch 069 | train_loss=0.5679 acc=0.7978 | val_loss=0.7008 acc=0.7148 | prec=0.7135 rec=0.7045 f1=0.7057 | time=14.6s\n",
            "Epoch 070 | train_loss=0.5494 acc=0.8080 | val_loss=0.6859 acc=0.7305 | prec=0.7270 rec=0.7269 f1=0.7269 | time=14.6s\n",
            "Epoch 071 | train_loss=0.5606 acc=0.7997 | val_loss=0.6862 acc=0.7227 | prec=0.7184 rec=0.7164 f1=0.7170 | time=14.6s\n",
            "Epoch 072 | train_loss=0.5642 acc=0.7929 | val_loss=0.7185 acc=0.7129 | prec=0.7244 rec=0.6972 f1=0.6985 | time=14.6s\n",
            "Epoch 073 | train_loss=0.5388 acc=0.8017 | val_loss=0.7021 acc=0.7090 | prec=0.7139 rec=0.6991 f1=0.7018 | time=14.7s\n",
            "Epoch 074 | train_loss=0.5207 acc=0.8173 | val_loss=0.7357 acc=0.6797 | prec=0.6809 rec=0.6727 f1=0.6693 | time=14.8s\n",
            "Epoch 075 | train_loss=0.5241 acc=0.8158 | val_loss=0.6937 acc=0.7148 | prec=0.7102 rec=0.7068 f1=0.7074 | time=14.7s\n",
            "Epoch 076 | train_loss=0.5322 acc=0.8114 | val_loss=0.7042 acc=0.7090 | prec=0.7131 rec=0.7120 f1=0.7058 | time=14.6s\n",
            "Epoch 077 | train_loss=0.5062 acc=0.8158 | val_loss=0.7017 acc=0.7109 | prec=0.7086 rec=0.7036 f1=0.7051 | time=14.4s\n",
            "Epoch 078 | train_loss=0.5079 acc=0.8202 | val_loss=0.7121 acc=0.7031 | prec=0.6986 rec=0.7005 f1=0.6974 | time=14.5s\n",
            "Epoch 079 | train_loss=0.5258 acc=0.8012 | val_loss=0.7050 acc=0.7070 | prec=0.7006 rec=0.6958 f1=0.6951 | time=14.5s\n",
            "Epoch 080 | train_loss=0.5202 acc=0.8041 | val_loss=0.6884 acc=0.7246 | prec=0.7216 rec=0.7210 f1=0.7211 | time=14.6s\n",
            "Epoch 081 | train_loss=0.5122 acc=0.8144 | val_loss=0.6942 acc=0.7129 | prec=0.7077 rec=0.7036 f1=0.7043 | time=14.7s\n",
            "Epoch 082 | train_loss=0.4874 acc=0.8334 | val_loss=0.6968 acc=0.7188 | prec=0.7149 rec=0.7160 f1=0.7151 | time=14.7s\n",
            "Epoch 083 | train_loss=0.5050 acc=0.8178 | val_loss=0.6848 acc=0.7168 | prec=0.7174 rec=0.7100 f1=0.7121 | time=14.7s\n",
            "Epoch 084 | train_loss=0.4864 acc=0.8315 | val_loss=0.6920 acc=0.7070 | prec=0.7023 rec=0.6986 f1=0.6995 | time=14.5s\n",
            "Epoch 085 | train_loss=0.4890 acc=0.8222 | val_loss=0.7091 acc=0.6953 | prec=0.6946 rec=0.6785 f1=0.6755 | time=14.9s\n",
            "Epoch 086 | train_loss=0.4755 acc=0.8261 | val_loss=0.6886 acc=0.7148 | prec=0.7131 rec=0.7151 f1=0.7112 | time=14.7s\n",
            "Epoch 087 | train_loss=0.4731 acc=0.8432 | val_loss=0.6941 acc=0.7070 | prec=0.7029 rec=0.6945 f1=0.6947 | time=15.4s\n",
            "Epoch 088 | train_loss=0.4525 acc=0.8417 | val_loss=0.7076 acc=0.7051 | prec=0.7060 rec=0.6885 f1=0.6867 | time=15.5s\n",
            "Epoch 089 | train_loss=0.4738 acc=0.8310 | val_loss=0.7163 acc=0.6953 | prec=0.6893 rec=0.6886 f1=0.6868 | time=15.3s\n",
            "Epoch 090 | train_loss=0.4498 acc=0.8515 | val_loss=0.6932 acc=0.7070 | prec=0.7019 rec=0.6995 f1=0.7003 | time=15.3s\n",
            "Epoch 091 | train_loss=0.4720 acc=0.8261 | val_loss=0.6998 acc=0.7070 | prec=0.7061 rec=0.6940 f1=0.6932 | time=15.2s\n",
            "Epoch 092 | train_loss=0.4566 acc=0.8422 | val_loss=0.6879 acc=0.7227 | prec=0.7230 rec=0.7178 f1=0.7194 | time=15.6s\n",
            "Epoch 093 | train_loss=0.4438 acc=0.8456 | val_loss=0.6941 acc=0.7188 | prec=0.7129 rec=0.7114 f1=0.7114 | time=15.6s\n",
            "Epoch 094 | train_loss=0.4523 acc=0.8432 | val_loss=0.6962 acc=0.7129 | prec=0.7079 rec=0.7050 f1=0.7056 | time=15.3s\n",
            "Epoch 095 | train_loss=0.4542 acc=0.8422 | val_loss=0.7294 acc=0.6855 | prec=0.6830 rec=0.6814 f1=0.6758 | time=15.4s\n",
            "Epoch 096 | train_loss=0.4344 acc=0.8530 | val_loss=0.6948 acc=0.7188 | prec=0.7145 rec=0.7095 f1=0.7104 | time=15.4s\n",
            "Epoch 097 | train_loss=0.4358 acc=0.8373 | val_loss=0.6911 acc=0.7168 | prec=0.7108 rec=0.7123 f1=0.7107 | time=15.7s\n",
            "Epoch 098 | train_loss=0.4429 acc=0.8456 | val_loss=0.6963 acc=0.7148 | prec=0.7086 rec=0.7073 f1=0.7072 | time=15.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▂▃▃▃▂▁▂▁▁▁▃▄▅▆▅▆▆▆▇▇▇▇▇▇▇▇█▇▇███████████</td></tr><tr><td>precision</td><td>▄▂▃▂▂▁▃▃▅▆█▆▇▇▇▇▇▇██████████████████████</td></tr><tr><td>recall</td><td>▁▂▁▂▂▁▁▁▁▁▄▄▄▅▆▆▇▇▇▇▇▇▇▇█▇██▇█▇███▇█████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▂▂▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇██▇███████</td></tr><tr><td>train_loss</td><td>██████████▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▅▅▅▅▆▆▇▇▇▇▇▇▇▇▇▇██████████████</td></tr><tr><td>val_loss</td><td>██████████▆▅▅▄▃▃▃▂▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>98</td></tr><tr><td>f1_score</td><td>0.70724</td></tr><tr><td>precision</td><td>0.70864</td></tr><tr><td>recall</td><td>0.70729</td></tr><tr><td>train_acc</td><td>0.84563</td></tr><tr><td>train_loss</td><td>0.44288</td></tr><tr><td>val_acc</td><td>0.71484</td></tr><tr><td>val_loss</td><td>0.69625</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/kpkmau9p' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/kpkmau9p</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_093055-kpkmau9p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 09:55:04,548] Trial 0 finished with values: [0.6847848817706108, 0.716796875] and parameters: {'lr': 2.786581334200445e-05, 'wd': 1.7316947862979625e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 1 hyperparams --> lr=6.72e-03, wd=5.21e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_095504-4gf3phfg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/4gf3phfg' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/4gf3phfg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/4gf3phfg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 1] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1339 acc=0.3483 | val_loss=1.1011 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 002 | train_loss=1.1354 acc=0.3322 | val_loss=1.0938 acc=0.3555 | prec=0.2021 rec=0.3315 f1=0.1782 | time=14.6s\n",
            "Epoch 003 | train_loss=1.1250 acc=0.3556 | val_loss=1.0930 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 004 | train_loss=1.1202 acc=0.3454 | val_loss=1.4812 acc=0.2852 | prec=0.0951 rec=0.3333 f1=0.1479 | time=14.8s\n",
            "Epoch 005 | train_loss=1.1380 acc=0.3190 | val_loss=1.0950 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 006 | train_loss=1.1038 acc=0.3605 | val_loss=1.0802 acc=0.4551 | prec=0.5690 rec=0.4327 f1=0.3876 | time=14.6s\n",
            "Epoch 007 | train_loss=1.0232 acc=0.5007 | val_loss=0.9188 acc=0.5977 | prec=0.6350 rec=0.5634 f1=0.5100 | time=14.6s\n",
            "Epoch 008 | train_loss=0.9624 acc=0.5442 | val_loss=1.0231 acc=0.4805 | prec=0.4049 rec=0.4481 f1=0.3615 | time=14.5s\n",
            "Epoch 009 | train_loss=0.9437 acc=0.5296 | val_loss=1.0084 acc=0.5020 | prec=0.5377 rec=0.4755 f1=0.4305 | time=14.6s\n",
            "Epoch 010 | train_loss=0.8387 acc=0.5794 | val_loss=0.8813 acc=0.6016 | prec=0.6099 rec=0.5924 f1=0.5755 | time=14.7s\n",
            "Epoch 011 | train_loss=0.8330 acc=0.6097 | val_loss=1.0702 acc=0.4531 | prec=0.6352 rec=0.4337 f1=0.3718 | time=14.8s\n",
            "Epoch 012 | train_loss=0.7841 acc=0.6268 | val_loss=1.4981 acc=0.4512 | prec=0.6194 rec=0.4522 f1=0.3623 | time=14.7s\n",
            "Epoch 013 | train_loss=0.7737 acc=0.6453 | val_loss=0.7927 acc=0.6523 | prec=0.6860 rec=0.6536 f1=0.6524 | time=14.7s\n",
            "Epoch 014 | train_loss=0.7403 acc=0.6717 | val_loss=1.8296 acc=0.3203 | prec=0.5579 rec=0.3661 f1=0.2143 | time=14.9s\n",
            "Epoch 015 | train_loss=0.7449 acc=0.6654 | val_loss=0.8493 acc=0.6914 | prec=0.7085 rec=0.6730 f1=0.6717 | time=14.9s\n",
            "Epoch 016 | train_loss=0.7186 acc=0.6849 | val_loss=0.8783 acc=0.6250 | prec=0.6742 rec=0.6313 f1=0.6016 | time=14.9s\n",
            "Epoch 017 | train_loss=0.7274 acc=0.6756 | val_loss=1.7988 acc=0.4453 | prec=0.6695 rec=0.4384 f1=0.3448 | time=15.0s\n",
            "Epoch 018 | train_loss=0.7483 acc=0.6600 | val_loss=0.8861 acc=0.5762 | prec=0.6244 rec=0.5586 f1=0.5437 | time=15.1s\n",
            "Epoch 019 | train_loss=0.7462 acc=0.6517 | val_loss=1.5329 acc=0.3594 | prec=0.7150 rec=0.4016 f1=0.2855 | time=14.9s\n",
            "Epoch 020 | train_loss=0.7245 acc=0.6800 | val_loss=1.3098 acc=0.4902 | prec=0.6467 rec=0.5057 f1=0.4410 | time=14.7s\n",
            "Epoch 021 | train_loss=0.6413 acc=0.7142 | val_loss=1.1157 acc=0.5605 | prec=0.6606 rec=0.5675 f1=0.5432 | time=14.8s\n",
            "Epoch 022 | train_loss=0.6751 acc=0.7020 | val_loss=2.1527 acc=0.3652 | prec=0.7028 rec=0.4071 f1=0.2891 | time=14.7s\n",
            "Epoch 023 | train_loss=0.6413 acc=0.7167 | val_loss=0.8687 acc=0.5273 | prec=0.7119 rec=0.5550 f1=0.4818 | time=14.8s\n",
            "Epoch 024 | train_loss=0.6306 acc=0.7264 | val_loss=1.0585 acc=0.6133 | prec=0.6981 rec=0.5779 f1=0.5203 | time=15.0s\n",
            "Epoch 025 | train_loss=0.6481 acc=0.7044 | val_loss=1.5657 acc=0.3730 | prec=0.2716 rec=0.3516 f1=0.2127 | time=15.0s\n",
            "Epoch 026 | train_loss=0.6343 acc=0.7215 | val_loss=0.9535 acc=0.5684 | prec=0.6307 rec=0.5351 f1=0.4764 | time=14.8s\n",
            "Epoch 027 | train_loss=0.6336 acc=0.7196 | val_loss=0.9620 acc=0.5566 | prec=0.6519 rec=0.5417 f1=0.5180 | time=14.9s\n",
            "Epoch 028 | train_loss=0.6051 acc=0.7338 | val_loss=1.2595 acc=0.6094 | prec=0.6732 rec=0.5743 f1=0.5155 | time=14.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▄▆▄▅▇▄▄█▂█▇▄▆▃▅▆▃▅▆▂▅▆▆</td></tr><tr><td>precision</td><td>▁▂▁▁▁▆▇▄▆▇▇▇█▆██▇▇█▇▇███▃▇▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁▃▆▃▄▆▃▃█▂█▇▃▆▂▅▆▃▆▆▁▅▅▆</td></tr><tr><td>train_acc</td><td>▁▁▂▁▁▂▄▅▅▅▆▆▇▇▇▇▇▇▇▇█▇██████</td></tr><tr><td>train_loss</td><td>██████▆▆▅▄▄▃▃▃▃▂▃▃▃▃▁▂▁▁▂▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▂▁▂▄▆▄▅▆▄▄▇▂█▇▄▆▂▅▆▂▅▇▃▆▆▇</td></tr><tr><td>val_loss</td><td>▃▃▃▅▃▂▂▂▂▁▂▅▁▆▁▁▆▁▅▄▃█▁▂▅▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>28</td></tr><tr><td>f1_score</td><td>0.5155</td></tr><tr><td>precision</td><td>0.67321</td></tr><tr><td>recall</td><td>0.57431</td></tr><tr><td>train_acc</td><td>0.73376</td></tr><tr><td>train_loss</td><td>0.6051</td></tr><tr><td>val_acc</td><td>0.60938</td></tr><tr><td>val_loss</td><td>1.25946</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/4gf3phfg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/4gf3phfg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_095504-4gf3phfg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 10:02:01,171] Trial 1 finished with values: [0.7927289269864559, 0.65234375] and parameters: {'lr': 0.006724487138239756, 'wd': 0.0005213565111464654}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 2 hyperparams --> lr=1.34e-04, wd=4.02e-03\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_100201-fllsmtap</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fllsmtap' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fllsmtap' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fllsmtap</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 2] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1281 acc=0.3464 | val_loss=1.1038 acc=0.3496 | prec=0.1945 rec=0.3274 f1=0.1846 | time=14.7s\n",
            "Epoch 002 | train_loss=1.1351 acc=0.3307 | val_loss=1.1003 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 003 | train_loss=1.1313 acc=0.3361 | val_loss=1.1016 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 004 | train_loss=1.1258 acc=0.3591 | val_loss=1.0986 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 005 | train_loss=1.1305 acc=0.3415 | val_loss=1.0998 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 006 | train_loss=1.1315 acc=0.3400 | val_loss=1.1011 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 007 | train_loss=1.1323 acc=0.3254 | val_loss=1.0964 acc=0.3398 | prec=0.2266 rec=0.3169 f1=0.2642 | time=14.6s\n",
            "Epoch 008 | train_loss=1.1337 acc=0.3249 | val_loss=1.0971 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 009 | train_loss=1.1243 acc=0.3425 | val_loss=1.0998 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 010 | train_loss=1.1129 acc=0.3713 | val_loss=1.0970 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 011 | train_loss=1.1187 acc=0.3586 | val_loss=1.0939 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 012 | train_loss=1.1123 acc=0.3659 | val_loss=1.0744 acc=0.4180 | prec=0.2757 rec=0.3898 f1=0.3010 | time=14.7s\n",
            "Epoch 013 | train_loss=1.0843 acc=0.4094 | val_loss=1.0291 acc=0.4512 | prec=0.3016 rec=0.4208 f1=0.3341 | time=14.6s\n",
            "Epoch 014 | train_loss=0.9983 acc=0.4919 | val_loss=0.9864 acc=0.5625 | prec=0.4040 rec=0.5246 f1=0.4427 | time=14.6s\n",
            "Epoch 015 | train_loss=0.9834 acc=0.5056 | val_loss=0.9514 acc=0.5762 | prec=0.7283 rec=0.5387 f1=0.4619 | time=14.7s\n",
            "Epoch 016 | train_loss=0.9515 acc=0.5217 | val_loss=0.9370 acc=0.5918 | prec=0.7264 rec=0.5570 f1=0.5014 | time=14.8s\n",
            "Epoch 017 | train_loss=0.9180 acc=0.5574 | val_loss=0.8895 acc=0.5957 | prec=0.6970 rec=0.5620 f1=0.5117 | time=14.6s\n",
            "Epoch 018 | train_loss=0.8874 acc=0.5740 | val_loss=0.8821 acc=0.5918 | prec=0.5903 rec=0.5644 f1=0.5418 | time=14.6s\n",
            "Epoch 019 | train_loss=0.8449 acc=0.6072 | val_loss=0.9308 acc=0.5293 | prec=0.6120 rec=0.5421 f1=0.5362 | time=14.7s\n",
            "Epoch 020 | train_loss=0.8258 acc=0.6326 | val_loss=0.8201 acc=0.6367 | prec=0.6385 rec=0.6169 f1=0.6114 | time=14.5s\n",
            "Epoch 021 | train_loss=0.7820 acc=0.6541 | val_loss=0.8261 acc=0.6387 | prec=0.6583 rec=0.6395 f1=0.6398 | time=14.5s\n",
            "Epoch 022 | train_loss=0.7715 acc=0.6463 | val_loss=0.7505 acc=0.6836 | prec=0.6800 rec=0.6666 f1=0.6630 | time=14.6s\n",
            "Epoch 023 | train_loss=0.7227 acc=0.6869 | val_loss=0.7844 acc=0.6621 | prec=0.6863 rec=0.6710 f1=0.6594 | time=14.8s\n",
            "Epoch 024 | train_loss=0.6966 acc=0.7054 | val_loss=0.7673 acc=0.6699 | prec=0.6886 rec=0.6502 f1=0.6458 | time=14.8s\n",
            "Epoch 025 | train_loss=0.6778 acc=0.7035 | val_loss=0.7386 acc=0.6875 | prec=0.6964 rec=0.6693 f1=0.6671 | time=14.6s\n",
            "Epoch 026 | train_loss=0.6642 acc=0.7088 | val_loss=0.7617 acc=0.6777 | prec=0.6877 rec=0.6759 f1=0.6769 | time=14.6s\n",
            "Epoch 027 | train_loss=0.6551 acc=0.7201 | val_loss=0.7558 acc=0.6875 | prec=0.6881 rec=0.6864 f1=0.6843 | time=14.6s\n",
            "Epoch 028 | train_loss=0.6338 acc=0.7367 | val_loss=0.8244 acc=0.6309 | prec=0.7398 rec=0.6507 f1=0.6334 | time=14.5s\n",
            "Epoch 029 | train_loss=0.6139 acc=0.7440 | val_loss=0.8378 acc=0.6445 | prec=0.6940 rec=0.6228 f1=0.6166 | time=14.5s\n",
            "Epoch 030 | train_loss=0.6098 acc=0.7318 | val_loss=0.7068 acc=0.6973 | prec=0.6992 rec=0.6941 f1=0.6949 | time=14.7s\n",
            "Epoch 031 | train_loss=0.5911 acc=0.7484 | val_loss=0.7537 acc=0.6758 | prec=0.7035 rec=0.6769 f1=0.6760 | time=14.8s\n",
            "Epoch 032 | train_loss=0.5444 acc=0.7792 | val_loss=0.6892 acc=0.7051 | prec=0.7219 rec=0.7111 f1=0.7042 | time=14.5s\n",
            "Epoch 033 | train_loss=0.5487 acc=0.7723 | val_loss=0.7128 acc=0.6973 | prec=0.6942 rec=0.6840 f1=0.6840 | time=14.7s\n",
            "Epoch 034 | train_loss=0.5228 acc=0.7899 | val_loss=0.6956 acc=0.7090 | prec=0.7294 rec=0.7120 f1=0.7109 | time=14.7s\n",
            "Epoch 035 | train_loss=0.5247 acc=0.7875 | val_loss=0.7536 acc=0.6855 | prec=0.6977 rec=0.6781 f1=0.6820 | time=14.6s\n",
            "Epoch 036 | train_loss=0.5041 acc=0.7973 | val_loss=0.7804 acc=0.7051 | prec=0.7171 rec=0.6880 f1=0.6873 | time=14.5s\n",
            "Epoch 037 | train_loss=0.5092 acc=0.7894 | val_loss=0.6832 acc=0.7324 | prec=0.7394 rec=0.7338 f1=0.7321 | time=14.7s\n",
            "Epoch 038 | train_loss=0.4734 acc=0.8085 | val_loss=0.7456 acc=0.7031 | prec=0.7133 rec=0.6959 f1=0.6990 | time=14.8s\n",
            "Epoch 039 | train_loss=0.4495 acc=0.8285 | val_loss=0.7493 acc=0.7109 | prec=0.7054 rec=0.7060 f1=0.7037 | time=14.6s\n",
            "Epoch 040 | train_loss=0.4376 acc=0.8222 | val_loss=0.7366 acc=0.7129 | prec=0.7226 rec=0.7087 f1=0.7118 | time=14.6s\n",
            "Epoch 041 | train_loss=0.4189 acc=0.8354 | val_loss=0.7824 acc=0.6934 | prec=0.6866 rec=0.6863 f1=0.6846 | time=14.6s\n",
            "Epoch 042 | train_loss=0.4348 acc=0.8285 | val_loss=1.0355 acc=0.6699 | prec=0.7063 rec=0.6460 f1=0.6386 | time=14.6s\n",
            "Epoch 043 | train_loss=0.4120 acc=0.8407 | val_loss=0.7609 acc=0.7031 | prec=0.7050 rec=0.6973 f1=0.6997 | time=14.6s\n",
            "Epoch 044 | train_loss=0.3755 acc=0.8534 | val_loss=0.7964 acc=0.6973 | prec=0.7070 rec=0.6923 f1=0.6954 | time=14.6s\n",
            "Epoch 045 | train_loss=0.3734 acc=0.8588 | val_loss=0.7807 acc=0.6914 | prec=0.6883 rec=0.6928 f1=0.6850 | time=14.9s\n",
            "Epoch 046 | train_loss=0.3972 acc=0.8378 | val_loss=0.8335 acc=0.6973 | prec=0.7168 rec=0.6895 f1=0.6945 | time=14.6s\n",
            "Epoch 047 | train_loss=0.3514 acc=0.8710 | val_loss=0.7760 acc=0.7070 | prec=0.7046 rec=0.7042 f1=0.7036 | time=14.6s\n",
            "Epoch 048 | train_loss=0.3322 acc=0.8686 | val_loss=0.7938 acc=0.7227 | prec=0.7175 rec=0.7155 f1=0.7160 | time=14.5s\n",
            "Epoch 049 | train_loss=0.3239 acc=0.8740 | val_loss=0.8519 acc=0.7090 | prec=0.7135 rec=0.6967 f1=0.6993 | time=14.6s\n",
            "Epoch 050 | train_loss=0.3341 acc=0.8764 | val_loss=0.8681 acc=0.7051 | prec=0.7099 rec=0.7000 f1=0.7025 | time=14.6s\n",
            "Epoch 051 | train_loss=0.3123 acc=0.8818 | val_loss=0.8865 acc=0.6797 | prec=0.6797 rec=0.6736 f1=0.6756 | time=14.7s\n",
            "Epoch 052 | train_loss=0.2977 acc=0.8788 | val_loss=0.9145 acc=0.6719 | prec=0.7038 rec=0.6801 f1=0.6730 | time=14.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▃▄▅▅▅▆▇▇▇▇▇▇█▇▇▇██████▇███████▇</td></tr><tr><td>precision</td><td>▂▁▁▁▁▂▁▁▁▁▃▄███▇▇▇▇▇▇▇█▇███████▇████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▂▄▅▅▅▅▆▇▇▇▇▇▆▇▇██▇▇█▇█▇▆▇▇▇▇█▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▂▁▂▃▃▃▄▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>███████████▆▆▆▆▅▅▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▅▅▅▅▅▆▇▇▇▇▆▆▇▇▇▇▇█▇█▇▇▇▇▇███▇▇</td></tr><tr><td>val_loss</td><td>██████████▆▅▅▄▄▃▃▂▃▂▂▂▃▄▁▁▁▂▃▁▂▂▃▇▂▃▄▃▃▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>52</td></tr><tr><td>f1_score</td><td>0.67297</td></tr><tr><td>precision</td><td>0.70378</td></tr><tr><td>recall</td><td>0.68014</td></tr><tr><td>train_acc</td><td>0.87885</td></tr><tr><td>train_loss</td><td>0.29765</td></tr><tr><td>val_acc</td><td>0.67188</td></tr><tr><td>val_loss</td><td>0.91455</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fllsmtap' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fllsmtap</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_100201-fllsmtap/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 10:14:45,349] Trial 2 finished with values: [0.6831636913120747, 0.732421875] and parameters: {'lr': 0.00013354248239129485, 'wd': 0.004022672318622753}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 3 hyperparams --> lr=2.90e-05, wd=2.05e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_101445-00ped7v2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/00ped7v2' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/00ped7v2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/00ped7v2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 3] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1417 acc=0.3512 | val_loss=1.1006 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 002 | train_loss=1.1563 acc=0.3376 | val_loss=1.1036 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 003 | train_loss=1.1419 acc=0.3532 | val_loss=1.1030 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 004 | train_loss=1.1478 acc=0.3356 | val_loss=1.1021 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 005 | train_loss=1.1375 acc=0.3561 | val_loss=1.1020 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 006 | train_loss=1.1334 acc=0.3464 | val_loss=1.0956 acc=0.3496 | prec=0.1728 rec=0.3260 f1=0.1758 | time=14.9s\n",
            "Epoch 007 | train_loss=1.1394 acc=0.3503 | val_loss=1.0975 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 008 | train_loss=1.1366 acc=0.3439 | val_loss=1.0951 acc=0.3574 | prec=0.2394 rec=0.3333 f1=0.2419 | time=14.5s\n",
            "Epoch 009 | train_loss=1.1316 acc=0.3610 | val_loss=1.0949 acc=0.3457 | prec=0.2271 rec=0.3224 f1=0.2296 | time=14.6s\n",
            "Epoch 010 | train_loss=1.1236 acc=0.3649 | val_loss=1.0944 acc=0.3477 | prec=0.2150 rec=0.3242 f1=0.2049 | time=14.6s\n",
            "Epoch 011 | train_loss=1.1246 acc=0.3620 | val_loss=1.0935 acc=0.3574 | prec=0.2587 rec=0.3333 f1=0.1990 | time=14.5s\n",
            "Epoch 012 | train_loss=1.1242 acc=0.3576 | val_loss=1.0936 acc=0.3477 | prec=0.1173 rec=0.3242 f1=0.1722 | time=14.7s\n",
            "Epoch 013 | train_loss=1.1359 acc=0.3425 | val_loss=1.0951 acc=0.3535 | prec=0.2251 rec=0.3297 f1=0.1978 | time=14.9s\n",
            "Epoch 014 | train_loss=1.1263 acc=0.3385 | val_loss=1.0938 acc=0.3496 | prec=0.2244 rec=0.3260 f1=0.2254 | time=14.6s\n",
            "Epoch 015 | train_loss=1.1259 acc=0.3429 | val_loss=1.0986 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 016 | train_loss=1.1201 acc=0.3522 | val_loss=1.0994 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 017 | train_loss=1.1227 acc=0.3459 | val_loss=1.0937 acc=0.3730 | prec=0.2496 rec=0.3479 f1=0.2854 | time=14.5s\n",
            "Epoch 018 | train_loss=1.1080 acc=0.3698 | val_loss=1.0940 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 019 | train_loss=1.1064 acc=0.3644 | val_loss=1.0962 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 020 | train_loss=1.1181 acc=0.3512 | val_loss=1.0882 acc=0.3691 | prec=0.2453 rec=0.3443 f1=0.2129 | time=14.8s\n",
            "Epoch 021 | train_loss=1.1165 acc=0.3635 | val_loss=1.0795 acc=0.4414 | prec=0.3182 rec=0.4117 f1=0.3375 | time=14.8s\n",
            "Epoch 022 | train_loss=1.0900 acc=0.3942 | val_loss=1.0658 acc=0.4746 | prec=0.3152 rec=0.4426 f1=0.3669 | time=14.7s\n",
            "Epoch 023 | train_loss=1.0887 acc=0.4069 | val_loss=1.0386 acc=0.4980 | prec=0.3308 rec=0.4645 f1=0.3860 | time=14.6s\n",
            "Epoch 024 | train_loss=1.0537 acc=0.4397 | val_loss=1.0182 acc=0.5293 | prec=0.3854 rec=0.4936 f1=0.4140 | time=14.5s\n",
            "Epoch 025 | train_loss=1.0574 acc=0.4446 | val_loss=1.0060 acc=0.5527 | prec=0.3713 rec=0.5155 f1=0.4312 | time=14.6s\n",
            "Epoch 026 | train_loss=1.0268 acc=0.4885 | val_loss=1.0292 acc=0.4570 | prec=0.3354 rec=0.4262 f1=0.3318 | time=14.7s\n",
            "Epoch 027 | train_loss=1.0207 acc=0.4841 | val_loss=0.9879 acc=0.5566 | prec=0.3717 rec=0.5191 f1=0.4311 | time=14.9s\n",
            "Epoch 028 | train_loss=1.0019 acc=0.4944 | val_loss=0.9792 acc=0.5508 | prec=0.3682 rec=0.5137 f1=0.4258 | time=14.7s\n",
            "Epoch 029 | train_loss=0.9871 acc=0.5232 | val_loss=0.9653 acc=0.5684 | prec=0.3879 rec=0.5301 f1=0.4448 | time=14.6s\n",
            "Epoch 030 | train_loss=0.9747 acc=0.5310 | val_loss=0.9442 acc=0.5645 | prec=0.3862 rec=0.5264 f1=0.4418 | time=14.6s\n",
            "Epoch 031 | train_loss=0.9524 acc=0.5374 | val_loss=0.9288 acc=0.5820 | prec=0.3991 rec=0.5428 f1=0.4561 | time=14.6s\n",
            "Epoch 032 | train_loss=0.9345 acc=0.5608 | val_loss=0.9309 acc=0.5684 | prec=0.7158 rec=0.5305 f1=0.4470 | time=14.5s\n",
            "Epoch 033 | train_loss=0.9100 acc=0.5706 | val_loss=0.9075 acc=0.5781 | prec=0.3969 rec=0.5392 f1=0.4534 | time=14.7s\n",
            "Epoch 034 | train_loss=0.9126 acc=0.5706 | val_loss=0.9005 acc=0.5781 | prec=0.4049 rec=0.5392 f1=0.4550 | time=14.8s\n",
            "Epoch 035 | train_loss=0.9022 acc=0.5696 | val_loss=0.9069 acc=0.5703 | prec=0.3839 rec=0.5319 f1=0.4452 | time=14.7s\n",
            "Epoch 036 | train_loss=0.8979 acc=0.5799 | val_loss=0.8982 acc=0.5664 | prec=0.4659 rec=0.5287 f1=0.4469 | time=14.6s\n",
            "Epoch 037 | train_loss=0.8883 acc=0.5657 | val_loss=0.8935 acc=0.5762 | prec=0.4184 rec=0.5373 f1=0.4550 | time=14.6s\n",
            "Epoch 038 | train_loss=0.8786 acc=0.5779 | val_loss=0.8784 acc=0.5918 | prec=0.6252 rec=0.5542 f1=0.4837 | time=14.6s\n",
            "Epoch 039 | train_loss=0.8703 acc=0.5794 | val_loss=0.8815 acc=0.5918 | prec=0.5523 rec=0.5653 f1=0.5422 | time=14.6s\n",
            "Epoch 040 | train_loss=0.8441 acc=0.6028 | val_loss=0.8673 acc=0.5938 | prec=0.6062 rec=0.5560 f1=0.4860 | time=14.6s\n",
            "Epoch 041 | train_loss=0.8464 acc=0.5984 | val_loss=0.8676 acc=0.5801 | prec=0.5206 rec=0.5424 f1=0.4706 | time=14.8s\n",
            "Epoch 042 | train_loss=0.8441 acc=0.5960 | val_loss=0.8481 acc=0.5898 | prec=0.5757 rec=0.5533 f1=0.4852 | time=14.6s\n",
            "Epoch 043 | train_loss=0.8396 acc=0.6111 | val_loss=0.8380 acc=0.6055 | prec=0.5874 rec=0.5674 f1=0.4983 | time=14.6s\n",
            "Epoch 044 | train_loss=0.8299 acc=0.6180 | val_loss=0.8532 acc=0.5938 | prec=0.7594 rec=0.5560 f1=0.4861 | time=14.6s\n",
            "Epoch 045 | train_loss=0.8272 acc=0.6180 | val_loss=0.8361 acc=0.5918 | prec=0.5796 rec=0.5547 f1=0.4901 | time=14.5s\n",
            "Epoch 046 | train_loss=0.7918 acc=0.6351 | val_loss=0.8226 acc=0.6152 | prec=0.6046 rec=0.5825 f1=0.5402 | time=14.6s\n",
            "Epoch 047 | train_loss=0.8022 acc=0.6287 | val_loss=0.8086 acc=0.6309 | prec=0.6509 rec=0.5985 f1=0.5603 | time=14.7s\n",
            "Epoch 048 | train_loss=0.7776 acc=0.6512 | val_loss=0.8083 acc=0.6309 | prec=0.6495 rec=0.6003 f1=0.5672 | time=14.8s\n",
            "Epoch 049 | train_loss=0.7637 acc=0.6771 | val_loss=0.8012 acc=0.6270 | prec=0.6203 rec=0.5985 f1=0.5738 | time=14.6s\n",
            "Epoch 050 | train_loss=0.7620 acc=0.6478 | val_loss=0.7951 acc=0.6328 | prec=0.6733 rec=0.6026 f1=0.5734 | time=14.6s\n",
            "Epoch 051 | train_loss=0.7381 acc=0.6795 | val_loss=0.7843 acc=0.6367 | prec=0.6591 rec=0.6067 f1=0.5765 | time=14.8s\n",
            "Epoch 052 | train_loss=0.7243 acc=0.6908 | val_loss=0.7758 acc=0.6680 | prec=0.6547 rec=0.6511 f1=0.6471 | time=14.6s\n",
            "Epoch 053 | train_loss=0.7362 acc=0.6746 | val_loss=0.7556 acc=0.6660 | prec=0.6711 rec=0.6428 f1=0.6325 | time=14.7s\n",
            "Epoch 054 | train_loss=0.7147 acc=0.6957 | val_loss=0.7623 acc=0.6660 | prec=0.6731 rec=0.6428 f1=0.6332 | time=14.8s\n",
            "Epoch 055 | train_loss=0.7128 acc=0.7040 | val_loss=0.7445 acc=0.6914 | prec=0.6823 rec=0.6790 f1=0.6785 | time=14.8s\n",
            "Epoch 056 | train_loss=0.7085 acc=0.7044 | val_loss=0.7528 acc=0.6816 | prec=0.6709 rec=0.6712 f1=0.6702 | time=14.6s\n",
            "Epoch 057 | train_loss=0.6994 acc=0.7005 | val_loss=0.7393 acc=0.6875 | prec=0.6786 rec=0.6744 f1=0.6743 | time=14.6s\n",
            "Epoch 058 | train_loss=0.6848 acc=0.7191 | val_loss=0.7340 acc=0.6992 | prec=0.6908 rec=0.6886 f1=0.6877 | time=14.6s\n",
            "Epoch 059 | train_loss=0.6665 acc=0.7274 | val_loss=0.7272 acc=0.6934 | prec=0.6957 rec=0.6771 f1=0.6745 | time=14.6s\n",
            "Epoch 060 | train_loss=0.6683 acc=0.7240 | val_loss=0.7310 acc=0.6973 | prec=0.6894 rec=0.6914 f1=0.6888 | time=14.6s\n",
            "Epoch 061 | train_loss=0.6565 acc=0.7352 | val_loss=0.7156 acc=0.6973 | prec=0.6967 rec=0.6812 f1=0.6796 | time=14.8s\n",
            "Epoch 062 | train_loss=0.6482 acc=0.7352 | val_loss=0.7166 acc=0.6992 | prec=0.7043 rec=0.6826 f1=0.6828 | time=14.8s\n",
            "Epoch 063 | train_loss=0.6510 acc=0.7367 | val_loss=0.7703 acc=0.6680 | prec=0.6853 rec=0.6760 f1=0.6569 | time=14.6s\n",
            "Epoch 064 | train_loss=0.6340 acc=0.7494 | val_loss=0.7167 acc=0.7031 | prec=0.6981 rec=0.7001 f1=0.6976 | time=14.6s\n",
            "Epoch 065 | train_loss=0.6213 acc=0.7582 | val_loss=0.7095 acc=0.7148 | prec=0.7088 rec=0.7105 f1=0.7093 | time=14.6s\n",
            "Epoch 066 | train_loss=0.6249 acc=0.7474 | val_loss=0.7123 acc=0.7188 | prec=0.7167 rec=0.7151 f1=0.7153 | time=14.6s\n",
            "Epoch 067 | train_loss=0.6201 acc=0.7494 | val_loss=0.7122 acc=0.7148 | prec=0.7132 rec=0.7147 f1=0.7110 | time=14.6s\n",
            "Epoch 068 | train_loss=0.6025 acc=0.7748 | val_loss=0.6969 acc=0.7266 | prec=0.7218 rec=0.7191 f1=0.7201 | time=14.8s\n",
            "Epoch 069 | train_loss=0.6131 acc=0.7445 | val_loss=0.7063 acc=0.6953 | prec=0.6935 rec=0.6849 f1=0.6844 | time=14.8s\n",
            "Epoch 070 | train_loss=0.5987 acc=0.7572 | val_loss=0.6982 acc=0.7109 | prec=0.7050 rec=0.7032 f1=0.7037 | time=14.6s\n",
            "Epoch 071 | train_loss=0.6067 acc=0.7582 | val_loss=0.7272 acc=0.6855 | prec=0.6998 rec=0.6777 f1=0.6812 | time=14.6s\n",
            "Epoch 072 | train_loss=0.5912 acc=0.7640 | val_loss=0.6969 acc=0.7246 | prec=0.7255 rec=0.7247 f1=0.7228 | time=14.7s\n",
            "Epoch 073 | train_loss=0.5917 acc=0.7670 | val_loss=0.7247 acc=0.7051 | prec=0.7199 rec=0.7028 f1=0.7043 | time=14.7s\n",
            "Epoch 074 | train_loss=0.5647 acc=0.7836 | val_loss=0.6818 acc=0.7266 | prec=0.7214 rec=0.7228 f1=0.7219 | time=14.7s\n",
            "Epoch 075 | train_loss=0.5759 acc=0.7763 | val_loss=0.6855 acc=0.7305 | prec=0.7311 rec=0.7251 f1=0.7271 | time=14.9s\n",
            "Epoch 076 | train_loss=0.5631 acc=0.7919 | val_loss=0.6990 acc=0.7188 | prec=0.7317 rec=0.7109 f1=0.7154 | time=14.8s\n",
            "Epoch 077 | train_loss=0.5391 acc=0.8051 | val_loss=0.6785 acc=0.7305 | prec=0.7287 rec=0.7246 f1=0.7262 | time=14.8s\n",
            "Epoch 078 | train_loss=0.5473 acc=0.7880 | val_loss=0.6908 acc=0.7012 | prec=0.7169 rec=0.7070 f1=0.7009 | time=14.6s\n",
            "Epoch 079 | train_loss=0.5387 acc=0.7978 | val_loss=0.6984 acc=0.7012 | prec=0.7123 rec=0.7065 f1=0.7009 | time=14.7s\n",
            "Epoch 080 | train_loss=0.5320 acc=0.7958 | val_loss=0.6811 acc=0.7246 | prec=0.7278 rec=0.7238 f1=0.7235 | time=14.7s\n",
            "Epoch 081 | train_loss=0.5217 acc=0.8061 | val_loss=0.6708 acc=0.7246 | prec=0.7255 rec=0.7247 f1=0.7228 | time=14.7s\n",
            "Epoch 082 | train_loss=0.5312 acc=0.8026 | val_loss=0.6762 acc=0.7227 | prec=0.7283 rec=0.7206 f1=0.7217 | time=14.8s\n",
            "Epoch 083 | train_loss=0.5157 acc=0.8061 | val_loss=0.6650 acc=0.7324 | prec=0.7302 rec=0.7288 f1=0.7291 | time=14.6s\n",
            "Epoch 084 | train_loss=0.5195 acc=0.8051 | val_loss=0.6703 acc=0.7266 | prec=0.7266 rec=0.7265 f1=0.7248 | time=14.6s\n",
            "Epoch 085 | train_loss=0.5215 acc=0.8002 | val_loss=0.6902 acc=0.6973 | prec=0.6963 rec=0.6997 f1=0.6912 | time=14.6s\n",
            "Epoch 086 | train_loss=0.5152 acc=0.8056 | val_loss=0.6691 acc=0.7344 | prec=0.7375 rec=0.7306 f1=0.7325 | time=14.6s\n",
            "Epoch 087 | train_loss=0.5163 acc=0.8139 | val_loss=0.6731 acc=0.7129 | prec=0.7140 rec=0.7147 f1=0.7106 | time=14.5s\n",
            "Epoch 088 | train_loss=0.5056 acc=0.8144 | val_loss=0.6660 acc=0.7129 | prec=0.7078 rec=0.7105 f1=0.7086 | time=14.6s\n",
            "Epoch 089 | train_loss=0.4904 acc=0.8168 | val_loss=0.6697 acc=0.7266 | prec=0.7343 rec=0.7224 f1=0.7254 | time=14.9s\n",
            "Epoch 090 | train_loss=0.4885 acc=0.8149 | val_loss=0.6593 acc=0.7324 | prec=0.7298 rec=0.7283 f1=0.7289 | time=14.7s\n",
            "Epoch 091 | train_loss=0.4965 acc=0.8192 | val_loss=0.6877 acc=0.7070 | prec=0.7049 rec=0.7078 f1=0.7000 | time=14.5s\n",
            "Epoch 092 | train_loss=0.4913 acc=0.8271 | val_loss=0.6745 acc=0.7266 | prec=0.7401 rec=0.7173 f1=0.7222 | time=14.6s\n",
            "Epoch 093 | train_loss=0.4984 acc=0.8109 | val_loss=0.6576 acc=0.7422 | prec=0.7414 rec=0.7397 f1=0.7402 | time=14.7s\n",
            "Epoch 094 | train_loss=0.4762 acc=0.8305 | val_loss=0.6671 acc=0.7246 | prec=0.7250 rec=0.7252 f1=0.7229 | time=14.6s\n",
            "Epoch 095 | train_loss=0.4887 acc=0.8188 | val_loss=0.6610 acc=0.7207 | prec=0.7176 rec=0.7178 f1=0.7175 | time=14.6s\n",
            "Epoch 096 | train_loss=0.4796 acc=0.8300 | val_loss=0.6699 acc=0.7363 | prec=0.7415 rec=0.7333 f1=0.7353 | time=14.8s\n",
            "Epoch 097 | train_loss=0.4794 acc=0.8144 | val_loss=0.6627 acc=0.7285 | prec=0.7252 rec=0.7219 f1=0.7229 | time=14.6s\n",
            "Epoch 098 | train_loss=0.4876 acc=0.8139 | val_loss=0.6716 acc=0.7207 | prec=0.7206 rec=0.7220 f1=0.7185 | time=14.5s\n",
            "Epoch 099 | train_loss=0.4642 acc=0.8349 | val_loss=0.6605 acc=0.7344 | prec=0.7293 rec=0.7320 f1=0.7300 | time=14.7s\n",
            "Epoch 100 | train_loss=0.4612 acc=0.8368 | val_loss=0.6620 acc=0.7207 | prec=0.7197 rec=0.7151 f1=0.7168 | time=14.6s\n",
            "Epoch 101 | train_loss=0.4706 acc=0.8202 | val_loss=0.6611 acc=0.7402 | prec=0.7443 rec=0.7356 f1=0.7381 | time=14.5s\n",
            "Epoch 102 | train_loss=0.4585 acc=0.8295 | val_loss=0.6622 acc=0.7227 | prec=0.7201 rec=0.7183 f1=0.7191 | time=14.7s\n",
            "Epoch 103 | train_loss=0.4584 acc=0.8324 | val_loss=0.6677 acc=0.7266 | prec=0.7259 rec=0.7210 f1=0.7228 | time=14.7s\n",
            "Epoch 104 | train_loss=0.4407 acc=0.8471 | val_loss=0.6659 acc=0.7324 | prec=0.7334 rec=0.7297 f1=0.7306 | time=14.6s\n",
            "Epoch 105 | train_loss=0.4493 acc=0.8368 | val_loss=0.6687 acc=0.7305 | prec=0.7311 rec=0.7269 f1=0.7282 | time=14.5s\n",
            "Epoch 106 | train_loss=0.4464 acc=0.8329 | val_loss=0.6708 acc=0.7266 | prec=0.7308 rec=0.7274 f1=0.7260 | time=14.6s\n",
            "Epoch 107 | train_loss=0.4503 acc=0.8339 | val_loss=0.6705 acc=0.7402 | prec=0.7470 rec=0.7360 f1=0.7388 | time=14.6s\n",
            "Epoch 108 | train_loss=0.4453 acc=0.8368 | val_loss=0.6604 acc=0.7148 | prec=0.7124 rec=0.7147 f1=0.7123 | time=14.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▂▁▂▁▁▁▄▄▄▄▆▅▅▆▆▇▇▇▇█████████████████</td></tr><tr><td>precision</td><td>▁▂▂▁▂▁▁▃▄▄▄▄▅▄▇▆▆▇▇▇▇▇▇█████████▇███████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▃▃▃▅▅▄▅▅▅▅▅▆▆▇▇▇▇█▇▇████▇████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▂▂▃▃▄▄▄▄▅▅▆▆▆▇▇▇▇▇▇▇█████████████</td></tr><tr><td>train_loss</td><td>█████▇▇▆▆▆▅▅▅▅▅▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▄▅▃▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇█▇▇████▇████████</td></tr><tr><td>val_loss</td><td>█████▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>108</td></tr><tr><td>f1_score</td><td>0.7123</td></tr><tr><td>precision</td><td>0.71241</td></tr><tr><td>recall</td><td>0.71467</td></tr><tr><td>train_acc</td><td>0.83683</td></tr><tr><td>train_loss</td><td>0.44531</td></tr><tr><td>val_acc</td><td>0.71484</td></tr><tr><td>val_loss</td><td>0.66045</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/00ped7v2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/00ped7v2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_101445-00ped7v2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 10:41:11,067] Trial 3 finished with values: [0.6575549207627773, 0.7421875] and parameters: {'lr': 2.8981260209177457e-05, 'wd': 2.0541260445442108e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 4 hyperparams --> lr=2.08e-05, wd=6.91e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_104111-2e3tknzu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/2e3tknzu' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/2e3tknzu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/2e3tknzu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 4] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.2023 acc=0.3034 | val_loss=1.1456 acc=0.2852 | prec=0.0951 rec=0.3333 f1=0.1479 | time=14.9s\n",
            "Epoch 002 | train_loss=1.1835 acc=0.3224 | val_loss=1.1181 acc=0.3613 | prec=0.2295 rec=0.3481 f1=0.2455 | time=14.6s\n",
            "Epoch 003 | train_loss=1.1651 acc=0.3302 | val_loss=1.1071 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 004 | train_loss=1.1551 acc=0.3337 | val_loss=1.1091 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 005 | train_loss=1.1528 acc=0.3249 | val_loss=1.1054 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 006 | train_loss=1.1391 acc=0.3332 | val_loss=1.1082 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 007 | train_loss=1.1640 acc=0.3083 | val_loss=1.1037 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 008 | train_loss=1.1553 acc=0.3214 | val_loss=1.0995 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.9s\n",
            "Epoch 009 | train_loss=1.1459 acc=0.3337 | val_loss=1.0983 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 010 | train_loss=1.1293 acc=0.3581 | val_loss=1.0989 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 011 | train_loss=1.1442 acc=0.3405 | val_loss=1.0983 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 012 | train_loss=1.1387 acc=0.3327 | val_loss=1.0968 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 013 | train_loss=1.1440 acc=0.3376 | val_loss=1.0991 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 014 | train_loss=1.1440 acc=0.3239 | val_loss=1.0987 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 015 | train_loss=1.1318 acc=0.3586 | val_loss=1.0970 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 016 | train_loss=1.1195 acc=0.3713 | val_loss=1.0970 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 017 | train_loss=1.1444 acc=0.3356 | val_loss=1.0968 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 018 | train_loss=1.1362 acc=0.3439 | val_loss=1.0951 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 019 | train_loss=1.1376 acc=0.3464 | val_loss=1.0988 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 020 | train_loss=1.1281 acc=0.3420 | val_loss=1.0948 acc=0.3594 | prec=0.4527 rec=0.3352 f1=0.1794 | time=14.6s\n",
            "Epoch 021 | train_loss=1.1298 acc=0.3610 | val_loss=1.0957 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 022 | train_loss=1.1265 acc=0.3517 | val_loss=1.0940 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 023 | train_loss=1.1261 acc=0.3591 | val_loss=1.0823 acc=0.3750 | prec=0.3080 rec=0.3497 f1=0.2124 | time=14.7s\n",
            "Epoch 024 | train_loss=1.1011 acc=0.3923 | val_loss=1.0562 acc=0.4824 | prec=0.4334 rec=0.4522 f1=0.3932 | time=14.7s\n",
            "Epoch 025 | train_loss=1.0863 acc=0.4094 | val_loss=1.0791 acc=0.4043 | prec=0.5542 rec=0.3835 f1=0.2773 | time=14.6s\n",
            "Epoch 026 | train_loss=1.0746 acc=0.4304 | val_loss=1.0053 acc=0.5508 | prec=0.5103 rec=0.5192 f1=0.4738 | time=14.6s\n",
            "Epoch 027 | train_loss=1.0488 acc=0.4431 | val_loss=1.0420 acc=0.4688 | prec=0.5159 rec=0.4399 f1=0.3630 | time=14.7s\n",
            "Epoch 028 | train_loss=1.0365 acc=0.4558 | val_loss=0.9928 acc=0.5332 | prec=0.4410 rec=0.4982 f1=0.4232 | time=14.7s\n",
            "Epoch 029 | train_loss=1.0231 acc=0.4748 | val_loss=0.9698 acc=0.5645 | prec=0.4461 rec=0.5269 f1=0.4478 | time=14.9s\n",
            "Epoch 030 | train_loss=1.0206 acc=0.4773 | val_loss=0.9610 acc=0.5625 | prec=0.3834 rec=0.5246 f1=0.4404 | time=14.7s\n",
            "Epoch 031 | train_loss=1.0039 acc=0.4973 | val_loss=0.9580 acc=0.5703 | prec=0.5044 rec=0.5323 f1=0.4510 | time=14.6s\n",
            "Epoch 032 | train_loss=0.9988 acc=0.4880 | val_loss=0.9577 acc=0.5469 | prec=0.4728 rec=0.5132 f1=0.4520 | time=14.6s\n",
            "Epoch 033 | train_loss=0.9738 acc=0.5071 | val_loss=0.9328 acc=0.5781 | prec=0.5881 rec=0.5419 f1=0.4723 | time=14.8s\n",
            "Epoch 034 | train_loss=0.9652 acc=0.5203 | val_loss=0.9256 acc=0.5762 | prec=0.5591 rec=0.5433 f1=0.4912 | time=14.7s\n",
            "Epoch 035 | train_loss=0.9424 acc=0.5452 | val_loss=0.9148 acc=0.5879 | prec=0.6615 rec=0.5524 f1=0.4874 | time=14.8s\n",
            "Epoch 036 | train_loss=0.9424 acc=0.5408 | val_loss=0.9183 acc=0.5801 | prec=0.6230 rec=0.5456 f1=0.4841 | time=14.7s\n",
            "Epoch 037 | train_loss=0.9410 acc=0.5540 | val_loss=0.8936 acc=0.5938 | prec=0.6535 rec=0.5593 f1=0.5036 | time=14.6s\n",
            "Epoch 038 | train_loss=0.9267 acc=0.5564 | val_loss=0.8963 acc=0.6074 | prec=0.6852 rec=0.5739 f1=0.5265 | time=14.6s\n",
            "Epoch 039 | train_loss=0.9262 acc=0.5388 | val_loss=0.8874 acc=0.6094 | prec=0.6476 rec=0.5748 f1=0.5218 | time=14.6s\n",
            "Epoch 040 | train_loss=0.9011 acc=0.5760 | val_loss=0.8672 acc=0.6113 | prec=0.6553 rec=0.5784 f1=0.5321 | time=14.6s\n",
            "Epoch 041 | train_loss=0.8995 acc=0.5882 | val_loss=0.8556 acc=0.6191 | prec=0.6218 rec=0.5913 f1=0.5664 | time=14.6s\n",
            "Epoch 042 | train_loss=0.8851 acc=0.6058 | val_loss=0.8568 acc=0.6191 | prec=0.6355 rec=0.5899 f1=0.5598 | time=14.8s\n",
            "Epoch 043 | train_loss=0.8801 acc=0.6023 | val_loss=0.8448 acc=0.6172 | prec=0.6183 rec=0.5876 f1=0.5577 | time=14.6s\n",
            "Epoch 044 | train_loss=0.8661 acc=0.6170 | val_loss=0.8714 acc=0.6016 | prec=0.6348 rec=0.5703 f1=0.5276 | time=14.6s\n",
            "Epoch 045 | train_loss=0.8783 acc=0.6106 | val_loss=0.8354 acc=0.6289 | prec=0.6382 rec=0.6018 f1=0.5799 | time=14.6s\n",
            "Epoch 046 | train_loss=0.8483 acc=0.6229 | val_loss=0.8410 acc=0.6211 | prec=0.6378 rec=0.5931 f1=0.5696 | time=14.7s\n",
            "Epoch 047 | train_loss=0.8353 acc=0.6390 | val_loss=0.8195 acc=0.6387 | prec=0.6530 rec=0.6136 f1=0.5981 | time=14.6s\n",
            "Epoch 048 | train_loss=0.8404 acc=0.6380 | val_loss=0.8244 acc=0.6602 | prec=0.6558 rec=0.6415 f1=0.6361 | time=14.6s\n",
            "Epoch 049 | train_loss=0.8274 acc=0.6429 | val_loss=0.8381 acc=0.6289 | prec=0.6421 rec=0.6138 f1=0.6138 | time=14.8s\n",
            "Epoch 050 | train_loss=0.8182 acc=0.6639 | val_loss=0.8025 acc=0.6738 | prec=0.6761 rec=0.6593 f1=0.6598 | time=14.6s\n",
            "Epoch 051 | train_loss=0.7996 acc=0.6659 | val_loss=0.7871 acc=0.6582 | prec=0.6648 rec=0.6392 f1=0.6358 | time=14.6s\n",
            "Epoch 052 | train_loss=0.7994 acc=0.6610 | val_loss=0.7905 acc=0.6562 | prec=0.6981 rec=0.6319 f1=0.6215 | time=14.5s\n",
            "Epoch 053 | train_loss=0.7837 acc=0.6766 | val_loss=0.7808 acc=0.6680 | prec=0.6742 rec=0.6530 f1=0.6537 | time=14.6s\n",
            "Epoch 054 | train_loss=0.7621 acc=0.7049 | val_loss=0.7820 acc=0.6562 | prec=0.6826 rec=0.6346 f1=0.6289 | time=14.7s\n",
            "Epoch 055 | train_loss=0.7544 acc=0.6917 | val_loss=0.7744 acc=0.7148 | prec=0.7226 rec=0.7165 f1=0.7149 | time=14.7s\n",
            "Epoch 056 | train_loss=0.7503 acc=0.6996 | val_loss=0.7494 acc=0.6758 | prec=0.6812 rec=0.6649 f1=0.6676 | time=14.8s\n",
            "Epoch 057 | train_loss=0.7379 acc=0.7103 | val_loss=0.7669 acc=0.6836 | prec=0.6943 rec=0.6745 f1=0.6778 | time=14.7s\n",
            "Epoch 058 | train_loss=0.7350 acc=0.6937 | val_loss=0.7529 acc=0.7207 | prec=0.7212 rec=0.7183 f1=0.7186 | time=14.5s\n",
            "Epoch 059 | train_loss=0.7203 acc=0.7123 | val_loss=0.7500 acc=0.6973 | prec=0.7064 rec=0.6830 f1=0.6847 | time=14.7s\n",
            "Epoch 060 | train_loss=0.7115 acc=0.7074 | val_loss=0.7242 acc=0.6953 | prec=0.6990 rec=0.6858 f1=0.6884 | time=14.6s\n",
            "Epoch 061 | train_loss=0.7042 acc=0.7318 | val_loss=0.7385 acc=0.6855 | prec=0.7034 rec=0.6680 f1=0.6678 | time=14.7s\n",
            "Epoch 062 | train_loss=0.6925 acc=0.7225 | val_loss=0.7257 acc=0.7012 | prec=0.7108 rec=0.6918 f1=0.6950 | time=14.8s\n",
            "Epoch 063 | train_loss=0.6839 acc=0.7333 | val_loss=0.7152 acc=0.6934 | prec=0.7027 rec=0.6813 f1=0.6842 | time=14.9s\n",
            "Epoch 064 | train_loss=0.6864 acc=0.7308 | val_loss=0.7293 acc=0.6797 | prec=0.7008 rec=0.6620 f1=0.6622 | time=14.6s\n",
            "Epoch 065 | train_loss=0.6852 acc=0.7279 | val_loss=0.7101 acc=0.7012 | prec=0.7042 rec=0.6922 f1=0.6946 | time=14.6s\n",
            "Epoch 066 | train_loss=0.6619 acc=0.7416 | val_loss=0.7225 acc=0.6855 | prec=0.7056 rec=0.6661 f1=0.6642 | time=14.7s\n",
            "Epoch 067 | train_loss=0.6780 acc=0.7338 | val_loss=0.7425 acc=0.6719 | prec=0.7044 rec=0.6543 f1=0.6547 | time=14.7s\n",
            "Epoch 068 | train_loss=0.6716 acc=0.7382 | val_loss=0.7066 acc=0.6992 | prec=0.7060 rec=0.6899 f1=0.6926 | time=14.6s\n",
            "Epoch 069 | train_loss=0.6507 acc=0.7513 | val_loss=0.7084 acc=0.6895 | prec=0.7094 rec=0.6716 f1=0.6716 | time=14.7s\n",
            "Epoch 070 | train_loss=0.6594 acc=0.7313 | val_loss=0.6896 acc=0.7188 | prec=0.7192 rec=0.7119 f1=0.7139 | time=14.9s\n",
            "Epoch 071 | train_loss=0.6514 acc=0.7440 | val_loss=0.7342 acc=0.6934 | prec=0.7174 rec=0.6817 f1=0.6844 | time=14.7s\n",
            "Epoch 072 | train_loss=0.6428 acc=0.7557 | val_loss=0.7100 acc=0.7090 | prec=0.7212 rec=0.7078 f1=0.7082 | time=14.7s\n",
            "Epoch 073 | train_loss=0.6265 acc=0.7606 | val_loss=0.6920 acc=0.7051 | prec=0.7226 rec=0.6922 f1=0.6957 | time=14.7s\n",
            "Epoch 074 | train_loss=0.6267 acc=0.7611 | val_loss=0.7183 acc=0.6895 | prec=0.7130 rec=0.6771 f1=0.6802 | time=14.7s\n",
            "Epoch 075 | train_loss=0.6390 acc=0.7553 | val_loss=0.6949 acc=0.7031 | prec=0.7142 rec=0.6954 f1=0.6983 | time=14.8s\n",
            "Epoch 076 | train_loss=0.6314 acc=0.7596 | val_loss=0.6876 acc=0.7129 | prec=0.7199 rec=0.7045 f1=0.7072 | time=14.8s\n",
            "Epoch 077 | train_loss=0.6065 acc=0.7709 | val_loss=0.7073 acc=0.7246 | prec=0.7386 rec=0.7238 f1=0.7238 | time=14.7s\n",
            "Epoch 078 | train_loss=0.6056 acc=0.7836 | val_loss=0.7167 acc=0.6777 | prec=0.7083 rec=0.6616 f1=0.6634 | time=14.5s\n",
            "Epoch 079 | train_loss=0.6141 acc=0.7699 | val_loss=0.6948 acc=0.7148 | prec=0.7239 rec=0.7119 f1=0.7134 | time=14.6s\n",
            "Epoch 080 | train_loss=0.5904 acc=0.7860 | val_loss=0.7007 acc=0.7148 | prec=0.7352 rec=0.7087 f1=0.7110 | time=14.7s\n",
            "Epoch 081 | train_loss=0.6083 acc=0.7616 | val_loss=0.6897 acc=0.7188 | prec=0.7285 rec=0.7132 f1=0.7154 | time=14.6s\n",
            "Epoch 082 | train_loss=0.5968 acc=0.7650 | val_loss=0.7067 acc=0.7090 | prec=0.7316 rec=0.6977 f1=0.7011 | time=14.7s\n",
            "Epoch 083 | train_loss=0.5907 acc=0.7792 | val_loss=0.6898 acc=0.7070 | prec=0.7273 rec=0.6940 f1=0.6973 | time=14.8s\n",
            "Epoch 084 | train_loss=0.5818 acc=0.7865 | val_loss=0.6905 acc=0.7168 | prec=0.7349 rec=0.7091 f1=0.7126 | time=14.6s\n",
            "Epoch 085 | train_loss=0.5913 acc=0.7704 | val_loss=0.6598 acc=0.7324 | prec=0.7370 rec=0.7288 f1=0.7303 | time=14.6s\n",
            "Epoch 086 | train_loss=0.5861 acc=0.7777 | val_loss=0.6605 acc=0.7344 | prec=0.7377 rec=0.7250 f1=0.7276 | time=14.7s\n",
            "Epoch 087 | train_loss=0.5718 acc=0.7816 | val_loss=0.6892 acc=0.6953 | prec=0.7182 rec=0.6835 f1=0.6874 | time=14.5s\n",
            "Epoch 088 | train_loss=0.5880 acc=0.7655 | val_loss=0.7134 acc=0.6836 | prec=0.7156 rec=0.6689 f1=0.6715 | time=14.6s\n",
            "Epoch 089 | train_loss=0.5842 acc=0.7855 | val_loss=0.7090 acc=0.6816 | prec=0.7208 rec=0.6634 f1=0.6649 | time=14.7s\n",
            "Epoch 090 | train_loss=0.5641 acc=0.7816 | val_loss=0.6681 acc=0.7129 | prec=0.7272 rec=0.7018 f1=0.7054 | time=14.8s\n",
            "Epoch 091 | train_loss=0.5522 acc=0.8021 | val_loss=0.6821 acc=0.7031 | prec=0.7242 rec=0.6922 f1=0.6966 | time=14.6s\n",
            "Epoch 092 | train_loss=0.5771 acc=0.7836 | val_loss=0.6542 acc=0.7305 | prec=0.7399 rec=0.7242 f1=0.7274 | time=14.6s\n",
            "Epoch 093 | train_loss=0.5609 acc=0.8017 | val_loss=0.6665 acc=0.7285 | prec=0.7400 rec=0.7247 f1=0.7268 | time=14.6s\n",
            "Epoch 094 | train_loss=0.5554 acc=0.8012 | val_loss=0.6773 acc=0.7188 | prec=0.7315 rec=0.7142 f1=0.7168 | time=14.6s\n",
            "Epoch 095 | train_loss=0.5582 acc=0.7904 | val_loss=0.7042 acc=0.6895 | prec=0.7153 rec=0.6753 f1=0.6782 | time=14.6s\n",
            "Epoch 096 | train_loss=0.5515 acc=0.8036 | val_loss=0.6767 acc=0.7207 | prec=0.7337 rec=0.7197 f1=0.7203 | time=14.5s\n",
            "Epoch 097 | train_loss=0.5590 acc=0.7929 | val_loss=0.6791 acc=0.7070 | prec=0.7233 rec=0.6982 f1=0.7018 | time=14.8s\n",
            "Epoch 098 | train_loss=0.5445 acc=0.8109 | val_loss=0.6842 acc=0.7012 | prec=0.7247 rec=0.6876 f1=0.6913 | time=14.6s\n",
            "Epoch 099 | train_loss=0.5489 acc=0.7919 | val_loss=0.6715 acc=0.7070 | prec=0.7216 rec=0.6968 f1=0.7005 | time=14.6s\n",
            "Epoch 100 | train_loss=0.5420 acc=0.7968 | val_loss=0.6772 acc=0.7031 | prec=0.7214 rec=0.6927 f1=0.6963 | time=14.5s\n",
            "Epoch 101 | train_loss=0.5290 acc=0.8080 | val_loss=0.6666 acc=0.7168 | prec=0.7302 rec=0.7100 f1=0.7134 | time=14.5s\n",
            "Epoch 102 | train_loss=0.5332 acc=0.8149 | val_loss=0.6614 acc=0.7148 | prec=0.7270 rec=0.7087 f1=0.7121 | time=14.5s\n",
            "Epoch 103 | train_loss=0.5404 acc=0.8056 | val_loss=0.6641 acc=0.7129 | prec=0.7276 rec=0.7045 f1=0.7086 | time=14.6s\n",
            "Epoch 104 | train_loss=0.5443 acc=0.7929 | val_loss=0.6659 acc=0.7207 | prec=0.7361 rec=0.7118 f1=0.7162 | time=14.8s\n",
            "Epoch 105 | train_loss=0.5246 acc=0.7968 | val_loss=0.6647 acc=0.7148 | prec=0.7284 rec=0.7059 f1=0.7099 | time=14.8s\n",
            "Epoch 106 | train_loss=0.5314 acc=0.7987 | val_loss=0.6624 acc=0.7129 | prec=0.7244 rec=0.7041 f1=0.7075 | time=14.6s\n",
            "Epoch 107 | train_loss=0.5087 acc=0.8241 | val_loss=0.6923 acc=0.7012 | prec=0.7255 rec=0.6867 f1=0.6896 | time=14.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▄▅▅▅▆▆▆▆▇▇▇▇▇█▇▇█▇▇▇█▇███▇███▇█</td></tr><tr><td>precision</td><td>▁▁▁▁▅▁▅▅▅▄▆▇▇▇▇▇▇▇█▇████████████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▄▄▅▅▅▆▆▇▇▇█▇▇▇█▇█▇██▇███▇▇█▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▁▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>███▇█▇▇▇▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▃▃▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇███▇▇▇█▇▇█▇█▇</td></tr><tr><td>val_loss</td><td>████████▇▆▆▆▅▅▅▄▄▄▄▃▄▃▃▂▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>107</td></tr><tr><td>f1_score</td><td>0.68962</td></tr><tr><td>precision</td><td>0.72554</td></tr><tr><td>recall</td><td>0.68669</td></tr><tr><td>train_acc</td><td>0.82413</td></tr><tr><td>train_loss</td><td>0.50866</td></tr><tr><td>val_acc</td><td>0.70117</td></tr><tr><td>val_loss</td><td>0.69234</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/2e3tknzu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/2e3tknzu</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_104111-2e3tknzu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 11:07:23,128] Trial 4 finished with values: [0.6542192865163088, 0.73046875] and parameters: {'lr': 2.0762925364621747e-05, 'wd': 6.908036690376855e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 5 hyperparams --> lr=2.93e-04, wd=2.20e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_110723-hx0ue3ss</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/hx0ue3ss' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/hx0ue3ss' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/hx0ue3ss</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 5] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1461 acc=0.3219 | val_loss=1.1046 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 002 | train_loss=1.1325 acc=0.3307 | val_loss=1.1038 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 003 | train_loss=1.1312 acc=0.3293 | val_loss=1.1004 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.9s\n",
            "Epoch 004 | train_loss=1.1264 acc=0.3376 | val_loss=1.1029 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 005 | train_loss=1.1216 acc=0.3512 | val_loss=1.0940 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 006 | train_loss=1.1266 acc=0.3293 | val_loss=1.1033 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 007 | train_loss=1.1133 acc=0.3522 | val_loss=1.1004 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 008 | train_loss=1.0909 acc=0.3947 | val_loss=1.0715 acc=0.3809 | prec=0.5674 rec=0.3598 f1=0.2295 | time=14.6s\n",
            "Epoch 009 | train_loss=1.0659 acc=0.4309 | val_loss=1.0181 acc=0.5723 | prec=0.7090 rec=0.5388 f1=0.4806 | time=14.7s\n",
            "Epoch 010 | train_loss=1.0356 acc=0.4880 | val_loss=0.9998 acc=0.5352 | prec=0.5174 rec=0.5166 f1=0.4999 | time=14.9s\n",
            "Epoch 011 | train_loss=0.9841 acc=0.5261 | val_loss=0.9477 acc=0.5840 | prec=0.5606 rec=0.5585 f1=0.5347 | time=14.7s\n",
            "Epoch 012 | train_loss=0.9501 acc=0.5437 | val_loss=0.9206 acc=0.6172 | prec=0.7060 rec=0.5862 f1=0.5506 | time=14.7s\n",
            "Epoch 013 | train_loss=0.9083 acc=0.5892 | val_loss=0.8715 acc=0.6289 | prec=0.6694 rec=0.6018 f1=0.5809 | time=14.6s\n",
            "Epoch 014 | train_loss=0.8651 acc=0.6160 | val_loss=0.8403 acc=0.6543 | prec=0.6481 rec=0.6462 f1=0.6426 | time=14.7s\n",
            "Epoch 015 | train_loss=0.8221 acc=0.6444 | val_loss=0.8314 acc=0.6230 | prec=0.6396 rec=0.6041 f1=0.5939 | time=14.6s\n",
            "Epoch 016 | train_loss=0.7777 acc=0.6663 | val_loss=0.7691 acc=0.6562 | prec=0.6544 rec=0.6388 f1=0.6353 | time=14.6s\n",
            "Epoch 017 | train_loss=0.7373 acc=0.6830 | val_loss=0.9658 acc=0.5215 | prec=0.6899 rec=0.4993 f1=0.4664 | time=14.9s\n",
            "Epoch 018 | train_loss=0.7575 acc=0.6737 | val_loss=0.7847 acc=0.6777 | prec=0.7040 rec=0.6865 f1=0.6648 | time=14.8s\n",
            "Epoch 019 | train_loss=0.6991 acc=0.6976 | val_loss=0.7787 acc=0.6406 | prec=0.7002 rec=0.6145 f1=0.5933 | time=14.5s\n",
            "Epoch 020 | train_loss=0.6355 acc=0.7421 | val_loss=0.7287 acc=0.6973 | prec=0.7089 rec=0.6877 f1=0.6854 | time=14.6s\n",
            "Epoch 021 | train_loss=0.6527 acc=0.7137 | val_loss=0.7383 acc=0.6836 | prec=0.7201 rec=0.6648 f1=0.6648 | time=14.6s\n",
            "Epoch 022 | train_loss=0.5928 acc=0.7504 | val_loss=0.6777 acc=0.7148 | prec=0.7174 rec=0.6994 f1=0.6989 | time=14.6s\n",
            "Epoch 023 | train_loss=0.5912 acc=0.7479 | val_loss=0.7003 acc=0.7031 | prec=0.7064 rec=0.7033 f1=0.6952 | time=14.6s\n",
            "Epoch 024 | train_loss=0.5270 acc=0.7787 | val_loss=0.7017 acc=0.7012 | prec=0.7198 rec=0.7028 f1=0.6894 | time=14.8s\n",
            "Epoch 025 | train_loss=0.5209 acc=0.7929 | val_loss=0.7482 acc=0.6699 | prec=0.6964 rec=0.6474 f1=0.6357 | time=14.9s\n",
            "Epoch 026 | train_loss=0.4929 acc=0.8061 | val_loss=0.6920 acc=0.7246 | prec=0.7345 rec=0.7302 f1=0.7191 | time=14.6s\n",
            "Epoch 027 | train_loss=0.5278 acc=0.7953 | val_loss=0.8441 acc=0.6738 | prec=0.7124 rec=0.6875 f1=0.6746 | time=14.6s\n",
            "Epoch 028 | train_loss=0.4613 acc=0.8251 | val_loss=0.9313 acc=0.6074 | prec=0.6860 rec=0.6214 f1=0.6117 | time=14.5s\n",
            "Epoch 029 | train_loss=0.4340 acc=0.8324 | val_loss=0.7537 acc=0.7207 | prec=0.7283 rec=0.7261 f1=0.7186 | time=14.6s\n",
            "Epoch 030 | train_loss=0.4133 acc=0.8437 | val_loss=0.7926 acc=0.6914 | prec=0.7022 rec=0.6960 f1=0.6906 | time=14.6s\n",
            "Epoch 031 | train_loss=0.3917 acc=0.8417 | val_loss=1.1305 acc=0.5859 | prec=0.6940 rec=0.6083 f1=0.5862 | time=14.8s\n",
            "Epoch 032 | train_loss=0.3751 acc=0.8515 | val_loss=0.8027 acc=0.6934 | prec=0.6953 rec=0.6859 f1=0.6886 | time=14.8s\n",
            "Epoch 033 | train_loss=0.3662 acc=0.8495 | val_loss=0.8118 acc=0.7090 | prec=0.7141 rec=0.7106 f1=0.7024 | time=14.4s\n",
            "Epoch 034 | train_loss=0.3350 acc=0.8740 | val_loss=0.8768 acc=0.6641 | prec=0.6856 rec=0.6562 f1=0.6587 | time=14.6s\n",
            "Epoch 035 | train_loss=0.3179 acc=0.8779 | val_loss=1.0306 acc=0.6543 | prec=0.6862 rec=0.6642 f1=0.6565 | time=14.6s\n",
            "Epoch 036 | train_loss=0.2874 acc=0.8847 | val_loss=0.9105 acc=0.6699 | prec=0.6709 rec=0.6714 f1=0.6649 | time=14.7s\n",
            "Epoch 037 | train_loss=0.3027 acc=0.8749 | val_loss=0.9532 acc=0.6680 | prec=0.6644 rec=0.6571 f1=0.6517 | time=14.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▂▅▅▆▆▆▇▆▇▅▇▆█▇███▇█▇▇██▆██▇▇▇▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▆█▆▆█▇▇▇▇▇██████████▇█████▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▅▄▅▅▆▇▆▆▄▇▆▇▇▇██▇█▇▆█▇▆▇█▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▂▂▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>████████▇▇▇▆▆▆▅▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▅▄▅▆▆▇▆▇▄▇▆▇▇███▇█▇▆█▇▅▇█▇▇▇▇</td></tr><tr><td>val_loss</td><td>████▇██▇▆▆▅▅▄▄▃▂▅▃▃▂▂▁▁▁▂▁▄▅▂▃█▃▃▄▆▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>f1_score</td><td>0.6517</td></tr><tr><td>precision</td><td>0.66443</td></tr><tr><td>recall</td><td>0.65711</td></tr><tr><td>train_acc</td><td>0.87494</td></tr><tr><td>train_loss</td><td>0.30265</td></tr><tr><td>val_acc</td><td>0.66797</td></tr><tr><td>val_loss</td><td>0.95324</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/hx0ue3ss' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/hx0ue3ss</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_110723-hx0ue3ss/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 11:16:28,177] Trial 5 finished with values: [0.6776970252394676, 0.71484375] and parameters: {'lr': 0.00029304564342522984, 'wd': 0.00021959051320832694}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 6 hyperparams --> lr=3.23e-03, wd=3.78e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_111628-64kk0han</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/64kk0han' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/64kk0han' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/64kk0han</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 6] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1493 acc=0.3498 | val_loss=1.0930 acc=0.3809 | prec=0.3263 rec=0.3552 f1=0.2249 | time=14.6s\n",
            "Epoch 002 | train_loss=1.1155 acc=0.3757 | val_loss=1.0729 acc=0.4355 | prec=0.4836 rec=0.4279 f1=0.3852 | time=14.6s\n",
            "Epoch 003 | train_loss=1.0414 acc=0.4797 | val_loss=0.9739 acc=0.5527 | prec=0.3871 rec=0.5155 f1=0.4338 | time=14.6s\n",
            "Epoch 004 | train_loss=0.9626 acc=0.5349 | val_loss=0.9241 acc=0.6113 | prec=0.6506 rec=0.5803 f1=0.5420 | time=14.6s\n",
            "Epoch 005 | train_loss=0.9338 acc=0.5642 | val_loss=0.9616 acc=0.5605 | prec=0.6226 rec=0.5329 f1=0.4942 | time=14.6s\n",
            "Epoch 006 | train_loss=0.8589 acc=0.6150 | val_loss=0.8526 acc=0.6152 | prec=0.6589 rec=0.5881 f1=0.5666 | time=14.7s\n",
            "Epoch 007 | train_loss=0.8291 acc=0.6321 | val_loss=0.7699 acc=0.6602 | prec=0.6543 rec=0.6364 f1=0.6224 | time=14.9s\n",
            "Epoch 008 | train_loss=0.8094 acc=0.6439 | val_loss=0.7543 acc=0.6855 | prec=0.6753 rec=0.6744 f1=0.6742 | time=14.6s\n",
            "Epoch 009 | train_loss=0.7486 acc=0.6639 | val_loss=0.8747 acc=0.6035 | prec=0.6414 rec=0.6062 f1=0.6067 | time=14.6s\n",
            "Epoch 010 | train_loss=0.7456 acc=0.6781 | val_loss=0.9098 acc=0.5820 | prec=0.6942 rec=0.6010 f1=0.5868 | time=14.5s\n",
            "Epoch 011 | train_loss=0.7489 acc=0.6610 | val_loss=1.1843 acc=0.5215 | prec=0.6536 rec=0.5131 f1=0.4750 | time=14.6s\n",
            "Epoch 012 | train_loss=0.7107 acc=0.6800 | val_loss=1.1501 acc=0.5469 | prec=0.6416 rec=0.5336 f1=0.5056 | time=14.7s\n",
            "Epoch 013 | train_loss=0.6810 acc=0.6859 | val_loss=1.2201 acc=0.4551 | prec=0.6941 rec=0.4373 f1=0.3711 | time=14.7s\n",
            "Epoch 014 | train_loss=0.7301 acc=0.6717 | val_loss=0.7705 acc=0.6348 | prec=0.7127 rec=0.6488 f1=0.6382 | time=14.8s\n",
            "Epoch 015 | train_loss=0.6285 acc=0.7157 | val_loss=0.8347 acc=0.6562 | prec=0.6547 rec=0.6402 f1=0.6342 | time=14.7s\n",
            "Epoch 016 | train_loss=0.6041 acc=0.7377 | val_loss=0.8625 acc=0.6367 | prec=0.7091 rec=0.6483 f1=0.6314 | time=14.6s\n",
            "Epoch 017 | train_loss=0.6317 acc=0.7118 | val_loss=0.8011 acc=0.6367 | prec=0.6546 rec=0.6377 f1=0.6379 | time=14.5s\n",
            "Epoch 018 | train_loss=0.5984 acc=0.7372 | val_loss=0.9658 acc=0.5977 | prec=0.6894 rec=0.6123 f1=0.5744 | time=14.7s\n",
            "Epoch 019 | train_loss=0.5231 acc=0.7792 | val_loss=0.7809 acc=0.6270 | prec=0.6491 rec=0.6281 f1=0.6157 | time=14.6s\n",
            "Epoch 020 | train_loss=0.5586 acc=0.7655 | val_loss=1.9341 acc=0.4980 | prec=0.6152 rec=0.4788 f1=0.4356 | time=14.8s\n",
            "Epoch 021 | train_loss=0.5573 acc=0.7655 | val_loss=2.1882 acc=0.4746 | prec=0.6044 rec=0.4814 f1=0.3990 | time=14.9s\n",
            "Epoch 022 | train_loss=0.5528 acc=0.7513 | val_loss=1.4692 acc=0.5762 | prec=0.6278 rec=0.5503 f1=0.5279 | time=14.8s\n",
            "Epoch 023 | train_loss=0.5454 acc=0.7719 | val_loss=0.9195 acc=0.6543 | prec=0.6433 rec=0.6384 f1=0.6336 | time=14.7s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▃▄▆▅▆▇█▇▇▅▅▃▇▇▇▇▆▇▄▄▆▇</td></tr><tr><td>precision</td><td>▁▄▂▇▆▇▇▇▇█▇▇██▇█▇█▇▆▆▆▇</td></tr><tr><td>recall</td><td>▁▃▅▆▅▆▇█▇▆▄▅▃▇▇▇▇▇▇▄▄▅▇</td></tr><tr><td>train_acc</td><td>▁▁▃▄▄▅▆▆▆▆▆▆▆▆▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>██▇▆▆▅▄▄▄▃▄▃▃▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▂▅▆▅▆▇█▆▆▄▅▃▇▇▇▇▆▇▄▃▅▇</td></tr><tr><td>val_loss</td><td>▃▃▂▂▂▁▁▁▂▂▃▃▃▁▁▂▁▂▁▇█▄▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>f1_score</td><td>0.63362</td></tr><tr><td>precision</td><td>0.64325</td></tr><tr><td>recall</td><td>0.63836</td></tr><tr><td>train_acc</td><td>0.77186</td></tr><tr><td>train_loss</td><td>0.54536</td></tr><tr><td>val_acc</td><td>0.6543</td></tr><tr><td>val_loss</td><td>0.91955</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/64kk0han' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/64kk0han</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_111628-64kk0han/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 11:22:08,031] Trial 6 finished with values: [0.754261925816536, 0.685546875] and parameters: {'lr': 0.0032283489940804085, 'wd': 3.7753078025933787e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 7 hyperparams --> lr=9.51e-05, wd=2.16e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_112208-fjhuto4l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fjhuto4l' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fjhuto4l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fjhuto4l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 7] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1777 acc=0.3596 | val_loss=1.0947 acc=0.3477 | prec=0.2230 rec=0.3242 f1=0.1948 | time=14.7s\n",
            "Epoch 002 | train_loss=1.1459 acc=0.3317 | val_loss=1.0948 acc=0.3633 | prec=0.2865 rec=0.3388 f1=0.1989 | time=14.6s\n",
            "Epoch 003 | train_loss=1.1423 acc=0.3341 | val_loss=1.0947 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 004 | train_loss=1.1434 acc=0.3346 | val_loss=1.0938 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 005 | train_loss=1.1375 acc=0.3327 | val_loss=1.0938 acc=0.3379 | prec=0.2200 rec=0.3151 f1=0.2411 | time=14.7s\n",
            "Epoch 006 | train_loss=1.1387 acc=0.3293 | val_loss=1.0958 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 007 | train_loss=1.1143 acc=0.3630 | val_loss=1.0950 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 008 | train_loss=1.1265 acc=0.3439 | val_loss=1.0952 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 009 | train_loss=1.1386 acc=0.3327 | val_loss=1.0932 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 010 | train_loss=1.1208 acc=0.3566 | val_loss=1.0928 acc=0.3613 | prec=0.4529 rec=0.3370 f1=0.1833 | time=14.6s\n",
            "Epoch 011 | train_loss=1.1267 acc=0.3468 | val_loss=1.0929 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.9s\n",
            "Epoch 012 | train_loss=1.1239 acc=0.3434 | val_loss=1.0901 acc=0.3691 | prec=0.3011 rec=0.3443 f1=0.2017 | time=14.8s\n",
            "Epoch 013 | train_loss=1.1069 acc=0.3801 | val_loss=1.0763 acc=0.4766 | prec=0.3332 rec=0.4444 f1=0.3712 | time=14.6s\n",
            "Epoch 014 | train_loss=1.1041 acc=0.3967 | val_loss=1.0731 acc=0.4238 | prec=0.3513 rec=0.3953 f1=0.2940 | time=14.6s\n",
            "Epoch 015 | train_loss=1.0789 acc=0.4182 | val_loss=1.0521 acc=0.5215 | prec=0.5133 rec=0.4882 f1=0.4192 | time=14.6s\n",
            "Epoch 016 | train_loss=1.0421 acc=0.4592 | val_loss=1.0156 acc=0.5566 | prec=0.7028 rec=0.5200 f1=0.4394 | time=14.5s\n",
            "Epoch 017 | train_loss=0.9943 acc=0.5125 | val_loss=0.9909 acc=0.5488 | prec=0.6669 rec=0.5169 f1=0.4605 | time=14.6s\n",
            "Epoch 018 | train_loss=0.9611 acc=0.5432 | val_loss=0.9452 acc=0.5820 | prec=0.6532 rec=0.5511 f1=0.5065 | time=14.9s\n",
            "Epoch 019 | train_loss=0.9270 acc=0.5559 | val_loss=0.9474 acc=0.5957 | prec=0.6112 rec=0.5971 f1=0.5934 | time=14.7s\n",
            "Epoch 020 | train_loss=0.9093 acc=0.5813 | val_loss=0.8699 acc=0.6387 | prec=0.6298 rec=0.6242 f1=0.6224 | time=14.5s\n",
            "Epoch 021 | train_loss=0.8715 acc=0.5979 | val_loss=0.8680 acc=0.6211 | prec=0.6356 rec=0.6217 f1=0.6114 | time=14.8s\n",
            "Epoch 022 | train_loss=0.8352 acc=0.6238 | val_loss=0.8325 acc=0.6484 | prec=0.6427 rec=0.6421 f1=0.6422 | time=14.6s\n",
            "Epoch 023 | train_loss=0.7988 acc=0.6429 | val_loss=0.8292 acc=0.6348 | prec=0.6383 rec=0.6340 f1=0.6275 | time=14.5s\n",
            "Epoch 024 | train_loss=0.7832 acc=0.6580 | val_loss=0.8120 acc=0.6543 | prec=0.6503 rec=0.6504 f1=0.6501 | time=14.7s\n",
            "Epoch 025 | train_loss=0.7571 acc=0.6761 | val_loss=0.7859 acc=0.6680 | prec=0.6871 rec=0.6756 f1=0.6673 | time=14.8s\n",
            "Epoch 026 | train_loss=0.7344 acc=0.6795 | val_loss=0.8684 acc=0.5879 | prec=0.7069 rec=0.5732 f1=0.5597 | time=14.7s\n",
            "Epoch 027 | train_loss=0.7093 acc=0.6878 | val_loss=0.7549 acc=0.7090 | prec=0.7251 rec=0.7138 f1=0.7100 | time=14.6s\n",
            "Epoch 028 | train_loss=0.6790 acc=0.7137 | val_loss=0.7191 acc=0.7109 | prec=0.7149 rec=0.7133 f1=0.7066 | time=14.6s\n",
            "Epoch 029 | train_loss=0.6763 acc=0.7240 | val_loss=0.7521 acc=0.6816 | prec=0.7247 rec=0.6934 f1=0.6777 | time=14.6s\n",
            "Epoch 030 | train_loss=0.6534 acc=0.7338 | val_loss=0.7450 acc=0.7188 | prec=0.7440 rec=0.7275 f1=0.7207 | time=14.7s\n",
            "Epoch 031 | train_loss=0.6246 acc=0.7504 | val_loss=0.7472 acc=0.6641 | prec=0.7184 rec=0.6784 f1=0.6528 | time=14.7s\n",
            "Epoch 032 | train_loss=0.5903 acc=0.7767 | val_loss=0.7033 acc=0.7090 | prec=0.7135 rec=0.7101 f1=0.7031 | time=14.8s\n",
            "Epoch 033 | train_loss=0.5778 acc=0.7670 | val_loss=0.7758 acc=0.6523 | prec=0.7240 rec=0.6707 f1=0.6491 | time=14.6s\n",
            "Epoch 034 | train_loss=0.5569 acc=0.7680 | val_loss=0.7660 acc=0.6855 | prec=0.7395 rec=0.7007 f1=0.6881 | time=14.7s\n",
            "Epoch 035 | train_loss=0.5705 acc=0.7626 | val_loss=0.7140 acc=0.6895 | prec=0.7148 rec=0.6965 f1=0.6920 | time=14.7s\n",
            "Epoch 036 | train_loss=0.5360 acc=0.7831 | val_loss=0.6852 acc=0.7266 | prec=0.7332 rec=0.7251 f1=0.7259 | time=14.6s\n",
            "Epoch 037 | train_loss=0.5180 acc=0.8012 | val_loss=0.6665 acc=0.7207 | prec=0.7183 rec=0.7063 f1=0.7049 | time=14.5s\n",
            "Epoch 038 | train_loss=0.4938 acc=0.8090 | val_loss=0.6677 acc=0.7422 | prec=0.7494 rec=0.7466 f1=0.7403 | time=14.8s\n",
            "Epoch 039 | train_loss=0.4919 acc=0.8075 | val_loss=0.6956 acc=0.7148 | prec=0.7305 rec=0.7151 f1=0.7158 | time=14.9s\n",
            "Epoch 040 | train_loss=0.4692 acc=0.8173 | val_loss=0.6466 acc=0.7266 | prec=0.7209 rec=0.7173 f1=0.7168 | time=14.7s\n",
            "Epoch 041 | train_loss=0.4653 acc=0.8134 | val_loss=0.7254 acc=0.6973 | prec=0.7324 rec=0.7075 f1=0.6997 | time=14.6s\n",
            "Epoch 042 | train_loss=0.4729 acc=0.8109 | val_loss=0.7268 acc=0.6836 | prec=0.7311 rec=0.6957 f1=0.6854 | time=14.5s\n",
            "Epoch 043 | train_loss=0.4548 acc=0.8207 | val_loss=0.6748 acc=0.7285 | prec=0.7314 rec=0.7270 f1=0.7270 | time=14.5s\n",
            "Epoch 044 | train_loss=0.4076 acc=0.8417 | val_loss=0.6967 acc=0.7148 | prec=0.7122 rec=0.7031 f1=0.7021 | time=14.8s\n",
            "Epoch 045 | train_loss=0.4474 acc=0.8246 | val_loss=0.7012 acc=0.7305 | prec=0.7419 rec=0.7371 f1=0.7272 | time=14.7s\n",
            "Epoch 046 | train_loss=0.4300 acc=0.8315 | val_loss=0.7113 acc=0.6992 | prec=0.7279 rec=0.7112 f1=0.6909 | time=14.9s\n",
            "Epoch 047 | train_loss=0.4112 acc=0.8373 | val_loss=0.6951 acc=0.7441 | prec=0.7418 rec=0.7365 f1=0.7381 | time=14.6s\n",
            "Epoch 048 | train_loss=0.4148 acc=0.8285 | val_loss=0.6670 acc=0.7383 | prec=0.7361 rec=0.7328 f1=0.7341 | time=14.5s\n",
            "Epoch 049 | train_loss=0.3777 acc=0.8632 | val_loss=0.6654 acc=0.7344 | prec=0.7289 rec=0.7292 f1=0.7290 | time=14.5s\n",
            "Epoch 050 | train_loss=0.3861 acc=0.8500 | val_loss=0.6811 acc=0.7441 | prec=0.7423 rec=0.7392 f1=0.7404 | time=14.4s\n",
            "Epoch 051 | train_loss=0.3800 acc=0.8451 | val_loss=0.6869 acc=0.7227 | prec=0.7201 rec=0.7220 f1=0.7186 | time=14.5s\n",
            "Epoch 052 | train_loss=0.3641 acc=0.8583 | val_loss=0.7679 acc=0.7207 | prec=0.7254 rec=0.7192 f1=0.7197 | time=14.7s\n",
            "Epoch 053 | train_loss=0.3460 acc=0.8652 | val_loss=0.7223 acc=0.7031 | prec=0.6978 rec=0.6996 f1=0.6936 | time=14.9s\n",
            "Epoch 054 | train_loss=0.3254 acc=0.8788 | val_loss=0.7118 acc=0.7188 | prec=0.7145 rec=0.7105 f1=0.7095 | time=14.7s\n",
            "Epoch 055 | train_loss=0.3346 acc=0.8759 | val_loss=0.7464 acc=0.7324 | prec=0.7267 rec=0.7288 f1=0.7272 | time=14.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▂▁▁▁▁▁▄▅▅▆▇▇▇▇▇▆▇█▇█▇▇███████▇████▇█</td></tr><tr><td>precision</td><td>▂▃▁▁▂▁▅▁▃▃▇▇▇▆▇▇▇▇▇█████████████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▃▄▄▄▅▆▆▆▆▇▅▇▇▇▇▇█▇█▇▇██▇█████▇█</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇█▇▇█▇█████</td></tr><tr><td>train_loss</td><td>█████▇████▇▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▄▅▅▅▅▆▆▆▆▇▇▇█▇▇▇▇███▇▇█▇██████</td></tr><tr><td>val_loss</td><td>██████████▇▆▆▆▄▄▃▃▄▂▂▂▂▂▃▂▁▁▁▁▂▁▂▂▂▁▁▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>55</td></tr><tr><td>f1_score</td><td>0.72722</td></tr><tr><td>precision</td><td>0.72668</td></tr><tr><td>recall</td><td>0.72876</td></tr><tr><td>train_acc</td><td>0.87592</td></tr><tr><td>train_loss</td><td>0.33462</td></tr><tr><td>val_acc</td><td>0.73242</td></tr><tr><td>val_loss</td><td>0.74644</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fjhuto4l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/fjhuto4l</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_112208-fjhuto4l/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 11:35:36,763] Trial 7 finished with values: [0.6466364953666925, 0.7265625] and parameters: {'lr': 9.51189829755495e-05, 'wd': 0.0002155981491337327}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 8 hyperparams --> lr=1.11e-04, wd=2.54e-03\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_113536-qh7tkpri</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/qh7tkpri' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/qh7tkpri' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/qh7tkpri</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 8] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1764 acc=0.3420 | val_loss=1.0984 acc=0.3672 | prec=0.2878 rec=0.3424 f1=0.2009 | time=14.7s\n",
            "Epoch 002 | train_loss=1.1434 acc=0.3625 | val_loss=1.0943 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 003 | train_loss=1.1390 acc=0.3732 | val_loss=1.0952 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 004 | train_loss=1.1432 acc=0.3512 | val_loss=1.0934 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 005 | train_loss=1.1372 acc=0.3571 | val_loss=1.0937 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 006 | train_loss=1.1406 acc=0.3488 | val_loss=1.0944 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 007 | train_loss=1.1273 acc=0.3552 | val_loss=1.0942 acc=0.3613 | prec=0.4529 rec=0.3370 f1=0.1833 | time=14.5s\n",
            "Epoch 008 | train_loss=1.1320 acc=0.3757 | val_loss=1.0940 acc=0.3535 | prec=0.2293 rec=0.3297 f1=0.1976 | time=14.6s\n",
            "Epoch 009 | train_loss=1.1404 acc=0.3395 | val_loss=1.0962 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 010 | train_loss=1.1396 acc=0.3395 | val_loss=1.0978 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 011 | train_loss=1.1230 acc=0.3527 | val_loss=1.0980 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 012 | train_loss=1.1272 acc=0.3537 | val_loss=1.0994 acc=0.3535 | prec=0.1190 rec=0.3297 f1=0.1749 | time=14.7s\n",
            "Epoch 013 | train_loss=1.1327 acc=0.3425 | val_loss=1.0941 acc=0.3516 | prec=0.2361 rec=0.3279 f1=0.2015 | time=14.6s\n",
            "Epoch 014 | train_loss=1.1192 acc=0.3635 | val_loss=1.0957 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 015 | train_loss=1.1154 acc=0.3693 | val_loss=1.0850 acc=0.3691 | prec=0.2808 rec=0.3443 f1=0.2122 | time=14.6s\n",
            "Epoch 016 | train_loss=1.0831 acc=0.4299 | val_loss=1.0399 acc=0.5059 | prec=0.3378 rec=0.4718 f1=0.3907 | time=14.6s\n",
            "Epoch 017 | train_loss=1.0429 acc=0.4656 | val_loss=1.0088 acc=0.5508 | prec=0.3669 rec=0.5137 f1=0.4278 | time=14.6s\n",
            "Epoch 018 | train_loss=1.0111 acc=0.5037 | val_loss=0.9802 acc=0.5605 | prec=0.6047 rec=0.5237 f1=0.4459 | time=14.9s\n",
            "Epoch 019 | train_loss=0.9673 acc=0.5242 | val_loss=0.9338 acc=0.5762 | prec=0.6820 rec=0.5420 f1=0.4806 | time=14.6s\n",
            "Epoch 020 | train_loss=0.9222 acc=0.5686 | val_loss=0.8990 acc=0.6016 | prec=0.6742 rec=0.5684 f1=0.5210 | time=14.6s\n",
            "Epoch 021 | train_loss=0.9002 acc=0.5833 | val_loss=0.9006 acc=0.5879 | prec=0.6820 rec=0.5557 f1=0.5058 | time=14.5s\n",
            "Epoch 022 | train_loss=0.8798 acc=0.6116 | val_loss=0.8552 acc=0.6270 | prec=0.6561 rec=0.6018 f1=0.5855 | time=14.6s\n",
            "Epoch 023 | train_loss=0.8606 acc=0.6229 | val_loss=0.8349 acc=0.6289 | prec=0.6719 rec=0.5999 f1=0.5729 | time=14.8s\n",
            "Epoch 024 | train_loss=0.8152 acc=0.6390 | val_loss=0.7752 acc=0.6523 | prec=0.6800 rec=0.6273 f1=0.6132 | time=14.7s\n",
            "Epoch 025 | train_loss=0.7656 acc=0.6795 | val_loss=0.7417 acc=0.6973 | prec=0.7115 rec=0.6780 f1=0.6753 | time=14.9s\n",
            "Epoch 026 | train_loss=0.7360 acc=0.6898 | val_loss=0.7405 acc=0.6836 | prec=0.6820 rec=0.6666 f1=0.6635 | time=14.6s\n",
            "Epoch 027 | train_loss=0.7323 acc=0.6991 | val_loss=0.7587 acc=0.6738 | prec=0.6928 rec=0.6626 f1=0.6622 | time=14.6s\n",
            "Epoch 028 | train_loss=0.7208 acc=0.7020 | val_loss=0.7349 acc=0.7031 | prec=0.7106 rec=0.7010 f1=0.7002 | time=14.6s\n",
            "Epoch 029 | train_loss=0.7022 acc=0.6981 | val_loss=0.7174 acc=0.7207 | prec=0.7303 rec=0.7095 f1=0.7128 | time=14.5s\n",
            "Epoch 030 | train_loss=0.6588 acc=0.7235 | val_loss=0.6905 acc=0.6914 | prec=0.7067 rec=0.6702 f1=0.6630 | time=14.6s\n",
            "Epoch 031 | train_loss=0.6315 acc=0.7362 | val_loss=0.6800 acc=0.7363 | prec=0.7401 rec=0.7361 f1=0.7353 | time=14.7s\n",
            "Epoch 032 | train_loss=0.5993 acc=0.7611 | val_loss=0.7754 acc=0.6797 | prec=0.7250 rec=0.6685 f1=0.6692 | time=14.9s\n",
            "Epoch 033 | train_loss=0.5827 acc=0.7606 | val_loss=0.6671 acc=0.7188 | prec=0.7165 rec=0.7109 f1=0.7106 | time=14.7s\n",
            "Epoch 034 | train_loss=0.5809 acc=0.7782 | val_loss=0.6437 acc=0.7344 | prec=0.7313 rec=0.7334 f1=0.7301 | time=14.6s\n",
            "Epoch 035 | train_loss=0.5446 acc=0.7914 | val_loss=0.6643 acc=0.7363 | prec=0.7525 rec=0.7426 f1=0.7374 | time=14.6s\n",
            "Epoch 036 | train_loss=0.5477 acc=0.7855 | val_loss=0.6774 acc=0.7168 | prec=0.7433 rec=0.7022 f1=0.7049 | time=14.7s\n",
            "Epoch 037 | train_loss=0.5073 acc=0.8085 | val_loss=0.6277 acc=0.7539 | prec=0.7570 rec=0.7465 f1=0.7492 | time=14.7s\n",
            "Epoch 038 | train_loss=0.4876 acc=0.8100 | val_loss=0.6650 acc=0.7285 | prec=0.7329 rec=0.7330 f1=0.7277 | time=14.6s\n",
            "Epoch 039 | train_loss=0.4855 acc=0.8090 | val_loss=0.6286 acc=0.7324 | prec=0.7359 rec=0.7325 f1=0.7315 | time=14.8s\n",
            "Epoch 040 | train_loss=0.4569 acc=0.8256 | val_loss=0.6898 acc=0.7129 | prec=0.7499 rec=0.7249 f1=0.7148 | time=14.7s\n",
            "Epoch 041 | train_loss=0.4485 acc=0.8290 | val_loss=0.6733 acc=0.7305 | prec=0.7296 rec=0.7274 f1=0.7247 | time=14.7s\n",
            "Epoch 042 | train_loss=0.4409 acc=0.8300 | val_loss=0.6700 acc=0.7285 | prec=0.7330 rec=0.7205 f1=0.7233 | time=14.6s\n",
            "Epoch 043 | train_loss=0.4475 acc=0.8222 | val_loss=0.7630 acc=0.6621 | prec=0.7255 rec=0.6803 f1=0.6564 | time=14.6s\n",
            "Epoch 044 | train_loss=0.4253 acc=0.8403 | val_loss=0.7124 acc=0.7090 | prec=0.7142 rec=0.6940 f1=0.6932 | time=14.6s\n",
            "Epoch 045 | train_loss=0.4240 acc=0.8349 | val_loss=0.6754 acc=0.7324 | prec=0.7339 rec=0.7237 f1=0.7255 | time=14.7s\n",
            "Epoch 046 | train_loss=0.3808 acc=0.8588 | val_loss=0.7337 acc=0.7148 | prec=0.7256 rec=0.7114 f1=0.7137 | time=14.9s\n",
            "Epoch 047 | train_loss=0.4051 acc=0.8451 | val_loss=0.7121 acc=0.7031 | prec=0.7358 rec=0.7153 f1=0.6996 | time=14.6s\n",
            "Epoch 048 | train_loss=0.3937 acc=0.8461 | val_loss=0.7140 acc=0.7246 | prec=0.7218 rec=0.7215 f1=0.7201 | time=14.6s\n",
            "Epoch 049 | train_loss=0.3696 acc=0.8481 | val_loss=0.7661 acc=0.7148 | prec=0.7357 rec=0.6994 f1=0.7007 | time=14.6s\n",
            "Epoch 050 | train_loss=0.3362 acc=0.8740 | val_loss=0.7371 acc=0.7148 | prec=0.7221 rec=0.7225 f1=0.7118 | time=14.6s\n",
            "Epoch 051 | train_loss=0.3684 acc=0.8617 | val_loss=0.8138 acc=0.6758 | prec=0.6921 rec=0.6667 f1=0.6654 | time=14.6s\n",
            "Epoch 052 | train_loss=0.3305 acc=0.8759 | val_loss=0.7125 acc=0.7344 | prec=0.7345 rec=0.7301 f1=0.7314 | time=14.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▅▆▆▆▇▇▇█▇███▇█████▇█▇█▇██</td></tr><tr><td>precision</td><td>▃▁▁▁▁▅▂▁▁▁▂▃▃▄▆▇▇▇▇▇▇▇█▇████████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▄▅▅▆▆▆▇▇▇▇█▇██▇████▇▇▇▇█▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>train_loss</td><td>████████████▇▇▇▆▆▆▆▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▄▅▅▆▅▆▆▆▇▇▇█▇▇████████▇█▇███</td></tr><tr><td>val_loss</td><td>███████████▇▇▆▆▅▄▄▃▃▃▂▂▂▃▁▂▂▁▂▂▂▃▂▂▂▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>52</td></tr><tr><td>f1_score</td><td>0.73141</td></tr><tr><td>precision</td><td>0.73453</td></tr><tr><td>recall</td><td>0.73012</td></tr><tr><td>train_acc</td><td>0.87592</td></tr><tr><td>train_loss</td><td>0.33046</td></tr><tr><td>val_acc</td><td>0.73438</td></tr><tr><td>val_loss</td><td>0.71249</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/qh7tkpri' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/qh7tkpri</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_113536-qh7tkpri/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 11:48:22,077] Trial 8 finished with values: [0.6277389880269766, 0.75390625] and parameters: {'lr': 0.00011111361926742459, 'wd': 0.002544246881680688}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 9 hyperparams --> lr=3.72e-05, wd=1.96e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_114822-v0yexkbx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/v0yexkbx' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/v0yexkbx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/v0yexkbx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 9] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1702 acc=0.3806 | val_loss=1.1143 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 002 | train_loss=1.1751 acc=0.3478 | val_loss=1.1044 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 003 | train_loss=1.1442 acc=0.3537 | val_loss=1.1021 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 004 | train_loss=1.1389 acc=0.3498 | val_loss=1.1005 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.5s\n",
            "Epoch 005 | train_loss=1.1258 acc=0.3488 | val_loss=1.0949 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 006 | train_loss=1.1381 acc=0.3381 | val_loss=1.0952 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 007 | train_loss=1.1158 acc=0.3688 | val_loss=1.0944 acc=0.3555 | prec=0.1192 rec=0.3315 f1=0.1753 | time=14.9s\n",
            "Epoch 008 | train_loss=1.1306 acc=0.3395 | val_loss=1.0958 acc=0.3340 | prec=0.1989 rec=0.3115 f1=0.2089 | time=14.6s\n",
            "Epoch 009 | train_loss=1.1316 acc=0.3239 | val_loss=1.0957 acc=0.3516 | prec=0.2157 rec=0.3279 f1=0.2136 | time=14.6s\n",
            "Epoch 010 | train_loss=1.1347 acc=0.3263 | val_loss=1.0961 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 011 | train_loss=1.1218 acc=0.3468 | val_loss=1.0953 acc=0.3516 | prec=0.2087 rec=0.3279 f1=0.1828 | time=14.6s\n",
            "Epoch 012 | train_loss=1.1302 acc=0.3449 | val_loss=1.0956 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.8s\n",
            "Epoch 013 | train_loss=1.1115 acc=0.3600 | val_loss=1.0953 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.7s\n",
            "Epoch 014 | train_loss=1.1193 acc=0.3434 | val_loss=1.0951 acc=0.3848 | prec=0.2786 rec=0.3588 f1=0.2589 | time=14.9s\n",
            "Epoch 015 | train_loss=1.1165 acc=0.3561 | val_loss=1.0961 acc=0.3809 | prec=0.3013 rec=0.3552 f1=0.2418 | time=14.7s\n",
            "Epoch 016 | train_loss=1.1287 acc=0.3327 | val_loss=1.0957 acc=0.3633 | prec=0.2354 rec=0.3388 f1=0.2657 | time=14.7s\n",
            "Epoch 017 | train_loss=1.1069 acc=0.3703 | val_loss=1.0951 acc=0.3711 | prec=0.2529 rec=0.3461 f1=0.2710 | time=14.5s\n",
            "Epoch 018 | train_loss=1.1175 acc=0.3537 | val_loss=1.0946 acc=0.3828 | prec=0.2573 rec=0.3570 f1=0.2899 | time=14.6s\n",
            "Epoch 019 | train_loss=1.1156 acc=0.3517 | val_loss=1.0948 acc=0.3711 | prec=0.2469 rec=0.3461 f1=0.2862 | time=14.7s\n",
            "Epoch 020 | train_loss=1.1016 acc=0.3845 | val_loss=1.0922 acc=0.3867 | prec=0.2577 rec=0.3607 f1=0.2967 | time=14.6s\n",
            "Epoch 021 | train_loss=1.1110 acc=0.3527 | val_loss=1.0889 acc=0.3984 | prec=0.2660 rec=0.3716 f1=0.3093 | time=14.9s\n",
            "Epoch 022 | train_loss=1.1029 acc=0.3742 | val_loss=1.0848 acc=0.4238 | prec=0.2831 rec=0.3953 f1=0.3299 | time=14.7s\n",
            "Epoch 023 | train_loss=1.0987 acc=0.3864 | val_loss=1.0776 acc=0.4473 | prec=0.3893 rec=0.4176 f1=0.3481 | time=14.6s\n",
            "Epoch 024 | train_loss=1.0918 acc=0.3952 | val_loss=1.0775 acc=0.4590 | prec=0.3269 rec=0.4281 f1=0.3503 | time=14.5s\n",
            "Epoch 025 | train_loss=1.0688 acc=0.4157 | val_loss=1.0672 acc=0.4512 | prec=0.5947 rec=0.4240 f1=0.3684 | time=14.7s\n",
            "Epoch 026 | train_loss=1.0616 acc=0.4323 | val_loss=1.0346 acc=0.5332 | prec=0.5646 rec=0.5023 f1=0.4530 | time=14.5s\n",
            "Epoch 027 | train_loss=1.0220 acc=0.4983 | val_loss=0.9896 acc=0.5820 | prec=0.5777 rec=0.5520 f1=0.5148 | time=14.7s\n",
            "Epoch 028 | train_loss=0.9873 acc=0.5252 | val_loss=0.9782 acc=0.5703 | prec=0.5375 rec=0.5425 f1=0.5118 | time=14.9s\n",
            "Epoch 029 | train_loss=0.9602 acc=0.5515 | val_loss=0.9478 acc=0.5703 | prec=0.5365 rec=0.5356 f1=0.4740 | time=14.7s\n",
            "Epoch 030 | train_loss=0.9442 acc=0.5618 | val_loss=0.9527 acc=0.5762 | prec=0.5701 rec=0.5517 f1=0.5346 | time=14.7s\n",
            "Epoch 031 | train_loss=0.9214 acc=0.5774 | val_loss=0.9259 acc=0.6055 | prec=0.6171 rec=0.5817 f1=0.5678 | time=14.6s\n",
            "Epoch 032 | train_loss=0.9044 acc=0.5867 | val_loss=0.9217 acc=0.5918 | prec=0.6601 rec=0.5611 f1=0.5232 | time=14.7s\n",
            "Epoch 033 | train_loss=0.8893 acc=0.6111 | val_loss=0.9199 acc=0.5996 | prec=0.6616 rec=0.5703 f1=0.5396 | time=14.8s\n",
            "Epoch 034 | train_loss=0.8709 acc=0.6097 | val_loss=0.8842 acc=0.6172 | prec=0.6158 rec=0.5991 f1=0.5955 | time=14.8s\n",
            "Epoch 035 | train_loss=0.8555 acc=0.6346 | val_loss=0.8806 acc=0.6055 | prec=0.6381 rec=0.5748 f1=0.5427 | time=14.8s\n",
            "Epoch 036 | train_loss=0.8195 acc=0.6585 | val_loss=0.9084 acc=0.5879 | prec=0.6573 rec=0.5593 f1=0.5311 | time=14.7s\n",
            "Epoch 037 | train_loss=0.8206 acc=0.6541 | val_loss=0.9274 acc=0.5781 | prec=0.7176 rec=0.5507 f1=0.5240 | time=14.7s\n",
            "Epoch 038 | train_loss=0.8049 acc=0.6497 | val_loss=0.8097 acc=0.6309 | prec=0.6651 rec=0.5999 f1=0.5665 | time=14.6s\n",
            "Epoch 039 | train_loss=0.7916 acc=0.6615 | val_loss=0.8938 acc=0.6191 | prec=0.6982 rec=0.6005 f1=0.5967 | time=14.7s\n",
            "Epoch 040 | train_loss=0.7720 acc=0.6781 | val_loss=0.8169 acc=0.6680 | prec=0.6829 rec=0.6479 f1=0.6449 | time=14.6s\n",
            "Epoch 041 | train_loss=0.7701 acc=0.6702 | val_loss=0.8350 acc=0.6426 | prec=0.6911 rec=0.6224 f1=0.6197 | time=14.7s\n",
            "Epoch 042 | train_loss=0.7621 acc=0.6834 | val_loss=0.7954 acc=0.6797 | prec=0.6930 rec=0.6717 f1=0.6758 | time=14.8s\n",
            "Epoch 043 | train_loss=0.7218 acc=0.7128 | val_loss=0.8036 acc=0.6582 | prec=0.6821 rec=0.6438 f1=0.6465 | time=14.6s\n",
            "Epoch 044 | train_loss=0.7278 acc=0.7040 | val_loss=0.7883 acc=0.6699 | prec=0.6887 rec=0.6557 f1=0.6582 | time=14.6s\n",
            "Epoch 045 | train_loss=0.7093 acc=0.7250 | val_loss=0.8199 acc=0.6621 | prec=0.7074 rec=0.6480 f1=0.6506 | time=14.5s\n",
            "Epoch 046 | train_loss=0.7054 acc=0.7255 | val_loss=0.7874 acc=0.6777 | prec=0.6946 rec=0.6741 f1=0.6764 | time=14.5s\n",
            "Epoch 047 | train_loss=0.6864 acc=0.7294 | val_loss=0.7649 acc=0.6836 | prec=0.7030 rec=0.6781 f1=0.6816 | time=14.6s\n",
            "Epoch 048 | train_loss=0.6561 acc=0.7489 | val_loss=0.7676 acc=0.6816 | prec=0.7008 rec=0.6722 f1=0.6761 | time=14.7s\n",
            "Epoch 049 | train_loss=0.6644 acc=0.7308 | val_loss=0.7233 acc=0.7188 | prec=0.7185 rec=0.7174 f1=0.7165 | time=14.8s\n",
            "Epoch 050 | train_loss=0.6511 acc=0.7518 | val_loss=0.7615 acc=0.6777 | prec=0.6939 rec=0.6662 f1=0.6696 | time=14.7s\n",
            "Epoch 051 | train_loss=0.6414 acc=0.7636 | val_loss=0.7703 acc=0.6738 | prec=0.7016 rec=0.6635 f1=0.6675 | time=14.6s\n",
            "Epoch 052 | train_loss=0.6335 acc=0.7601 | val_loss=0.7814 acc=0.6699 | prec=0.6997 rec=0.6571 f1=0.6606 | time=14.8s\n",
            "Epoch 053 | train_loss=0.6216 acc=0.7616 | val_loss=0.7764 acc=0.6992 | prec=0.7284 rec=0.6858 f1=0.6898 | time=14.5s\n",
            "Epoch 054 | train_loss=0.6273 acc=0.7523 | val_loss=0.8099 acc=0.6660 | prec=0.7095 rec=0.6576 f1=0.6613 | time=14.6s\n",
            "Epoch 055 | train_loss=0.6210 acc=0.7699 | val_loss=0.8078 acc=0.6680 | prec=0.7138 rec=0.6557 f1=0.6596 | time=14.6s\n",
            "Epoch 056 | train_loss=0.6047 acc=0.7709 | val_loss=0.7373 acc=0.7188 | prec=0.7360 rec=0.7202 f1=0.7196 | time=14.8s\n",
            "Epoch 057 | train_loss=0.5935 acc=0.7704 | val_loss=0.7177 acc=0.7188 | prec=0.7301 rec=0.7215 f1=0.7192 | time=14.6s\n",
            "Epoch 058 | train_loss=0.5842 acc=0.7855 | val_loss=0.7566 acc=0.6895 | prec=0.7068 rec=0.6702 f1=0.6680 | time=14.8s\n",
            "Epoch 059 | train_loss=0.5618 acc=0.7875 | val_loss=0.7522 acc=0.6914 | prec=0.7100 rec=0.6822 f1=0.6849 | time=14.5s\n",
            "Epoch 060 | train_loss=0.5596 acc=0.8017 | val_loss=0.7361 acc=0.7031 | prec=0.7208 rec=0.6913 f1=0.6948 | time=14.6s\n",
            "Epoch 061 | train_loss=0.5540 acc=0.7929 | val_loss=0.7261 acc=0.7168 | prec=0.7285 rec=0.7105 f1=0.7136 | time=14.7s\n",
            "Epoch 062 | train_loss=0.5366 acc=0.8144 | val_loss=0.7602 acc=0.6914 | prec=0.7252 rec=0.6813 f1=0.6851 | time=14.6s\n",
            "Epoch 063 | train_loss=0.5303 acc=0.8056 | val_loss=0.7351 acc=0.7188 | prec=0.7329 rec=0.7137 f1=0.7165 | time=14.8s\n",
            "Epoch 064 | train_loss=0.5254 acc=0.8192 | val_loss=0.7651 acc=0.7109 | prec=0.7406 rec=0.7147 f1=0.7131 | time=14.8s\n",
            "Epoch 065 | train_loss=0.5048 acc=0.8251 | val_loss=0.7435 acc=0.7148 | prec=0.7323 rec=0.7110 f1=0.7133 | time=14.5s\n",
            "Epoch 066 | train_loss=0.5177 acc=0.8056 | val_loss=0.8289 acc=0.6777 | prec=0.7246 rec=0.6662 f1=0.6675 | time=14.6s\n",
            "Epoch 067 | train_loss=0.5067 acc=0.8144 | val_loss=0.6979 acc=0.7168 | prec=0.7234 rec=0.7100 f1=0.7131 | time=14.5s\n",
            "Epoch 068 | train_loss=0.5125 acc=0.8168 | val_loss=0.8730 acc=0.6426 | prec=0.7050 rec=0.6320 f1=0.6306 | time=14.6s\n",
            "Epoch 069 | train_loss=0.5070 acc=0.8246 | val_loss=0.7239 acc=0.7266 | prec=0.7353 rec=0.7228 f1=0.7249 | time=14.5s\n",
            "Epoch 070 | train_loss=0.4960 acc=0.8280 | val_loss=0.7559 acc=0.7070 | prec=0.7227 rec=0.6991 f1=0.7022 | time=14.8s\n",
            "Epoch 071 | train_loss=0.4983 acc=0.8124 | val_loss=0.7215 acc=0.7227 | prec=0.7345 rec=0.7215 f1=0.7221 | time=14.8s\n",
            "Epoch 072 | train_loss=0.4791 acc=0.8236 | val_loss=0.7827 acc=0.6816 | prec=0.7186 rec=0.6791 f1=0.6810 | time=14.6s\n",
            "Epoch 073 | train_loss=0.4688 acc=0.8359 | val_loss=0.7722 acc=0.7051 | prec=0.7322 rec=0.6982 f1=0.7017 | time=14.6s\n",
            "Epoch 074 | train_loss=0.4678 acc=0.8305 | val_loss=0.7739 acc=0.7070 | prec=0.7274 rec=0.7014 f1=0.7045 | time=14.5s\n",
            "Epoch 075 | train_loss=0.4851 acc=0.8144 | val_loss=0.7517 acc=0.7168 | prec=0.7305 rec=0.7073 f1=0.7107 | time=14.7s\n",
            "Epoch 076 | train_loss=0.4543 acc=0.8442 | val_loss=0.7361 acc=0.7109 | prec=0.7239 rec=0.7073 f1=0.7095 | time=14.8s\n",
            "Epoch 077 | train_loss=0.4595 acc=0.8383 | val_loss=0.7592 acc=0.7148 | prec=0.7339 rec=0.7124 f1=0.7137 | time=14.8s\n",
            "Epoch 078 | train_loss=0.4401 acc=0.8510 | val_loss=0.7997 acc=0.7051 | prec=0.7344 rec=0.7014 f1=0.7033 | time=14.8s\n",
            "Epoch 079 | train_loss=0.4700 acc=0.8266 | val_loss=0.7121 acc=0.7266 | prec=0.7302 rec=0.7228 f1=0.7242 | time=14.6s\n",
            "Epoch 080 | train_loss=0.4191 acc=0.8583 | val_loss=0.7466 acc=0.7227 | prec=0.7383 rec=0.7146 f1=0.7180 | time=14.9s\n",
            "Epoch 081 | train_loss=0.4209 acc=0.8500 | val_loss=0.7718 acc=0.7090 | prec=0.7295 rec=0.7051 f1=0.7078 | time=14.7s\n",
            "Epoch 082 | train_loss=0.4184 acc=0.8588 | val_loss=0.7474 acc=0.7129 | prec=0.7369 rec=0.7092 f1=0.7111 | time=15.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▁▁▂▂▂▃▃▆▆▆▅▆▇▇▇▇▇▇▇▇▇██▇███▇███████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▃▃▂▃▃▃▄▆▆▆▇█▇█▇███▇████████████████</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁▂▁▁▂▂▃▃▄▅▅▅▅▆▆▇▆▇▇▇▇▇█████▇███████</td></tr><tr><td>train_acc</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▄▄▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>train_loss</td><td>█████▇▇█▇▇▇▇▇▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▂▂▂▂▃▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇█▇██████▇██████</td></tr><tr><td>val_loss</td><td>█████████████▆▅▄▅▅▃▄▃▃▃▃▂▁▂▂▂▁▂▂▁▁▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>82</td></tr><tr><td>f1_score</td><td>0.71112</td></tr><tr><td>precision</td><td>0.7369</td></tr><tr><td>recall</td><td>0.70916</td></tr><tr><td>train_acc</td><td>0.85882</td></tr><tr><td>train_loss</td><td>0.41841</td></tr><tr><td>val_acc</td><td>0.71289</td></tr><tr><td>val_loss</td><td>0.74741</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/v0yexkbx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2/runs/v0yexkbx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_114822-v0yexkbx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 12:08:28,889] Trial 9 finished with values: [0.6978908181190491, 0.716796875] and parameters: {'lr': 3.7214819629441235e-05, 'wd': 0.00019609787248213046}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "All trials: hyperparams → (val_loss, val_acc)\n",
            "  Trial 0: lr=2.79e-05, wd=1.73e-05 → (0.6848, 0.7168)\n",
            "  Trial 1: lr=6.72e-03, wd=5.21e-04 → (0.7927, 0.6523)\n",
            "  Trial 2: lr=1.34e-04, wd=4.02e-03 → (0.6832, 0.7324)\n",
            "  Trial 3: lr=2.90e-05, wd=2.05e-05 → (0.6576, 0.7422)\n",
            "  Trial 4: lr=2.08e-05, wd=6.91e-05 → (0.6542, 0.7305)\n",
            "  Trial 5: lr=2.93e-04, wd=2.20e-04 → (0.6777, 0.7148)\n",
            "  Trial 6: lr=3.23e-03, wd=3.78e-05 → (0.7543, 0.6855)\n",
            "  Trial 7: lr=9.51e-05, wd=2.16e-04 → (0.6466, 0.7266)\n",
            "  Trial 8: lr=1.11e-04, wd=2.54e-03 → (0.6277, 0.7539)\n",
            "  Trial 9: lr=3.72e-05, wd=1.96e-04 → (0.6979, 0.7168)\n",
            "\n",
            "=== Selected Trial #8 ===\n",
            " val_loss=0.6277, val_acc=0.7539\n",
            " Best hyperparameters: {'lr': 0.00011111361926742459, 'wd': 0.002544246881680688}\n",
            "\n",
            "=== Final Evaluation on Test Sets ===\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "2",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-e28872b682dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_within\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_within_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"test_cross\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cross_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"-- {name} -- total={len(metas)}  A={counts[0]}, C={counts[1]}, F={counts[2]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Acc={res['acc']:.4f}  Prec={res['precision']:.4f}  Rec={res['recall']:.4f}  F1={res['f1']:.4f}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-e28872b682dc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_within\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_within_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"test_cross\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_cross_meta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m         \u001b[0mcounts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_labels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlabel_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"label\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmetas\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"-- {name} -- total={len(metas)}  A={counts[0]}, C={counts[1]}, F={counts[2]}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\" Acc={res['acc']:.4f}  Prec={res['precision']:.4f}  Rec={res['recall']:.4f}  F1={res['f1']:.4f}\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 2"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tbPuKrNOn8tC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### AD vs CN vs FTD (Best Trial test)"
      ],
      "metadata": {
        "id": "VGUX9lIM_kYB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "PCT_START   = 0.2  # for OneCycleLR\n",
        "\n",
        "# ─── Fixed best hyperparameters ─────────────────────────────────\n",
        "BEST_LR = 2.90e-05\n",
        "BEST_WD = 2.05e-05\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class EEGMultiDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load metadata & split ───────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_data       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "label_map = {'A':0,'C':1,'F':2}\n",
        "\n",
        "# ─── Manual Class Balancing ──────────────────────────────────────\n",
        "train_data_A = [d for d in train_data if d['label']=='A']\n",
        "train_data_C = [d for d in train_data if d['label']=='C']\n",
        "train_data_F = [d for d in train_data if d['label']=='F']\n",
        "\n",
        "min_samples = min(\n",
        "    (len(train_data_A)+len(train_data_C)) / 2,\n",
        "    (len(train_data_A)+len(train_data_F)) / 2,\n",
        "    (len(train_data_C)+len(train_data_F)) / 2\n",
        ")\n",
        "\n",
        "a_index = int(min(min_samples, len(train_data_A)))\n",
        "c_index = int(min(min_samples, len(train_data_C)))\n",
        "f_index = int(min(min_samples, len(train_data_F)))\n",
        "\n",
        "balanced_train_data = (\n",
        "    random.sample(train_data_A, a_index) +\n",
        "    random.sample(train_data_C, c_index) +\n",
        "    random.sample(train_data_F, f_index)\n",
        ")\n",
        "\n",
        "print(f'Before Balancing  A={len(train_data_A)}, C={len(train_data_C)}, F={len(train_data_F)}')\n",
        "print(f'After  Balancing  A={a_index}, C={c_index}, F={f_index}  Total={len(balanced_train_data)}')\n",
        "\n",
        "for d in balanced_train_data:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_train_data)\n",
        "dataset_train = EEGMultiDataset(raw_ds_train, balanced_train_data)\n",
        "labels_train  = np.array([d['label'] for d in balanced_train_data])\n",
        "\n",
        "# ─── Utility: count labels ───────────────────────────────────────\n",
        "def count_labels(meta_list):\n",
        "    counts = {0:0, 1:0, 2:0}\n",
        "    for d in meta_list:\n",
        "        counts[d['label']] += 1\n",
        "    return counts\n",
        "\n",
        "# ─── Optuna objective (fixed hyperparams) ───────────────────────\n",
        "def objective(trial):\n",
        "    lr = BEST_LR\n",
        "    wd = BEST_WD\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams (fixed) --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-multiclass-AD_CN_FTD_fixed',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr,'wd':wd,'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2, stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "\n",
        "    tr_counts = count_labels([balanced_train_data[i] for i in tr_idx])\n",
        "    va_counts = count_labels([balanced_train_data[i] for i in va_idx])\n",
        "    print(f\"[Trial {trial.number}] TRAIN n={len(tr_idx)} A={tr_counts[0]}, C={tr_counts[1]}, F={tr_counts[2]} | \"\n",
        "          f\"VAL n={len(va_idx)} A={va_counts[0]}, C={va_counts[1]}, F={va_counts[2]}\")\n",
        "\n",
        "    train_loader = DataLoader(Subset(dataset_train, tr_idx),\n",
        "                              batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "    val_loader   = DataLoader(Subset(dataset_train, va_idx),\n",
        "                              batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_state    = None\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "        train_loss = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            train_loss   += loss.item()\n",
        "            preds        = logits.argmax(1)\n",
        "            train_correct+= (preds==y).sum().item()\n",
        "            train_total  += y.size(0)\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_acc   = train_correct / train_total\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                val_loss += criterion(logits, y).item()\n",
        "                all_preds.append(logits.argmax(1).cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "\n",
        "        val_acc = accuracy_score(labs, preds)\n",
        "        prec    = precision_score(labs, preds, average='macro', zero_division=0)\n",
        "        rec     = recall_score(labs, preds, average='macro', zero_division=0)\n",
        "        f1      = f1_score(labs, preds, average='macro', zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | prec={prec:.4f} rec={rec:.4f} f1={f1:.4f} | \"\n",
        "              f\"time={elapsed:.1f}s\")\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':      epoch,\n",
        "            'train_loss': train_loss,\n",
        "            'train_acc':  train_acc,\n",
        "            'val_loss':   val_loss,\n",
        "            'val_acc':    val_acc,\n",
        "            'precision':  prec,\n",
        "            'recall':     rec,\n",
        "            'f1_score':   f1\n",
        "        })\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "            es_count      = 0\n",
        "            os.makedirs('ckpts', exist_ok=True)\n",
        "            ckpt_path = f'ckpts/trial{trial.number}_best.pth'\n",
        "            torch.save(best_state, ckpt_path)\n",
        "            trial.set_user_attr('ckpt_path', ckpt_path)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, scheduler, train_loader, val_loader\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # return best checkpoint loss & acc\n",
        "    return best_val_loss, val_acc\n",
        "\n",
        "# ─── Run Optuna & Evaluate Best ─────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=[\"minimize\",\"maximize\"],\n",
        "        study_name=\"eeg_multiobj_AD_CN_FTD_fixed\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=1)\n",
        "\n",
        "    # best trial is trial 0\n",
        "    t = study.trials[0]\n",
        "    ckpt_path = t.user_attrs[\"ckpt_path\"]\n",
        "    print(f\"\\nFixed-hparam run → lr={BEST_LR:.2e}, wd={BEST_WD:.2e} → \"\n",
        "          f\"(val_loss={t.values[0]:.4f}, val_acc={t.values[1]:.4f})\")\n",
        "\n",
        "    # load best checkpoint\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model_val = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=3\n",
        "    ).to(DEVICE)\n",
        "    model_val.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    model_val.eval()\n",
        "\n",
        "    def evaluate(model, metas):\n",
        "        metas_copy = []\n",
        "        for d in metas:\n",
        "            new_d = d.copy()\n",
        "            if isinstance(new_d['label'], str):\n",
        "                new_d['label'] = label_map[new_d['label']]\n",
        "            metas_copy.append(new_d)\n",
        "\n",
        "        ds     = EEGMultiDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                all_preds.append(logits.argmax(1).cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        metrics = {\n",
        "            \"acc\":       accuracy_score(labs, preds),\n",
        "            \"precision\": precision_score(labs, preds, average='macro', zero_division=0),\n",
        "            \"recall\":    recall_score(labs, preds, average='macro', zero_division=0),\n",
        "            \"f1\":        f1_score(labs, preds, average='macro', zero_division=0),\n",
        "        }\n",
        "        counts = count_labels(metas_copy)\n",
        "        return metrics, counts\n",
        "\n",
        "    # evaluate model trained with validation\n",
        "    print(\"\\n=== Test performance (model with validation) ===\")\n",
        "    m_val_within, c_val_within = evaluate(model_val, test_within_meta)\n",
        "    print(f\" test_within: total={len(test_within_meta)}  A={c_val_within[0]}, C={c_val_within[1]}, F={c_val_within[2]}\")\n",
        "    print(f\"    Acc={m_val_within['acc']:.4f}  Prec={m_val_within['precision']:.4f}  \"\n",
        "          f\"Rec={m_val_within['recall']:.4f}  F1={m_val_within['f1']:.4f}\")\n",
        "\n",
        "    m_val_cross, c_val_cross = evaluate(model_val, test_cross_meta)\n",
        "    print(f\" test_cross:  total={len(test_cross_meta)}  A={c_val_cross[0]}, C={c_val_cross[1]}, F={c_val_cross[2]}\")\n",
        "    print(f\"    Acc={m_val_cross['acc']:.4f}  Prec={m_val_cross['precision']:.4f}  \"\n",
        "          f\"Rec={m_val_cross['recall']:.4f}  F1={m_val_cross['f1']:.4f}\")\n",
        "\n",
        "    # ─── Full train on all training data, no validation ─────────────\n",
        "    print(\"\\n=== Full-train performance (no validation) ===\")\n",
        "    full_loader = DataLoader(dataset_train, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    model_full = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model_full.parameters(),\n",
        "                                  lr=BEST_LR, weight_decay=BEST_WD)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=BEST_LR, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(full_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # train for MAX_EPOCHS on full data\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        model_full.train()\n",
        "        for X, y in full_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model_full(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "    model_full.eval()\n",
        "    m_full_within, c_full_within = evaluate(model_full, test_within_meta)\n",
        "    print(f\" test_within: total={len(test_within_meta)}  A={c_full_within[0]}, C={c_full_within[1]}, F={c_full_within[2]}\")\n",
        "    print(f\"    Acc={m_full_within['acc']:.4f}  Prec={m_full_within['precision']:.4f}  \"\n",
        "          f\"Rec={m_full_within['recall']:.4f}  F1={m_full_within['f1']:.4f}\")\n",
        "\n",
        "    m_full_cross, c_full_cross = evaluate(model_full, test_cross_meta)\n",
        "    print(f\" test_cross:  total={len(test_cross_meta)}  A={c_full_cross[0]}, C={c_full_cross[1]}, F={c_full_cross[2]}\")\n",
        "    print(f\"    Acc={m_full_cross['acc']:.4f}  Prec={m_full_cross['precision']:.4f}  \"\n",
        "          f\"Rec={m_full_cross['recall']:.4f}  F1={m_full_cross['f1']:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0yY6NvY1n8uu",
        "outputId": "fe5a7a2f-915b-4bc5-878d-7b9297af2278"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 21:10:08,122] A new study created in memory with name: eeg_multiobj_AD_CN_FTD_fixed\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before Balancing  A=1388, C=1102, F=729\n",
            "After  Balancing  A=915, C=915, F=729  Total=2559\n",
            "\n",
            "--- Trial 0 hyperparams (fixed) --> lr=2.90e-05, wd=2.05e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250510_211008-8oqqlvzy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed/runs/8oqqlvzy' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed/runs/8oqqlvzy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed/runs/8oqqlvzy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 0] TRAIN n=2047 A=732, C=732, F=583 | VAL n=512 A=183, C=183, F=146\n",
            "Epoch 001 | train_loss=1.1655 acc=0.3283 | val_loss=1.0987 acc=0.3242 | prec=0.5474 rec=0.3347 f1=0.2640 | time=492.3s\n",
            "Epoch 002 | train_loss=1.1677 acc=0.3205 | val_loss=1.0976 acc=0.3379 | prec=0.3361 rec=0.3239 f1=0.2838 | time=15.4s\n",
            "Epoch 003 | train_loss=1.1707 acc=0.3327 | val_loss=1.0980 acc=0.3613 | prec=0.3829 rec=0.3767 f1=0.3485 | time=15.4s\n",
            "Epoch 004 | train_loss=1.1717 acc=0.3258 | val_loss=1.0986 acc=0.3086 | prec=0.3722 rec=0.3432 f1=0.2414 | time=15.1s\n",
            "Epoch 005 | train_loss=1.1814 acc=0.3009 | val_loss=1.1003 acc=0.2871 | prec=0.2615 rec=0.3342 f1=0.1578 | time=14.7s\n",
            "Epoch 006 | train_loss=1.1509 acc=0.3468 | val_loss=1.0978 acc=0.3438 | prec=0.2870 rec=0.3234 f1=0.2092 | time=14.8s\n",
            "Epoch 007 | train_loss=1.1518 acc=0.3385 | val_loss=1.1008 acc=0.2773 | prec=0.1676 rec=0.3224 f1=0.1575 | time=14.5s\n",
            "Epoch 008 | train_loss=1.1391 acc=0.3454 | val_loss=1.1003 acc=0.2832 | prec=0.1864 rec=0.2877 f1=0.2261 | time=14.5s\n",
            "Epoch 009 | train_loss=1.1649 acc=0.3219 | val_loss=1.1001 acc=0.3574 | prec=0.2291 rec=0.3403 f1=0.2249 | time=14.5s\n",
            "Epoch 010 | train_loss=1.1515 acc=0.3254 | val_loss=1.0957 acc=0.3574 | prec=0.1194 rec=0.3333 f1=0.1758 | time=14.6s\n",
            "Epoch 011 | train_loss=1.1555 acc=0.3214 | val_loss=1.0955 acc=0.3633 | prec=0.2427 rec=0.3388 f1=0.2824 | time=14.5s\n",
            "Epoch 012 | train_loss=1.1303 acc=0.3400 | val_loss=1.0938 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 013 | train_loss=1.1378 acc=0.3337 | val_loss=1.0926 acc=0.3691 | prec=0.2462 rec=0.3443 f1=0.2837 | time=14.7s\n",
            "Epoch 014 | train_loss=1.1462 acc=0.3337 | val_loss=1.0928 acc=0.3672 | prec=0.2985 rec=0.3424 f1=0.2034 | time=14.7s\n",
            "Epoch 015 | train_loss=1.1387 acc=0.3322 | val_loss=1.0937 acc=0.3652 | prec=0.3428 rec=0.3406 f1=0.1912 | time=14.6s\n",
            "Epoch 016 | train_loss=1.1356 acc=0.3434 | val_loss=1.0932 acc=0.3789 | prec=0.2569 rec=0.3534 f1=0.2899 | time=14.6s\n",
            "Epoch 017 | train_loss=1.1071 acc=0.3752 | val_loss=1.0945 acc=0.3867 | prec=0.2581 rec=0.3607 f1=0.2923 | time=14.6s\n",
            "Epoch 018 | train_loss=1.1290 acc=0.3444 | val_loss=1.1013 acc=0.3574 | prec=0.1191 rec=0.3333 f1=0.1755 | time=14.6s\n",
            "Epoch 019 | train_loss=1.1286 acc=0.3522 | val_loss=1.0934 acc=0.3594 | prec=0.4527 rec=0.3352 f1=0.1794 | time=14.5s\n",
            "Epoch 020 | train_loss=1.1174 acc=0.3610 | val_loss=1.0734 acc=0.4668 | prec=0.3099 rec=0.4353 f1=0.3613 | time=14.7s\n",
            "Epoch 021 | train_loss=1.0883 acc=0.3898 | val_loss=1.0474 acc=0.4590 | prec=0.4093 rec=0.4336 f1=0.3971 | time=14.7s\n",
            "Epoch 022 | train_loss=1.0701 acc=0.4265 | val_loss=1.0162 acc=0.5352 | prec=0.3753 rec=0.4991 f1=0.4193 | time=14.6s\n",
            "Epoch 023 | train_loss=1.0262 acc=0.4748 | val_loss=1.0018 acc=0.5508 | prec=0.3959 rec=0.5137 f1=0.4323 | time=14.6s\n",
            "Epoch 024 | train_loss=1.0089 acc=0.5017 | val_loss=0.9945 acc=0.5254 | prec=0.5387 rec=0.4969 f1=0.4500 | time=14.6s\n",
            "Epoch 025 | train_loss=0.9902 acc=0.5159 | val_loss=0.9859 acc=0.5391 | prec=0.5344 rec=0.5166 f1=0.4918 | time=14.6s\n",
            "Epoch 026 | train_loss=0.9607 acc=0.5374 | val_loss=0.9571 acc=0.5527 | prec=0.5321 rec=0.5270 f1=0.4988 | time=14.6s\n",
            "Epoch 027 | train_loss=0.9540 acc=0.5471 | val_loss=0.9307 acc=0.5703 | prec=0.5485 rec=0.5397 f1=0.4981 | time=14.7s\n",
            "Epoch 028 | train_loss=0.9528 acc=0.5486 | val_loss=0.9439 acc=0.5684 | prec=0.5595 rec=0.5564 f1=0.5496 | time=14.6s\n",
            "Epoch 029 | train_loss=0.9147 acc=0.5721 | val_loss=0.9172 acc=0.5586 | prec=0.5470 rec=0.5385 f1=0.5237 | time=14.5s\n",
            "Epoch 030 | train_loss=0.9174 acc=0.5613 | val_loss=0.9087 acc=0.5762 | prec=0.5857 rec=0.5503 f1=0.5244 | time=14.6s\n",
            "Epoch 031 | train_loss=0.8985 acc=0.5740 | val_loss=0.8946 acc=0.5781 | prec=0.5855 rec=0.5502 f1=0.5180 | time=14.7s\n",
            "Epoch 032 | train_loss=0.8849 acc=0.6014 | val_loss=0.8804 acc=0.5996 | prec=0.5876 rec=0.5818 f1=0.5726 | time=14.5s\n",
            "Epoch 033 | train_loss=0.8541 acc=0.6229 | val_loss=0.8834 acc=0.5938 | prec=0.5880 rec=0.5722 f1=0.5574 | time=14.7s\n",
            "Epoch 034 | train_loss=0.8453 acc=0.6141 | val_loss=0.8731 acc=0.6016 | prec=0.6081 rec=0.5758 f1=0.5527 | time=14.8s\n",
            "Epoch 035 | train_loss=0.8230 acc=0.6351 | val_loss=0.8583 acc=0.6074 | prec=0.6371 rec=0.5803 f1=0.5539 | time=14.8s\n",
            "Epoch 036 | train_loss=0.8267 acc=0.6287 | val_loss=0.8530 acc=0.6270 | prec=0.6177 rec=0.6105 f1=0.6049 | time=14.6s\n",
            "Epoch 037 | train_loss=0.8184 acc=0.6370 | val_loss=0.8337 acc=0.6191 | prec=0.6048 rec=0.6046 f1=0.6013 | time=14.7s\n",
            "Epoch 038 | train_loss=0.7997 acc=0.6458 | val_loss=0.8271 acc=0.6328 | prec=0.6232 rec=0.6183 f1=0.6151 | time=14.6s\n",
            "Epoch 039 | train_loss=0.7823 acc=0.6668 | val_loss=0.8333 acc=0.6445 | prec=0.6337 rec=0.6306 f1=0.6292 | time=14.7s\n",
            "Epoch 040 | train_loss=0.7641 acc=0.6737 | val_loss=0.8163 acc=0.6445 | prec=0.6362 rec=0.6329 f1=0.6327 | time=14.7s\n",
            "Epoch 041 | train_loss=0.7552 acc=0.6849 | val_loss=0.8411 acc=0.6328 | prec=0.6401 rec=0.6234 f1=0.6247 | time=14.8s\n",
            "Epoch 042 | train_loss=0.7600 acc=0.6898 | val_loss=0.7931 acc=0.6582 | prec=0.6449 rec=0.6462 f1=0.6439 | time=14.6s\n",
            "Epoch 043 | train_loss=0.7437 acc=0.6820 | val_loss=0.7948 acc=0.6621 | prec=0.6556 rec=0.6484 f1=0.6475 | time=14.7s\n",
            "Epoch 044 | train_loss=0.7252 acc=0.6966 | val_loss=0.7889 acc=0.6621 | prec=0.6604 rec=0.6567 f1=0.6576 | time=14.6s\n",
            "Epoch 045 | train_loss=0.7195 acc=0.6908 | val_loss=0.7987 acc=0.6562 | prec=0.6671 rec=0.6503 f1=0.6524 | time=14.6s\n",
            "Epoch 046 | train_loss=0.7227 acc=0.6952 | val_loss=0.7765 acc=0.6738 | prec=0.6732 rec=0.6695 f1=0.6702 | time=14.8s\n",
            "Epoch 047 | train_loss=0.6954 acc=0.7220 | val_loss=0.7839 acc=0.6719 | prec=0.6789 rec=0.6594 f1=0.6610 | time=14.8s\n",
            "Epoch 048 | train_loss=0.6908 acc=0.7245 | val_loss=0.7820 acc=0.6738 | prec=0.6736 rec=0.6557 f1=0.6524 | time=14.8s\n",
            "Epoch 049 | train_loss=0.6858 acc=0.7181 | val_loss=0.7698 acc=0.6934 | prec=0.7001 rec=0.6794 f1=0.6808 | time=14.7s\n",
            "Epoch 050 | train_loss=0.6676 acc=0.7294 | val_loss=0.7569 acc=0.6973 | prec=0.6974 rec=0.6895 f1=0.6916 | time=14.5s\n",
            "Epoch 051 | train_loss=0.6485 acc=0.7528 | val_loss=0.7606 acc=0.6953 | prec=0.6974 rec=0.6928 f1=0.6929 | time=14.6s\n",
            "Epoch 052 | train_loss=0.6600 acc=0.7362 | val_loss=0.7847 acc=0.6758 | prec=0.6805 rec=0.6764 f1=0.6714 | time=14.6s\n",
            "Epoch 053 | train_loss=0.6474 acc=0.7440 | val_loss=0.7454 acc=0.7051 | prec=0.7067 rec=0.6954 f1=0.6976 | time=14.6s\n",
            "Epoch 054 | train_loss=0.6454 acc=0.7509 | val_loss=0.7452 acc=0.6992 | prec=0.6932 rec=0.6950 f1=0.6933 | time=14.7s\n",
            "Epoch 055 | train_loss=0.6430 acc=0.7450 | val_loss=0.7575 acc=0.6895 | prec=0.7021 rec=0.6822 f1=0.6848 | time=15.0s\n",
            "Epoch 056 | train_loss=0.6234 acc=0.7596 | val_loss=0.7431 acc=0.7012 | prec=0.7068 rec=0.6941 f1=0.6967 | time=14.7s\n",
            "Epoch 057 | train_loss=0.6113 acc=0.7626 | val_loss=0.7355 acc=0.7051 | prec=0.7010 rec=0.6982 f1=0.6992 | time=14.6s\n",
            "Epoch 058 | train_loss=0.6125 acc=0.7640 | val_loss=0.7804 acc=0.6738 | prec=0.6822 rec=0.6584 f1=0.6584 | time=14.5s\n",
            "Epoch 059 | train_loss=0.6018 acc=0.7787 | val_loss=0.7958 acc=0.6914 | prec=0.7147 rec=0.6720 f1=0.6705 | time=14.6s\n",
            "Epoch 060 | train_loss=0.6013 acc=0.7763 | val_loss=0.7353 acc=0.6934 | prec=0.6910 rec=0.6845 f1=0.6859 | time=14.6s\n",
            "Epoch 061 | train_loss=0.5925 acc=0.7675 | val_loss=0.7378 acc=0.7070 | prec=0.7098 rec=0.7055 f1=0.7052 | time=14.7s\n",
            "Epoch 062 | train_loss=0.5916 acc=0.7831 | val_loss=0.7305 acc=0.7070 | prec=0.7125 rec=0.7032 f1=0.7045 | time=14.8s\n",
            "Epoch 063 | train_loss=0.5808 acc=0.7811 | val_loss=0.7139 acc=0.7305 | prec=0.7282 rec=0.7256 f1=0.7263 | time=14.5s\n",
            "Epoch 064 | train_loss=0.5741 acc=0.7846 | val_loss=0.7438 acc=0.6953 | prec=0.7091 rec=0.6918 f1=0.6943 | time=14.5s\n",
            "Epoch 065 | train_loss=0.5522 acc=0.7992 | val_loss=0.7332 acc=0.7090 | prec=0.7087 rec=0.7027 f1=0.7044 | time=14.6s\n",
            "Epoch 066 | train_loss=0.5856 acc=0.7802 | val_loss=0.7415 acc=0.7051 | prec=0.7065 rec=0.6931 f1=0.6950 | time=14.5s\n",
            "Epoch 067 | train_loss=0.5475 acc=0.7953 | val_loss=0.7491 acc=0.7012 | prec=0.7090 rec=0.6853 f1=0.6861 | time=14.6s\n",
            "Epoch 068 | train_loss=0.5617 acc=0.7875 | val_loss=0.6994 acc=0.7227 | prec=0.7164 rec=0.7164 f1=0.7164 | time=14.7s\n",
            "Epoch 069 | train_loss=0.5529 acc=0.7982 | val_loss=0.7235 acc=0.7109 | prec=0.7082 rec=0.7027 f1=0.7045 | time=14.9s\n",
            "Epoch 070 | train_loss=0.5303 acc=0.8153 | val_loss=0.7187 acc=0.7129 | prec=0.7125 rec=0.7115 f1=0.7104 | time=14.7s\n",
            "Epoch 071 | train_loss=0.5401 acc=0.8056 | val_loss=0.7377 acc=0.7051 | prec=0.7090 rec=0.6931 f1=0.6955 | time=14.7s\n",
            "Epoch 072 | train_loss=0.5385 acc=0.7938 | val_loss=0.7953 acc=0.6953 | prec=0.7172 rec=0.6821 f1=0.6858 | time=14.6s\n",
            "Epoch 073 | train_loss=0.5189 acc=0.8075 | val_loss=0.7613 acc=0.6953 | prec=0.7053 rec=0.6835 f1=0.6868 | time=14.6s\n",
            "Epoch 074 | train_loss=0.5169 acc=0.8119 | val_loss=0.7400 acc=0.7031 | prec=0.6966 rec=0.6977 f1=0.6957 | time=14.7s\n",
            "Epoch 075 | train_loss=0.5152 acc=0.8105 | val_loss=0.7386 acc=0.7031 | prec=0.7089 rec=0.6922 f1=0.6951 | time=14.7s\n",
            "Epoch 076 | train_loss=0.5108 acc=0.8354 | val_loss=0.7136 acc=0.7148 | prec=0.7088 rec=0.7096 f1=0.7090 | time=14.8s\n",
            "Epoch 077 | train_loss=0.4879 acc=0.8354 | val_loss=0.7419 acc=0.7090 | prec=0.7109 rec=0.7023 f1=0.7047 | time=14.8s\n",
            "Epoch 078 | train_loss=0.5018 acc=0.8183 | val_loss=0.7168 acc=0.7109 | prec=0.7034 rec=0.7004 f1=0.7007 | time=14.6s\n",
            "Epoch 079 | train_loss=0.5068 acc=0.8075 | val_loss=0.7456 acc=0.7070 | prec=0.7094 rec=0.6958 f1=0.6983 | time=14.7s\n",
            "Epoch 080 | train_loss=0.4908 acc=0.8202 | val_loss=0.7081 acc=0.7129 | prec=0.7091 rec=0.7078 f1=0.7082 | time=14.7s\n",
            "Epoch 081 | train_loss=0.4998 acc=0.8158 | val_loss=0.7192 acc=0.7070 | prec=0.7013 rec=0.6963 f1=0.6972 | time=14.7s\n",
            "Epoch 082 | train_loss=0.4719 acc=0.8447 | val_loss=0.7178 acc=0.7148 | prec=0.7130 rec=0.7114 f1=0.7114 | time=14.8s\n",
            "Epoch 083 | train_loss=0.4916 acc=0.8227 | val_loss=0.7246 acc=0.7090 | prec=0.7049 rec=0.7009 f1=0.7023 | time=14.8s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▃▃▁▂▂▃▃▁▁▄▅▅▆▅▆▇▆▇▇▇▇▇▇▇██▇██▇██████████</td></tr><tr><td>precision</td><td>▃▂▂▁▂▄▃▃▃▄▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██▇████████████</td></tr><tr><td>recall</td><td>▂▁▁▁▁▁▁▂▁▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇████▇██▇████████</td></tr><tr><td>train_acc</td><td>▁▁▁▁▂▁▁▁▁▂▂▂▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇█▇▇███████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▇▇▅▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▁▂▁▂▂▂▂▃▂▅▅▅▅▆▆▆▆▇▇▇▇▇▇█▇█▇███████████</td></tr><tr><td>val_loss</td><td>█████████▆▆▅▅▅▅▄▄▄▃▃▃▃▃▂▂▂▂▂▂▃▂▁▁▂▃▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>83</td></tr><tr><td>f1_score</td><td>0.70227</td></tr><tr><td>precision</td><td>0.70492</td></tr><tr><td>recall</td><td>0.7009</td></tr><tr><td>train_acc</td><td>0.82267</td></tr><tr><td>train_loss</td><td>0.49159</td></tr><tr><td>val_acc</td><td>0.70898</td></tr><tr><td>val_loss</td><td>0.72458</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed/runs/8oqqlvzy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed/runs/8oqqlvzy</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-multiclass-AD_CN_FTD_fixed</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250510_211008-8oqqlvzy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-10 21:38:45,203] Trial 0 finished with values: [0.6993866618722677, 0.708984375] and parameters: {}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Fixed-hparam run → lr=2.90e-05, wd=2.05e-05 → (val_loss=0.6994, val_acc=0.7090)\n",
            "\n",
            "=== Test performance (model with validation) ===\n",
            " test_within: total=344  A=146, C=126, F=72\n",
            "    Acc=0.7703  Prec=0.7527  Rec=0.7703  F1=0.7557\n",
            " test_cross:  total=873  A=319, C=307, F=247\n",
            "    Acc=0.5178  Prec=0.5082  Rec=0.5086  F1=0.5012\n",
            "\n",
            "=== Full-train performance (no validation) ===\n",
            " test_within: total=344  A=146, C=126, F=72\n",
            "    Acc=0.7820  Prec=0.7672  Rec=0.7475  F1=0.7542\n",
            " test_cross:  total=873  A=319, C=307, F=247\n",
            "    Acc=0.5441  Prec=0.5787  Rec=0.5216  F1=0.5033\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QKxlvFGEXROt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Classification Performance of Leave-One-Subject-Out (LOSO) validation for the AD-CN problem"
      ],
      "metadata": {
        "id": "GNurs3MnP0I9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── 재현성 설정 ─────────────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(SEED + worker_id)\n",
        "    random.seed(SEED + worker_id)\n",
        "\n",
        "# ─── 설정 ─────────────────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE   = 32\n",
        "MAX_EPOCHS   = 150   # 최대 epoch\n",
        "LR           = 2.2e-05\n",
        "WD           = 2.3e-06\n",
        "PCT_START    = 0.2\n",
        "ES_PATIENCE  = 15    # validation_loss 개선 없을 때 조기종료\n",
        "VAL_SIZE     = 0.2   # train_meta 중 validation 비율\n",
        "\n",
        "# ─── Dataset 래퍼 클래스 ──────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, meta):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.meta   = meta\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y    = self.meta[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── subject_id 추출 함수 ─────────────────────────────────────────\n",
        "def extract_subject_id(m):\n",
        "    fn = os.path.basename(m['file_name'])\n",
        "    return fn.split('_')[0]\n",
        "\n",
        "# ─── 메타데이터 로드 및 필터링 ─────────────────────────────────────\n",
        "all_meta = json.load(open(os.path.join(DATA_DIR, LABEL_FILE), 'r'))\n",
        "all_meta = [m for m in all_meta if m['label'] in ('A','C')]\n",
        "groups   = [extract_subject_id(m) for m in all_meta]\n",
        "subjects = sorted(set(groups))\n",
        "\n",
        "# ─── LOSO 검증 셋업 ──────────────────────────────────────────────\n",
        "gkf = GroupKFold(n_splits=len(subjects))\n",
        "total_tp = total_tn = total_fp = total_fn = 0\n",
        "\n",
        "# ─── Fold별 학습/평가 루프 ───────────────────────────────────────\n",
        "for fold, (train_idx, test_idx) in enumerate(gkf.split(all_meta, groups=groups), 1):\n",
        "    print(f\"\\n>>> Fold {fold}/{len(subjects)}  (leave out {subjects[fold-1]})\")\n",
        "\n",
        "    # 메타 분할 및 라벨 매핑\n",
        "    train_meta = [copy.deepcopy(all_meta[i]) for i in train_idx]\n",
        "    test_meta  = [copy.deepcopy(all_meta[i]) for i in test_idx]\n",
        "    for m in train_meta: m['label'] = 0 if m['label']=='A' else 1\n",
        "    for m in test_meta:  m['label'] = 0 if m['label']=='A' else 1\n",
        "\n",
        "    # train_meta 안에서 validation split by subject\n",
        "    groups_train = [extract_subject_id(m) for m in train_meta]\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=SEED)\n",
        "    tr_idx2, val_idx = next(gss.split(train_meta, groups=groups_train))\n",
        "    val_meta   = [train_meta[i] for i in val_idx]\n",
        "    train_meta = [train_meta[i] for i in tr_idx2]\n",
        "\n",
        "    # DataLoader 준비\n",
        "    raw_train = EEGDataset(DATA_DIR, train_meta)\n",
        "    raw_val   = EEGDataset(DATA_DIR, val_meta)\n",
        "    raw_test  = EEGDataset(DATA_DIR, test_meta)\n",
        "    train_ds  = BinaryEEGDataset(raw_train, train_meta)\n",
        "    val_ds    = BinaryEEGDataset(raw_val,   val_meta)\n",
        "    test_ds   = BinaryEEGDataset(raw_test,  test_meta)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=4, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    # 모델/옵티마이저/스케줄러/손실 함수\n",
        "    input_len = train_ds[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=LR, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping 및 체크포인트 변수\n",
        "    best_val_loss = float('inf')\n",
        "    best_state    = None\n",
        "    es_count      = 0\n",
        "\n",
        "    # ─── 학습 + 검증 루프 ──────────────────────────────────────────\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = WD * (cur_lr / LR)\n",
        "            train_loss_sum += loss.item()\n",
        "        avg_train_loss = train_loss_sum / len(train_loader)\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_loss_sum = val_correct = val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                val_loss_sum += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                val_correct += (preds == y).sum().item()\n",
        "                val_total += y.size(0)\n",
        "        avg_val_loss = val_loss_sum / len(val_loader)\n",
        "        avg_val_acc  = val_correct / val_total if val_total>0 else 0.0\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(f\"  Fold {fold} Epoch {epoch}/{MAX_EPOCHS}  \"\n",
        "              f\"Train Loss={avg_train_loss:.4f}  \"\n",
        "              f\"Val Loss={avg_val_loss:.4f}  \"\n",
        "              f\"Val Acc={avg_val_acc:.4f}  \"\n",
        "              f\"Time={elapsed:.1f}s\")\n",
        "\n",
        "        # Early stopping & checkpoint on val_loss\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "            es_count      = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"  → Early stopping at epoch {epoch} (no val_loss improvement)\")\n",
        "                break\n",
        "\n",
        "    # 최적의 validation 시점 모델 로드\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # ─── 최종 테스트 & 혼동행렬 누적 ─────────────────────────────────\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            preds = model(X).argmax(1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(all_preds)\n",
        "    labs  = np.concatenate(all_labels)\n",
        "    tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "    total_tp += tp\n",
        "    total_tn += tn\n",
        "    total_fp += fp\n",
        "    total_fn += fn\n",
        "\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    print(f\"  Fold {fold} Final ACC = {acc:.4f}   (TP={tp}  TN={tn}  FP={fp}  FN={fn})\")\n",
        "\n",
        "    # 메모리 해제\n",
        "    del model, optimizer, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ─── 전체 LOSO 지표 계산 ─────────────────────────────────────────\n",
        "ACC  = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn)\n",
        "SENS = total_tp / (total_tp + total_fn)\n",
        "SPEC = total_tn / (total_tn + total_fp)\n",
        "F1   = 2 * total_tp / (2*total_tp + total_fp + total_fn)\n",
        "\n",
        "print(\"\\n=== LOSO 평가 결과 ===\")\n",
        "print(f\"Accuracy   = {ACC:.4%}\")\n",
        "print(f\"Sensitivity= {SENS:.4%}\")\n",
        "print(f\"Specificity= {SPEC:.4%}\")\n",
        "print(f\"F1-score   = {F1:.4%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oBJyQEFYoVdE",
        "outputId": "e0a078c6-13f8-4ac8-94d6-6efff851e42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            ">>> Fold 1/65  (leave out sub-001)\n",
            "  Fold 1 Epoch 1/150  Train Loss=0.7415  Val Loss=0.6998  Val Acc=0.4133  Time=19.5s\n",
            "  Fold 1 Epoch 2/150  Train Loss=0.7539  Val Loss=0.7006  Val Acc=0.4103  Time=18.6s\n",
            "  Fold 1 Epoch 3/150  Train Loss=0.7284  Val Loss=0.6975  Val Acc=0.4042  Time=18.8s\n",
            "  Fold 1 Epoch 4/150  Train Loss=0.7352  Val Loss=0.6868  Val Acc=0.5882  Time=18.7s\n",
            "  Fold 1 Epoch 5/150  Train Loss=0.7336  Val Loss=0.6915  Val Acc=0.5747  Time=18.8s\n",
            "  Fold 1 Epoch 6/150  Train Loss=0.7357  Val Loss=0.6831  Val Acc=0.5897  Time=18.6s\n",
            "  Fold 1 Epoch 7/150  Train Loss=0.7300  Val Loss=0.6843  Val Acc=0.5897  Time=18.7s\n",
            "  Fold 1 Epoch 8/150  Train Loss=0.7197  Val Loss=0.6828  Val Acc=0.5897  Time=18.7s\n",
            "  Fold 1 Epoch 9/150  Train Loss=0.7256  Val Loss=0.6858  Val Acc=0.5897  Time=18.8s\n",
            "  Fold 1 Epoch 10/150  Train Loss=0.7307  Val Loss=0.6821  Val Acc=0.5897  Time=18.6s\n",
            "  Fold 1 Epoch 11/150  Train Loss=0.7114  Val Loss=0.6832  Val Acc=0.5897  Time=18.8s\n",
            "  Fold 1 Epoch 12/150  Train Loss=0.7183  Val Loss=0.6970  Val Acc=0.4087  Time=18.5s\n",
            "  Fold 1 Epoch 13/150  Train Loss=0.7277  Val Loss=0.6874  Val Acc=0.5897  Time=18.6s\n",
            "  Fold 1 Epoch 14/150  Train Loss=0.7168  Val Loss=0.6861  Val Acc=0.5913  Time=18.6s\n",
            "  Fold 1 Epoch 15/150  Train Loss=0.7170  Val Loss=0.6829  Val Acc=0.5897  Time=18.6s\n",
            "  Fold 1 Epoch 16/150  Train Loss=0.7183  Val Loss=0.6824  Val Acc=0.5897  Time=18.8s\n",
            "  Fold 1 Epoch 17/150  Train Loss=0.7155  Val Loss=0.6843  Val Acc=0.5913  Time=18.6s\n",
            "  Fold 1 Epoch 18/150  Train Loss=0.7164  Val Loss=0.6870  Val Acc=0.6003  Time=18.6s\n",
            "  Fold 1 Epoch 19/150  Train Loss=0.7223  Val Loss=0.6869  Val Acc=0.5822  Time=18.6s\n",
            "  Fold 1 Epoch 20/150  Train Loss=0.7083  Val Loss=0.6664  Val Acc=0.6440  Time=18.7s\n",
            "  Fold 1 Epoch 21/150  Train Loss=0.6894  Val Loss=0.6119  Val Acc=0.6652  Time=18.6s\n",
            "  Fold 1 Epoch 22/150  Train Loss=0.6274  Val Loss=0.5780  Val Acc=0.7179  Time=18.9s\n",
            "  Fold 1 Epoch 23/150  Train Loss=0.5846  Val Loss=0.5624  Val Acc=0.7179  Time=18.5s\n",
            "  Fold 1 Epoch 24/150  Train Loss=0.5599  Val Loss=0.5076  Val Acc=0.7345  Time=18.9s\n",
            "  Fold 1 Epoch 25/150  Train Loss=0.5323  Val Loss=0.5600  Val Acc=0.7406  Time=18.6s\n",
            "  Fold 1 Epoch 26/150  Train Loss=0.5214  Val Loss=0.5345  Val Acc=0.7421  Time=18.8s\n",
            "  Fold 1 Epoch 27/150  Train Loss=0.5020  Val Loss=0.5107  Val Acc=0.7526  Time=18.6s\n",
            "  Fold 1 Epoch 28/150  Train Loss=0.4837  Val Loss=0.5002  Val Acc=0.7541  Time=18.8s\n",
            "  Fold 1 Epoch 29/150  Train Loss=0.4795  Val Loss=0.4900  Val Acc=0.7511  Time=18.7s\n",
            "  Fold 1 Epoch 30/150  Train Loss=0.4797  Val Loss=0.5600  Val Acc=0.7240  Time=18.7s\n",
            "  Fold 1 Epoch 31/150  Train Loss=0.4584  Val Loss=0.4925  Val Acc=0.7466  Time=18.8s\n",
            "  Fold 1 Epoch 32/150  Train Loss=0.4494  Val Loss=0.4614  Val Acc=0.7602  Time=18.6s\n",
            "  Fold 1 Epoch 33/150  Train Loss=0.4548  Val Loss=0.4690  Val Acc=0.7602  Time=18.6s\n",
            "  Fold 1 Epoch 34/150  Train Loss=0.4189  Val Loss=0.4789  Val Acc=0.7632  Time=18.6s\n",
            "  Fold 1 Epoch 35/150  Train Loss=0.4098  Val Loss=0.4453  Val Acc=0.7798  Time=18.7s\n",
            "  Fold 1 Epoch 36/150  Train Loss=0.4130  Val Loss=0.4692  Val Acc=0.7722  Time=18.5s\n",
            "  Fold 1 Epoch 37/150  Train Loss=0.3955  Val Loss=0.4475  Val Acc=0.7858  Time=18.8s\n",
            "  Fold 1 Epoch 38/150  Train Loss=0.3967  Val Loss=0.5107  Val Acc=0.7179  Time=18.6s\n",
            "  Fold 1 Epoch 39/150  Train Loss=0.3835  Val Loss=0.4416  Val Acc=0.7873  Time=18.8s\n",
            "  Fold 1 Epoch 40/150  Train Loss=0.3709  Val Loss=0.4765  Val Acc=0.7783  Time=18.6s\n",
            "  Fold 1 Epoch 41/150  Train Loss=0.3750  Val Loss=0.4256  Val Acc=0.8130  Time=18.9s\n",
            "  Fold 1 Epoch 42/150  Train Loss=0.3562  Val Loss=0.4478  Val Acc=0.7798  Time=18.6s\n",
            "  Fold 1 Epoch 43/150  Train Loss=0.3501  Val Loss=0.5545  Val Acc=0.6968  Time=18.6s\n",
            "  Fold 1 Epoch 44/150  Train Loss=0.3379  Val Loss=0.4327  Val Acc=0.8130  Time=18.7s\n",
            "  Fold 1 Epoch 45/150  Train Loss=0.3358  Val Loss=0.4490  Val Acc=0.7949  Time=18.5s\n",
            "  Fold 1 Epoch 46/150  Train Loss=0.3213  Val Loss=0.4463  Val Acc=0.8009  Time=18.7s\n",
            "  Fold 1 Epoch 47/150  Train Loss=0.3180  Val Loss=0.4493  Val Acc=0.7903  Time=18.6s\n",
            "  Fold 1 Epoch 48/150  Train Loss=0.3192  Val Loss=0.4405  Val Acc=0.8024  Time=18.9s\n",
            "  Fold 1 Epoch 49/150  Train Loss=0.3120  Val Loss=0.4438  Val Acc=0.8175  Time=18.5s\n",
            "  Fold 1 Epoch 50/150  Train Loss=0.3110  Val Loss=0.4623  Val Acc=0.8100  Time=18.8s\n",
            "  Fold 1 Epoch 51/150  Train Loss=0.3179  Val Loss=0.4507  Val Acc=0.8205  Time=18.4s\n",
            "  Fold 1 Epoch 52/150  Train Loss=0.2937  Val Loss=0.4356  Val Acc=0.8039  Time=19.0s\n",
            "  Fold 1 Epoch 53/150  Train Loss=0.2871  Val Loss=0.4294  Val Acc=0.8235  Time=18.5s\n",
            "  Fold 1 Epoch 54/150  Train Loss=0.3036  Val Loss=0.4856  Val Acc=0.7662  Time=18.8s\n",
            "  Fold 1 Epoch 55/150  Train Loss=0.2929  Val Loss=0.4390  Val Acc=0.8130  Time=18.6s\n",
            "  Fold 1 Epoch 56/150  Train Loss=0.2793  Val Loss=0.4961  Val Acc=0.7572  Time=18.7s\n",
            "  → Early stopping at epoch 56 (no val_loss improvement)\n",
            "  Fold 1 Final ACC = 0.9157   (TP=0  TN=76  FP=7  FN=0)\n",
            "\n",
            ">>> Fold 2/65  (leave out sub-002)\n",
            "  Fold 2 Epoch 1/150  Train Loss=0.7705  Val Loss=0.7123  Val Acc=0.4304  Time=18.7s\n",
            "  Fold 2 Epoch 2/150  Train Loss=0.7501  Val Loss=0.7045  Val Acc=0.4304  Time=18.8s\n",
            "  Fold 2 Epoch 3/150  Train Loss=0.7375  Val Loss=0.7041  Val Acc=0.4304  Time=18.7s\n",
            "  Fold 2 Epoch 4/150  Train Loss=0.7375  Val Loss=0.6999  Val Acc=0.4304  Time=18.7s\n",
            "  Fold 2 Epoch 5/150  Train Loss=0.7326  Val Loss=0.6974  Val Acc=0.4272  Time=18.9s\n",
            "  Fold 2 Epoch 6/150  Train Loss=0.7250  Val Loss=0.6920  Val Acc=0.5364  Time=18.7s\n",
            "  Fold 2 Epoch 7/150  Train Loss=0.7136  Val Loss=0.6895  Val Acc=0.5680  Time=18.8s\n",
            "  Fold 2 Epoch 8/150  Train Loss=0.7099  Val Loss=0.6887  Val Acc=0.5696  Time=18.7s\n",
            "  Fold 2 Epoch 9/150  Train Loss=0.7159  Val Loss=0.6905  Val Acc=0.5665  Time=18.9s\n",
            "  Fold 2 Epoch 10/150  Train Loss=0.7174  Val Loss=0.6926  Val Acc=0.5111  Time=18.8s\n",
            "  Fold 2 Epoch 11/150  Train Loss=0.7184  Val Loss=0.6868  Val Acc=0.5696  Time=18.8s\n",
            "  Fold 2 Epoch 12/150  Train Loss=0.7071  Val Loss=0.6867  Val Acc=0.5696  Time=18.6s\n",
            "  Fold 2 Epoch 13/150  Train Loss=0.7133  Val Loss=0.6866  Val Acc=0.5696  Time=18.9s\n",
            "  Fold 2 Epoch 14/150  Train Loss=0.7073  Val Loss=0.6825  Val Acc=0.5696  Time=18.7s\n",
            "  Fold 2 Epoch 15/150  Train Loss=0.7021  Val Loss=0.6736  Val Acc=0.6028  Time=19.0s\n",
            "  Fold 2 Epoch 16/150  Train Loss=0.6900  Val Loss=0.6499  Val Acc=0.6377  Time=18.9s\n",
            "  Fold 2 Epoch 17/150  Train Loss=0.6513  Val Loss=0.6258  Val Acc=0.6566  Time=18.8s\n",
            "  Fold 2 Epoch 18/150  Train Loss=0.6312  Val Loss=0.5816  Val Acc=0.7278  Time=18.7s\n",
            "  Fold 2 Epoch 19/150  Train Loss=0.5914  Val Loss=0.5540  Val Acc=0.7358  Time=18.8s\n",
            "  Fold 2 Epoch 20/150  Train Loss=0.5751  Val Loss=0.5392  Val Acc=0.7453  Time=18.7s\n",
            "  Fold 2 Epoch 21/150  Train Loss=0.5580  Val Loss=0.5176  Val Acc=0.7500  Time=18.9s\n",
            "  Fold 2 Epoch 22/150  Train Loss=0.5445  Val Loss=0.5109  Val Acc=0.7547  Time=18.6s\n",
            "  Fold 2 Epoch 23/150  Train Loss=0.5324  Val Loss=0.4920  Val Acc=0.7658  Time=18.7s\n",
            "  Fold 2 Epoch 24/150  Train Loss=0.5241  Val Loss=0.4781  Val Acc=0.7801  Time=18.8s\n",
            "  Fold 2 Epoch 25/150  Train Loss=0.5181  Val Loss=0.4956  Val Acc=0.7500  Time=18.7s\n",
            "  Fold 2 Epoch 26/150  Train Loss=0.5074  Val Loss=0.4592  Val Acc=0.7959  Time=18.8s\n",
            "  Fold 2 Epoch 27/150  Train Loss=0.5013  Val Loss=0.4577  Val Acc=0.7959  Time=18.6s\n",
            "  Fold 2 Epoch 28/150  Train Loss=0.4869  Val Loss=0.4518  Val Acc=0.8038  Time=18.8s\n",
            "  Fold 2 Epoch 29/150  Train Loss=0.4785  Val Loss=0.4469  Val Acc=0.8022  Time=18.6s\n",
            "  Fold 2 Epoch 30/150  Train Loss=0.4644  Val Loss=0.4366  Val Acc=0.8117  Time=19.0s\n",
            "  Fold 2 Epoch 31/150  Train Loss=0.4525  Val Loss=0.4419  Val Acc=0.8165  Time=18.8s\n",
            "  Fold 2 Epoch 32/150  Train Loss=0.4428  Val Loss=0.5110  Val Acc=0.7247  Time=18.9s\n",
            "  Fold 2 Epoch 33/150  Train Loss=0.4346  Val Loss=0.4808  Val Acc=0.7547  Time=18.7s\n",
            "  Fold 2 Epoch 34/150  Train Loss=0.4116  Val Loss=0.4506  Val Acc=0.7753  Time=18.9s\n",
            "  Fold 2 Epoch 35/150  Train Loss=0.3969  Val Loss=0.4589  Val Acc=0.7785  Time=18.7s\n",
            "  Fold 2 Epoch 36/150  Train Loss=0.4105  Val Loss=0.5951  Val Acc=0.6677  Time=18.7s\n",
            "  Fold 2 Epoch 37/150  Train Loss=0.3912  Val Loss=0.4443  Val Acc=0.7896  Time=18.7s\n",
            "  Fold 2 Epoch 38/150  Train Loss=0.3684  Val Loss=0.4970  Val Acc=0.7405  Time=18.9s\n",
            "  Fold 2 Epoch 39/150  Train Loss=0.3664  Val Loss=0.5207  Val Acc=0.7152  Time=18.9s\n",
            "  Fold 2 Epoch 40/150  Train Loss=0.3622  Val Loss=0.4538  Val Acc=0.7848  Time=18.8s\n",
            "  Fold 2 Epoch 41/150  Train Loss=0.3488  Val Loss=0.5099  Val Acc=0.7294  Time=18.7s\n",
            "  Fold 2 Epoch 42/150  Train Loss=0.3418  Val Loss=0.4628  Val Acc=0.7864  Time=18.8s\n",
            "  Fold 2 Epoch 43/150  Train Loss=0.3329  Val Loss=0.4432  Val Acc=0.7943  Time=18.8s\n",
            "  Fold 2 Epoch 44/150  Train Loss=0.3261  Val Loss=0.4228  Val Acc=0.8165  Time=18.7s\n",
            "  Fold 2 Epoch 45/150  Train Loss=0.3127  Val Loss=0.5045  Val Acc=0.7421  Time=18.8s\n",
            "  Fold 2 Epoch 46/150  Train Loss=0.3105  Val Loss=0.5720  Val Acc=0.7041  Time=18.7s\n",
            "  Fold 2 Epoch 47/150  Train Loss=0.3189  Val Loss=0.4673  Val Acc=0.7737  Time=18.8s\n",
            "  Fold 2 Epoch 48/150  Train Loss=0.3175  Val Loss=0.4504  Val Acc=0.7927  Time=18.7s\n",
            "  Fold 2 Epoch 49/150  Train Loss=0.2912  Val Loss=0.5253  Val Acc=0.7421  Time=19.0s\n",
            "  Fold 2 Epoch 50/150  Train Loss=0.2967  Val Loss=0.5734  Val Acc=0.7278  Time=18.7s\n",
            "  Fold 2 Epoch 51/150  Train Loss=0.2976  Val Loss=0.4827  Val Acc=0.7816  Time=18.9s\n",
            "  Fold 2 Epoch 52/150  Train Loss=0.2695  Val Loss=0.5324  Val Acc=0.7373  Time=18.7s\n",
            "  Fold 2 Epoch 53/150  Train Loss=0.2820  Val Loss=0.4724  Val Acc=0.7864  Time=19.2s\n",
            "  Fold 2 Epoch 54/150  Train Loss=0.2744  Val Loss=0.5131  Val Acc=0.7658  Time=19.0s\n",
            "  Fold 2 Epoch 55/150  Train Loss=0.2581  Val Loss=0.5631  Val Acc=0.7373  Time=19.0s\n",
            "  Fold 2 Epoch 56/150  Train Loss=0.2685  Val Loss=0.5077  Val Acc=0.7722  Time=18.5s\n",
            "  Fold 2 Epoch 57/150  Train Loss=0.2599  Val Loss=0.4935  Val Acc=0.7816  Time=18.8s\n",
            "  Fold 2 Epoch 58/150  Train Loss=0.2630  Val Loss=0.5438  Val Acc=0.7579  Time=19.0s\n",
            "  Fold 2 Epoch 59/150  Train Loss=0.2479  Val Loss=0.5425  Val Acc=0.7532  Time=18.8s\n",
            "  → Early stopping at epoch 59 (no val_loss improvement)\n",
            "  Fold 2 Final ACC = 0.4730   (TP=0  TN=35  FP=39  FN=0)\n",
            "\n",
            ">>> Fold 3/65  (leave out sub-003)\n",
            "  Fold 3 Epoch 1/150  Train Loss=0.7772  Val Loss=0.6922  Val Acc=0.5203  Time=18.9s\n",
            "  Fold 3 Epoch 2/150  Train Loss=0.7491  Val Loss=0.6858  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 3 Epoch 3/150  Train Loss=0.7633  Val Loss=0.6819  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 3 Epoch 4/150  Train Loss=0.7600  Val Loss=0.6790  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 3 Epoch 5/150  Train Loss=0.7484  Val Loss=0.6793  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 3 Epoch 6/150  Train Loss=0.7433  Val Loss=0.6785  Val Acc=0.5844  Time=18.6s\n",
            "  Fold 3 Epoch 7/150  Train Loss=0.7472  Val Loss=0.6798  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 3 Epoch 8/150  Train Loss=0.7447  Val Loss=0.6785  Val Acc=0.5844  Time=18.7s\n",
            "  Fold 3 Epoch 9/150  Train Loss=0.7366  Val Loss=0.6778  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 3 Epoch 10/150  Train Loss=0.7363  Val Loss=0.6779  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 3 Epoch 11/150  Train Loss=0.7350  Val Loss=0.6781  Val Acc=0.5844  Time=19.2s\n",
            "  Fold 3 Epoch 12/150  Train Loss=0.7382  Val Loss=0.6775  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 3 Epoch 13/150  Train Loss=0.7436  Val Loss=0.6767  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 3 Epoch 14/150  Train Loss=0.7372  Val Loss=0.6827  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 3 Epoch 15/150  Train Loss=0.7266  Val Loss=0.6767  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 3 Epoch 16/150  Train Loss=0.7292  Val Loss=0.6790  Val Acc=0.5844  Time=19.2s\n",
            "  Fold 3 Epoch 17/150  Train Loss=0.7310  Val Loss=0.6779  Val Acc=0.5844  Time=19.3s\n",
            "  Fold 3 Epoch 18/150  Train Loss=0.7243  Val Loss=0.6794  Val Acc=0.5844  Time=19.3s\n",
            "  Fold 3 Epoch 19/150  Train Loss=0.7120  Val Loss=0.6756  Val Acc=0.5875  Time=19.5s\n",
            "  Fold 3 Epoch 20/150  Train Loss=0.7297  Val Loss=0.6681  Val Acc=0.5844  Time=19.6s\n",
            "  Fold 3 Epoch 21/150  Train Loss=0.7134  Val Loss=0.6463  Val Acc=0.6406  Time=19.4s\n",
            "  Fold 3 Epoch 22/150  Train Loss=0.7032  Val Loss=0.5684  Val Acc=0.7594  Time=19.5s\n",
            "  Fold 3 Epoch 23/150  Train Loss=0.6624  Val Loss=0.5404  Val Acc=0.7828  Time=19.6s\n",
            "  Fold 3 Epoch 24/150  Train Loss=0.6414  Val Loss=0.4759  Val Acc=0.8359  Time=19.6s\n",
            "  Fold 3 Epoch 25/150  Train Loss=0.6084  Val Loss=0.4550  Val Acc=0.8656  Time=19.6s\n",
            "  Fold 3 Epoch 26/150  Train Loss=0.5824  Val Loss=0.4356  Val Acc=0.8703  Time=19.7s\n",
            "  Fold 3 Epoch 27/150  Train Loss=0.5512  Val Loss=0.4565  Val Acc=0.8750  Time=19.4s\n",
            "  Fold 3 Epoch 28/150  Train Loss=0.5426  Val Loss=0.4517  Val Acc=0.8156  Time=19.7s\n",
            "  Fold 3 Epoch 29/150  Train Loss=0.5321  Val Loss=0.3626  Val Acc=0.8891  Time=19.6s\n",
            "  Fold 3 Epoch 30/150  Train Loss=0.5060  Val Loss=0.3840  Val Acc=0.8656  Time=19.6s\n",
            "  Fold 3 Epoch 31/150  Train Loss=0.5174  Val Loss=0.4426  Val Acc=0.7922  Time=19.5s\n",
            "  Fold 3 Epoch 32/150  Train Loss=0.4964  Val Loss=0.4163  Val Acc=0.8078  Time=19.5s\n",
            "  Fold 3 Epoch 33/150  Train Loss=0.4824  Val Loss=0.3843  Val Acc=0.8625  Time=19.5s\n",
            "  Fold 3 Epoch 34/150  Train Loss=0.4664  Val Loss=0.4288  Val Acc=0.7672  Time=19.6s\n",
            "  Fold 3 Epoch 35/150  Train Loss=0.4584  Val Loss=0.3587  Val Acc=0.8516  Time=19.7s\n",
            "  Fold 3 Epoch 36/150  Train Loss=0.4521  Val Loss=0.4459  Val Acc=0.7516  Time=19.5s\n",
            "  Fold 3 Epoch 37/150  Train Loss=0.4341  Val Loss=0.3917  Val Acc=0.8234  Time=19.7s\n",
            "  Fold 3 Epoch 38/150  Train Loss=0.4272  Val Loss=0.3596  Val Acc=0.8641  Time=19.6s\n",
            "  Fold 3 Epoch 39/150  Train Loss=0.4218  Val Loss=0.4245  Val Acc=0.7719  Time=19.7s\n",
            "  Fold 3 Epoch 40/150  Train Loss=0.4049  Val Loss=0.3670  Val Acc=0.8359  Time=19.5s\n",
            "  Fold 3 Epoch 41/150  Train Loss=0.3935  Val Loss=0.4032  Val Acc=0.7969  Time=19.4s\n",
            "  Fold 3 Epoch 42/150  Train Loss=0.3818  Val Loss=0.3414  Val Acc=0.8719  Time=18.7s\n",
            "  Fold 3 Epoch 43/150  Train Loss=0.3746  Val Loss=0.3813  Val Acc=0.8219  Time=19.1s\n",
            "  Fold 3 Epoch 44/150  Train Loss=0.3665  Val Loss=0.3537  Val Acc=0.8516  Time=18.7s\n",
            "  Fold 3 Epoch 45/150  Train Loss=0.3527  Val Loss=0.3987  Val Acc=0.8172  Time=19.0s\n",
            "  Fold 3 Epoch 46/150  Train Loss=0.3511  Val Loss=0.3642  Val Acc=0.8422  Time=18.7s\n",
            "  Fold 3 Epoch 47/150  Train Loss=0.3550  Val Loss=0.4169  Val Acc=0.8000  Time=18.9s\n",
            "  Fold 3 Epoch 48/150  Train Loss=0.3379  Val Loss=0.3705  Val Acc=0.8344  Time=18.7s\n",
            "  Fold 3 Epoch 49/150  Train Loss=0.3425  Val Loss=0.5402  Val Acc=0.7203  Time=18.9s\n",
            "  Fold 3 Epoch 50/150  Train Loss=0.3244  Val Loss=0.4184  Val Acc=0.8000  Time=18.9s\n",
            "  Fold 3 Epoch 51/150  Train Loss=0.3322  Val Loss=0.6115  Val Acc=0.6687  Time=18.6s\n",
            "  Fold 3 Epoch 52/150  Train Loss=0.3165  Val Loss=0.4545  Val Acc=0.7828  Time=18.9s\n",
            "  Fold 3 Epoch 53/150  Train Loss=0.3154  Val Loss=0.4363  Val Acc=0.8094  Time=18.8s\n",
            "  Fold 3 Epoch 54/150  Train Loss=0.3155  Val Loss=0.4723  Val Acc=0.7719  Time=18.9s\n",
            "  Fold 3 Epoch 55/150  Train Loss=0.3163  Val Loss=0.3999  Val Acc=0.8234  Time=18.7s\n",
            "  Fold 3 Epoch 56/150  Train Loss=0.3040  Val Loss=0.4586  Val Acc=0.7937  Time=18.9s\n",
            "  Fold 3 Epoch 57/150  Train Loss=0.3000  Val Loss=0.4671  Val Acc=0.7875  Time=18.7s\n",
            "  → Early stopping at epoch 57 (no val_loss improvement)\n",
            "  Fold 3 Final ACC = 0.4844   (TP=31  TN=0  FP=0  FN=33)\n",
            "\n",
            ">>> Fold 4/65  (leave out sub-004)\n",
            "  Fold 4 Epoch 1/150  Train Loss=0.7542  Val Loss=0.6869  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 2/150  Train Loss=0.7497  Val Loss=0.6870  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 3/150  Train Loss=0.7387  Val Loss=0.6867  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 4/150  Train Loss=0.7371  Val Loss=0.6882  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 5/150  Train Loss=0.7350  Val Loss=0.6876  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 6/150  Train Loss=0.7241  Val Loss=0.6876  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 7/150  Train Loss=0.7210  Val Loss=0.6867  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 8/150  Train Loss=0.7139  Val Loss=0.6879  Val Acc=0.5835  Time=18.7s\n",
            "  Fold 4 Epoch 9/150  Train Loss=0.7045  Val Loss=0.6902  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 10/150  Train Loss=0.7120  Val Loss=0.6896  Val Acc=0.5835  Time=18.6s\n",
            "  Fold 4 Epoch 11/150  Train Loss=0.6998  Val Loss=0.6900  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 12/150  Train Loss=0.6979  Val Loss=0.6889  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 13/150  Train Loss=0.7017  Val Loss=0.6900  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 4 Epoch 14/150  Train Loss=0.7039  Val Loss=0.6896  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 15/150  Train Loss=0.7034  Val Loss=0.6888  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 16/150  Train Loss=0.7052  Val Loss=0.6888  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 17/150  Train Loss=0.7118  Val Loss=0.6883  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 18/150  Train Loss=0.7084  Val Loss=0.6891  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 19/150  Train Loss=0.7095  Val Loss=0.6862  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 20/150  Train Loss=0.6960  Val Loss=0.6906  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 21/150  Train Loss=0.7008  Val Loss=0.6853  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 22/150  Train Loss=0.6994  Val Loss=0.6779  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 4 Epoch 23/150  Train Loss=0.6996  Val Loss=0.6830  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 4 Epoch 24/150  Train Loss=0.6736  Val Loss=0.6374  Val Acc=0.6412  Time=18.8s\n",
            "  Fold 4 Epoch 25/150  Train Loss=0.6260  Val Loss=0.5376  Val Acc=0.7582  Time=18.8s\n",
            "  Fold 4 Epoch 26/150  Train Loss=0.5777  Val Loss=0.5147  Val Acc=0.7363  Time=18.8s\n",
            "  Fold 4 Epoch 27/150  Train Loss=0.5587  Val Loss=0.4828  Val Acc=0.8159  Time=18.7s\n",
            "  Fold 4 Epoch 28/150  Train Loss=0.5352  Val Loss=0.4454  Val Acc=0.8222  Time=18.9s\n",
            "  Fold 4 Epoch 29/150  Train Loss=0.5171  Val Loss=0.4353  Val Acc=0.8315  Time=18.7s\n",
            "  Fold 4 Epoch 30/150  Train Loss=0.5009  Val Loss=0.4204  Val Acc=0.8190  Time=18.9s\n",
            "  Fold 4 Epoch 31/150  Train Loss=0.4762  Val Loss=0.4247  Val Acc=0.8175  Time=18.8s\n",
            "  Fold 4 Epoch 32/150  Train Loss=0.4736  Val Loss=0.4118  Val Acc=0.8159  Time=18.9s\n",
            "  Fold 4 Epoch 33/150  Train Loss=0.4569  Val Loss=0.4145  Val Acc=0.8362  Time=18.8s\n",
            "  Fold 4 Epoch 34/150  Train Loss=0.4376  Val Loss=0.4243  Val Acc=0.8159  Time=19.0s\n",
            "  Fold 4 Epoch 35/150  Train Loss=0.4235  Val Loss=0.3983  Val Acc=0.8393  Time=18.7s\n",
            "  Fold 4 Epoch 36/150  Train Loss=0.4161  Val Loss=0.3944  Val Acc=0.8300  Time=19.0s\n",
            "  Fold 4 Epoch 37/150  Train Loss=0.4015  Val Loss=0.5131  Val Acc=0.7020  Time=18.7s\n",
            "  Fold 4 Epoch 38/150  Train Loss=0.3902  Val Loss=0.4072  Val Acc=0.8112  Time=18.8s\n",
            "  Fold 4 Epoch 39/150  Train Loss=0.3656  Val Loss=0.4988  Val Acc=0.7285  Time=18.9s\n",
            "  Fold 4 Epoch 40/150  Train Loss=0.3629  Val Loss=0.4019  Val Acc=0.8175  Time=18.9s\n",
            "  Fold 4 Epoch 41/150  Train Loss=0.3652  Val Loss=0.4774  Val Acc=0.7426  Time=18.7s\n",
            "  Fold 4 Epoch 42/150  Train Loss=0.3525  Val Loss=0.4157  Val Acc=0.8144  Time=18.8s\n",
            "  Fold 4 Epoch 43/150  Train Loss=0.3501  Val Loss=0.4049  Val Acc=0.8190  Time=18.8s\n",
            "  Fold 4 Epoch 44/150  Train Loss=0.3370  Val Loss=0.4272  Val Acc=0.7988  Time=18.9s\n",
            "  Fold 4 Epoch 45/150  Train Loss=0.3279  Val Loss=0.4075  Val Acc=0.8159  Time=18.8s\n",
            "  Fold 4 Epoch 46/150  Train Loss=0.3351  Val Loss=0.4218  Val Acc=0.8066  Time=18.8s\n",
            "  Fold 4 Epoch 47/150  Train Loss=0.3225  Val Loss=0.5361  Val Acc=0.7270  Time=18.8s\n",
            "  Fold 4 Epoch 48/150  Train Loss=0.3131  Val Loss=0.6368  Val Acc=0.6739  Time=18.8s\n",
            "  Fold 4 Epoch 49/150  Train Loss=0.3091  Val Loss=0.4432  Val Acc=0.7894  Time=18.6s\n",
            "  Fold 4 Epoch 50/150  Train Loss=0.2985  Val Loss=0.4539  Val Acc=0.7910  Time=18.8s\n",
            "  Fold 4 Epoch 51/150  Train Loss=0.3016  Val Loss=0.4722  Val Acc=0.7754  Time=19.0s\n",
            "  → Early stopping at epoch 51 (no val_loss improvement)\n",
            "  Fold 4 Final ACC = 0.9839   (TP=61  TN=0  FP=0  FN=1)\n",
            "\n",
            ">>> Fold 5/65  (leave out sub-005)\n",
            "  Fold 5 Epoch 1/150  Train Loss=0.7479  Val Loss=0.6876  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 2/150  Train Loss=0.7276  Val Loss=0.6878  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 3/150  Train Loss=0.7353  Val Loss=0.6874  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 4/150  Train Loss=0.7432  Val Loss=0.6873  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 5/150  Train Loss=0.7230  Val Loss=0.6867  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 6/150  Train Loss=0.7354  Val Loss=0.6869  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 5 Epoch 7/150  Train Loss=0.7163  Val Loss=0.6881  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 8/150  Train Loss=0.7151  Val Loss=0.6886  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 5 Epoch 9/150  Train Loss=0.7302  Val Loss=0.6879  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 10/150  Train Loss=0.7239  Val Loss=0.6897  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 5 Epoch 11/150  Train Loss=0.7154  Val Loss=0.6876  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 12/150  Train Loss=0.7197  Val Loss=0.6877  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 13/150  Train Loss=0.7203  Val Loss=0.6878  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 5 Epoch 14/150  Train Loss=0.7128  Val Loss=0.6878  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 15/150  Train Loss=0.7340  Val Loss=0.6879  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 5 Epoch 16/150  Train Loss=0.7233  Val Loss=0.6895  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 17/150  Train Loss=0.7079  Val Loss=0.6875  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 18/150  Train Loss=0.7186  Val Loss=0.6873  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 5 Epoch 19/150  Train Loss=0.7162  Val Loss=0.6868  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 5 Epoch 20/150  Train Loss=0.7122  Val Loss=0.6877  Val Acc=0.5789  Time=18.7s\n",
            "  → Early stopping at epoch 20 (no val_loss improvement)\n",
            "  Fold 5 Final ACC = 0.0000   (TP=0  TN=0  FP=0  FN=62)\n",
            "\n",
            ">>> Fold 6/65  (leave out sub-006)\n",
            "  Fold 6 Epoch 1/150  Train Loss=0.7205  Val Loss=0.6831  Val Acc=0.5737  Time=18.7s\n",
            "  Fold 6 Epoch 2/150  Train Loss=0.7310  Val Loss=0.6832  Val Acc=0.5737  Time=18.8s\n",
            "  Fold 6 Epoch 3/150  Train Loss=0.7318  Val Loss=0.6838  Val Acc=0.5737  Time=18.7s\n",
            "  Fold 6 Epoch 4/150  Train Loss=0.7308  Val Loss=0.6837  Val Acc=0.5737  Time=18.8s\n",
            "  Fold 6 Epoch 5/150  Train Loss=0.7180  Val Loss=0.6842  Val Acc=0.5737  Time=18.7s\n",
            "  Fold 6 Epoch 6/150  Train Loss=0.7277  Val Loss=0.6841  Val Acc=0.5737  Time=18.7s\n",
            "  Fold 6 Epoch 7/150  Train Loss=0.7295  Val Loss=0.6834  Val Acc=0.5737  Time=18.9s\n",
            "  Fold 6 Epoch 8/150  Train Loss=0.7232  Val Loss=0.6834  Val Acc=0.5737  Time=18.7s\n",
            "  Fold 6 Epoch 9/150  Train Loss=0.7204  Val Loss=0.6840  Val Acc=0.5737  Time=18.9s\n",
            "  Fold 6 Epoch 10/150  Train Loss=0.7205  Val Loss=0.6857  Val Acc=0.5737  Time=18.8s\n",
            "  Fold 6 Epoch 11/150  Train Loss=0.7245  Val Loss=0.6839  Val Acc=0.5737  Time=19.0s\n",
            "  Fold 6 Epoch 12/150  Train Loss=0.7204  Val Loss=0.6843  Val Acc=0.5737  Time=18.9s\n",
            "  Fold 6 Epoch 13/150  Train Loss=0.7224  Val Loss=0.6845  Val Acc=0.5737  Time=19.0s\n",
            "  Fold 6 Epoch 14/150  Train Loss=0.7258  Val Loss=0.6848  Val Acc=0.5737  Time=18.8s\n",
            "  Fold 6 Epoch 15/150  Train Loss=0.7227  Val Loss=0.6838  Val Acc=0.5737  Time=18.8s\n",
            "  Fold 6 Epoch 16/150  Train Loss=0.7171  Val Loss=0.6832  Val Acc=0.5737  Time=18.9s\n",
            "  → Early stopping at epoch 16 (no val_loss improvement)\n",
            "  Fold 6 Final ACC = 1.0000   (TP=0  TN=62  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 7/65  (leave out sub-007)\n",
            "  Fold 7 Epoch 1/150  Train Loss=0.7591  Val Loss=0.7187  Val Acc=0.4084  Time=19.1s\n",
            "  Fold 7 Epoch 2/150  Train Loss=0.7386  Val Loss=0.7133  Val Acc=0.4084  Time=18.7s\n",
            "  Fold 7 Epoch 3/150  Train Loss=0.7435  Val Loss=0.7040  Val Acc=0.4069  Time=19.0s\n",
            "  Fold 7 Epoch 4/150  Train Loss=0.7370  Val Loss=0.7026  Val Acc=0.4099  Time=18.7s\n",
            "  Fold 7 Epoch 5/150  Train Loss=0.7348  Val Loss=0.6944  Val Acc=0.4474  Time=18.9s\n",
            "  Fold 7 Epoch 6/150  Train Loss=0.7229  Val Loss=0.6913  Val Acc=0.5676  Time=19.0s\n",
            "  Fold 7 Epoch 7/150  Train Loss=0.7331  Val Loss=0.6914  Val Acc=0.5676  Time=19.0s\n",
            "  Fold 7 Epoch 8/150  Train Loss=0.7271  Val Loss=0.6909  Val Acc=0.5616  Time=18.8s\n",
            "  Fold 7 Epoch 9/150  Train Loss=0.7279  Val Loss=0.6887  Val Acc=0.6021  Time=18.9s\n",
            "  Fold 7 Epoch 10/150  Train Loss=0.7294  Val Loss=0.6863  Val Acc=0.5946  Time=18.9s\n",
            "  Fold 7 Epoch 11/150  Train Loss=0.7216  Val Loss=0.6908  Val Acc=0.5360  Time=18.8s\n",
            "  Fold 7 Epoch 12/150  Train Loss=0.7157  Val Loss=0.6900  Val Acc=0.6426  Time=18.8s\n",
            "  Fold 7 Epoch 13/150  Train Loss=0.7155  Val Loss=0.6837  Val Acc=0.6366  Time=18.9s\n",
            "  Fold 7 Epoch 14/150  Train Loss=0.7206  Val Loss=0.6807  Val Acc=0.6667  Time=18.8s\n",
            "  Fold 7 Epoch 15/150  Train Loss=0.7051  Val Loss=0.6326  Val Acc=0.6592  Time=18.7s\n",
            "  Fold 7 Epoch 16/150  Train Loss=0.6821  Val Loss=0.6966  Val Acc=0.4294  Time=19.0s\n",
            "  Fold 7 Epoch 17/150  Train Loss=0.6360  Val Loss=0.6376  Val Acc=0.7012  Time=18.8s\n",
            "  Fold 7 Epoch 18/150  Train Loss=0.6026  Val Loss=0.6173  Val Acc=0.7042  Time=18.9s\n",
            "  Fold 7 Epoch 19/150  Train Loss=0.5786  Val Loss=0.5589  Val Acc=0.7072  Time=18.7s\n",
            "  Fold 7 Epoch 20/150  Train Loss=0.5683  Val Loss=0.6044  Val Acc=0.7012  Time=18.9s\n",
            "  Fold 7 Epoch 21/150  Train Loss=0.5570  Val Loss=0.5331  Val Acc=0.7147  Time=18.9s\n",
            "  Fold 7 Epoch 22/150  Train Loss=0.5383  Val Loss=0.5379  Val Acc=0.7177  Time=19.0s\n",
            "  Fold 7 Epoch 23/150  Train Loss=0.5166  Val Loss=0.5539  Val Acc=0.7117  Time=18.8s\n",
            "  Fold 7 Epoch 24/150  Train Loss=0.5107  Val Loss=0.5307  Val Acc=0.7147  Time=19.0s\n",
            "  Fold 7 Epoch 25/150  Train Loss=0.4963  Val Loss=0.5146  Val Acc=0.7297  Time=18.7s\n",
            "  Fold 7 Epoch 26/150  Train Loss=0.4927  Val Loss=0.5141  Val Acc=0.7237  Time=19.1s\n",
            "  Fold 7 Epoch 27/150  Train Loss=0.4654  Val Loss=0.4915  Val Acc=0.7417  Time=18.8s\n",
            "  Fold 7 Epoch 28/150  Train Loss=0.4584  Val Loss=0.4747  Val Acc=0.7523  Time=18.9s\n",
            "  Fold 7 Epoch 29/150  Train Loss=0.4465  Val Loss=0.5293  Val Acc=0.7267  Time=18.8s\n",
            "  Fold 7 Epoch 30/150  Train Loss=0.4422  Val Loss=0.5547  Val Acc=0.6892  Time=19.0s\n",
            "  Fold 7 Epoch 31/150  Train Loss=0.4176  Val Loss=0.4606  Val Acc=0.7658  Time=18.9s\n",
            "  Fold 7 Epoch 32/150  Train Loss=0.4248  Val Loss=0.4912  Val Acc=0.7538  Time=19.0s\n",
            "  Fold 7 Epoch 33/150  Train Loss=0.4125  Val Loss=0.5060  Val Acc=0.7312  Time=18.9s\n",
            "  Fold 7 Epoch 34/150  Train Loss=0.4056  Val Loss=0.4776  Val Acc=0.7508  Time=19.0s\n",
            "  Fold 7 Epoch 35/150  Train Loss=0.3977  Val Loss=0.5017  Val Acc=0.7402  Time=19.0s\n",
            "  Fold 7 Epoch 36/150  Train Loss=0.3797  Val Loss=0.4842  Val Acc=0.7447  Time=18.9s\n",
            "  Fold 7 Epoch 37/150  Train Loss=0.3759  Val Loss=0.5162  Val Acc=0.7282  Time=18.8s\n",
            "  Fold 7 Epoch 38/150  Train Loss=0.3911  Val Loss=0.4464  Val Acc=0.7838  Time=18.9s\n",
            "  Fold 7 Epoch 39/150  Train Loss=0.3596  Val Loss=0.4465  Val Acc=0.7838  Time=18.9s\n",
            "  Fold 7 Epoch 40/150  Train Loss=0.3454  Val Loss=0.5382  Val Acc=0.7237  Time=18.8s\n",
            "  Fold 7 Epoch 41/150  Train Loss=0.3471  Val Loss=0.4430  Val Acc=0.7913  Time=18.8s\n",
            "  Fold 7 Epoch 42/150  Train Loss=0.3397  Val Loss=0.4684  Val Acc=0.7703  Time=19.0s\n",
            "  Fold 7 Epoch 43/150  Train Loss=0.3391  Val Loss=0.4682  Val Acc=0.7718  Time=18.9s\n",
            "  Fold 7 Epoch 44/150  Train Loss=0.3353  Val Loss=0.4957  Val Acc=0.7402  Time=18.8s\n",
            "  Fold 7 Epoch 45/150  Train Loss=0.3439  Val Loss=0.4475  Val Acc=0.7898  Time=18.9s\n",
            "  Fold 7 Epoch 46/150  Train Loss=0.3259  Val Loss=0.4486  Val Acc=0.7973  Time=18.8s\n",
            "  Fold 7 Epoch 47/150  Train Loss=0.3133  Val Loss=0.4491  Val Acc=0.8018  Time=19.0s\n",
            "  Fold 7 Epoch 48/150  Train Loss=0.3150  Val Loss=0.4515  Val Acc=0.8003  Time=18.9s\n",
            "  Fold 7 Epoch 49/150  Train Loss=0.3181  Val Loss=0.4646  Val Acc=0.7838  Time=19.0s\n",
            "  Fold 7 Epoch 50/150  Train Loss=0.3217  Val Loss=0.4634  Val Acc=0.7973  Time=18.9s\n",
            "  Fold 7 Epoch 51/150  Train Loss=0.3307  Val Loss=0.4719  Val Acc=0.7868  Time=19.2s\n",
            "  Fold 7 Epoch 52/150  Train Loss=0.3138  Val Loss=0.4842  Val Acc=0.7883  Time=18.9s\n",
            "  Fold 7 Epoch 53/150  Train Loss=0.3081  Val Loss=0.4717  Val Acc=0.7988  Time=19.1s\n",
            "  Fold 7 Epoch 54/150  Train Loss=0.3312  Val Loss=0.4780  Val Acc=0.7853  Time=18.8s\n",
            "  Fold 7 Epoch 55/150  Train Loss=0.2948  Val Loss=0.4889  Val Acc=0.7808  Time=18.9s\n",
            "  Fold 7 Epoch 56/150  Train Loss=0.2822  Val Loss=0.5225  Val Acc=0.7477  Time=18.9s\n",
            "  → Early stopping at epoch 56 (no val_loss improvement)\n",
            "  Fold 7 Final ACC = 1.0000   (TP=0  TN=62  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 8/65  (leave out sub-008)\n",
            "  Fold 8 Epoch 1/150  Train Loss=0.7410  Val Loss=0.6807  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 8 Epoch 2/150  Train Loss=0.7552  Val Loss=0.6807  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 3/150  Train Loss=0.7494  Val Loss=0.6842  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 8 Epoch 4/150  Train Loss=0.7331  Val Loss=0.6954  Val Acc=0.4129  Time=18.8s\n",
            "  Fold 8 Epoch 5/150  Train Loss=0.7247  Val Loss=0.6889  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 8 Epoch 6/150  Train Loss=0.7337  Val Loss=0.6899  Val Acc=0.5886  Time=18.7s\n",
            "  Fold 8 Epoch 7/150  Train Loss=0.7302  Val Loss=0.6875  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 8/150  Train Loss=0.7222  Val Loss=0.6854  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 8 Epoch 9/150  Train Loss=0.7210  Val Loss=0.6884  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 10/150  Train Loss=0.7100  Val Loss=0.6894  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 11/150  Train Loss=0.7136  Val Loss=0.6882  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 8 Epoch 12/150  Train Loss=0.7218  Val Loss=0.6873  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 8 Epoch 13/150  Train Loss=0.7068  Val Loss=0.6929  Val Acc=0.5195  Time=18.8s\n",
            "  Fold 8 Epoch 14/150  Train Loss=0.7086  Val Loss=0.6886  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 15/150  Train Loss=0.7055  Val Loss=0.6805  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 16/150  Train Loss=0.6993  Val Loss=0.6802  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 8 Epoch 17/150  Train Loss=0.6975  Val Loss=0.6784  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 8 Epoch 18/150  Train Loss=0.6937  Val Loss=0.6732  Val Acc=0.6066  Time=18.9s\n",
            "  Fold 8 Epoch 19/150  Train Loss=0.6792  Val Loss=0.6700  Val Acc=0.6712  Time=18.8s\n",
            "  Fold 8 Epoch 20/150  Train Loss=0.6698  Val Loss=0.6317  Val Acc=0.6832  Time=19.0s\n",
            "  Fold 8 Epoch 21/150  Train Loss=0.6431  Val Loss=0.5939  Val Acc=0.7117  Time=18.7s\n",
            "  Fold 8 Epoch 22/150  Train Loss=0.5994  Val Loss=0.6005  Val Acc=0.7012  Time=18.9s\n",
            "  Fold 8 Epoch 23/150  Train Loss=0.5672  Val Loss=0.5754  Val Acc=0.7057  Time=18.8s\n",
            "  Fold 8 Epoch 24/150  Train Loss=0.5489  Val Loss=0.5263  Val Acc=0.7177  Time=19.0s\n",
            "  Fold 8 Epoch 25/150  Train Loss=0.5344  Val Loss=0.5357  Val Acc=0.7057  Time=18.7s\n",
            "  Fold 8 Epoch 26/150  Train Loss=0.5141  Val Loss=0.5145  Val Acc=0.7207  Time=18.9s\n",
            "  Fold 8 Epoch 27/150  Train Loss=0.4980  Val Loss=0.5212  Val Acc=0.7162  Time=18.7s\n",
            "  Fold 8 Epoch 28/150  Train Loss=0.4955  Val Loss=0.4721  Val Acc=0.7703  Time=18.9s\n",
            "  Fold 8 Epoch 29/150  Train Loss=0.4869  Val Loss=0.4874  Val Acc=0.7417  Time=18.8s\n",
            "  Fold 8 Epoch 30/150  Train Loss=0.4705  Val Loss=0.5162  Val Acc=0.7267  Time=18.9s\n",
            "  Fold 8 Epoch 31/150  Train Loss=0.4604  Val Loss=0.4762  Val Acc=0.7477  Time=18.8s\n",
            "  Fold 8 Epoch 32/150  Train Loss=0.4487  Val Loss=0.4686  Val Acc=0.7538  Time=18.9s\n",
            "  Fold 8 Epoch 33/150  Train Loss=0.4539  Val Loss=0.4803  Val Acc=0.7492  Time=18.7s\n",
            "  Fold 8 Epoch 34/150  Train Loss=0.4249  Val Loss=0.4633  Val Acc=0.7643  Time=18.8s\n",
            "  Fold 8 Epoch 35/150  Train Loss=0.4059  Val Loss=0.5001  Val Acc=0.7417  Time=19.0s\n",
            "  Fold 8 Epoch 36/150  Train Loss=0.4156  Val Loss=0.4437  Val Acc=0.7943  Time=18.7s\n",
            "  Fold 8 Epoch 37/150  Train Loss=0.4077  Val Loss=0.4479  Val Acc=0.7718  Time=18.7s\n",
            "  Fold 8 Epoch 38/150  Train Loss=0.4054  Val Loss=0.5257  Val Acc=0.7192  Time=18.8s\n",
            "  Fold 8 Epoch 39/150  Train Loss=0.3875  Val Loss=0.5167  Val Acc=0.7207  Time=18.8s\n",
            "  Fold 8 Epoch 40/150  Train Loss=0.3790  Val Loss=0.4818  Val Acc=0.7508  Time=18.6s\n",
            "  Fold 8 Epoch 41/150  Train Loss=0.3769  Val Loss=0.4374  Val Acc=0.7838  Time=19.0s\n",
            "  Fold 8 Epoch 42/150  Train Loss=0.3639  Val Loss=0.4766  Val Acc=0.7568  Time=18.8s\n",
            "  Fold 8 Epoch 43/150  Train Loss=0.3488  Val Loss=0.5745  Val Acc=0.6922  Time=19.0s\n",
            "  Fold 8 Epoch 44/150  Train Loss=0.3550  Val Loss=0.5118  Val Acc=0.7282  Time=18.9s\n",
            "  Fold 8 Epoch 45/150  Train Loss=0.3481  Val Loss=0.4543  Val Acc=0.7793  Time=18.9s\n",
            "  Fold 8 Epoch 46/150  Train Loss=0.3227  Val Loss=0.4515  Val Acc=0.7793  Time=18.9s\n",
            "  Fold 8 Epoch 47/150  Train Loss=0.3357  Val Loss=0.4592  Val Acc=0.7748  Time=19.0s\n",
            "  Fold 8 Epoch 48/150  Train Loss=0.3396  Val Loss=0.4590  Val Acc=0.7763  Time=18.8s\n",
            "  Fold 8 Epoch 49/150  Train Loss=0.3304  Val Loss=0.5096  Val Acc=0.7462  Time=19.3s\n",
            "  Fold 8 Epoch 50/150  Train Loss=0.3273  Val Loss=0.4824  Val Acc=0.7643  Time=18.9s\n",
            "  Fold 8 Epoch 51/150  Train Loss=0.3063  Val Loss=0.4613  Val Acc=0.7868  Time=19.1s\n",
            "  Fold 8 Epoch 52/150  Train Loss=0.3099  Val Loss=0.4944  Val Acc=0.7643  Time=18.8s\n",
            "  Fold 8 Epoch 53/150  Train Loss=0.2999  Val Loss=0.4654  Val Acc=0.7793  Time=18.9s\n",
            "  Fold 8 Epoch 54/150  Train Loss=0.2938  Val Loss=0.5647  Val Acc=0.7357  Time=18.8s\n",
            "  Fold 8 Epoch 55/150  Train Loss=0.2857  Val Loss=0.4684  Val Acc=0.7778  Time=18.9s\n",
            "  Fold 8 Epoch 56/150  Train Loss=0.2943  Val Loss=0.5237  Val Acc=0.7523  Time=18.8s\n",
            "  → Early stopping at epoch 56 (no val_loss improvement)\n",
            "  Fold 8 Final ACC = 1.0000   (TP=0  TN=60  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 9/65  (leave out sub-009)\n",
            "  Fold 9 Epoch 1/150  Train Loss=0.7452  Val Loss=0.7627  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 9 Epoch 2/150  Train Loss=0.7282  Val Loss=0.7376  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 9 Epoch 3/150  Train Loss=0.7254  Val Loss=0.7362  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 9 Epoch 4/150  Train Loss=0.7271  Val Loss=0.7343  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 9 Epoch 5/150  Train Loss=0.7200  Val Loss=0.7214  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 9 Epoch 6/150  Train Loss=0.7287  Val Loss=0.7280  Val Acc=0.4084  Time=18.7s\n",
            "  Fold 9 Epoch 7/150  Train Loss=0.7201  Val Loss=0.7214  Val Acc=0.4084  Time=18.8s\n",
            "  Fold 9 Epoch 8/150  Train Loss=0.7260  Val Loss=0.7237  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 9 Epoch 9/150  Train Loss=0.7168  Val Loss=0.7115  Val Acc=0.4084  Time=18.8s\n",
            "  Fold 9 Epoch 10/150  Train Loss=0.7183  Val Loss=0.7185  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 9 Epoch 11/150  Train Loss=0.7180  Val Loss=0.7171  Val Acc=0.4084  Time=18.8s\n",
            "  Fold 9 Epoch 12/150  Train Loss=0.7131  Val Loss=0.7143  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 9 Epoch 13/150  Train Loss=0.7173  Val Loss=0.7125  Val Acc=0.4084  Time=18.7s\n",
            "  Fold 9 Epoch 14/150  Train Loss=0.7129  Val Loss=0.7146  Val Acc=0.4084  Time=18.8s\n",
            "  Fold 9 Epoch 15/150  Train Loss=0.7181  Val Loss=0.7274  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 9 Epoch 16/150  Train Loss=0.7143  Val Loss=0.7104  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 9 Epoch 17/150  Train Loss=0.7115  Val Loss=0.6913  Val Acc=0.5420  Time=18.8s\n",
            "  Fold 9 Epoch 18/150  Train Loss=0.7080  Val Loss=0.6841  Val Acc=0.6727  Time=19.0s\n",
            "  Fold 9 Epoch 19/150  Train Loss=0.6999  Val Loss=0.6811  Val Acc=0.5300  Time=18.8s\n",
            "  Fold 9 Epoch 20/150  Train Loss=0.6563  Val Loss=0.6151  Val Acc=0.6892  Time=19.0s\n",
            "  Fold 9 Epoch 21/150  Train Loss=0.6167  Val Loss=0.5978  Val Acc=0.7042  Time=18.8s\n",
            "  Fold 9 Epoch 22/150  Train Loss=0.5722  Val Loss=0.6189  Val Acc=0.6742  Time=18.9s\n",
            "  Fold 9 Epoch 23/150  Train Loss=0.5302  Val Loss=0.5863  Val Acc=0.7042  Time=18.9s\n",
            "  Fold 9 Epoch 24/150  Train Loss=0.5136  Val Loss=0.5238  Val Acc=0.7252  Time=19.1s\n",
            "  Fold 9 Epoch 25/150  Train Loss=0.5025  Val Loss=0.5432  Val Acc=0.7162  Time=18.9s\n",
            "  Fold 9 Epoch 26/150  Train Loss=0.4830  Val Loss=0.5134  Val Acc=0.7357  Time=19.1s\n",
            "  Fold 9 Epoch 27/150  Train Loss=0.4599  Val Loss=0.5310  Val Acc=0.7237  Time=18.7s\n",
            "  Fold 9 Epoch 28/150  Train Loss=0.4556  Val Loss=0.5004  Val Acc=0.7523  Time=19.1s\n",
            "  Fold 9 Epoch 29/150  Train Loss=0.4470  Val Loss=0.5409  Val Acc=0.6997  Time=18.8s\n",
            "  Fold 9 Epoch 30/150  Train Loss=0.4250  Val Loss=0.4890  Val Acc=0.7583  Time=18.9s\n",
            "  Fold 9 Epoch 31/150  Train Loss=0.4164  Val Loss=0.5101  Val Acc=0.7432  Time=18.8s\n",
            "  Fold 9 Epoch 32/150  Train Loss=0.4058  Val Loss=0.5369  Val Acc=0.7177  Time=19.0s\n",
            "  Fold 9 Epoch 33/150  Train Loss=0.3987  Val Loss=0.5036  Val Acc=0.7342  Time=18.8s\n",
            "  Fold 9 Epoch 34/150  Train Loss=0.3917  Val Loss=0.5086  Val Acc=0.7327  Time=18.7s\n",
            "  Fold 9 Epoch 35/150  Train Loss=0.3760  Val Loss=0.4942  Val Acc=0.7523  Time=18.8s\n",
            "  Fold 9 Epoch 36/150  Train Loss=0.3799  Val Loss=0.4730  Val Acc=0.7643  Time=18.7s\n",
            "  Fold 9 Epoch 37/150  Train Loss=0.3664  Val Loss=0.4835  Val Acc=0.7462  Time=18.7s\n",
            "  Fold 9 Epoch 38/150  Train Loss=0.3548  Val Loss=0.4669  Val Acc=0.7643  Time=18.8s\n",
            "  Fold 9 Epoch 39/150  Train Loss=0.3627  Val Loss=0.4581  Val Acc=0.7913  Time=18.9s\n",
            "  Fold 9 Epoch 40/150  Train Loss=0.3457  Val Loss=0.5251  Val Acc=0.7432  Time=18.7s\n",
            "  Fold 9 Epoch 41/150  Train Loss=0.3455  Val Loss=0.4854  Val Acc=0.7598  Time=18.9s\n",
            "  Fold 9 Epoch 42/150  Train Loss=0.3393  Val Loss=0.4796  Val Acc=0.7748  Time=18.6s\n",
            "  Fold 9 Epoch 43/150  Train Loss=0.3228  Val Loss=0.4777  Val Acc=0.7793  Time=19.0s\n",
            "  Fold 9 Epoch 44/150  Train Loss=0.3161  Val Loss=0.4714  Val Acc=0.7838  Time=18.6s\n",
            "  Fold 9 Epoch 45/150  Train Loss=0.3133  Val Loss=0.4673  Val Acc=0.7928  Time=19.0s\n",
            "  Fold 9 Epoch 46/150  Train Loss=0.3171  Val Loss=0.4645  Val Acc=0.7808  Time=18.6s\n",
            "  Fold 9 Epoch 47/150  Train Loss=0.3127  Val Loss=0.4711  Val Acc=0.7688  Time=19.0s\n",
            "  Fold 9 Epoch 48/150  Train Loss=0.2968  Val Loss=0.4832  Val Acc=0.7823  Time=18.8s\n",
            "  Fold 9 Epoch 49/150  Train Loss=0.3007  Val Loss=0.4683  Val Acc=0.7898  Time=19.0s\n",
            "  Fold 9 Epoch 50/150  Train Loss=0.3014  Val Loss=0.4859  Val Acc=0.7793  Time=18.8s\n",
            "  Fold 9 Epoch 51/150  Train Loss=0.2840  Val Loss=0.4606  Val Acc=0.7943  Time=19.1s\n",
            "  Fold 9 Epoch 52/150  Train Loss=0.2824  Val Loss=0.4718  Val Acc=0.7808  Time=18.8s\n",
            "  Fold 9 Epoch 53/150  Train Loss=0.2640  Val Loss=0.4907  Val Acc=0.7808  Time=18.9s\n",
            "  Fold 9 Epoch 54/150  Train Loss=0.2840  Val Loss=0.4821  Val Acc=0.7943  Time=18.9s\n",
            "  → Early stopping at epoch 54 (no val_loss improvement)\n",
            "  Fold 9 Final ACC = 0.5932   (TP=0  TN=35  FP=24  FN=0)\n",
            "\n",
            ">>> Fold 10/65  (leave out sub-010)\n",
            "  Fold 10 Epoch 1/150  Train Loss=0.7544  Val Loss=0.6927  Val Acc=0.5300  Time=18.9s\n",
            "  Fold 10 Epoch 2/150  Train Loss=0.7326  Val Loss=0.6834  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 10 Epoch 3/150  Train Loss=0.7240  Val Loss=0.6850  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 10 Epoch 4/150  Train Loss=0.7210  Val Loss=0.6828  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 10 Epoch 5/150  Train Loss=0.7199  Val Loss=0.6798  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 10 Epoch 6/150  Train Loss=0.7139  Val Loss=0.6802  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 10 Epoch 7/150  Train Loss=0.7194  Val Loss=0.6784  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 10 Epoch 8/150  Train Loss=0.7196  Val Loss=0.6777  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 10 Epoch 9/150  Train Loss=0.7139  Val Loss=0.6776  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 10 Epoch 10/150  Train Loss=0.7219  Val Loss=0.6775  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 10 Epoch 11/150  Train Loss=0.7071  Val Loss=0.6794  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 10 Epoch 12/150  Train Loss=0.7143  Val Loss=0.6829  Val Acc=0.5991  Time=18.9s\n",
            "  Fold 10 Epoch 13/150  Train Loss=0.7004  Val Loss=0.6923  Val Acc=0.4700  Time=18.6s\n",
            "  Fold 10 Epoch 14/150  Train Loss=0.6973  Val Loss=0.6774  Val Acc=0.6306  Time=18.9s\n",
            "  Fold 10 Epoch 15/150  Train Loss=0.6968  Val Loss=0.6677  Val Acc=0.6351  Time=18.8s\n",
            "  Fold 10 Epoch 16/150  Train Loss=0.6783  Val Loss=0.6310  Val Acc=0.6321  Time=19.0s\n",
            "  Fold 10 Epoch 17/150  Train Loss=0.6420  Val Loss=0.5990  Val Acc=0.6727  Time=18.8s\n",
            "  Fold 10 Epoch 18/150  Train Loss=0.6058  Val Loss=0.6761  Val Acc=0.5541  Time=18.8s\n",
            "  Fold 10 Epoch 19/150  Train Loss=0.5610  Val Loss=0.6029  Val Acc=0.6952  Time=18.9s\n",
            "  Fold 10 Epoch 20/150  Train Loss=0.5523  Val Loss=0.5314  Val Acc=0.7282  Time=19.1s\n",
            "  Fold 10 Epoch 21/150  Train Loss=0.5340  Val Loss=0.5353  Val Acc=0.7207  Time=18.8s\n",
            "  Fold 10 Epoch 22/150  Train Loss=0.5195  Val Loss=0.5773  Val Acc=0.6952  Time=19.1s\n",
            "  Fold 10 Epoch 23/150  Train Loss=0.5082  Val Loss=0.5159  Val Acc=0.7327  Time=18.8s\n",
            "  Fold 10 Epoch 24/150  Train Loss=0.4949  Val Loss=0.5284  Val Acc=0.7147  Time=19.1s\n",
            "  Fold 10 Epoch 25/150  Train Loss=0.4824  Val Loss=0.5259  Val Acc=0.7072  Time=18.7s\n",
            "  Fold 10 Epoch 26/150  Train Loss=0.4768  Val Loss=0.5343  Val Acc=0.7087  Time=19.0s\n",
            "  Fold 10 Epoch 27/150  Train Loss=0.4695  Val Loss=0.5305  Val Acc=0.7282  Time=18.8s\n",
            "  Fold 10 Epoch 28/150  Train Loss=0.4645  Val Loss=0.5460  Val Acc=0.7192  Time=19.0s\n",
            "  Fold 10 Epoch 29/150  Train Loss=0.4539  Val Loss=0.5140  Val Acc=0.7342  Time=18.8s\n",
            "  Fold 10 Epoch 30/150  Train Loss=0.4493  Val Loss=0.5384  Val Acc=0.7342  Time=19.0s\n",
            "  Fold 10 Epoch 31/150  Train Loss=0.4469  Val Loss=0.5298  Val Acc=0.7312  Time=18.9s\n",
            "  Fold 10 Epoch 32/150  Train Loss=0.4331  Val Loss=0.4975  Val Acc=0.7477  Time=18.9s\n",
            "  Fold 10 Epoch 33/150  Train Loss=0.4220  Val Loss=0.5801  Val Acc=0.6952  Time=18.8s\n",
            "  Fold 10 Epoch 34/150  Train Loss=0.4204  Val Loss=0.5374  Val Acc=0.7147  Time=18.9s\n",
            "  Fold 10 Epoch 35/150  Train Loss=0.3921  Val Loss=0.5239  Val Acc=0.7297  Time=18.8s\n",
            "  Fold 10 Epoch 36/150  Train Loss=0.3885  Val Loss=0.6411  Val Acc=0.6847  Time=18.8s\n",
            "  Fold 10 Epoch 37/150  Train Loss=0.3882  Val Loss=0.6160  Val Acc=0.6922  Time=18.8s\n",
            "  Fold 10 Epoch 38/150  Train Loss=0.3659  Val Loss=0.5112  Val Acc=0.7402  Time=18.7s\n",
            "  Fold 10 Epoch 39/150  Train Loss=0.3569  Val Loss=0.5126  Val Acc=0.7342  Time=18.9s\n",
            "  Fold 10 Epoch 40/150  Train Loss=0.3522  Val Loss=0.5198  Val Acc=0.7402  Time=18.9s\n",
            "  Fold 10 Epoch 41/150  Train Loss=0.3532  Val Loss=0.5885  Val Acc=0.7057  Time=18.9s\n",
            "  Fold 10 Epoch 42/150  Train Loss=0.3453  Val Loss=0.5243  Val Acc=0.7477  Time=18.7s\n",
            "  Fold 10 Epoch 43/150  Train Loss=0.3415  Val Loss=0.6168  Val Acc=0.7012  Time=18.9s\n",
            "  Fold 10 Epoch 44/150  Train Loss=0.3323  Val Loss=0.6845  Val Acc=0.6712  Time=18.8s\n",
            "  Fold 10 Epoch 45/150  Train Loss=0.3186  Val Loss=0.4818  Val Acc=0.7598  Time=18.9s\n",
            "  Fold 10 Epoch 46/150  Train Loss=0.3138  Val Loss=0.5813  Val Acc=0.7192  Time=18.7s\n",
            "  Fold 10 Epoch 47/150  Train Loss=0.3197  Val Loss=0.6251  Val Acc=0.6997  Time=19.2s\n",
            "  Fold 10 Epoch 48/150  Train Loss=0.3138  Val Loss=0.5401  Val Acc=0.7297  Time=18.8s\n",
            "  Fold 10 Epoch 49/150  Train Loss=0.2945  Val Loss=0.5366  Val Acc=0.7282  Time=19.0s\n",
            "  Fold 10 Epoch 50/150  Train Loss=0.2967  Val Loss=0.5338  Val Acc=0.7402  Time=18.9s\n",
            "  Fold 10 Epoch 51/150  Train Loss=0.2888  Val Loss=0.5450  Val Acc=0.7387  Time=18.9s\n",
            "  Fold 10 Epoch 52/150  Train Loss=0.2957  Val Loss=0.5765  Val Acc=0.7402  Time=18.9s\n",
            "  Fold 10 Epoch 53/150  Train Loss=0.2907  Val Loss=0.5403  Val Acc=0.7432  Time=19.0s\n",
            "  Fold 10 Epoch 54/150  Train Loss=0.2819  Val Loss=0.5123  Val Acc=0.7628  Time=18.9s\n",
            "  Fold 10 Epoch 55/150  Train Loss=0.2818  Val Loss=0.6580  Val Acc=0.6952  Time=18.9s\n",
            "  Fold 10 Epoch 56/150  Train Loss=0.2785  Val Loss=0.6239  Val Acc=0.7252  Time=18.9s\n",
            "  Fold 10 Epoch 57/150  Train Loss=0.2767  Val Loss=0.6399  Val Acc=0.7132  Time=18.9s\n",
            "  Fold 10 Epoch 58/150  Train Loss=0.2739  Val Loss=0.5556  Val Acc=0.7402  Time=18.8s\n",
            "  Fold 10 Epoch 59/150  Train Loss=0.2616  Val Loss=0.5905  Val Acc=0.7267  Time=19.0s\n",
            "  Fold 10 Epoch 60/150  Train Loss=0.2536  Val Loss=0.6699  Val Acc=0.7192  Time=18.8s\n",
            "  → Early stopping at epoch 60 (no val_loss improvement)\n",
            "  Fold 10 Final ACC = 0.9661   (TP=0  TN=57  FP=2  FN=0)\n",
            "\n",
            ">>> Fold 11/65  (leave out sub-011)\n",
            "  Fold 11 Epoch 1/150  Train Loss=0.7917  Val Loss=0.6809  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 11 Epoch 2/150  Train Loss=0.7805  Val Loss=0.6798  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 3/150  Train Loss=0.7567  Val Loss=0.6788  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 11 Epoch 4/150  Train Loss=0.7595  Val Loss=0.6783  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 5/150  Train Loss=0.7544  Val Loss=0.6777  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 6/150  Train Loss=0.7425  Val Loss=0.6784  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 11 Epoch 7/150  Train Loss=0.7394  Val Loss=0.6780  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 11 Epoch 8/150  Train Loss=0.7255  Val Loss=0.6791  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 11 Epoch 9/150  Train Loss=0.7238  Val Loss=0.6799  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 10/150  Train Loss=0.7216  Val Loss=0.6816  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 11 Epoch 11/150  Train Loss=0.7276  Val Loss=0.6801  Val Acc=0.5916  Time=18.6s\n",
            "  Fold 11 Epoch 12/150  Train Loss=0.7229  Val Loss=0.6794  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 13/150  Train Loss=0.7241  Val Loss=0.6791  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 14/150  Train Loss=0.7240  Val Loss=0.6797  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 15/150  Train Loss=0.7170  Val Loss=0.6817  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 11 Epoch 16/150  Train Loss=0.7151  Val Loss=0.6790  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 11 Epoch 17/150  Train Loss=0.7247  Val Loss=0.6803  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 11 Epoch 18/150  Train Loss=0.7162  Val Loss=0.6720  Val Acc=0.6036  Time=19.0s\n",
            "  Fold 11 Epoch 19/150  Train Loss=0.7049  Val Loss=0.6534  Val Acc=0.6787  Time=19.0s\n",
            "  Fold 11 Epoch 20/150  Train Loss=0.6702  Val Loss=0.5810  Val Acc=0.6997  Time=19.0s\n",
            "  Fold 11 Epoch 21/150  Train Loss=0.6077  Val Loss=0.5592  Val Acc=0.7102  Time=18.9s\n",
            "  Fold 11 Epoch 22/150  Train Loss=0.5471  Val Loss=0.5781  Val Acc=0.6937  Time=18.9s\n",
            "  Fold 11 Epoch 23/150  Train Loss=0.5392  Val Loss=0.5306  Val Acc=0.7192  Time=18.7s\n",
            "  Fold 11 Epoch 24/150  Train Loss=0.5092  Val Loss=0.4977  Val Acc=0.7417  Time=19.0s\n",
            "  Fold 11 Epoch 25/150  Train Loss=0.4995  Val Loss=0.5225  Val Acc=0.7207  Time=18.7s\n",
            "  Fold 11 Epoch 26/150  Train Loss=0.4798  Val Loss=0.5296  Val Acc=0.7102  Time=19.1s\n",
            "  Fold 11 Epoch 27/150  Train Loss=0.4701  Val Loss=0.5212  Val Acc=0.7207  Time=18.7s\n",
            "  Fold 11 Epoch 28/150  Train Loss=0.4577  Val Loss=0.4842  Val Acc=0.7658  Time=19.0s\n",
            "  Fold 11 Epoch 29/150  Train Loss=0.4632  Val Loss=0.5088  Val Acc=0.7237  Time=18.8s\n",
            "  Fold 11 Epoch 30/150  Train Loss=0.4482  Val Loss=0.5030  Val Acc=0.7402  Time=19.1s\n",
            "  Fold 11 Epoch 31/150  Train Loss=0.4352  Val Loss=0.4778  Val Acc=0.7523  Time=18.8s\n",
            "  Fold 11 Epoch 32/150  Train Loss=0.4363  Val Loss=0.4649  Val Acc=0.7763  Time=18.9s\n",
            "  Fold 11 Epoch 33/150  Train Loss=0.4170  Val Loss=0.4782  Val Acc=0.7553  Time=18.9s\n",
            "  Fold 11 Epoch 34/150  Train Loss=0.4112  Val Loss=0.4738  Val Acc=0.7553  Time=18.9s\n",
            "  Fold 11 Epoch 35/150  Train Loss=0.4114  Val Loss=0.4517  Val Acc=0.7718  Time=18.8s\n",
            "  Fold 11 Epoch 36/150  Train Loss=0.3945  Val Loss=0.4777  Val Acc=0.7523  Time=18.9s\n",
            "  Fold 11 Epoch 37/150  Train Loss=0.3887  Val Loss=0.4469  Val Acc=0.7823  Time=19.0s\n",
            "  Fold 11 Epoch 38/150  Train Loss=0.3768  Val Loss=0.4728  Val Acc=0.7583  Time=18.8s\n",
            "  Fold 11 Epoch 39/150  Train Loss=0.3750  Val Loss=0.4667  Val Acc=0.7447  Time=19.0s\n",
            "  Fold 11 Epoch 40/150  Train Loss=0.3608  Val Loss=0.4537  Val Acc=0.7688  Time=19.0s\n",
            "  Fold 11 Epoch 41/150  Train Loss=0.3618  Val Loss=0.4655  Val Acc=0.7658  Time=18.9s\n",
            "  Fold 11 Epoch 42/150  Train Loss=0.3596  Val Loss=0.4320  Val Acc=0.8018  Time=19.0s\n",
            "  Fold 11 Epoch 43/150  Train Loss=0.3413  Val Loss=0.4505  Val Acc=0.7898  Time=18.9s\n",
            "  Fold 11 Epoch 44/150  Train Loss=0.3467  Val Loss=0.4521  Val Acc=0.7883  Time=18.8s\n",
            "  Fold 11 Epoch 45/150  Train Loss=0.3212  Val Loss=0.4550  Val Acc=0.7793  Time=18.8s\n",
            "  Fold 11 Epoch 46/150  Train Loss=0.3225  Val Loss=0.4697  Val Acc=0.7763  Time=18.7s\n",
            "  Fold 11 Epoch 47/150  Train Loss=0.3174  Val Loss=0.4408  Val Acc=0.7883  Time=18.9s\n",
            "  Fold 11 Epoch 48/150  Train Loss=0.3147  Val Loss=0.4819  Val Acc=0.7718  Time=18.9s\n",
            "  Fold 11 Epoch 49/150  Train Loss=0.3214  Val Loss=0.4927  Val Acc=0.7748  Time=18.9s\n",
            "  Fold 11 Epoch 50/150  Train Loss=0.3084  Val Loss=0.4995  Val Acc=0.7688  Time=18.9s\n",
            "  Fold 11 Epoch 51/150  Train Loss=0.3115  Val Loss=0.4919  Val Acc=0.7748  Time=19.0s\n",
            "  Fold 11 Epoch 52/150  Train Loss=0.2914  Val Loss=0.4874  Val Acc=0.7808  Time=18.9s\n",
            "  Fold 11 Epoch 53/150  Train Loss=0.2906  Val Loss=0.4737  Val Acc=0.7853  Time=19.1s\n",
            "  Fold 11 Epoch 54/150  Train Loss=0.2991  Val Loss=0.4913  Val Acc=0.7688  Time=18.8s\n",
            "  Fold 11 Epoch 55/150  Train Loss=0.2859  Val Loss=0.4732  Val Acc=0.7838  Time=19.0s\n",
            "  Fold 11 Epoch 56/150  Train Loss=0.2874  Val Loss=0.4960  Val Acc=0.7763  Time=18.8s\n",
            "  Fold 11 Epoch 57/150  Train Loss=0.2810  Val Loss=0.5121  Val Acc=0.7748  Time=19.0s\n",
            "  → Early stopping at epoch 57 (no val_loss improvement)\n",
            "  Fold 11 Final ACC = 0.6034   (TP=0  TN=35  FP=23  FN=0)\n",
            "\n",
            ">>> Fold 12/65  (leave out sub-012)\n",
            "  Fold 12 Epoch 1/150  Train Loss=0.7317  Val Loss=0.7082  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 12 Epoch 2/150  Train Loss=0.7313  Val Loss=0.6968  Val Acc=0.4234  Time=19.0s\n",
            "  Fold 12 Epoch 3/150  Train Loss=0.7137  Val Loss=0.6941  Val Acc=0.4734  Time=18.8s\n",
            "  Fold 12 Epoch 4/150  Train Loss=0.7132  Val Loss=0.6925  Val Acc=0.5141  Time=19.0s\n",
            "  Fold 12 Epoch 5/150  Train Loss=0.7052  Val Loss=0.6890  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 12 Epoch 6/150  Train Loss=0.7173  Val Loss=0.6855  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 12 Epoch 7/150  Train Loss=0.7114  Val Loss=0.6858  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 8/150  Train Loss=0.7179  Val Loss=0.6835  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 9/150  Train Loss=0.7103  Val Loss=0.6844  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 12 Epoch 10/150  Train Loss=0.7139  Val Loss=0.6818  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 11/150  Train Loss=0.7071  Val Loss=0.6804  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 12 Epoch 12/150  Train Loss=0.7063  Val Loss=0.6849  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 13/150  Train Loss=0.7150  Val Loss=0.6849  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 14/150  Train Loss=0.7071  Val Loss=0.6923  Val Acc=0.5375  Time=18.9s\n",
            "  Fold 12 Epoch 15/150  Train Loss=0.7077  Val Loss=0.6922  Val Acc=0.5563  Time=18.8s\n",
            "  Fold 12 Epoch 16/150  Train Loss=0.7158  Val Loss=0.6855  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 17/150  Train Loss=0.7162  Val Loss=0.6832  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 18/150  Train Loss=0.7112  Val Loss=0.6850  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 12 Epoch 19/150  Train Loss=0.7100  Val Loss=0.6839  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 12 Epoch 20/150  Train Loss=0.7103  Val Loss=0.6832  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 12 Epoch 21/150  Train Loss=0.7087  Val Loss=0.6839  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 12 Epoch 22/150  Train Loss=0.7012  Val Loss=0.6816  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 12 Epoch 23/150  Train Loss=0.6979  Val Loss=0.6875  Val Acc=0.5891  Time=18.7s\n",
            "  Fold 12 Epoch 24/150  Train Loss=0.7040  Val Loss=0.6817  Val Acc=0.5844  Time=18.7s\n",
            "  Fold 12 Epoch 25/150  Train Loss=0.7062  Val Loss=0.6802  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 12 Epoch 26/150  Train Loss=0.6981  Val Loss=0.6779  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 12 Epoch 27/150  Train Loss=0.6988  Val Loss=0.6741  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 12 Epoch 28/150  Train Loss=0.6973  Val Loss=0.6866  Val Acc=0.6687  Time=18.9s\n",
            "  Fold 12 Epoch 29/150  Train Loss=0.7020  Val Loss=0.6793  Val Acc=0.6422  Time=18.9s\n",
            "  Fold 12 Epoch 30/150  Train Loss=0.6960  Val Loss=0.6435  Val Acc=0.7266  Time=18.9s\n",
            "  Fold 12 Epoch 31/150  Train Loss=0.6630  Val Loss=0.6010  Val Acc=0.7953  Time=18.8s\n",
            "  Fold 12 Epoch 32/150  Train Loss=0.6272  Val Loss=0.5467  Val Acc=0.8484  Time=18.8s\n",
            "  Fold 12 Epoch 33/150  Train Loss=0.5991  Val Loss=0.4883  Val Acc=0.8453  Time=18.8s\n",
            "  Fold 12 Epoch 34/150  Train Loss=0.5852  Val Loss=0.4987  Val Acc=0.8672  Time=18.9s\n",
            "  Fold 12 Epoch 35/150  Train Loss=0.5667  Val Loss=0.4930  Val Acc=0.8781  Time=18.9s\n",
            "  Fold 12 Epoch 36/150  Train Loss=0.5472  Val Loss=0.5165  Val Acc=0.7984  Time=18.8s\n",
            "  Fold 12 Epoch 37/150  Train Loss=0.5343  Val Loss=0.4253  Val Acc=0.8875  Time=18.8s\n",
            "  Fold 12 Epoch 38/150  Train Loss=0.5205  Val Loss=0.4514  Val Acc=0.8781  Time=18.8s\n",
            "  Fold 12 Epoch 39/150  Train Loss=0.5192  Val Loss=0.4311  Val Acc=0.8875  Time=18.8s\n",
            "  Fold 12 Epoch 40/150  Train Loss=0.5005  Val Loss=0.4438  Val Acc=0.8938  Time=18.9s\n",
            "  Fold 12 Epoch 41/150  Train Loss=0.4857  Val Loss=0.4300  Val Acc=0.8922  Time=18.8s\n",
            "  Fold 12 Epoch 42/150  Train Loss=0.4848  Val Loss=0.4248  Val Acc=0.8984  Time=18.9s\n",
            "  Fold 12 Epoch 43/150  Train Loss=0.4693  Val Loss=0.4501  Val Acc=0.8812  Time=18.8s\n",
            "  Fold 12 Epoch 44/150  Train Loss=0.4592  Val Loss=0.4517  Val Acc=0.8812  Time=18.9s\n",
            "  Fold 12 Epoch 45/150  Train Loss=0.4645  Val Loss=0.4834  Val Acc=0.7594  Time=18.8s\n",
            "  Fold 12 Epoch 46/150  Train Loss=0.4543  Val Loss=0.4501  Val Acc=0.8344  Time=19.0s\n",
            "  Fold 12 Epoch 47/150  Train Loss=0.4356  Val Loss=0.3964  Val Acc=0.8922  Time=18.7s\n",
            "  Fold 12 Epoch 48/150  Train Loss=0.4255  Val Loss=0.4233  Val Acc=0.8344  Time=19.0s\n",
            "  Fold 12 Epoch 49/150  Train Loss=0.4210  Val Loss=0.4471  Val Acc=0.8094  Time=18.7s\n",
            "  Fold 12 Epoch 50/150  Train Loss=0.4179  Val Loss=0.4877  Val Acc=0.7531  Time=19.1s\n",
            "  Fold 12 Epoch 51/150  Train Loss=0.4078  Val Loss=0.3752  Val Acc=0.8797  Time=18.9s\n",
            "  Fold 12 Epoch 52/150  Train Loss=0.3940  Val Loss=0.4471  Val Acc=0.7859  Time=18.8s\n",
            "  Fold 12 Epoch 53/150  Train Loss=0.3900  Val Loss=0.3955  Val Acc=0.8609  Time=18.9s\n",
            "  Fold 12 Epoch 54/150  Train Loss=0.3821  Val Loss=0.4042  Val Acc=0.8422  Time=19.0s\n",
            "  Fold 12 Epoch 55/150  Train Loss=0.3802  Val Loss=0.4358  Val Acc=0.8156  Time=18.8s\n",
            "  Fold 12 Epoch 56/150  Train Loss=0.3823  Val Loss=0.4670  Val Acc=0.7547  Time=18.9s\n",
            "  Fold 12 Epoch 57/150  Train Loss=0.3680  Val Loss=0.4028  Val Acc=0.8375  Time=18.8s\n",
            "  Fold 12 Epoch 58/150  Train Loss=0.3667  Val Loss=0.4217  Val Acc=0.7969  Time=18.8s\n",
            "  Fold 12 Epoch 59/150  Train Loss=0.3620  Val Loss=0.4204  Val Acc=0.8078  Time=18.9s\n",
            "  Fold 12 Epoch 60/150  Train Loss=0.3578  Val Loss=0.4343  Val Acc=0.7875  Time=19.0s\n",
            "  Fold 12 Epoch 61/150  Train Loss=0.3508  Val Loss=0.3747  Val Acc=0.8422  Time=18.8s\n",
            "  Fold 12 Epoch 62/150  Train Loss=0.3517  Val Loss=0.3746  Val Acc=0.8406  Time=18.8s\n",
            "  Fold 12 Epoch 63/150  Train Loss=0.3386  Val Loss=0.4057  Val Acc=0.8297  Time=18.9s\n",
            "  Fold 12 Epoch 64/150  Train Loss=0.3390  Val Loss=0.3479  Val Acc=0.8594  Time=18.9s\n",
            "  Fold 12 Epoch 65/150  Train Loss=0.3265  Val Loss=0.4290  Val Acc=0.7891  Time=18.9s\n",
            "  Fold 12 Epoch 66/150  Train Loss=0.3352  Val Loss=0.4598  Val Acc=0.7688  Time=18.8s\n",
            "  Fold 12 Epoch 67/150  Train Loss=0.3181  Val Loss=0.3700  Val Acc=0.8438  Time=18.9s\n",
            "  Fold 12 Epoch 68/150  Train Loss=0.3249  Val Loss=0.5492  Val Acc=0.7234  Time=18.8s\n",
            "  Fold 12 Epoch 69/150  Train Loss=0.3182  Val Loss=0.4068  Val Acc=0.8156  Time=19.0s\n",
            "  Fold 12 Epoch 70/150  Train Loss=0.3047  Val Loss=0.3839  Val Acc=0.8234  Time=18.9s\n",
            "  Fold 12 Epoch 71/150  Train Loss=0.3105  Val Loss=0.4184  Val Acc=0.7984  Time=19.0s\n",
            "  Fold 12 Epoch 72/150  Train Loss=0.2991  Val Loss=0.4171  Val Acc=0.7953  Time=18.8s\n",
            "  Fold 12 Epoch 73/150  Train Loss=0.2893  Val Loss=0.4616  Val Acc=0.7672  Time=19.0s\n",
            "  Fold 12 Epoch 74/150  Train Loss=0.3004  Val Loss=0.4273  Val Acc=0.8000  Time=18.8s\n",
            "  Fold 12 Epoch 75/150  Train Loss=0.2988  Val Loss=0.3832  Val Acc=0.8359  Time=18.9s\n",
            "  Fold 12 Epoch 76/150  Train Loss=0.2893  Val Loss=0.3913  Val Acc=0.8219  Time=18.9s\n",
            "  Fold 12 Epoch 77/150  Train Loss=0.2909  Val Loss=0.4257  Val Acc=0.8000  Time=18.9s\n",
            "  Fold 12 Epoch 78/150  Train Loss=0.2869  Val Loss=0.3929  Val Acc=0.8219  Time=18.8s\n",
            "  Fold 12 Epoch 79/150  Train Loss=0.2838  Val Loss=0.4332  Val Acc=0.7781  Time=19.0s\n",
            "  → Early stopping at epoch 79 (no val_loss improvement)\n",
            "  Fold 12 Final ACC = 0.5088   (TP=29  TN=0  FP=0  FN=28)\n",
            "\n",
            ">>> Fold 13/65  (leave out sub-013)\n",
            "  Fold 13 Epoch 1/150  Train Loss=0.7472  Val Loss=0.6841  Val Acc=0.5961  Time=18.9s\n",
            "  Fold 13 Epoch 2/150  Train Loss=0.7173  Val Loss=0.6872  Val Acc=0.5917  Time=19.1s\n",
            "  Fold 13 Epoch 3/150  Train Loss=0.7116  Val Loss=0.6949  Val Acc=0.4605  Time=19.0s\n",
            "  Fold 13 Epoch 4/150  Train Loss=0.7225  Val Loss=0.6902  Val Acc=0.5753  Time=18.9s\n",
            "  Fold 13 Epoch 5/150  Train Loss=0.7308  Val Loss=0.6872  Val Acc=0.5917  Time=18.9s\n",
            "  Fold 13 Epoch 6/150  Train Loss=0.7153  Val Loss=0.6855  Val Acc=0.5946  Time=18.9s\n",
            "  Fold 13 Epoch 7/150  Train Loss=0.7181  Val Loss=0.6869  Val Acc=0.5887  Time=18.9s\n",
            "  Fold 13 Epoch 8/150  Train Loss=0.7137  Val Loss=0.6805  Val Acc=0.5946  Time=18.9s\n",
            "  Fold 13 Epoch 9/150  Train Loss=0.7193  Val Loss=0.6892  Val Acc=0.5902  Time=19.0s\n",
            "  Fold 13 Epoch 10/150  Train Loss=0.7233  Val Loss=0.6890  Val Acc=0.5917  Time=18.9s\n",
            "  Fold 13 Epoch 11/150  Train Loss=0.7179  Val Loss=0.6952  Val Acc=0.4352  Time=19.0s\n",
            "  Fold 13 Epoch 12/150  Train Loss=0.7188  Val Loss=0.6935  Val Acc=0.4739  Time=18.9s\n",
            "  Fold 13 Epoch 13/150  Train Loss=0.7127  Val Loss=0.6919  Val Acc=0.5678  Time=19.1s\n",
            "  Fold 13 Epoch 14/150  Train Loss=0.7195  Val Loss=0.6893  Val Acc=0.6051  Time=18.9s\n",
            "  Fold 13 Epoch 15/150  Train Loss=0.7104  Val Loss=0.6881  Val Acc=0.6259  Time=19.2s\n",
            "  Fold 13 Epoch 16/150  Train Loss=0.7162  Val Loss=0.6860  Val Acc=0.6200  Time=18.8s\n",
            "  Fold 13 Epoch 17/150  Train Loss=0.7061  Val Loss=0.6803  Val Acc=0.6453  Time=19.0s\n",
            "  Fold 13 Epoch 18/150  Train Loss=0.7092  Val Loss=0.6812  Val Acc=0.5961  Time=19.0s\n",
            "  Fold 13 Epoch 19/150  Train Loss=0.6777  Val Loss=0.6439  Val Acc=0.6662  Time=19.0s\n",
            "  Fold 13 Epoch 20/150  Train Loss=0.6574  Val Loss=0.6231  Val Acc=0.6811  Time=19.0s\n",
            "  Fold 13 Epoch 21/150  Train Loss=0.6053  Val Loss=0.5779  Val Acc=0.6975  Time=19.0s\n",
            "  Fold 13 Epoch 22/150  Train Loss=0.5893  Val Loss=0.5889  Val Acc=0.6975  Time=18.8s\n",
            "  Fold 13 Epoch 23/150  Train Loss=0.5570  Val Loss=0.5821  Val Acc=0.6841  Time=19.0s\n",
            "  Fold 13 Epoch 24/150  Train Loss=0.5293  Val Loss=0.6499  Val Acc=0.6498  Time=18.9s\n",
            "  Fold 13 Epoch 25/150  Train Loss=0.5081  Val Loss=0.6181  Val Acc=0.6602  Time=19.1s\n",
            "  Fold 13 Epoch 26/150  Train Loss=0.5108  Val Loss=0.5852  Val Acc=0.6781  Time=18.8s\n",
            "  Fold 13 Epoch 27/150  Train Loss=0.5250  Val Loss=0.5585  Val Acc=0.7109  Time=19.1s\n",
            "  Fold 13 Epoch 28/150  Train Loss=0.4926  Val Loss=0.5756  Val Acc=0.6841  Time=19.0s\n",
            "  Fold 13 Epoch 29/150  Train Loss=0.4863  Val Loss=0.5789  Val Acc=0.6826  Time=19.3s\n",
            "  Fold 13 Epoch 30/150  Train Loss=0.4731  Val Loss=0.5773  Val Acc=0.6811  Time=19.0s\n",
            "  Fold 13 Epoch 31/150  Train Loss=0.4555  Val Loss=0.5296  Val Acc=0.7198  Time=19.2s\n",
            "  Fold 13 Epoch 32/150  Train Loss=0.4519  Val Loss=0.6032  Val Acc=0.6602  Time=18.8s\n",
            "  Fold 13 Epoch 33/150  Train Loss=0.4452  Val Loss=0.5443  Val Acc=0.7094  Time=19.2s\n",
            "  Fold 13 Epoch 34/150  Train Loss=0.4425  Val Loss=0.6038  Val Acc=0.6572  Time=18.9s\n",
            "  Fold 13 Epoch 35/150  Train Loss=0.4161  Val Loss=0.6655  Val Acc=0.6185  Time=19.1s\n",
            "  Fold 13 Epoch 36/150  Train Loss=0.4079  Val Loss=0.5623  Val Acc=0.6945  Time=18.9s\n",
            "  Fold 13 Epoch 37/150  Train Loss=0.4007  Val Loss=0.6345  Val Acc=0.6274  Time=19.2s\n",
            "  Fold 13 Epoch 38/150  Train Loss=0.3890  Val Loss=0.6231  Val Acc=0.6408  Time=18.7s\n",
            "  Fold 13 Epoch 39/150  Train Loss=0.3836  Val Loss=0.5835  Val Acc=0.6841  Time=19.0s\n",
            "  Fold 13 Epoch 40/150  Train Loss=0.3643  Val Loss=0.5320  Val Acc=0.7303  Time=18.9s\n",
            "  Fold 13 Epoch 41/150  Train Loss=0.3722  Val Loss=0.6369  Val Acc=0.6453  Time=19.1s\n",
            "  Fold 13 Epoch 42/150  Train Loss=0.3630  Val Loss=0.5533  Val Acc=0.7198  Time=18.9s\n",
            "  Fold 13 Epoch 43/150  Train Loss=0.3563  Val Loss=0.6162  Val Acc=0.6632  Time=19.0s\n",
            "  Fold 13 Epoch 44/150  Train Loss=0.3348  Val Loss=0.6411  Val Acc=0.6408  Time=19.1s\n",
            "  Fold 13 Epoch 45/150  Train Loss=0.3411  Val Loss=0.5346  Val Acc=0.7243  Time=19.0s\n",
            "  Fold 13 Epoch 46/150  Train Loss=0.3506  Val Loss=0.7384  Val Acc=0.6110  Time=18.8s\n",
            "  → Early stopping at epoch 46 (no val_loss improvement)\n",
            "  Fold 13 Final ACC = 0.9825   (TP=0  TN=56  FP=1  FN=0)\n",
            "\n",
            ">>> Fold 14/65  (leave out sub-014)\n",
            "  Fold 14 Epoch 1/150  Train Loss=0.7504  Val Loss=0.6964  Val Acc=0.4272  Time=18.9s\n",
            "  Fold 14 Epoch 2/150  Train Loss=0.7463  Val Loss=0.6893  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 14 Epoch 3/150  Train Loss=0.7360  Val Loss=0.6886  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 14 Epoch 4/150  Train Loss=0.7308  Val Loss=0.6894  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 14 Epoch 5/150  Train Loss=0.7374  Val Loss=0.6868  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 14 Epoch 6/150  Train Loss=0.7382  Val Loss=0.6881  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 14 Epoch 7/150  Train Loss=0.7359  Val Loss=0.6883  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 14 Epoch 8/150  Train Loss=0.7278  Val Loss=0.6885  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 14 Epoch 9/150  Train Loss=0.7300  Val Loss=0.6901  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 14 Epoch 10/150  Train Loss=0.7339  Val Loss=0.6883  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 14 Epoch 11/150  Train Loss=0.7227  Val Loss=0.6874  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 14 Epoch 12/150  Train Loss=0.7240  Val Loss=0.6864  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 14 Epoch 13/150  Train Loss=0.7121  Val Loss=0.6864  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 14 Epoch 14/150  Train Loss=0.7160  Val Loss=0.6858  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 14 Epoch 15/150  Train Loss=0.7169  Val Loss=0.6849  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 14 Epoch 16/150  Train Loss=0.7174  Val Loss=0.6838  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 14 Epoch 17/150  Train Loss=0.7029  Val Loss=0.6830  Val Acc=0.5820  Time=18.7s\n",
            "  Fold 14 Epoch 18/150  Train Loss=0.7140  Val Loss=0.6707  Val Acc=0.6022  Time=19.0s\n",
            "  Fold 14 Epoch 19/150  Train Loss=0.7073  Val Loss=0.6529  Val Acc=0.6037  Time=18.7s\n",
            "  Fold 14 Epoch 20/150  Train Loss=0.6782  Val Loss=0.5903  Val Acc=0.7090  Time=19.1s\n",
            "  Fold 14 Epoch 21/150  Train Loss=0.6313  Val Loss=0.5088  Val Acc=0.8173  Time=18.8s\n",
            "  Fold 14 Epoch 22/150  Train Loss=0.5904  Val Loss=0.5758  Val Acc=0.6207  Time=19.0s\n",
            "  Fold 14 Epoch 23/150  Train Loss=0.5604  Val Loss=0.4477  Val Acc=0.8065  Time=18.8s\n",
            "  Fold 14 Epoch 24/150  Train Loss=0.5274  Val Loss=0.4315  Val Acc=0.8204  Time=19.0s\n",
            "  Fold 14 Epoch 25/150  Train Loss=0.5168  Val Loss=0.4210  Val Acc=0.8282  Time=18.9s\n",
            "  Fold 14 Epoch 26/150  Train Loss=0.5137  Val Loss=0.4186  Val Acc=0.8328  Time=18.8s\n",
            "  Fold 14 Epoch 27/150  Train Loss=0.5008  Val Loss=0.4093  Val Acc=0.8313  Time=18.8s\n",
            "  Fold 14 Epoch 28/150  Train Loss=0.4828  Val Loss=0.4112  Val Acc=0.8266  Time=18.8s\n",
            "  Fold 14 Epoch 29/150  Train Loss=0.4734  Val Loss=0.4021  Val Acc=0.8313  Time=18.9s\n",
            "  Fold 14 Epoch 30/150  Train Loss=0.4566  Val Loss=0.4072  Val Acc=0.8266  Time=18.8s\n",
            "  Fold 14 Epoch 31/150  Train Loss=0.4489  Val Loss=0.4366  Val Acc=0.7848  Time=18.8s\n",
            "  Fold 14 Epoch 32/150  Train Loss=0.4219  Val Loss=0.3943  Val Acc=0.8251  Time=18.8s\n",
            "  Fold 14 Epoch 33/150  Train Loss=0.4186  Val Loss=0.3993  Val Acc=0.8142  Time=18.8s\n",
            "  Fold 14 Epoch 34/150  Train Loss=0.4202  Val Loss=0.3959  Val Acc=0.8204  Time=18.9s\n",
            "  Fold 14 Epoch 35/150  Train Loss=0.4009  Val Loss=0.4014  Val Acc=0.8158  Time=18.9s\n",
            "  Fold 14 Epoch 36/150  Train Loss=0.4020  Val Loss=0.4027  Val Acc=0.8050  Time=18.9s\n",
            "  Fold 14 Epoch 37/150  Train Loss=0.3928  Val Loss=0.4163  Val Acc=0.7879  Time=19.0s\n",
            "  Fold 14 Epoch 38/150  Train Loss=0.3863  Val Loss=0.4377  Val Acc=0.7786  Time=18.9s\n",
            "  Fold 14 Epoch 39/150  Train Loss=0.3789  Val Loss=0.4676  Val Acc=0.7461  Time=19.0s\n",
            "  Fold 14 Epoch 40/150  Train Loss=0.3718  Val Loss=0.4245  Val Acc=0.7740  Time=18.8s\n",
            "  Fold 14 Epoch 41/150  Train Loss=0.3623  Val Loss=0.4456  Val Acc=0.7724  Time=18.9s\n",
            "  Fold 14 Epoch 42/150  Train Loss=0.3592  Val Loss=0.4186  Val Acc=0.8111  Time=18.8s\n",
            "  Fold 14 Epoch 43/150  Train Loss=0.3318  Val Loss=0.4231  Val Acc=0.8080  Time=19.2s\n",
            "  Fold 14 Epoch 44/150  Train Loss=0.3371  Val Loss=0.4175  Val Acc=0.8111  Time=18.9s\n",
            "  Fold 14 Epoch 45/150  Train Loss=0.3359  Val Loss=0.4224  Val Acc=0.8127  Time=19.1s\n",
            "  Fold 14 Epoch 46/150  Train Loss=0.3311  Val Loss=0.4225  Val Acc=0.8080  Time=18.7s\n",
            "  Fold 14 Epoch 47/150  Train Loss=0.3314  Val Loss=0.4993  Val Acc=0.7941  Time=19.1s\n",
            "  → Early stopping at epoch 47 (no val_loss improvement)\n",
            "  Fold 14 Final ACC = 0.5263   (TP=30  TN=0  FP=0  FN=27)\n",
            "\n",
            ">>> Fold 15/65  (leave out sub-015)\n",
            "  Fold 15 Epoch 1/150  Train Loss=0.8107  Val Loss=0.7191  Val Acc=0.4211  Time=18.7s\n",
            "  Fold 15 Epoch 2/150  Train Loss=0.7520  Val Loss=0.6936  Val Acc=0.4954  Time=19.1s\n",
            "  Fold 15 Epoch 3/150  Train Loss=0.7361  Val Loss=0.6919  Val Acc=0.5820  Time=18.8s\n",
            "  Fold 15 Epoch 4/150  Train Loss=0.7346  Val Loss=0.6933  Val Acc=0.4969  Time=19.0s\n",
            "  Fold 15 Epoch 5/150  Train Loss=0.7356  Val Loss=0.6928  Val Acc=0.5372  Time=18.7s\n",
            "  Fold 15 Epoch 6/150  Train Loss=0.7244  Val Loss=0.6915  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 7/150  Train Loss=0.7283  Val Loss=0.6894  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 8/150  Train Loss=0.7220  Val Loss=0.6890  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 15 Epoch 9/150  Train Loss=0.7148  Val Loss=0.6897  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 10/150  Train Loss=0.7214  Val Loss=0.6887  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 15 Epoch 11/150  Train Loss=0.7093  Val Loss=0.6891  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 12/150  Train Loss=0.7016  Val Loss=0.6875  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 13/150  Train Loss=0.7085  Val Loss=0.6878  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 15 Epoch 14/150  Train Loss=0.7024  Val Loss=0.6865  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 15/150  Train Loss=0.7028  Val Loss=0.6841  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 16/150  Train Loss=0.6995  Val Loss=0.6864  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 15 Epoch 17/150  Train Loss=0.7043  Val Loss=0.6772  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 15 Epoch 18/150  Train Loss=0.6881  Val Loss=0.6646  Val Acc=0.5960  Time=18.7s\n",
            "  Fold 15 Epoch 19/150  Train Loss=0.6679  Val Loss=0.6083  Val Acc=0.7492  Time=19.0s\n",
            "  Fold 15 Epoch 20/150  Train Loss=0.6237  Val Loss=0.5637  Val Acc=0.7988  Time=18.7s\n",
            "  Fold 15 Epoch 21/150  Train Loss=0.5928  Val Loss=0.5167  Val Acc=0.8111  Time=19.0s\n",
            "  Fold 15 Epoch 22/150  Train Loss=0.5814  Val Loss=0.5183  Val Acc=0.8127  Time=18.8s\n",
            "  Fold 15 Epoch 23/150  Train Loss=0.5616  Val Loss=0.5077  Val Acc=0.8204  Time=19.0s\n",
            "  Fold 15 Epoch 24/150  Train Loss=0.5544  Val Loss=0.4988  Val Acc=0.7972  Time=18.7s\n",
            "  Fold 15 Epoch 25/150  Train Loss=0.5288  Val Loss=0.4653  Val Acc=0.8313  Time=18.9s\n",
            "  Fold 15 Epoch 26/150  Train Loss=0.5152  Val Loss=0.4637  Val Acc=0.8173  Time=18.8s\n",
            "  Fold 15 Epoch 27/150  Train Loss=0.5045  Val Loss=0.4304  Val Acc=0.8406  Time=18.8s\n",
            "  Fold 15 Epoch 28/150  Train Loss=0.4937  Val Loss=0.4294  Val Acc=0.8406  Time=18.8s\n",
            "  Fold 15 Epoch 29/150  Train Loss=0.4878  Val Loss=0.4289  Val Acc=0.8375  Time=18.9s\n",
            "  Fold 15 Epoch 30/150  Train Loss=0.4709  Val Loss=0.4104  Val Acc=0.8235  Time=18.8s\n",
            "  Fold 15 Epoch 31/150  Train Loss=0.4504  Val Loss=0.4377  Val Acc=0.8080  Time=18.9s\n",
            "  Fold 15 Epoch 32/150  Train Loss=0.4309  Val Loss=0.4159  Val Acc=0.8452  Time=18.9s\n",
            "  Fold 15 Epoch 33/150  Train Loss=0.4213  Val Loss=0.4130  Val Acc=0.8297  Time=18.8s\n",
            "  Fold 15 Epoch 34/150  Train Loss=0.4227  Val Loss=0.4622  Val Acc=0.7663  Time=18.8s\n",
            "  Fold 15 Epoch 35/150  Train Loss=0.3919  Val Loss=0.4277  Val Acc=0.8080  Time=18.8s\n",
            "  Fold 15 Epoch 36/150  Train Loss=0.3912  Val Loss=0.4233  Val Acc=0.8189  Time=18.9s\n",
            "  Fold 15 Epoch 37/150  Train Loss=0.3769  Val Loss=0.4238  Val Acc=0.8003  Time=18.9s\n",
            "  Fold 15 Epoch 38/150  Train Loss=0.3836  Val Loss=0.4640  Val Acc=0.7570  Time=19.0s\n",
            "  Fold 15 Epoch 39/150  Train Loss=0.3689  Val Loss=0.4864  Val Acc=0.7492  Time=18.9s\n",
            "  Fold 15 Epoch 40/150  Train Loss=0.3746  Val Loss=0.4223  Val Acc=0.8080  Time=19.0s\n",
            "  Fold 15 Epoch 41/150  Train Loss=0.3566  Val Loss=0.4270  Val Acc=0.8173  Time=18.7s\n",
            "  Fold 15 Epoch 42/150  Train Loss=0.3489  Val Loss=0.5001  Val Acc=0.7368  Time=19.0s\n",
            "  Fold 15 Epoch 43/150  Train Loss=0.3340  Val Loss=0.4963  Val Acc=0.7446  Time=18.9s\n",
            "  Fold 15 Epoch 44/150  Train Loss=0.3424  Val Loss=0.4478  Val Acc=0.7786  Time=18.9s\n",
            "  Fold 15 Epoch 45/150  Train Loss=0.3267  Val Loss=0.4218  Val Acc=0.8080  Time=18.9s\n",
            "  → Early stopping at epoch 45 (no val_loss improvement)\n",
            "  Fold 15 Final ACC = 1.0000   (TP=57  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 16/65  (leave out sub-016)\n",
            "  Fold 16 Epoch 1/150  Train Loss=0.7617  Val Loss=0.7053  Val Acc=0.4184  Time=19.0s\n",
            "  Fold 16 Epoch 2/150  Train Loss=0.7557  Val Loss=0.7132  Val Acc=0.4184  Time=18.8s\n",
            "  Fold 16 Epoch 3/150  Train Loss=0.7459  Val Loss=0.6977  Val Acc=0.4215  Time=19.1s\n",
            "  Fold 16 Epoch 4/150  Train Loss=0.7399  Val Loss=0.6905  Val Acc=0.5645  Time=18.8s\n",
            "  Fold 16 Epoch 5/150  Train Loss=0.7432  Val Loss=0.6974  Val Acc=0.4292  Time=19.1s\n",
            "  Fold 16 Epoch 6/150  Train Loss=0.7383  Val Loss=0.6932  Val Acc=0.5070  Time=18.8s\n",
            "  Fold 16 Epoch 7/150  Train Loss=0.7355  Val Loss=0.6763  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 16 Epoch 8/150  Train Loss=0.7267  Val Loss=0.6892  Val Acc=0.5754  Time=18.8s\n",
            "  Fold 16 Epoch 9/150  Train Loss=0.7232  Val Loss=0.6904  Val Acc=0.5770  Time=19.0s\n",
            "  Fold 16 Epoch 10/150  Train Loss=0.7196  Val Loss=0.6887  Val Acc=0.5816  Time=18.8s\n",
            "  Fold 16 Epoch 11/150  Train Loss=0.7139  Val Loss=0.6812  Val Acc=0.5816  Time=19.1s\n",
            "  Fold 16 Epoch 12/150  Train Loss=0.7113  Val Loss=0.6791  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 16 Epoch 13/150  Train Loss=0.7104  Val Loss=0.6848  Val Acc=0.5910  Time=19.0s\n",
            "  Fold 16 Epoch 14/150  Train Loss=0.7228  Val Loss=0.6829  Val Acc=0.6003  Time=18.9s\n",
            "  Fold 16 Epoch 15/150  Train Loss=0.7115  Val Loss=0.6698  Val Acc=0.5832  Time=18.9s\n",
            "  Fold 16 Epoch 16/150  Train Loss=0.6994  Val Loss=0.6407  Val Acc=0.6065  Time=18.9s\n",
            "  Fold 16 Epoch 17/150  Train Loss=0.6628  Val Loss=0.5897  Val Acc=0.7900  Time=18.9s\n",
            "  Fold 16 Epoch 18/150  Train Loss=0.6378  Val Loss=0.5153  Val Acc=0.8118  Time=19.0s\n",
            "  Fold 16 Epoch 19/150  Train Loss=0.6017  Val Loss=0.4289  Val Acc=0.8554  Time=18.9s\n",
            "  Fold 16 Epoch 20/150  Train Loss=0.5789  Val Loss=0.4932  Val Acc=0.7527  Time=18.9s\n",
            "  Fold 16 Epoch 21/150  Train Loss=0.5453  Val Loss=0.4293  Val Acc=0.8834  Time=18.9s\n",
            "  Fold 16 Epoch 22/150  Train Loss=0.5456  Val Loss=0.4047  Val Acc=0.8678  Time=18.9s\n",
            "  Fold 16 Epoch 23/150  Train Loss=0.5326  Val Loss=0.4494  Val Acc=0.7589  Time=18.8s\n",
            "  Fold 16 Epoch 24/150  Train Loss=0.5025  Val Loss=0.3704  Val Acc=0.8802  Time=18.9s\n",
            "  Fold 16 Epoch 25/150  Train Loss=0.4982  Val Loss=0.3455  Val Acc=0.9020  Time=19.0s\n",
            "  Fold 16 Epoch 26/150  Train Loss=0.4777  Val Loss=0.3897  Val Acc=0.8600  Time=18.8s\n",
            "  Fold 16 Epoch 27/150  Train Loss=0.5115  Val Loss=0.3891  Val Acc=0.8631  Time=18.9s\n",
            "  Fold 16 Epoch 28/150  Train Loss=0.4767  Val Loss=0.3593  Val Acc=0.8989  Time=19.0s\n",
            "  Fold 16 Epoch 29/150  Train Loss=0.4486  Val Loss=0.3944  Val Acc=0.8383  Time=18.9s\n",
            "  Fold 16 Epoch 30/150  Train Loss=0.4516  Val Loss=0.4535  Val Acc=0.7294  Time=19.0s\n",
            "  Fold 16 Epoch 31/150  Train Loss=0.4418  Val Loss=0.3739  Val Acc=0.8523  Time=18.8s\n",
            "  Fold 16 Epoch 32/150  Train Loss=0.4318  Val Loss=0.4147  Val Acc=0.8103  Time=19.0s\n",
            "  Fold 16 Epoch 33/150  Train Loss=0.4304  Val Loss=0.3972  Val Acc=0.8243  Time=18.8s\n",
            "  Fold 16 Epoch 34/150  Train Loss=0.4017  Val Loss=0.4459  Val Acc=0.7776  Time=19.0s\n",
            "  Fold 16 Epoch 35/150  Train Loss=0.4021  Val Loss=0.3765  Val Acc=0.8460  Time=19.1s\n",
            "  Fold 16 Epoch 36/150  Train Loss=0.3946  Val Loss=0.3755  Val Acc=0.8491  Time=19.1s\n",
            "  Fold 16 Epoch 37/150  Train Loss=0.3841  Val Loss=0.4041  Val Acc=0.8196  Time=18.8s\n",
            "  Fold 16 Epoch 38/150  Train Loss=0.3761  Val Loss=0.4408  Val Acc=0.8040  Time=19.0s\n",
            "  Fold 16 Epoch 39/150  Train Loss=0.3872  Val Loss=0.4563  Val Acc=0.7776  Time=18.8s\n",
            "  Fold 16 Epoch 40/150  Train Loss=0.3554  Val Loss=0.5168  Val Acc=0.7170  Time=18.9s\n",
            "  → Early stopping at epoch 40 (no val_loss improvement)\n",
            "  Fold 16 Final ACC = 0.3750   (TP=21  TN=0  FP=0  FN=35)\n",
            "\n",
            ">>> Fold 17/65  (leave out sub-017)\n",
            "  Fold 17 Epoch 1/150  Train Loss=0.7227  Val Loss=0.6850  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 2/150  Train Loss=0.7290  Val Loss=0.6860  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 17 Epoch 3/150  Train Loss=0.7142  Val Loss=0.6870  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 4/150  Train Loss=0.7102  Val Loss=0.6869  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 5/150  Train Loss=0.7217  Val Loss=0.6865  Val Acc=0.5835  Time=18.7s\n",
            "  Fold 17 Epoch 6/150  Train Loss=0.7138  Val Loss=0.6881  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 17 Epoch 7/150  Train Loss=0.7147  Val Loss=0.6864  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 17 Epoch 8/150  Train Loss=0.7074  Val Loss=0.6865  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 9/150  Train Loss=0.7070  Val Loss=0.6870  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 17 Epoch 10/150  Train Loss=0.7048  Val Loss=0.6870  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 11/150  Train Loss=0.7055  Val Loss=0.6869  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 12/150  Train Loss=0.7101  Val Loss=0.6868  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 17 Epoch 13/150  Train Loss=0.7128  Val Loss=0.6861  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 17 Epoch 14/150  Train Loss=0.7018  Val Loss=0.6860  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 17 Epoch 15/150  Train Loss=0.7030  Val Loss=0.6854  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 17 Epoch 16/150  Train Loss=0.7001  Val Loss=0.6856  Val Acc=0.5835  Time=18.9s\n",
            "  → Early stopping at epoch 16 (no val_loss improvement)\n",
            "  Fold 17 Final ACC = 0.0000   (TP=0  TN=0  FP=0  FN=56)\n",
            "\n",
            ">>> Fold 18/65  (leave out sub-018)\n",
            "  Fold 18 Epoch 1/150  Train Loss=0.8209  Val Loss=0.7523  Val Acc=0.4103  Time=18.9s\n",
            "  Fold 18 Epoch 2/150  Train Loss=0.7596  Val Loss=0.7344  Val Acc=0.4103  Time=18.9s\n",
            "  Fold 18 Epoch 3/150  Train Loss=0.7548  Val Loss=0.7133  Val Acc=0.4103  Time=19.6s\n",
            "  Fold 18 Epoch 4/150  Train Loss=0.7564  Val Loss=0.7099  Val Acc=0.4103  Time=19.3s\n",
            "  Fold 18 Epoch 5/150  Train Loss=0.7442  Val Loss=0.7102  Val Acc=0.4103  Time=19.6s\n",
            "  Fold 18 Epoch 6/150  Train Loss=0.7398  Val Loss=0.7098  Val Acc=0.4103  Time=19.6s\n",
            "  Fold 18 Epoch 7/150  Train Loss=0.7544  Val Loss=0.7110  Val Acc=0.4103  Time=19.7s\n",
            "  Fold 18 Epoch 8/150  Train Loss=0.7425  Val Loss=0.6977  Val Acc=0.4072  Time=19.6s\n",
            "  Fold 18 Epoch 9/150  Train Loss=0.7389  Val Loss=0.7006  Val Acc=0.4103  Time=19.8s\n",
            "  Fold 18 Epoch 10/150  Train Loss=0.7406  Val Loss=0.7191  Val Acc=0.4103  Time=19.9s\n",
            "  Fold 18 Epoch 11/150  Train Loss=0.7311  Val Loss=0.7002  Val Acc=0.4103  Time=19.6s\n",
            "  Fold 18 Epoch 12/150  Train Loss=0.7301  Val Loss=0.7035  Val Acc=0.4103  Time=19.8s\n",
            "  Fold 18 Epoch 13/150  Train Loss=0.7334  Val Loss=0.6984  Val Acc=0.4103  Time=19.5s\n",
            "  Fold 18 Epoch 14/150  Train Loss=0.7311  Val Loss=0.6910  Val Acc=0.5943  Time=19.8s\n",
            "  Fold 18 Epoch 15/150  Train Loss=0.7256  Val Loss=0.6907  Val Acc=0.5988  Time=19.6s\n",
            "  Fold 18 Epoch 16/150  Train Loss=0.7250  Val Loss=0.6873  Val Acc=0.5897  Time=19.8s\n",
            "  Fold 18 Epoch 17/150  Train Loss=0.7154  Val Loss=0.6835  Val Acc=0.5897  Time=19.7s\n",
            "  Fold 18 Epoch 18/150  Train Loss=0.7171  Val Loss=0.6814  Val Acc=0.5897  Time=19.7s\n",
            "  Fold 18 Epoch 19/150  Train Loss=0.7158  Val Loss=0.6817  Val Acc=0.5897  Time=19.8s\n",
            "  Fold 18 Epoch 20/150  Train Loss=0.7042  Val Loss=0.6684  Val Acc=0.6048  Time=19.8s\n",
            "  Fold 18 Epoch 21/150  Train Loss=0.6815  Val Loss=0.6156  Val Acc=0.6727  Time=19.9s\n",
            "  Fold 18 Epoch 22/150  Train Loss=0.6421  Val Loss=0.5891  Val Acc=0.7179  Time=19.6s\n",
            "  Fold 18 Epoch 23/150  Train Loss=0.5961  Val Loss=0.5198  Val Acc=0.7436  Time=19.8s\n",
            "  Fold 18 Epoch 24/150  Train Loss=0.5507  Val Loss=0.5240  Val Acc=0.7451  Time=19.6s\n",
            "  Fold 18 Epoch 25/150  Train Loss=0.5380  Val Loss=0.5360  Val Acc=0.7345  Time=19.0s\n",
            "  Fold 18 Epoch 26/150  Train Loss=0.5256  Val Loss=0.5296  Val Acc=0.7436  Time=18.7s\n",
            "  Fold 18 Epoch 27/150  Train Loss=0.5070  Val Loss=0.4707  Val Acc=0.7783  Time=19.1s\n",
            "  Fold 18 Epoch 28/150  Train Loss=0.4930  Val Loss=0.4611  Val Acc=0.7828  Time=18.8s\n",
            "  Fold 18 Epoch 29/150  Train Loss=0.4857  Val Loss=0.5585  Val Acc=0.7255  Time=19.1s\n",
            "  Fold 18 Epoch 30/150  Train Loss=0.4804  Val Loss=0.4934  Val Acc=0.7572  Time=18.9s\n",
            "  Fold 18 Epoch 31/150  Train Loss=0.4625  Val Loss=0.4584  Val Acc=0.7753  Time=19.1s\n",
            "  Fold 18 Epoch 32/150  Train Loss=0.4522  Val Loss=0.5440  Val Acc=0.7179  Time=18.9s\n",
            "  Fold 18 Epoch 33/150  Train Loss=0.4538  Val Loss=0.4937  Val Acc=0.7572  Time=19.2s\n",
            "  Fold 18 Epoch 34/150  Train Loss=0.4340  Val Loss=0.4587  Val Acc=0.7903  Time=18.9s\n",
            "  Fold 18 Epoch 35/150  Train Loss=0.4122  Val Loss=0.4647  Val Acc=0.7722  Time=19.0s\n",
            "  Fold 18 Epoch 36/150  Train Loss=0.4084  Val Loss=0.4334  Val Acc=0.7903  Time=19.0s\n",
            "  Fold 18 Epoch 37/150  Train Loss=0.4043  Val Loss=0.4455  Val Acc=0.7903  Time=18.9s\n",
            "  Fold 18 Epoch 38/150  Train Loss=0.3940  Val Loss=0.4936  Val Acc=0.7602  Time=18.9s\n",
            "  Fold 18 Epoch 39/150  Train Loss=0.3830  Val Loss=0.4619  Val Acc=0.7873  Time=18.9s\n",
            "  Fold 18 Epoch 40/150  Train Loss=0.3881  Val Loss=0.4567  Val Acc=0.7888  Time=18.8s\n",
            "  Fold 18 Epoch 41/150  Train Loss=0.3685  Val Loss=0.4496  Val Acc=0.7964  Time=18.9s\n",
            "  Fold 18 Epoch 42/150  Train Loss=0.3533  Val Loss=0.4674  Val Acc=0.7798  Time=18.9s\n",
            "  Fold 18 Epoch 43/150  Train Loss=0.3576  Val Loss=0.5244  Val Acc=0.7511  Time=18.9s\n",
            "  Fold 18 Epoch 44/150  Train Loss=0.3553  Val Loss=0.4176  Val Acc=0.8039  Time=18.7s\n",
            "  Fold 18 Epoch 45/150  Train Loss=0.3375  Val Loss=0.4520  Val Acc=0.7888  Time=18.9s\n",
            "  Fold 18 Epoch 46/150  Train Loss=0.3380  Val Loss=0.4383  Val Acc=0.7919  Time=18.8s\n",
            "  Fold 18 Epoch 47/150  Train Loss=0.3257  Val Loss=0.4420  Val Acc=0.7934  Time=19.0s\n",
            "  Fold 18 Epoch 48/150  Train Loss=0.3215  Val Loss=0.4160  Val Acc=0.8039  Time=19.0s\n",
            "  Fold 18 Epoch 49/150  Train Loss=0.3241  Val Loss=0.4867  Val Acc=0.7677  Time=18.7s\n",
            "  Fold 18 Epoch 50/150  Train Loss=0.3106  Val Loss=0.4297  Val Acc=0.7994  Time=18.9s\n",
            "  Fold 18 Epoch 51/150  Train Loss=0.3154  Val Loss=0.4332  Val Acc=0.7994  Time=19.0s\n",
            "  Fold 18 Epoch 52/150  Train Loss=0.3165  Val Loss=0.4809  Val Acc=0.7783  Time=18.9s\n",
            "  Fold 18 Epoch 53/150  Train Loss=0.3190  Val Loss=0.5309  Val Acc=0.7511  Time=18.8s\n",
            "  Fold 18 Epoch 54/150  Train Loss=0.2952  Val Loss=0.4579  Val Acc=0.7903  Time=19.0s\n",
            "  Fold 18 Epoch 55/150  Train Loss=0.2793  Val Loss=0.5482  Val Acc=0.7481  Time=18.8s\n",
            "  Fold 18 Epoch 56/150  Train Loss=0.3026  Val Loss=0.4483  Val Acc=0.7919  Time=19.0s\n",
            "  Fold 18 Epoch 57/150  Train Loss=0.2823  Val Loss=0.4279  Val Acc=0.8039  Time=18.8s\n",
            "  Fold 18 Epoch 58/150  Train Loss=0.2985  Val Loss=0.4467  Val Acc=0.7934  Time=18.9s\n",
            "  Fold 18 Epoch 59/150  Train Loss=0.2701  Val Loss=0.5291  Val Acc=0.7677  Time=18.9s\n",
            "  Fold 18 Epoch 60/150  Train Loss=0.2786  Val Loss=0.4571  Val Acc=0.7919  Time=19.0s\n",
            "  Fold 18 Epoch 61/150  Train Loss=0.2676  Val Loss=0.4908  Val Acc=0.7813  Time=18.8s\n",
            "  Fold 18 Epoch 62/150  Train Loss=0.2666  Val Loss=0.4249  Val Acc=0.8115  Time=19.1s\n",
            "  Fold 18 Epoch 63/150  Train Loss=0.2724  Val Loss=0.4834  Val Acc=0.7813  Time=18.8s\n",
            "  → Early stopping at epoch 63 (no val_loss improvement)\n",
            "  Fold 18 Final ACC = 0.5179   (TP=0  TN=29  FP=27  FN=0)\n",
            "\n",
            ">>> Fold 19/65  (leave out sub-019)\n",
            "  Fold 19 Epoch 1/150  Train Loss=0.7571  Val Loss=0.6792  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 19 Epoch 2/150  Train Loss=0.7628  Val Loss=0.6794  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 19 Epoch 3/150  Train Loss=0.7549  Val Loss=0.6796  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 19 Epoch 4/150  Train Loss=0.7397  Val Loss=0.6809  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 19 Epoch 5/150  Train Loss=0.7348  Val Loss=0.6846  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 19 Epoch 6/150  Train Loss=0.7486  Val Loss=0.6802  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 19 Epoch 7/150  Train Loss=0.7349  Val Loss=0.6821  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 19 Epoch 8/150  Train Loss=0.7413  Val Loss=0.6808  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 19 Epoch 9/150  Train Loss=0.7395  Val Loss=0.6793  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 19 Epoch 10/150  Train Loss=0.7400  Val Loss=0.6799  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 19 Epoch 11/150  Train Loss=0.7337  Val Loss=0.6792  Val Acc=0.5916  Time=19.1s\n",
            "  Fold 19 Epoch 12/150  Train Loss=0.7358  Val Loss=0.6818  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 19 Epoch 13/150  Train Loss=0.7190  Val Loss=0.6810  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 19 Epoch 14/150  Train Loss=0.7257  Val Loss=0.6798  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 19 Epoch 15/150  Train Loss=0.7263  Val Loss=0.6797  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 19 Epoch 16/150  Train Loss=0.7234  Val Loss=0.6846  Val Acc=0.5916  Time=18.8s\n",
            "  → Early stopping at epoch 16 (no val_loss improvement)\n",
            "  Fold 19 Final ACC = 1.0000   (TP=0  TN=55  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 20/65  (leave out sub-020)\n",
            "  Fold 20 Epoch 1/150  Train Loss=0.7364  Val Loss=0.6892  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 2/150  Train Loss=0.7206  Val Loss=0.6875  Val Acc=0.5816  Time=18.8s\n",
            "  Fold 20 Epoch 3/150  Train Loss=0.7219  Val Loss=0.6878  Val Acc=0.5816  Time=18.8s\n",
            "  Fold 20 Epoch 4/150  Train Loss=0.7160  Val Loss=0.6877  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 5/150  Train Loss=0.7175  Val Loss=0.6873  Val Acc=0.5816  Time=18.7s\n",
            "  Fold 20 Epoch 6/150  Train Loss=0.7241  Val Loss=0.6876  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 7/150  Train Loss=0.7128  Val Loss=0.6870  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 8/150  Train Loss=0.7137  Val Loss=0.6871  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 9/150  Train Loss=0.7109  Val Loss=0.6869  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 10/150  Train Loss=0.7006  Val Loss=0.6873  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 11/150  Train Loss=0.7145  Val Loss=0.6871  Val Acc=0.5816  Time=18.8s\n",
            "  Fold 20 Epoch 12/150  Train Loss=0.7109  Val Loss=0.6868  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 13/150  Train Loss=0.7110  Val Loss=0.6873  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 14/150  Train Loss=0.7107  Val Loss=0.6865  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 15/150  Train Loss=0.7068  Val Loss=0.6872  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 16/150  Train Loss=0.7015  Val Loss=0.6868  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 17/150  Train Loss=0.7051  Val Loss=0.6870  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 18/150  Train Loss=0.7127  Val Loss=0.6854  Val Acc=0.5816  Time=19.0s\n",
            "  Fold 20 Epoch 19/150  Train Loss=0.7061  Val Loss=0.6863  Val Acc=0.5816  Time=18.9s\n",
            "  Fold 20 Epoch 20/150  Train Loss=0.7037  Val Loss=0.6838  Val Acc=0.5816  Time=19.2s\n",
            "  Fold 20 Epoch 21/150  Train Loss=0.7029  Val Loss=0.6763  Val Acc=0.5848  Time=18.8s\n",
            "  Fold 20 Epoch 22/150  Train Loss=0.6915  Val Loss=0.6636  Val Acc=0.7869  Time=19.1s\n",
            "  Fold 20 Epoch 23/150  Train Loss=0.6711  Val Loss=0.5852  Val Acc=0.8025  Time=18.9s\n",
            "  Fold 20 Epoch 24/150  Train Loss=0.6358  Val Loss=0.4948  Val Acc=0.8585  Time=19.0s\n",
            "  Fold 20 Epoch 25/150  Train Loss=0.5912  Val Loss=0.4871  Val Acc=0.8911  Time=18.8s\n",
            "  Fold 20 Epoch 26/150  Train Loss=0.5824  Val Loss=0.4609  Val Acc=0.8880  Time=19.0s\n",
            "  Fold 20 Epoch 27/150  Train Loss=0.5502  Val Loss=0.4601  Val Acc=0.8911  Time=18.9s\n",
            "  Fold 20 Epoch 28/150  Train Loss=0.5312  Val Loss=0.4193  Val Acc=0.8756  Time=18.8s\n",
            "  Fold 20 Epoch 29/150  Train Loss=0.5148  Val Loss=0.4247  Val Acc=0.8616  Time=19.0s\n",
            "  Fold 20 Epoch 30/150  Train Loss=0.4991  Val Loss=0.3649  Val Acc=0.8709  Time=18.8s\n",
            "  Fold 20 Epoch 31/150  Train Loss=0.4825  Val Loss=0.3954  Val Acc=0.8818  Time=18.8s\n",
            "  Fold 20 Epoch 32/150  Train Loss=0.4522  Val Loss=0.3601  Val Acc=0.8849  Time=18.8s\n",
            "  Fold 20 Epoch 33/150  Train Loss=0.4613  Val Loss=0.4098  Val Acc=0.8274  Time=18.9s\n",
            "  Fold 20 Epoch 34/150  Train Loss=0.4482  Val Loss=0.3437  Val Acc=0.9051  Time=18.7s\n",
            "  Fold 20 Epoch 35/150  Train Loss=0.4210  Val Loss=0.4707  Val Acc=0.6952  Time=18.9s\n",
            "  Fold 20 Epoch 36/150  Train Loss=0.4079  Val Loss=0.3952  Val Acc=0.8087  Time=18.8s\n",
            "  Fold 20 Epoch 37/150  Train Loss=0.4022  Val Loss=0.4034  Val Acc=0.7947  Time=18.9s\n",
            "  Fold 20 Epoch 38/150  Train Loss=0.3904  Val Loss=0.3551  Val Acc=0.8554  Time=18.9s\n",
            "  Fold 20 Epoch 39/150  Train Loss=0.3848  Val Loss=0.3414  Val Acc=0.8787  Time=19.0s\n",
            "  Fold 20 Epoch 40/150  Train Loss=0.3699  Val Loss=0.4067  Val Acc=0.7854  Time=18.7s\n",
            "  Fold 20 Epoch 41/150  Train Loss=0.3640  Val Loss=0.3515  Val Acc=0.8523  Time=19.0s\n",
            "  Fold 20 Epoch 42/150  Train Loss=0.3521  Val Loss=0.4505  Val Acc=0.7496  Time=18.7s\n",
            "  Fold 20 Epoch 43/150  Train Loss=0.3482  Val Loss=0.3292  Val Acc=0.8725  Time=18.9s\n",
            "  Fold 20 Epoch 44/150  Train Loss=0.3299  Val Loss=0.3621  Val Acc=0.8367  Time=18.7s\n",
            "  Fold 20 Epoch 45/150  Train Loss=0.3231  Val Loss=0.3744  Val Acc=0.8258  Time=19.0s\n",
            "  Fold 20 Epoch 46/150  Train Loss=0.3266  Val Loss=0.3284  Val Acc=0.8616  Time=18.8s\n",
            "  Fold 20 Epoch 47/150  Train Loss=0.3251  Val Loss=0.3033  Val Acc=0.8725  Time=18.8s\n",
            "  Fold 20 Epoch 48/150  Train Loss=0.3279  Val Loss=0.3862  Val Acc=0.8180  Time=18.8s\n",
            "  Fold 20 Epoch 49/150  Train Loss=0.3047  Val Loss=0.3232  Val Acc=0.8663  Time=19.0s\n",
            "  Fold 20 Epoch 50/150  Train Loss=0.3062  Val Loss=0.4198  Val Acc=0.7932  Time=18.8s\n",
            "  Fold 20 Epoch 51/150  Train Loss=0.2954  Val Loss=0.4017  Val Acc=0.8289  Time=18.9s\n",
            "  Fold 20 Epoch 52/150  Train Loss=0.2814  Val Loss=0.4988  Val Acc=0.7543  Time=18.8s\n",
            "  Fold 20 Epoch 53/150  Train Loss=0.2778  Val Loss=0.3806  Val Acc=0.8351  Time=18.8s\n",
            "  Fold 20 Epoch 54/150  Train Loss=0.2751  Val Loss=0.4232  Val Acc=0.8118  Time=18.9s\n",
            "  Fold 20 Epoch 55/150  Train Loss=0.2772  Val Loss=0.3266  Val Acc=0.8678  Time=18.8s\n",
            "  Fold 20 Epoch 56/150  Train Loss=0.2751  Val Loss=0.3129  Val Acc=0.8663  Time=18.9s\n",
            "  Fold 20 Epoch 57/150  Train Loss=0.2791  Val Loss=0.4384  Val Acc=0.7947  Time=19.0s\n",
            "  Fold 20 Epoch 58/150  Train Loss=0.2731  Val Loss=0.4602  Val Acc=0.7885  Time=18.9s\n",
            "  Fold 20 Epoch 59/150  Train Loss=0.2637  Val Loss=0.3865  Val Acc=0.8336  Time=18.8s\n",
            "  Fold 20 Epoch 60/150  Train Loss=0.2703  Val Loss=0.5252  Val Acc=0.7512  Time=18.9s\n",
            "  Fold 20 Epoch 61/150  Train Loss=0.2441  Val Loss=0.3698  Val Acc=0.8445  Time=18.8s\n",
            "  Fold 20 Epoch 62/150  Train Loss=0.2440  Val Loss=0.4195  Val Acc=0.8134  Time=18.9s\n",
            "  → Early stopping at epoch 62 (no val_loss improvement)\n",
            "  Fold 20 Final ACC = 0.2037   (TP=11  TN=0  FP=0  FN=43)\n",
            "\n",
            ">>> Fold 21/65  (leave out sub-021)\n",
            "  Fold 21 Epoch 1/150  Train Loss=0.7770  Val Loss=0.7578  Val Acc=0.4184  Time=18.8s\n",
            "  Fold 21 Epoch 2/150  Train Loss=0.7492  Val Loss=0.7464  Val Acc=0.4184  Time=18.9s\n",
            "  Fold 21 Epoch 3/150  Train Loss=0.7521  Val Loss=0.7207  Val Acc=0.4184  Time=18.9s\n",
            "  Fold 21 Epoch 4/150  Train Loss=0.7597  Val Loss=0.7157  Val Acc=0.4184  Time=18.9s\n",
            "  Fold 21 Epoch 5/150  Train Loss=0.7379  Val Loss=0.7182  Val Acc=0.4184  Time=18.8s\n",
            "  Fold 21 Epoch 6/150  Train Loss=0.7293  Val Loss=0.7143  Val Acc=0.4184  Time=19.0s\n",
            "  Fold 21 Epoch 7/150  Train Loss=0.7254  Val Loss=0.7224  Val Acc=0.4184  Time=18.9s\n",
            "  Fold 21 Epoch 8/150  Train Loss=0.7286  Val Loss=0.7056  Val Acc=0.4184  Time=18.9s\n",
            "  Fold 21 Epoch 9/150  Train Loss=0.7213  Val Loss=0.6945  Val Acc=0.4681  Time=18.8s\n",
            "  Fold 21 Epoch 10/150  Train Loss=0.7214  Val Loss=0.6954  Val Acc=0.4386  Time=19.0s\n",
            "  Fold 21 Epoch 11/150  Train Loss=0.7184  Val Loss=0.6989  Val Acc=0.4184  Time=18.8s\n",
            "  Fold 21 Epoch 12/150  Train Loss=0.7098  Val Loss=0.6933  Val Acc=0.5132  Time=18.9s\n",
            "  Fold 21 Epoch 13/150  Train Loss=0.7061  Val Loss=0.6879  Val Acc=0.5801  Time=18.8s\n",
            "  Fold 21 Epoch 14/150  Train Loss=0.7122  Val Loss=0.6902  Val Acc=0.5988  Time=19.0s\n",
            "  Fold 21 Epoch 15/150  Train Loss=0.7125  Val Loss=0.6925  Val Acc=0.5132  Time=18.9s\n",
            "  Fold 21 Epoch 16/150  Train Loss=0.7032  Val Loss=0.6868  Val Acc=0.6516  Time=18.9s\n",
            "  Fold 21 Epoch 17/150  Train Loss=0.7020  Val Loss=0.6790  Val Acc=0.6003  Time=18.9s\n",
            "  Fold 21 Epoch 18/150  Train Loss=0.7027  Val Loss=0.6765  Val Acc=0.6625  Time=19.1s\n",
            "  Fold 21 Epoch 19/150  Train Loss=0.6812  Val Loss=0.6461  Val Acc=0.7061  Time=18.9s\n",
            "  Fold 21 Epoch 20/150  Train Loss=0.6347  Val Loss=0.5177  Val Acc=0.8445  Time=18.9s\n",
            "  Fold 21 Epoch 21/150  Train Loss=0.5879  Val Loss=0.4954  Val Acc=0.8398  Time=18.9s\n",
            "  Fold 21 Epoch 22/150  Train Loss=0.5765  Val Loss=0.6132  Val Acc=0.5739  Time=18.9s\n",
            "  Fold 21 Epoch 23/150  Train Loss=0.5850  Val Loss=0.4857  Val Acc=0.8243  Time=18.9s\n",
            "  Fold 21 Epoch 24/150  Train Loss=0.5583  Val Loss=0.4779  Val Acc=0.8040  Time=18.9s\n",
            "  Fold 21 Epoch 25/150  Train Loss=0.5439  Val Loss=0.4335  Val Acc=0.8600  Time=19.0s\n",
            "  Fold 21 Epoch 26/150  Train Loss=0.5339  Val Loss=0.3931  Val Acc=0.8787  Time=18.9s\n",
            "  Fold 21 Epoch 27/150  Train Loss=0.5280  Val Loss=0.3907  Val Acc=0.8834  Time=18.9s\n",
            "  Fold 21 Epoch 28/150  Train Loss=0.5115  Val Loss=0.4442  Val Acc=0.8056  Time=18.9s\n",
            "  Fold 21 Epoch 29/150  Train Loss=0.5013  Val Loss=0.4346  Val Acc=0.8180  Time=18.9s\n",
            "  Fold 21 Epoch 30/150  Train Loss=0.4903  Val Loss=0.3946  Val Acc=0.8756  Time=18.8s\n",
            "  Fold 21 Epoch 31/150  Train Loss=0.4757  Val Loss=0.3898  Val Acc=0.8320  Time=18.9s\n",
            "  Fold 21 Epoch 32/150  Train Loss=0.4529  Val Loss=0.4193  Val Acc=0.7963  Time=18.7s\n",
            "  Fold 21 Epoch 33/150  Train Loss=0.4610  Val Loss=0.4043  Val Acc=0.8072  Time=18.9s\n",
            "  Fold 21 Epoch 34/150  Train Loss=0.4446  Val Loss=0.3753  Val Acc=0.8616  Time=18.8s\n",
            "  Fold 21 Epoch 35/150  Train Loss=0.4258  Val Loss=0.3610  Val Acc=0.8740  Time=18.9s\n",
            "  Fold 21 Epoch 36/150  Train Loss=0.4150  Val Loss=0.3999  Val Acc=0.8134  Time=18.8s\n",
            "  Fold 21 Epoch 37/150  Train Loss=0.4166  Val Loss=0.3717  Val Acc=0.8616  Time=18.9s\n",
            "  Fold 21 Epoch 38/150  Train Loss=0.3984  Val Loss=0.4672  Val Acc=0.7481  Time=18.7s\n",
            "  Fold 21 Epoch 39/150  Train Loss=0.3916  Val Loss=0.3774  Val Acc=0.8507  Time=18.9s\n",
            "  Fold 21 Epoch 40/150  Train Loss=0.3773  Val Loss=0.3893  Val Acc=0.8398  Time=18.9s\n",
            "  Fold 21 Epoch 41/150  Train Loss=0.3784  Val Loss=0.3771  Val Acc=0.8460  Time=19.0s\n",
            "  Fold 21 Epoch 42/150  Train Loss=0.3824  Val Loss=0.3763  Val Acc=0.8398  Time=18.8s\n",
            "  Fold 21 Epoch 43/150  Train Loss=0.3695  Val Loss=0.3758  Val Acc=0.8289  Time=19.0s\n",
            "  Fold 21 Epoch 44/150  Train Loss=0.3502  Val Loss=0.4602  Val Acc=0.7729  Time=18.9s\n",
            "  Fold 21 Epoch 45/150  Train Loss=0.3533  Val Loss=0.4073  Val Acc=0.7978  Time=18.9s\n",
            "  Fold 21 Epoch 46/150  Train Loss=0.3346  Val Loss=0.4047  Val Acc=0.8118  Time=18.9s\n",
            "  Fold 21 Epoch 47/150  Train Loss=0.3413  Val Loss=0.3936  Val Acc=0.8227  Time=18.9s\n",
            "  Fold 21 Epoch 48/150  Train Loss=0.3413  Val Loss=0.4018  Val Acc=0.8040  Time=18.9s\n",
            "  Fold 21 Epoch 49/150  Train Loss=0.3134  Val Loss=0.3963  Val Acc=0.8149  Time=18.7s\n",
            "  Fold 21 Epoch 50/150  Train Loss=0.3159  Val Loss=0.4240  Val Acc=0.7963  Time=18.9s\n",
            "  → Early stopping at epoch 50 (no val_loss improvement)\n",
            "  Fold 21 Final ACC = 1.0000   (TP=54  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 22/65  (leave out sub-022)\n",
            "  Fold 22 Epoch 1/150  Train Loss=0.7387  Val Loss=0.6791  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 22 Epoch 2/150  Train Loss=0.7310  Val Loss=0.6801  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 22 Epoch 3/150  Train Loss=0.7275  Val Loss=0.6828  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 22 Epoch 4/150  Train Loss=0.7388  Val Loss=0.6814  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 22 Epoch 5/150  Train Loss=0.7295  Val Loss=0.6828  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 22 Epoch 6/150  Train Loss=0.7324  Val Loss=0.6834  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 22 Epoch 7/150  Train Loss=0.7278  Val Loss=0.6863  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 22 Epoch 8/150  Train Loss=0.7227  Val Loss=0.6846  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 22 Epoch 9/150  Train Loss=0.7205  Val Loss=0.6787  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 22 Epoch 10/150  Train Loss=0.7296  Val Loss=0.6849  Val Acc=0.5916  Time=18.9s\n",
            "  Fold 22 Epoch 11/150  Train Loss=0.7226  Val Loss=0.6778  Val Acc=0.5916  Time=18.7s\n",
            "  Fold 22 Epoch 12/150  Train Loss=0.7191  Val Loss=0.6818  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 22 Epoch 13/150  Train Loss=0.7220  Val Loss=0.6772  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 22 Epoch 14/150  Train Loss=0.7280  Val Loss=0.6779  Val Acc=0.5916  Time=19.1s\n",
            "  Fold 22 Epoch 15/150  Train Loss=0.7224  Val Loss=0.6774  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 22 Epoch 16/150  Train Loss=0.7167  Val Loss=0.6744  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 22 Epoch 17/150  Train Loss=0.7105  Val Loss=0.6685  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 22 Epoch 18/150  Train Loss=0.7135  Val Loss=0.6607  Val Acc=0.5976  Time=19.0s\n",
            "  Fold 22 Epoch 19/150  Train Loss=0.7025  Val Loss=0.6256  Val Acc=0.6592  Time=18.9s\n",
            "  Fold 22 Epoch 20/150  Train Loss=0.6635  Val Loss=0.5943  Val Acc=0.6892  Time=19.1s\n",
            "  Fold 22 Epoch 21/150  Train Loss=0.6271  Val Loss=0.5738  Val Acc=0.7177  Time=18.8s\n",
            "  Fold 22 Epoch 22/150  Train Loss=0.5919  Val Loss=0.5740  Val Acc=0.6982  Time=19.0s\n",
            "  Fold 22 Epoch 23/150  Train Loss=0.5620  Val Loss=0.5810  Val Acc=0.6997  Time=19.1s\n",
            "  Fold 22 Epoch 24/150  Train Loss=0.5535  Val Loss=0.5225  Val Acc=0.7072  Time=19.1s\n",
            "  Fold 22 Epoch 25/150  Train Loss=0.5400  Val Loss=0.5233  Val Acc=0.7222  Time=18.9s\n",
            "  Fold 22 Epoch 26/150  Train Loss=0.5297  Val Loss=0.5075  Val Acc=0.7282  Time=18.9s\n",
            "  Fold 22 Epoch 27/150  Train Loss=0.5151  Val Loss=0.5100  Val Acc=0.7207  Time=18.9s\n",
            "  Fold 22 Epoch 28/150  Train Loss=0.4937  Val Loss=0.5355  Val Acc=0.7207  Time=18.9s\n",
            "  Fold 22 Epoch 29/150  Train Loss=0.4874  Val Loss=0.4921  Val Acc=0.7372  Time=18.9s\n",
            "  Fold 22 Epoch 30/150  Train Loss=0.4697  Val Loss=0.5320  Val Acc=0.7252  Time=18.8s\n",
            "  Fold 22 Epoch 31/150  Train Loss=0.4747  Val Loss=0.5290  Val Acc=0.7357  Time=18.9s\n",
            "  Fold 22 Epoch 32/150  Train Loss=0.4464  Val Loss=0.5582  Val Acc=0.7117  Time=18.9s\n",
            "  Fold 22 Epoch 33/150  Train Loss=0.4452  Val Loss=0.5669  Val Acc=0.7132  Time=18.8s\n",
            "  Fold 22 Epoch 34/150  Train Loss=0.4287  Val Loss=0.5117  Val Acc=0.7312  Time=19.0s\n",
            "  Fold 22 Epoch 35/150  Train Loss=0.4194  Val Loss=0.5393  Val Acc=0.7252  Time=18.9s\n",
            "  Fold 22 Epoch 36/150  Train Loss=0.4003  Val Loss=0.5365  Val Acc=0.7252  Time=19.0s\n",
            "  Fold 22 Epoch 37/150  Train Loss=0.3982  Val Loss=0.4836  Val Acc=0.7402  Time=19.0s\n",
            "  Fold 22 Epoch 38/150  Train Loss=0.3765  Val Loss=0.5182  Val Acc=0.7267  Time=18.9s\n",
            "  Fold 22 Epoch 39/150  Train Loss=0.3635  Val Loss=0.5489  Val Acc=0.7132  Time=19.0s\n",
            "  Fold 22 Epoch 40/150  Train Loss=0.3709  Val Loss=0.5258  Val Acc=0.7192  Time=18.8s\n",
            "  Fold 22 Epoch 41/150  Train Loss=0.3603  Val Loss=0.5076  Val Acc=0.7237  Time=19.0s\n",
            "  Fold 22 Epoch 42/150  Train Loss=0.3548  Val Loss=0.5180  Val Acc=0.7207  Time=19.0s\n",
            "  Fold 22 Epoch 43/150  Train Loss=0.3392  Val Loss=0.4995  Val Acc=0.7372  Time=19.0s\n",
            "  Fold 22 Epoch 44/150  Train Loss=0.3371  Val Loss=0.5567  Val Acc=0.7087  Time=18.9s\n",
            "  Fold 22 Epoch 45/150  Train Loss=0.3318  Val Loss=0.5604  Val Acc=0.7117  Time=19.0s\n",
            "  Fold 22 Epoch 46/150  Train Loss=0.3259  Val Loss=0.5215  Val Acc=0.7252  Time=18.8s\n",
            "  Fold 22 Epoch 47/150  Train Loss=0.3264  Val Loss=0.5757  Val Acc=0.7147  Time=19.1s\n",
            "  Fold 22 Epoch 48/150  Train Loss=0.3108  Val Loss=0.5246  Val Acc=0.7357  Time=19.0s\n",
            "  Fold 22 Epoch 49/150  Train Loss=0.3219  Val Loss=0.6286  Val Acc=0.7072  Time=19.1s\n",
            "  Fold 22 Epoch 50/150  Train Loss=0.3050  Val Loss=0.5602  Val Acc=0.7177  Time=18.7s\n",
            "  Fold 22 Epoch 51/150  Train Loss=0.3102  Val Loss=0.5028  Val Acc=0.7673  Time=19.0s\n",
            "  Fold 22 Epoch 52/150  Train Loss=0.2977  Val Loss=0.5879  Val Acc=0.7162  Time=18.8s\n",
            "  → Early stopping at epoch 52 (no val_loss improvement)\n",
            "  Fold 22 Final ACC = 1.0000   (TP=0  TN=54  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 23/65  (leave out sub-023)\n",
            "  Fold 23 Epoch 1/150  Train Loss=0.7855  Val Loss=0.7120  Val Acc=0.4084  Time=19.1s\n",
            "  Fold 23 Epoch 2/150  Train Loss=0.7554  Val Loss=0.7042  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 23 Epoch 3/150  Train Loss=0.7339  Val Loss=0.7011  Val Acc=0.4084  Time=19.1s\n",
            "  Fold 23 Epoch 4/150  Train Loss=0.7440  Val Loss=0.6996  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 23 Epoch 5/150  Train Loss=0.7321  Val Loss=0.6977  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 23 Epoch 6/150  Train Loss=0.7310  Val Loss=0.6913  Val Acc=0.5856  Time=18.9s\n",
            "  Fold 23 Epoch 7/150  Train Loss=0.7251  Val Loss=0.6951  Val Acc=0.4069  Time=19.1s\n",
            "  Fold 23 Epoch 8/150  Train Loss=0.7308  Val Loss=0.7001  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 23 Epoch 9/150  Train Loss=0.7240  Val Loss=0.6992  Val Acc=0.4099  Time=19.1s\n",
            "  Fold 23 Epoch 10/150  Train Loss=0.7159  Val Loss=0.7049  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 23 Epoch 11/150  Train Loss=0.7177  Val Loss=0.6972  Val Acc=0.4069  Time=19.0s\n",
            "  Fold 23 Epoch 12/150  Train Loss=0.7123  Val Loss=0.6956  Val Acc=0.4459  Time=18.9s\n",
            "  Fold 23 Epoch 13/150  Train Loss=0.7120  Val Loss=0.6981  Val Acc=0.4009  Time=19.0s\n",
            "  Fold 23 Epoch 14/150  Train Loss=0.7150  Val Loss=0.7008  Val Acc=0.4084  Time=19.1s\n",
            "  Fold 23 Epoch 15/150  Train Loss=0.7224  Val Loss=0.6958  Val Acc=0.4084  Time=18.9s\n",
            "  Fold 23 Epoch 16/150  Train Loss=0.7070  Val Loss=0.6918  Val Acc=0.5661  Time=18.9s\n",
            "  Fold 23 Epoch 17/150  Train Loss=0.7034  Val Loss=0.6955  Val Acc=0.4054  Time=19.0s\n",
            "  Fold 23 Epoch 18/150  Train Loss=0.7107  Val Loss=0.6977  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 23 Epoch 19/150  Train Loss=0.7026  Val Loss=0.7066  Val Acc=0.4084  Time=18.8s\n",
            "  Fold 23 Epoch 20/150  Train Loss=0.6935  Val Loss=0.6975  Val Acc=0.4264  Time=19.1s\n",
            "  Fold 23 Epoch 21/150  Train Loss=0.6919  Val Loss=0.6642  Val Acc=0.6637  Time=19.0s\n",
            "  Fold 23 Epoch 22/150  Train Loss=0.6583  Val Loss=0.6474  Val Acc=0.6577  Time=18.9s\n",
            "  Fold 23 Epoch 23/150  Train Loss=0.6203  Val Loss=0.6598  Val Acc=0.6111  Time=18.9s\n",
            "  Fold 23 Epoch 24/150  Train Loss=0.5622  Val Loss=0.6405  Val Acc=0.6622  Time=19.0s\n",
            "  Fold 23 Epoch 25/150  Train Loss=0.5358  Val Loss=0.6070  Val Acc=0.6922  Time=18.9s\n",
            "  Fold 23 Epoch 26/150  Train Loss=0.5176  Val Loss=0.5111  Val Acc=0.7447  Time=18.9s\n",
            "  Fold 23 Epoch 27/150  Train Loss=0.5064  Val Loss=0.5230  Val Acc=0.7057  Time=18.8s\n",
            "  Fold 23 Epoch 28/150  Train Loss=0.4867  Val Loss=0.5568  Val Acc=0.7072  Time=19.0s\n",
            "  Fold 23 Epoch 29/150  Train Loss=0.4796  Val Loss=0.5450  Val Acc=0.7117  Time=18.8s\n",
            "  Fold 23 Epoch 30/150  Train Loss=0.4692  Val Loss=0.5639  Val Acc=0.6877  Time=19.1s\n",
            "  Fold 23 Epoch 31/150  Train Loss=0.4513  Val Loss=0.5025  Val Acc=0.7252  Time=19.0s\n",
            "  Fold 23 Epoch 32/150  Train Loss=0.4445  Val Loss=0.5458  Val Acc=0.7072  Time=19.1s\n",
            "  Fold 23 Epoch 33/150  Train Loss=0.4320  Val Loss=0.5521  Val Acc=0.7087  Time=19.0s\n",
            "  Fold 23 Epoch 34/150  Train Loss=0.4273  Val Loss=0.5824  Val Acc=0.6892  Time=19.0s\n",
            "  Fold 23 Epoch 35/150  Train Loss=0.4101  Val Loss=0.5326  Val Acc=0.7252  Time=18.9s\n",
            "  Fold 23 Epoch 36/150  Train Loss=0.4120  Val Loss=0.5103  Val Acc=0.7312  Time=19.0s\n",
            "  Fold 23 Epoch 37/150  Train Loss=0.3939  Val Loss=0.5492  Val Acc=0.7072  Time=19.0s\n",
            "  Fold 23 Epoch 38/150  Train Loss=0.3788  Val Loss=0.4975  Val Acc=0.7447  Time=19.1s\n",
            "  Fold 23 Epoch 39/150  Train Loss=0.3712  Val Loss=0.5219  Val Acc=0.7162  Time=18.9s\n",
            "  Fold 23 Epoch 40/150  Train Loss=0.3603  Val Loss=0.5002  Val Acc=0.7432  Time=19.1s\n",
            "  Fold 23 Epoch 41/150  Train Loss=0.3730  Val Loss=0.4926  Val Acc=0.7462  Time=18.9s\n",
            "  Fold 23 Epoch 42/150  Train Loss=0.3629  Val Loss=0.4835  Val Acc=0.7553  Time=19.0s\n",
            "  Fold 23 Epoch 43/150  Train Loss=0.3425  Val Loss=0.5048  Val Acc=0.7508  Time=18.9s\n",
            "  Fold 23 Epoch 44/150  Train Loss=0.3359  Val Loss=0.6294  Val Acc=0.6982  Time=19.1s\n",
            "  Fold 23 Epoch 45/150  Train Loss=0.3276  Val Loss=0.5161  Val Acc=0.7402  Time=19.0s\n",
            "  Fold 23 Epoch 46/150  Train Loss=0.3222  Val Loss=0.4922  Val Acc=0.7538  Time=19.1s\n",
            "  Fold 23 Epoch 47/150  Train Loss=0.3178  Val Loss=0.4707  Val Acc=0.7748  Time=19.0s\n",
            "  Fold 23 Epoch 48/150  Train Loss=0.3156  Val Loss=0.5930  Val Acc=0.6997  Time=19.1s\n",
            "  Fold 23 Epoch 49/150  Train Loss=0.3030  Val Loss=0.5205  Val Acc=0.7297  Time=19.0s\n",
            "  Fold 23 Epoch 50/150  Train Loss=0.2971  Val Loss=0.4733  Val Acc=0.7748  Time=19.1s\n",
            "  Fold 23 Epoch 51/150  Train Loss=0.3033  Val Loss=0.5107  Val Acc=0.7553  Time=19.0s\n",
            "  Fold 23 Epoch 52/150  Train Loss=0.2941  Val Loss=0.4779  Val Acc=0.7748  Time=19.1s\n",
            "  Fold 23 Epoch 53/150  Train Loss=0.2961  Val Loss=0.5484  Val Acc=0.7222  Time=19.1s\n",
            "  Fold 23 Epoch 54/150  Train Loss=0.2904  Val Loss=0.6100  Val Acc=0.7042  Time=18.9s\n",
            "  Fold 23 Epoch 55/150  Train Loss=0.2863  Val Loss=0.5326  Val Acc=0.7312  Time=18.8s\n",
            "  Fold 23 Epoch 56/150  Train Loss=0.2897  Val Loss=0.5262  Val Acc=0.7462  Time=19.0s\n",
            "  Fold 23 Epoch 57/150  Train Loss=0.2683  Val Loss=0.4832  Val Acc=0.7763  Time=19.1s\n",
            "  Fold 23 Epoch 58/150  Train Loss=0.2765  Val Loss=0.5081  Val Acc=0.7492  Time=18.9s\n",
            "  Fold 23 Epoch 59/150  Train Loss=0.2688  Val Loss=0.5538  Val Acc=0.7357  Time=18.8s\n",
            "  Fold 23 Epoch 60/150  Train Loss=0.2691  Val Loss=0.4843  Val Acc=0.7793  Time=18.8s\n",
            "  Fold 23 Epoch 61/150  Train Loss=0.2564  Val Loss=0.6020  Val Acc=0.7192  Time=18.9s\n",
            "  Fold 23 Epoch 62/150  Train Loss=0.2593  Val Loss=0.5853  Val Acc=0.7072  Time=18.9s\n",
            "  → Early stopping at epoch 62 (no val_loss improvement)\n",
            "  Fold 23 Final ACC = 1.0000   (TP=0  TN=54  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 24/65  (leave out sub-024)\n",
            "  Fold 24 Epoch 1/150  Train Loss=0.8111  Val Loss=0.7232  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 24 Epoch 2/150  Train Loss=0.8132  Val Loss=0.7224  Val Acc=0.4211  Time=18.7s\n",
            "  Fold 24 Epoch 3/150  Train Loss=0.7914  Val Loss=0.7130  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 24 Epoch 4/150  Train Loss=0.7757  Val Loss=0.7048  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 24 Epoch 5/150  Train Loss=0.7703  Val Loss=0.7018  Val Acc=0.4211  Time=18.9s\n",
            "  Fold 24 Epoch 6/150  Train Loss=0.7530  Val Loss=0.6998  Val Acc=0.4211  Time=18.7s\n",
            "  Fold 24 Epoch 7/150  Train Loss=0.7460  Val Loss=0.6984  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 24 Epoch 8/150  Train Loss=0.7440  Val Loss=0.6976  Val Acc=0.4211  Time=18.9s\n",
            "  Fold 24 Epoch 9/150  Train Loss=0.7392  Val Loss=0.6961  Val Acc=0.4211  Time=18.9s\n",
            "  Fold 24 Epoch 10/150  Train Loss=0.7414  Val Loss=0.6998  Val Acc=0.4211  Time=18.9s\n",
            "  Fold 24 Epoch 11/150  Train Loss=0.7359  Val Loss=0.6977  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 24 Epoch 12/150  Train Loss=0.7451  Val Loss=0.7008  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 24 Epoch 13/150  Train Loss=0.7373  Val Loss=0.7019  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 24 Epoch 14/150  Train Loss=0.7266  Val Loss=0.6980  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 24 Epoch 15/150  Train Loss=0.7325  Val Loss=0.6935  Val Acc=0.4443  Time=19.0s\n",
            "  Fold 24 Epoch 16/150  Train Loss=0.7331  Val Loss=0.6959  Val Acc=0.4211  Time=18.7s\n",
            "  Fold 24 Epoch 17/150  Train Loss=0.7305  Val Loss=0.6920  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 24 Epoch 18/150  Train Loss=0.7168  Val Loss=0.6913  Val Acc=0.5557  Time=18.8s\n",
            "  Fold 24 Epoch 19/150  Train Loss=0.7234  Val Loss=0.6896  Val Acc=0.6146  Time=18.9s\n",
            "  Fold 24 Epoch 20/150  Train Loss=0.7135  Val Loss=0.6884  Val Acc=0.5743  Time=18.8s\n",
            "  Fold 24 Epoch 21/150  Train Loss=0.7155  Val Loss=0.6843  Val Acc=0.6192  Time=18.9s\n",
            "  Fold 24 Epoch 22/150  Train Loss=0.6988  Val Loss=0.6888  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 24 Epoch 23/150  Train Loss=0.6752  Val Loss=0.5910  Val Acc=0.7817  Time=18.7s\n",
            "  Fold 24 Epoch 24/150  Train Loss=0.6216  Val Loss=0.5080  Val Acc=0.8034  Time=18.8s\n",
            "  Fold 24 Epoch 25/150  Train Loss=0.5949  Val Loss=0.5106  Val Acc=0.8111  Time=18.8s\n",
            "  Fold 24 Epoch 26/150  Train Loss=0.5766  Val Loss=0.5410  Val Acc=0.7647  Time=18.9s\n",
            "  Fold 24 Epoch 27/150  Train Loss=0.5524  Val Loss=0.5015  Val Acc=0.7926  Time=18.8s\n",
            "  Fold 24 Epoch 28/150  Train Loss=0.5455  Val Loss=0.5000  Val Acc=0.7941  Time=18.9s\n",
            "  Fold 24 Epoch 29/150  Train Loss=0.5347  Val Loss=0.4360  Val Acc=0.8359  Time=18.7s\n",
            "  Fold 24 Epoch 30/150  Train Loss=0.5184  Val Loss=0.4312  Val Acc=0.8390  Time=18.9s\n",
            "  Fold 24 Epoch 31/150  Train Loss=0.5049  Val Loss=0.4662  Val Acc=0.8266  Time=18.8s\n",
            "  Fold 24 Epoch 32/150  Train Loss=0.4881  Val Loss=0.4650  Val Acc=0.7895  Time=18.9s\n",
            "  Fold 24 Epoch 33/150  Train Loss=0.4860  Val Loss=0.4372  Val Acc=0.8406  Time=18.8s\n",
            "  Fold 24 Epoch 34/150  Train Loss=0.4871  Val Loss=0.4034  Val Acc=0.8467  Time=18.9s\n",
            "  Fold 24 Epoch 35/150  Train Loss=0.4663  Val Loss=0.4693  Val Acc=0.7663  Time=18.7s\n",
            "  Fold 24 Epoch 36/150  Train Loss=0.4566  Val Loss=0.4429  Val Acc=0.7926  Time=18.9s\n",
            "  Fold 24 Epoch 37/150  Train Loss=0.4486  Val Loss=0.4751  Val Acc=0.7368  Time=18.7s\n",
            "  Fold 24 Epoch 38/150  Train Loss=0.4301  Val Loss=0.4642  Val Acc=0.7678  Time=18.9s\n",
            "  Fold 24 Epoch 39/150  Train Loss=0.4181  Val Loss=0.4391  Val Acc=0.7879  Time=18.9s\n",
            "  Fold 24 Epoch 40/150  Train Loss=0.4154  Val Loss=0.4333  Val Acc=0.7988  Time=19.0s\n",
            "  Fold 24 Epoch 41/150  Train Loss=0.4040  Val Loss=0.4050  Val Acc=0.8421  Time=18.6s\n",
            "  Fold 24 Epoch 42/150  Train Loss=0.3897  Val Loss=0.4229  Val Acc=0.8019  Time=19.0s\n",
            "  Fold 24 Epoch 43/150  Train Loss=0.3935  Val Loss=0.4238  Val Acc=0.8096  Time=18.9s\n",
            "  Fold 24 Epoch 44/150  Train Loss=0.3778  Val Loss=0.4491  Val Acc=0.7771  Time=18.8s\n",
            "  Fold 24 Epoch 45/150  Train Loss=0.3703  Val Loss=0.4176  Val Acc=0.8204  Time=18.9s\n",
            "  Fold 24 Epoch 46/150  Train Loss=0.3640  Val Loss=0.4535  Val Acc=0.7771  Time=18.8s\n",
            "  Fold 24 Epoch 47/150  Train Loss=0.3584  Val Loss=0.4212  Val Acc=0.8096  Time=18.9s\n",
            "  Fold 24 Epoch 48/150  Train Loss=0.3332  Val Loss=0.4228  Val Acc=0.8065  Time=18.8s\n",
            "  Fold 24 Epoch 49/150  Train Loss=0.3408  Val Loss=0.4270  Val Acc=0.8189  Time=18.9s\n",
            "  → Early stopping at epoch 49 (no val_loss improvement)\n",
            "  Fold 24 Final ACC = 0.5370   (TP=29  TN=0  FP=0  FN=25)\n",
            "\n",
            ">>> Fold 25/65  (leave out sub-025)\n",
            "  Fold 25 Epoch 1/150  Train Loss=0.7651  Val Loss=0.7198  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 2/150  Train Loss=0.7569  Val Loss=0.7048  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 3/150  Train Loss=0.7455  Val Loss=0.6994  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 4/150  Train Loss=0.7395  Val Loss=0.6993  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 25 Epoch 5/150  Train Loss=0.7397  Val Loss=0.7009  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 6/150  Train Loss=0.7314  Val Loss=0.7013  Val Acc=0.4211  Time=18.9s\n",
            "  Fold 25 Epoch 7/150  Train Loss=0.7332  Val Loss=0.7067  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 8/150  Train Loss=0.7257  Val Loss=0.7035  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 25 Epoch 9/150  Train Loss=0.7253  Val Loss=0.7019  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 10/150  Train Loss=0.7194  Val Loss=0.7024  Val Acc=0.4211  Time=18.9s\n",
            "  Fold 25 Epoch 11/150  Train Loss=0.7195  Val Loss=0.6975  Val Acc=0.4211  Time=18.8s\n",
            "  Fold 25 Epoch 12/150  Train Loss=0.7217  Val Loss=0.6940  Val Acc=0.4211  Time=19.0s\n",
            "  Fold 25 Epoch 13/150  Train Loss=0.7168  Val Loss=0.6918  Val Acc=0.4783  Time=18.8s\n",
            "  Fold 25 Epoch 14/150  Train Loss=0.7239  Val Loss=0.6892  Val Acc=0.5341  Time=18.9s\n",
            "  Fold 25 Epoch 15/150  Train Loss=0.7164  Val Loss=0.6822  Val Acc=0.6873  Time=18.9s\n",
            "  Fold 25 Epoch 16/150  Train Loss=0.7028  Val Loss=0.6620  Val Acc=0.6966  Time=18.9s\n",
            "  Fold 25 Epoch 17/150  Train Loss=0.6782  Val Loss=0.5838  Val Acc=0.7368  Time=18.9s\n",
            "  Fold 25 Epoch 18/150  Train Loss=0.6228  Val Loss=0.5436  Val Acc=0.7663  Time=19.0s\n",
            "  Fold 25 Epoch 19/150  Train Loss=0.5912  Val Loss=0.4903  Val Acc=0.8065  Time=18.9s\n",
            "  Fold 25 Epoch 20/150  Train Loss=0.5618  Val Loss=0.4745  Val Acc=0.8065  Time=18.9s\n",
            "  Fold 25 Epoch 21/150  Train Loss=0.5450  Val Loss=0.4887  Val Acc=0.7539  Time=19.0s\n",
            "  Fold 25 Epoch 22/150  Train Loss=0.5331  Val Loss=0.4496  Val Acc=0.8142  Time=18.8s\n",
            "  Fold 25 Epoch 23/150  Train Loss=0.5119  Val Loss=0.4504  Val Acc=0.8080  Time=18.8s\n",
            "  Fold 25 Epoch 24/150  Train Loss=0.5046  Val Loss=0.4539  Val Acc=0.8111  Time=18.9s\n",
            "  Fold 25 Epoch 25/150  Train Loss=0.5031  Val Loss=0.4582  Val Acc=0.7802  Time=19.0s\n",
            "  Fold 25 Epoch 26/150  Train Loss=0.4895  Val Loss=0.4381  Val Acc=0.8266  Time=18.8s\n",
            "  Fold 25 Epoch 27/150  Train Loss=0.4845  Val Loss=0.4334  Val Acc=0.8158  Time=18.9s\n",
            "  Fold 25 Epoch 28/150  Train Loss=0.4638  Val Loss=0.4314  Val Acc=0.8220  Time=19.0s\n",
            "  Fold 25 Epoch 29/150  Train Loss=0.4432  Val Loss=0.4387  Val Acc=0.7988  Time=18.8s\n",
            "  Fold 25 Epoch 30/150  Train Loss=0.4388  Val Loss=0.4284  Val Acc=0.8096  Time=18.9s\n",
            "  Fold 25 Epoch 31/150  Train Loss=0.4242  Val Loss=0.4335  Val Acc=0.8080  Time=18.9s\n",
            "  Fold 25 Epoch 32/150  Train Loss=0.4130  Val Loss=0.4354  Val Acc=0.8019  Time=18.6s\n",
            "  Fold 25 Epoch 33/150  Train Loss=0.4087  Val Loss=0.4318  Val Acc=0.8050  Time=18.9s\n",
            "  Fold 25 Epoch 34/150  Train Loss=0.3888  Val Loss=0.4269  Val Acc=0.8065  Time=18.8s\n",
            "  Fold 25 Epoch 35/150  Train Loss=0.3854  Val Loss=0.4307  Val Acc=0.8019  Time=19.0s\n",
            "  Fold 25 Epoch 36/150  Train Loss=0.3698  Val Loss=0.5332  Val Acc=0.7105  Time=18.8s\n",
            "  Fold 25 Epoch 37/150  Train Loss=0.3756  Val Loss=0.4382  Val Acc=0.7895  Time=19.0s\n",
            "  Fold 25 Epoch 38/150  Train Loss=0.3548  Val Loss=0.4475  Val Acc=0.7972  Time=18.8s\n",
            "  Fold 25 Epoch 39/150  Train Loss=0.3376  Val Loss=0.4473  Val Acc=0.7941  Time=19.0s\n",
            "  Fold 25 Epoch 40/150  Train Loss=0.3418  Val Loss=0.4317  Val Acc=0.8019  Time=18.8s\n",
            "  Fold 25 Epoch 41/150  Train Loss=0.3403  Val Loss=0.4662  Val Acc=0.7864  Time=18.9s\n",
            "  Fold 25 Epoch 42/150  Train Loss=0.3270  Val Loss=0.4970  Val Acc=0.7755  Time=19.0s\n",
            "  Fold 25 Epoch 43/150  Train Loss=0.3143  Val Loss=0.4761  Val Acc=0.7709  Time=19.0s\n",
            "  Fold 25 Epoch 44/150  Train Loss=0.3061  Val Loss=0.6061  Val Acc=0.6842  Time=18.9s\n",
            "  Fold 25 Epoch 45/150  Train Loss=0.3122  Val Loss=0.4901  Val Acc=0.7786  Time=18.7s\n",
            "  Fold 25 Epoch 46/150  Train Loss=0.2969  Val Loss=0.5948  Val Acc=0.7136  Time=18.9s\n",
            "  Fold 25 Epoch 47/150  Train Loss=0.3046  Val Loss=0.4534  Val Acc=0.7926  Time=18.8s\n",
            "  Fold 25 Epoch 48/150  Train Loss=0.2925  Val Loss=0.4821  Val Acc=0.7740  Time=19.0s\n",
            "  Fold 25 Epoch 49/150  Train Loss=0.2787  Val Loss=0.5120  Val Acc=0.7678  Time=18.8s\n",
            "  → Early stopping at epoch 49 (no val_loss improvement)\n",
            "  Fold 25 Final ACC = 0.1481   (TP=0  TN=8  FP=46  FN=0)\n",
            "\n",
            ">>> Fold 26/65  (leave out sub-026)\n",
            "  Fold 26 Epoch 1/150  Train Loss=0.9239  Val Loss=0.8342  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 26 Epoch 2/150  Train Loss=0.8700  Val Loss=0.8284  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 26 Epoch 3/150  Train Loss=0.8339  Val Loss=0.7950  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 26 Epoch 4/150  Train Loss=0.8024  Val Loss=0.7581  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 5/150  Train Loss=0.7780  Val Loss=0.7168  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 6/150  Train Loss=0.7588  Val Loss=0.7161  Val Acc=0.4156  Time=18.7s\n",
            "  Fold 26 Epoch 7/150  Train Loss=0.7429  Val Loss=0.7153  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 8/150  Train Loss=0.7442  Val Loss=0.7177  Val Acc=0.4156  Time=18.7s\n",
            "  Fold 26 Epoch 9/150  Train Loss=0.7471  Val Loss=0.7167  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 10/150  Train Loss=0.7384  Val Loss=0.7100  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 11/150  Train Loss=0.7353  Val Loss=0.7101  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 12/150  Train Loss=0.7252  Val Loss=0.7059  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 26 Epoch 13/150  Train Loss=0.7358  Val Loss=0.7098  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 26 Epoch 14/150  Train Loss=0.7250  Val Loss=0.7091  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 26 Epoch 15/150  Train Loss=0.7263  Val Loss=0.7016  Val Acc=0.4156  Time=19.2s\n",
            "  Fold 26 Epoch 16/150  Train Loss=0.7203  Val Loss=0.7070  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 17/150  Train Loss=0.7262  Val Loss=0.7036  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 26 Epoch 18/150  Train Loss=0.7210  Val Loss=0.7024  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 26 Epoch 19/150  Train Loss=0.7247  Val Loss=0.6965  Val Acc=0.4156  Time=19.1s\n",
            "  Fold 26 Epoch 20/150  Train Loss=0.7145  Val Loss=0.6999  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 26 Epoch 21/150  Train Loss=0.7137  Val Loss=0.6905  Val Acc=0.5891  Time=19.1s\n",
            "  Fold 26 Epoch 22/150  Train Loss=0.7111  Val Loss=0.6890  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 26 Epoch 23/150  Train Loss=0.7179  Val Loss=0.6919  Val Acc=0.6234  Time=19.0s\n",
            "  Fold 26 Epoch 24/150  Train Loss=0.7146  Val Loss=0.6895  Val Acc=0.6078  Time=18.9s\n",
            "  Fold 26 Epoch 25/150  Train Loss=0.7097  Val Loss=0.6793  Val Acc=0.6641  Time=18.9s\n",
            "  Fold 26 Epoch 26/150  Train Loss=0.6919  Val Loss=0.6361  Val Acc=0.8109  Time=19.0s\n",
            "  Fold 26 Epoch 27/150  Train Loss=0.6714  Val Loss=0.5896  Val Acc=0.8531  Time=18.9s\n",
            "  Fold 26 Epoch 28/150  Train Loss=0.6262  Val Loss=0.5385  Val Acc=0.8734  Time=18.9s\n",
            "  Fold 26 Epoch 29/150  Train Loss=0.6133  Val Loss=0.5028  Val Acc=0.8688  Time=18.9s\n",
            "  Fold 26 Epoch 30/150  Train Loss=0.5866  Val Loss=0.4815  Val Acc=0.8938  Time=19.0s\n",
            "  Fold 26 Epoch 31/150  Train Loss=0.5758  Val Loss=0.4586  Val Acc=0.8922  Time=19.0s\n",
            "  Fold 26 Epoch 32/150  Train Loss=0.5651  Val Loss=0.4387  Val Acc=0.9016  Time=18.9s\n",
            "  Fold 26 Epoch 33/150  Train Loss=0.5433  Val Loss=0.4286  Val Acc=0.9031  Time=19.1s\n",
            "  Fold 26 Epoch 34/150  Train Loss=0.5384  Val Loss=0.4237  Val Acc=0.8906  Time=18.9s\n",
            "  Fold 26 Epoch 35/150  Train Loss=0.5240  Val Loss=0.4104  Val Acc=0.9031  Time=18.9s\n",
            "  Fold 26 Epoch 36/150  Train Loss=0.5028  Val Loss=0.4508  Val Acc=0.8063  Time=18.9s\n",
            "  Fold 26 Epoch 37/150  Train Loss=0.4922  Val Loss=0.4038  Val Acc=0.8719  Time=18.8s\n",
            "  Fold 26 Epoch 38/150  Train Loss=0.4792  Val Loss=0.4064  Val Acc=0.8609  Time=18.9s\n",
            "  Fold 26 Epoch 39/150  Train Loss=0.4661  Val Loss=0.3979  Val Acc=0.8625  Time=18.9s\n",
            "  Fold 26 Epoch 40/150  Train Loss=0.4515  Val Loss=0.4024  Val Acc=0.8594  Time=18.9s\n",
            "  Fold 26 Epoch 41/150  Train Loss=0.4520  Val Loss=0.3748  Val Acc=0.8750  Time=19.0s\n",
            "  Fold 26 Epoch 42/150  Train Loss=0.4340  Val Loss=0.4212  Val Acc=0.8219  Time=19.3s\n",
            "  Fold 26 Epoch 43/150  Train Loss=0.4187  Val Loss=0.6087  Val Acc=0.6734  Time=19.2s\n",
            "  Fold 26 Epoch 44/150  Train Loss=0.4183  Val Loss=0.4205  Val Acc=0.8203  Time=19.2s\n",
            "  Fold 26 Epoch 45/150  Train Loss=0.3970  Val Loss=0.4441  Val Acc=0.8016  Time=19.0s\n",
            "  Fold 26 Epoch 46/150  Train Loss=0.3926  Val Loss=0.4107  Val Acc=0.8234  Time=19.0s\n",
            "  Fold 26 Epoch 47/150  Train Loss=0.4012  Val Loss=0.3846  Val Acc=0.8547  Time=19.0s\n",
            "  Fold 26 Epoch 48/150  Train Loss=0.3815  Val Loss=0.4363  Val Acc=0.8016  Time=19.2s\n",
            "  Fold 26 Epoch 49/150  Train Loss=0.3701  Val Loss=0.5078  Val Acc=0.7703  Time=19.1s\n",
            "  Fold 26 Epoch 50/150  Train Loss=0.3590  Val Loss=0.6170  Val Acc=0.6922  Time=19.2s\n",
            "  Fold 26 Epoch 51/150  Train Loss=0.3656  Val Loss=0.5166  Val Acc=0.7703  Time=19.1s\n",
            "  Fold 26 Epoch 52/150  Train Loss=0.3464  Val Loss=0.4104  Val Acc=0.8141  Time=19.1s\n",
            "  Fold 26 Epoch 53/150  Train Loss=0.3499  Val Loss=0.5810  Val Acc=0.7359  Time=19.1s\n",
            "  Fold 26 Epoch 54/150  Train Loss=0.3421  Val Loss=0.4481  Val Acc=0.7984  Time=19.2s\n",
            "  Fold 26 Epoch 55/150  Train Loss=0.3265  Val Loss=0.7067  Val Acc=0.6859  Time=19.0s\n",
            "  Fold 26 Epoch 56/150  Train Loss=0.3262  Val Loss=0.5586  Val Acc=0.7578  Time=19.3s\n",
            "  → Early stopping at epoch 56 (no val_loss improvement)\n",
            "  Fold 26 Final ACC = 0.4528   (TP=24  TN=0  FP=0  FN=29)\n",
            "\n",
            ">>> Fold 27/65  (leave out sub-027)\n",
            "  Fold 27 Epoch 1/150  Train Loss=0.7490  Val Loss=0.6995  Val Acc=0.4114  Time=19.2s\n",
            "  Fold 27 Epoch 2/150  Train Loss=0.7382  Val Loss=0.6873  Val Acc=0.6096  Time=19.3s\n",
            "  Fold 27 Epoch 3/150  Train Loss=0.7298  Val Loss=0.6828  Val Acc=0.5916  Time=19.3s\n",
            "  Fold 27 Epoch 4/150  Train Loss=0.7263  Val Loss=0.6810  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 27 Epoch 5/150  Train Loss=0.7346  Val Loss=0.6794  Val Acc=0.5916  Time=19.3s\n",
            "  Fold 27 Epoch 6/150  Train Loss=0.7223  Val Loss=0.6824  Val Acc=0.5916  Time=19.4s\n",
            "  Fold 27 Epoch 7/150  Train Loss=0.7275  Val Loss=0.6826  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 27 Epoch 8/150  Train Loss=0.7195  Val Loss=0.6874  Val Acc=0.5916  Time=19.3s\n",
            "  Fold 27 Epoch 9/150  Train Loss=0.7318  Val Loss=0.6901  Val Acc=0.5856  Time=19.2s\n",
            "  Fold 27 Epoch 10/150  Train Loss=0.7227  Val Loss=0.6890  Val Acc=0.5931  Time=19.2s\n",
            "  Fold 27 Epoch 11/150  Train Loss=0.7275  Val Loss=0.6963  Val Acc=0.4174  Time=19.2s\n",
            "  Fold 27 Epoch 12/150  Train Loss=0.7157  Val Loss=0.6935  Val Acc=0.4580  Time=19.3s\n",
            "  Fold 27 Epoch 13/150  Train Loss=0.7186  Val Loss=0.6934  Val Acc=0.4700  Time=19.0s\n",
            "  Fold 27 Epoch 14/150  Train Loss=0.7227  Val Loss=0.6972  Val Acc=0.4084  Time=19.3s\n",
            "  Fold 27 Epoch 15/150  Train Loss=0.7200  Val Loss=0.6856  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 27 Epoch 16/150  Train Loss=0.7119  Val Loss=0.6872  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 27 Epoch 17/150  Train Loss=0.7141  Val Loss=0.6840  Val Acc=0.5916  Time=19.4s\n",
            "  Fold 27 Epoch 18/150  Train Loss=0.7171  Val Loss=0.6855  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 27 Epoch 19/150  Train Loss=0.7176  Val Loss=0.6867  Val Acc=0.5916  Time=19.2s\n",
            "  Fold 27 Epoch 20/150  Train Loss=0.7084  Val Loss=0.6910  Val Acc=0.4805  Time=19.3s\n",
            "  → Early stopping at epoch 20 (no val_loss improvement)\n",
            "  Fold 27 Final ACC = 1.0000   (TP=0  TN=53  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 28/65  (leave out sub-028)\n",
            "  Fold 28 Epoch 1/150  Train Loss=0.7869  Val Loss=0.6853  Val Acc=0.5946  Time=19.4s\n",
            "  Fold 28 Epoch 2/150  Train Loss=0.7750  Val Loss=0.6789  Val Acc=0.5946  Time=19.2s\n",
            "  Fold 28 Epoch 3/150  Train Loss=0.7633  Val Loss=0.6774  Val Acc=0.5946  Time=19.4s\n",
            "  Fold 28 Epoch 4/150  Train Loss=0.7526  Val Loss=0.6767  Val Acc=0.5946  Time=19.4s\n",
            "  Fold 28 Epoch 5/150  Train Loss=0.7471  Val Loss=0.6758  Val Acc=0.5946  Time=19.3s\n",
            "  Fold 28 Epoch 6/150  Train Loss=0.7352  Val Loss=0.6759  Val Acc=0.5946  Time=19.1s\n",
            "  Fold 28 Epoch 7/150  Train Loss=0.7321  Val Loss=0.6753  Val Acc=0.5946  Time=19.5s\n",
            "  Fold 28 Epoch 8/150  Train Loss=0.7356  Val Loss=0.6752  Val Acc=0.5946  Time=19.2s\n",
            "  Fold 28 Epoch 9/150  Train Loss=0.7301  Val Loss=0.6747  Val Acc=0.5946  Time=19.1s\n",
            "  Fold 28 Epoch 10/150  Train Loss=0.7270  Val Loss=0.6744  Val Acc=0.5946  Time=18.9s\n",
            "  Fold 28 Epoch 11/150  Train Loss=0.7317  Val Loss=0.6724  Val Acc=0.5946  Time=19.1s\n",
            "  Fold 28 Epoch 12/150  Train Loss=0.7292  Val Loss=0.6716  Val Acc=0.5946  Time=19.2s\n",
            "  Fold 28 Epoch 13/150  Train Loss=0.7216  Val Loss=0.6681  Val Acc=0.5946  Time=19.1s\n",
            "  Fold 28 Epoch 14/150  Train Loss=0.7248  Val Loss=0.6592  Val Acc=0.5991  Time=19.1s\n",
            "  Fold 28 Epoch 15/150  Train Loss=0.6990  Val Loss=0.6433  Val Acc=0.6572  Time=19.1s\n",
            "  Fold 28 Epoch 16/150  Train Loss=0.6762  Val Loss=0.5992  Val Acc=0.6855  Time=19.0s\n",
            "  Fold 28 Epoch 17/150  Train Loss=0.6324  Val Loss=0.5639  Val Acc=0.7228  Time=19.0s\n",
            "  Fold 28 Epoch 18/150  Train Loss=0.6031  Val Loss=0.5542  Val Acc=0.7168  Time=18.9s\n",
            "  Fold 28 Epoch 19/150  Train Loss=0.5833  Val Loss=0.5517  Val Acc=0.7079  Time=19.0s\n",
            "  Fold 28 Epoch 20/150  Train Loss=0.5635  Val Loss=0.5152  Val Acc=0.7735  Time=18.9s\n",
            "  Fold 28 Epoch 21/150  Train Loss=0.5569  Val Loss=0.5280  Val Acc=0.7332  Time=19.0s\n",
            "  Fold 28 Epoch 22/150  Train Loss=0.5330  Val Loss=0.5187  Val Acc=0.7481  Time=19.0s\n",
            "  Fold 28 Epoch 23/150  Train Loss=0.5172  Val Loss=0.5138  Val Acc=0.7437  Time=18.9s\n",
            "  Fold 28 Epoch 24/150  Train Loss=0.5193  Val Loss=0.4915  Val Acc=0.7884  Time=19.0s\n",
            "  Fold 28 Epoch 25/150  Train Loss=0.5220  Val Loss=0.4998  Val Acc=0.7601  Time=19.1s\n",
            "  Fold 28 Epoch 26/150  Train Loss=0.4879  Val Loss=0.5191  Val Acc=0.7377  Time=19.1s\n",
            "  Fold 28 Epoch 27/150  Train Loss=0.4833  Val Loss=0.5247  Val Acc=0.7377  Time=19.0s\n",
            "  Fold 28 Epoch 28/150  Train Loss=0.4845  Val Loss=0.5178  Val Acc=0.7377  Time=19.0s\n",
            "  Fold 28 Epoch 29/150  Train Loss=0.4579  Val Loss=0.5108  Val Acc=0.7392  Time=18.9s\n",
            "  Fold 28 Epoch 30/150  Train Loss=0.4441  Val Loss=0.5230  Val Acc=0.7288  Time=18.8s\n",
            "  Fold 28 Epoch 31/150  Train Loss=0.4368  Val Loss=0.4640  Val Acc=0.8092  Time=19.0s\n",
            "  Fold 28 Epoch 32/150  Train Loss=0.4330  Val Loss=0.5260  Val Acc=0.7079  Time=18.9s\n",
            "  Fold 28 Epoch 33/150  Train Loss=0.4201  Val Loss=0.5067  Val Acc=0.7511  Time=18.8s\n",
            "  Fold 28 Epoch 34/150  Train Loss=0.4007  Val Loss=0.4972  Val Acc=0.7541  Time=19.1s\n",
            "  Fold 28 Epoch 35/150  Train Loss=0.4027  Val Loss=0.4756  Val Acc=0.7720  Time=18.8s\n",
            "  Fold 28 Epoch 36/150  Train Loss=0.3911  Val Loss=0.4954  Val Acc=0.7541  Time=19.0s\n",
            "  Fold 28 Epoch 37/150  Train Loss=0.3707  Val Loss=0.4844  Val Acc=0.7660  Time=19.0s\n",
            "  Fold 28 Epoch 38/150  Train Loss=0.3698  Val Loss=0.5431  Val Acc=0.7213  Time=18.9s\n",
            "  Fold 28 Epoch 39/150  Train Loss=0.3628  Val Loss=0.5006  Val Acc=0.7347  Time=18.9s\n",
            "  Fold 28 Epoch 40/150  Train Loss=0.3558  Val Loss=0.5008  Val Acc=0.7571  Time=19.1s\n",
            "  Fold 28 Epoch 41/150  Train Loss=0.3485  Val Loss=0.5110  Val Acc=0.7481  Time=19.0s\n",
            "  Fold 28 Epoch 42/150  Train Loss=0.3535  Val Loss=0.4853  Val Acc=0.7824  Time=19.0s\n",
            "  Fold 28 Epoch 43/150  Train Loss=0.3225  Val Loss=0.4894  Val Acc=0.7794  Time=19.0s\n",
            "  Fold 28 Epoch 44/150  Train Loss=0.3273  Val Loss=0.5129  Val Acc=0.7466  Time=19.2s\n",
            "  Fold 28 Epoch 45/150  Train Loss=0.3344  Val Loss=0.5666  Val Acc=0.7303  Time=19.0s\n",
            "  Fold 28 Epoch 46/150  Train Loss=0.3268  Val Loss=0.5257  Val Acc=0.7437  Time=19.1s\n",
            "  → Early stopping at epoch 46 (no val_loss improvement)\n",
            "  Fold 28 Final ACC = 1.0000   (TP=0  TN=53  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 29/65  (leave out sub-029)\n",
            "  Fold 29 Epoch 1/150  Train Loss=0.7245  Val Loss=0.6903  Val Acc=0.5616  Time=18.8s\n",
            "  Fold 29 Epoch 2/150  Train Loss=0.7291  Val Loss=0.6903  Val Acc=0.5676  Time=19.0s\n",
            "  Fold 29 Epoch 3/150  Train Loss=0.7251  Val Loss=0.6941  Val Acc=0.4775  Time=18.8s\n",
            "  Fold 29 Epoch 4/150  Train Loss=0.7233  Val Loss=0.6900  Val Acc=0.5736  Time=19.0s\n",
            "  Fold 29 Epoch 5/150  Train Loss=0.7118  Val Loss=0.6912  Val Acc=0.5646  Time=18.8s\n",
            "  Fold 29 Epoch 6/150  Train Loss=0.7226  Val Loss=0.6898  Val Acc=0.5931  Time=19.2s\n",
            "  Fold 29 Epoch 7/150  Train Loss=0.7166  Val Loss=0.6906  Val Acc=0.5811  Time=18.8s\n",
            "  Fold 29 Epoch 8/150  Train Loss=0.7127  Val Loss=0.6939  Val Acc=0.4790  Time=19.0s\n",
            "  Fold 29 Epoch 9/150  Train Loss=0.7234  Val Loss=0.6973  Val Acc=0.4129  Time=18.9s\n",
            "  Fold 29 Epoch 10/150  Train Loss=0.7153  Val Loss=0.6890  Val Acc=0.5931  Time=19.0s\n",
            "  Fold 29 Epoch 11/150  Train Loss=0.7078  Val Loss=0.6972  Val Acc=0.4054  Time=18.9s\n",
            "  Fold 29 Epoch 12/150  Train Loss=0.7162  Val Loss=0.6961  Val Acc=0.4159  Time=19.0s\n",
            "  Fold 29 Epoch 13/150  Train Loss=0.7117  Val Loss=0.6890  Val Acc=0.5901  Time=18.7s\n",
            "  Fold 29 Epoch 14/150  Train Loss=0.7140  Val Loss=0.6887  Val Acc=0.5856  Time=19.1s\n",
            "  Fold 29 Epoch 15/150  Train Loss=0.7127  Val Loss=0.6898  Val Acc=0.6036  Time=19.0s\n",
            "  Fold 29 Epoch 16/150  Train Loss=0.7125  Val Loss=0.6994  Val Acc=0.4084  Time=19.0s\n",
            "  Fold 29 Epoch 17/150  Train Loss=0.7171  Val Loss=0.6946  Val Acc=0.4369  Time=18.9s\n",
            "  Fold 29 Epoch 18/150  Train Loss=0.7120  Val Loss=0.6944  Val Acc=0.4444  Time=19.1s\n",
            "  Fold 29 Epoch 19/150  Train Loss=0.7119  Val Loss=0.6793  Val Acc=0.6652  Time=18.9s\n",
            "  Fold 29 Epoch 20/150  Train Loss=0.6992  Val Loss=0.6886  Val Acc=0.4865  Time=18.8s\n",
            "  Fold 29 Epoch 21/150  Train Loss=0.6686  Val Loss=0.6347  Val Acc=0.6502  Time=18.9s\n",
            "  Fold 29 Epoch 22/150  Train Loss=0.6376  Val Loss=0.6917  Val Acc=0.5150  Time=18.9s\n",
            "  Fold 29 Epoch 23/150  Train Loss=0.5986  Val Loss=0.5826  Val Acc=0.6967  Time=18.9s\n",
            "  Fold 29 Epoch 24/150  Train Loss=0.5637  Val Loss=0.5759  Val Acc=0.6952  Time=18.7s\n",
            "  Fold 29 Epoch 25/150  Train Loss=0.5404  Val Loss=0.5587  Val Acc=0.7012  Time=18.9s\n",
            "  Fold 29 Epoch 26/150  Train Loss=0.5293  Val Loss=0.5559  Val Acc=0.7027  Time=19.0s\n",
            "  Fold 29 Epoch 27/150  Train Loss=0.5169  Val Loss=0.5565  Val Acc=0.7012  Time=18.8s\n",
            "  Fold 29 Epoch 28/150  Train Loss=0.5032  Val Loss=0.5127  Val Acc=0.7252  Time=18.9s\n",
            "  Fold 29 Epoch 29/150  Train Loss=0.4892  Val Loss=0.5381  Val Acc=0.7057  Time=18.9s\n",
            "  Fold 29 Epoch 30/150  Train Loss=0.4698  Val Loss=0.5380  Val Acc=0.6922  Time=19.0s\n",
            "  Fold 29 Epoch 31/150  Train Loss=0.4732  Val Loss=0.5257  Val Acc=0.7072  Time=18.9s\n",
            "  Fold 29 Epoch 32/150  Train Loss=0.4632  Val Loss=0.4863  Val Acc=0.7523  Time=18.9s\n",
            "  Fold 29 Epoch 33/150  Train Loss=0.4595  Val Loss=0.4814  Val Acc=0.7568  Time=19.0s\n",
            "  Fold 29 Epoch 34/150  Train Loss=0.4394  Val Loss=0.5126  Val Acc=0.7297  Time=18.9s\n",
            "  Fold 29 Epoch 35/150  Train Loss=0.4321  Val Loss=0.4818  Val Acc=0.7477  Time=19.0s\n",
            "  Fold 29 Epoch 36/150  Train Loss=0.4181  Val Loss=0.5035  Val Acc=0.7297  Time=18.9s\n",
            "  Fold 29 Epoch 37/150  Train Loss=0.4055  Val Loss=0.4701  Val Acc=0.7462  Time=19.0s\n",
            "  Fold 29 Epoch 38/150  Train Loss=0.3888  Val Loss=0.4514  Val Acc=0.7868  Time=18.8s\n",
            "  Fold 29 Epoch 39/150  Train Loss=0.3827  Val Loss=0.4826  Val Acc=0.7477  Time=19.1s\n",
            "  Fold 29 Epoch 40/150  Train Loss=0.3743  Val Loss=0.4953  Val Acc=0.7312  Time=18.9s\n",
            "  Fold 29 Epoch 41/150  Train Loss=0.3626  Val Loss=0.4543  Val Acc=0.7688  Time=19.2s\n",
            "  Fold 29 Epoch 42/150  Train Loss=0.3668  Val Loss=0.4668  Val Acc=0.7703  Time=18.9s\n",
            "  Fold 29 Epoch 43/150  Train Loss=0.3467  Val Loss=0.4556  Val Acc=0.7628  Time=19.1s\n",
            "  Fold 29 Epoch 44/150  Train Loss=0.3355  Val Loss=0.4610  Val Acc=0.7628  Time=18.9s\n",
            "  Fold 29 Epoch 45/150  Train Loss=0.3429  Val Loss=0.4531  Val Acc=0.7718  Time=19.0s\n",
            "  Fold 29 Epoch 46/150  Train Loss=0.3418  Val Loss=0.4496  Val Acc=0.7778  Time=18.8s\n",
            "  Fold 29 Epoch 47/150  Train Loss=0.3352  Val Loss=0.4520  Val Acc=0.7823  Time=19.0s\n",
            "  Fold 29 Epoch 48/150  Train Loss=0.3250  Val Loss=0.4648  Val Acc=0.7673  Time=18.9s\n",
            "  Fold 29 Epoch 49/150  Train Loss=0.3096  Val Loss=0.4668  Val Acc=0.7643  Time=19.1s\n",
            "  Fold 29 Epoch 50/150  Train Loss=0.3174  Val Loss=0.4599  Val Acc=0.7763  Time=18.9s\n",
            "  Fold 29 Epoch 51/150  Train Loss=0.2980  Val Loss=0.4603  Val Acc=0.7688  Time=19.2s\n",
            "  Fold 29 Epoch 52/150  Train Loss=0.2955  Val Loss=0.4655  Val Acc=0.7973  Time=18.9s\n",
            "  Fold 29 Epoch 53/150  Train Loss=0.2946  Val Loss=0.4673  Val Acc=0.7958  Time=19.1s\n",
            "  Fold 29 Epoch 54/150  Train Loss=0.2900  Val Loss=0.4651  Val Acc=0.7913  Time=18.9s\n",
            "  Fold 29 Epoch 55/150  Train Loss=0.2943  Val Loss=0.4694  Val Acc=0.7748  Time=19.1s\n",
            "  Fold 29 Epoch 56/150  Train Loss=0.2819  Val Loss=0.4850  Val Acc=0.7748  Time=19.0s\n",
            "  Fold 29 Epoch 57/150  Train Loss=0.2796  Val Loss=0.4894  Val Acc=0.7733  Time=19.1s\n",
            "  Fold 29 Epoch 58/150  Train Loss=0.2723  Val Loss=0.4782  Val Acc=0.7778  Time=18.9s\n",
            "  Fold 29 Epoch 59/150  Train Loss=0.2825  Val Loss=0.5386  Val Acc=0.8003  Time=19.1s\n",
            "  Fold 29 Epoch 60/150  Train Loss=0.2701  Val Loss=0.5108  Val Acc=0.7928  Time=18.9s\n",
            "  Fold 29 Epoch 61/150  Train Loss=0.2611  Val Loss=0.4830  Val Acc=0.7763  Time=19.1s\n",
            "  → Early stopping at epoch 61 (no val_loss improvement)\n",
            "  Fold 29 Final ACC = 0.8679   (TP=0  TN=46  FP=7  FN=0)\n",
            "\n",
            ">>> Fold 30/65  (leave out sub-030)\n",
            "  Fold 30 Epoch 1/150  Train Loss=0.7506  Val Loss=0.6853  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 30 Epoch 2/150  Train Loss=0.7246  Val Loss=0.6886  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 30 Epoch 3/150  Train Loss=0.7229  Val Loss=0.6875  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 30 Epoch 4/150  Train Loss=0.7254  Val Loss=0.6861  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 30 Epoch 5/150  Train Loss=0.7267  Val Loss=0.6865  Val Acc=0.5835  Time=18.7s\n",
            "  Fold 30 Epoch 6/150  Train Loss=0.7168  Val Loss=0.6851  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 30 Epoch 7/150  Train Loss=0.7189  Val Loss=0.6851  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 30 Epoch 8/150  Train Loss=0.7179  Val Loss=0.6871  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 30 Epoch 9/150  Train Loss=0.7091  Val Loss=0.6848  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 30 Epoch 10/150  Train Loss=0.7203  Val Loss=0.6849  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 30 Epoch 11/150  Train Loss=0.7124  Val Loss=0.6857  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 30 Epoch 12/150  Train Loss=0.7084  Val Loss=0.6860  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 30 Epoch 13/150  Train Loss=0.7128  Val Loss=0.6849  Val Acc=0.5835  Time=18.8s\n",
            "  Fold 30 Epoch 14/150  Train Loss=0.7126  Val Loss=0.6841  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 30 Epoch 15/150  Train Loss=0.7073  Val Loss=0.6838  Val Acc=0.5835  Time=18.7s\n",
            "  Fold 30 Epoch 16/150  Train Loss=0.7014  Val Loss=0.6813  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 30 Epoch 17/150  Train Loss=0.7020  Val Loss=0.6764  Val Acc=0.5835  Time=18.9s\n",
            "  Fold 30 Epoch 18/150  Train Loss=0.7037  Val Loss=0.6689  Val Acc=0.5835  Time=19.0s\n",
            "  Fold 30 Epoch 19/150  Train Loss=0.6897  Val Loss=0.6467  Val Acc=0.6568  Time=18.8s\n",
            "  Fold 30 Epoch 20/150  Train Loss=0.6703  Val Loss=0.6233  Val Acc=0.6989  Time=18.9s\n",
            "  Fold 30 Epoch 21/150  Train Loss=0.6415  Val Loss=0.5736  Val Acc=0.7301  Time=18.8s\n",
            "  Fold 30 Epoch 22/150  Train Loss=0.6040  Val Loss=0.5372  Val Acc=0.7832  Time=18.9s\n",
            "  Fold 30 Epoch 23/150  Train Loss=0.5671  Val Loss=0.5063  Val Acc=0.8144  Time=18.9s\n",
            "  Fold 30 Epoch 24/150  Train Loss=0.5651  Val Loss=0.4817  Val Acc=0.8237  Time=19.0s\n",
            "  Fold 30 Epoch 25/150  Train Loss=0.5437  Val Loss=0.4911  Val Acc=0.8144  Time=18.8s\n",
            "  Fold 30 Epoch 26/150  Train Loss=0.5385  Val Loss=0.4694  Val Acc=0.8206  Time=18.7s\n",
            "  Fold 30 Epoch 27/150  Train Loss=0.5336  Val Loss=0.4683  Val Acc=0.8253  Time=18.7s\n",
            "  Fold 30 Epoch 28/150  Train Loss=0.5229  Val Loss=0.4489  Val Acc=0.8284  Time=19.0s\n",
            "  Fold 30 Epoch 29/150  Train Loss=0.5164  Val Loss=0.4594  Val Acc=0.8237  Time=18.8s\n",
            "  Fold 30 Epoch 30/150  Train Loss=0.5063  Val Loss=0.4526  Val Acc=0.8190  Time=18.9s\n",
            "  Fold 30 Epoch 31/150  Train Loss=0.4906  Val Loss=0.4492  Val Acc=0.8237  Time=18.9s\n",
            "  Fold 30 Epoch 32/150  Train Loss=0.4802  Val Loss=0.4435  Val Acc=0.8284  Time=19.1s\n",
            "  Fold 30 Epoch 33/150  Train Loss=0.4721  Val Loss=0.4351  Val Acc=0.8268  Time=18.8s\n",
            "  Fold 30 Epoch 34/150  Train Loss=0.4633  Val Loss=0.4289  Val Acc=0.8253  Time=18.9s\n",
            "  Fold 30 Epoch 35/150  Train Loss=0.4561  Val Loss=0.4225  Val Acc=0.8268  Time=18.7s\n",
            "  Fold 30 Epoch 36/150  Train Loss=0.4427  Val Loss=0.4204  Val Acc=0.8268  Time=18.9s\n",
            "  Fold 30 Epoch 37/150  Train Loss=0.4308  Val Loss=0.4297  Val Acc=0.8237  Time=18.8s\n",
            "  Fold 30 Epoch 38/150  Train Loss=0.4187  Val Loss=0.4393  Val Acc=0.7832  Time=18.9s\n",
            "  Fold 30 Epoch 39/150  Train Loss=0.4099  Val Loss=0.4146  Val Acc=0.8190  Time=19.0s\n",
            "  Fold 30 Epoch 40/150  Train Loss=0.4177  Val Loss=0.4300  Val Acc=0.7941  Time=19.0s\n",
            "  Fold 30 Epoch 41/150  Train Loss=0.3973  Val Loss=0.4192  Val Acc=0.8066  Time=18.8s\n",
            "  Fold 30 Epoch 42/150  Train Loss=0.3819  Val Loss=0.4129  Val Acc=0.8175  Time=18.9s\n",
            "  Fold 30 Epoch 43/150  Train Loss=0.3677  Val Loss=0.4255  Val Acc=0.7972  Time=18.7s\n",
            "  Fold 30 Epoch 44/150  Train Loss=0.3622  Val Loss=0.4225  Val Acc=0.7972  Time=19.0s\n",
            "  Fold 30 Epoch 45/150  Train Loss=0.3576  Val Loss=0.4402  Val Acc=0.8159  Time=18.9s\n",
            "  Fold 30 Epoch 46/150  Train Loss=0.3561  Val Loss=0.4303  Val Acc=0.7800  Time=18.9s\n",
            "  Fold 30 Epoch 47/150  Train Loss=0.3461  Val Loss=0.4281  Val Acc=0.8034  Time=18.7s\n",
            "  Fold 30 Epoch 48/150  Train Loss=0.3499  Val Loss=0.4236  Val Acc=0.8159  Time=18.9s\n",
            "  Fold 30 Epoch 49/150  Train Loss=0.3390  Val Loss=0.4375  Val Acc=0.7878  Time=18.8s\n",
            "  Fold 30 Epoch 50/150  Train Loss=0.3230  Val Loss=0.4639  Val Acc=0.7457  Time=18.9s\n",
            "  Fold 30 Epoch 51/150  Train Loss=0.3241  Val Loss=0.4215  Val Acc=0.8144  Time=18.7s\n",
            "  Fold 30 Epoch 52/150  Train Loss=0.3213  Val Loss=0.4354  Val Acc=0.7972  Time=18.9s\n",
            "  Fold 30 Epoch 53/150  Train Loss=0.3276  Val Loss=0.5310  Val Acc=0.6958  Time=18.8s\n",
            "  Fold 30 Epoch 54/150  Train Loss=0.3062  Val Loss=0.4431  Val Acc=0.7863  Time=19.0s\n",
            "  Fold 30 Epoch 55/150  Train Loss=0.3011  Val Loss=0.4535  Val Acc=0.7738  Time=18.8s\n",
            "  Fold 30 Epoch 56/150  Train Loss=0.2952  Val Loss=0.4740  Val Acc=0.7722  Time=18.9s\n",
            "  Fold 30 Epoch 57/150  Train Loss=0.2963  Val Loss=0.4365  Val Acc=0.8112  Time=18.7s\n",
            "  → Early stopping at epoch 57 (no val_loss improvement)\n",
            "  Fold 30 Final ACC = 0.2642   (TP=14  TN=0  FP=0  FN=39)\n",
            "\n",
            ">>> Fold 31/65  (leave out sub-031)\n",
            "  Fold 31 Epoch 1/150  Train Loss=0.7470  Val Loss=0.7136  Val Acc=0.4048  Time=18.9s\n",
            "  Fold 31 Epoch 2/150  Train Loss=0.7425  Val Loss=0.6968  Val Acc=0.4092  Time=18.9s\n",
            "  Fold 31 Epoch 3/150  Train Loss=0.7392  Val Loss=0.6886  Val Acc=0.5833  Time=18.9s\n",
            "  Fold 31 Epoch 4/150  Train Loss=0.7320  Val Loss=0.6817  Val Acc=0.5952  Time=18.9s\n",
            "  Fold 31 Epoch 5/150  Train Loss=0.7323  Val Loss=0.6793  Val Acc=0.5952  Time=19.1s\n",
            "  Fold 31 Epoch 6/150  Train Loss=0.7214  Val Loss=0.6768  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 7/150  Train Loss=0.7270  Val Loss=0.6772  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 8/150  Train Loss=0.7190  Val Loss=0.6906  Val Acc=0.5818  Time=18.8s\n",
            "  Fold 31 Epoch 9/150  Train Loss=0.7173  Val Loss=0.6766  Val Acc=0.5952  Time=19.1s\n",
            "  Fold 31 Epoch 10/150  Train Loss=0.7290  Val Loss=0.6806  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 11/150  Train Loss=0.7200  Val Loss=0.6868  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 12/150  Train Loss=0.7223  Val Loss=0.6848  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 13/150  Train Loss=0.7118  Val Loss=0.6814  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 14/150  Train Loss=0.7099  Val Loss=0.6848  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 15/150  Train Loss=0.7179  Val Loss=0.6894  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 16/150  Train Loss=0.7196  Val Loss=0.6866  Val Acc=0.5952  Time=19.0s\n",
            "  Fold 31 Epoch 17/150  Train Loss=0.7136  Val Loss=0.6868  Val Acc=0.6161  Time=19.0s\n",
            "  Fold 31 Epoch 18/150  Train Loss=0.7154  Val Loss=0.6707  Val Acc=0.5982  Time=18.9s\n",
            "  Fold 31 Epoch 19/150  Train Loss=0.6973  Val Loss=0.6425  Val Acc=0.6488  Time=18.9s\n",
            "  Fold 31 Epoch 20/150  Train Loss=0.6505  Val Loss=0.5936  Val Acc=0.6890  Time=18.9s\n",
            "  Fold 31 Epoch 21/150  Train Loss=0.6046  Val Loss=0.5670  Val Acc=0.7188  Time=19.0s\n",
            "  Fold 31 Epoch 22/150  Train Loss=0.5768  Val Loss=0.5652  Val Acc=0.6979  Time=19.0s\n",
            "  Fold 31 Epoch 23/150  Train Loss=0.5437  Val Loss=0.5730  Val Acc=0.6890  Time=18.9s\n",
            "  Fold 31 Epoch 24/150  Train Loss=0.5298  Val Loss=0.5362  Val Acc=0.7158  Time=19.0s\n",
            "  Fold 31 Epoch 25/150  Train Loss=0.5056  Val Loss=0.5158  Val Acc=0.7440  Time=18.9s\n",
            "  Fold 31 Epoch 26/150  Train Loss=0.4967  Val Loss=0.5048  Val Acc=0.7426  Time=19.0s\n",
            "  Fold 31 Epoch 27/150  Train Loss=0.4855  Val Loss=0.5309  Val Acc=0.7039  Time=19.0s\n",
            "  Fold 31 Epoch 28/150  Train Loss=0.4700  Val Loss=0.5106  Val Acc=0.7277  Time=19.0s\n",
            "  Fold 31 Epoch 29/150  Train Loss=0.4569  Val Loss=0.5062  Val Acc=0.7292  Time=19.0s\n",
            "  Fold 31 Epoch 30/150  Train Loss=0.4474  Val Loss=0.4911  Val Acc=0.7857  Time=19.0s\n",
            "  Fold 31 Epoch 31/150  Train Loss=0.4416  Val Loss=0.4989  Val Acc=0.7470  Time=19.0s\n",
            "  Fold 31 Epoch 32/150  Train Loss=0.4352  Val Loss=0.5129  Val Acc=0.7292  Time=19.1s\n",
            "  Fold 31 Epoch 33/150  Train Loss=0.4044  Val Loss=0.4768  Val Acc=0.7768  Time=19.0s\n",
            "  Fold 31 Epoch 34/150  Train Loss=0.4149  Val Loss=0.4700  Val Acc=0.7827  Time=19.2s\n",
            "  Fold 31 Epoch 35/150  Train Loss=0.4080  Val Loss=0.4825  Val Acc=0.7887  Time=18.9s\n",
            "  Fold 31 Epoch 36/150  Train Loss=0.3907  Val Loss=0.4823  Val Acc=0.7664  Time=19.1s\n",
            "  Fold 31 Epoch 37/150  Train Loss=0.3871  Val Loss=0.4986  Val Acc=0.7545  Time=18.8s\n",
            "  Fold 31 Epoch 38/150  Train Loss=0.3661  Val Loss=0.4891  Val Acc=0.7649  Time=19.1s\n",
            "  Fold 31 Epoch 39/150  Train Loss=0.3474  Val Loss=0.5108  Val Acc=0.7470  Time=18.9s\n",
            "  Fold 31 Epoch 40/150  Train Loss=0.3463  Val Loss=0.4922  Val Acc=0.7545  Time=19.1s\n",
            "  Fold 31 Epoch 41/150  Train Loss=0.3515  Val Loss=0.4914  Val Acc=0.7664  Time=19.0s\n",
            "  Fold 31 Epoch 42/150  Train Loss=0.3302  Val Loss=0.5038  Val Acc=0.7530  Time=19.1s\n",
            "  Fold 31 Epoch 43/150  Train Loss=0.3349  Val Loss=0.5151  Val Acc=0.7500  Time=18.9s\n",
            "  Fold 31 Epoch 44/150  Train Loss=0.3350  Val Loss=0.4980  Val Acc=0.7679  Time=19.1s\n",
            "  Fold 31 Epoch 45/150  Train Loss=0.3247  Val Loss=0.5613  Val Acc=0.7247  Time=18.9s\n",
            "  Fold 31 Epoch 46/150  Train Loss=0.3276  Val Loss=0.5938  Val Acc=0.7009  Time=19.0s\n",
            "  Fold 31 Epoch 47/150  Train Loss=0.3238  Val Loss=0.5182  Val Acc=0.7604  Time=18.9s\n",
            "  Fold 31 Epoch 48/150  Train Loss=0.2980  Val Loss=0.5727  Val Acc=0.7232  Time=19.1s\n",
            "  Fold 31 Epoch 49/150  Train Loss=0.3156  Val Loss=0.6046  Val Acc=0.7039  Time=18.9s\n",
            "  → Early stopping at epoch 49 (no val_loss improvement)\n",
            "  Fold 31 Final ACC = 1.0000   (TP=0  TN=52  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 32/65  (leave out sub-032)\n",
            "  Fold 32 Epoch 1/150  Train Loss=0.7457  Val Loss=0.7062  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 32 Epoch 2/150  Train Loss=0.7313  Val Loss=0.6949  Val Acc=0.4203  Time=18.8s\n",
            "  Fold 32 Epoch 3/150  Train Loss=0.7234  Val Loss=0.6875  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 32 Epoch 4/150  Train Loss=0.7207  Val Loss=0.6881  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 32 Epoch 5/150  Train Loss=0.7194  Val Loss=0.6851  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 32 Epoch 6/150  Train Loss=0.7196  Val Loss=0.6834  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 32 Epoch 7/150  Train Loss=0.7124  Val Loss=0.6854  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 8/150  Train Loss=0.7097  Val Loss=0.6861  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 32 Epoch 9/150  Train Loss=0.7102  Val Loss=0.6851  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 10/150  Train Loss=0.7096  Val Loss=0.6818  Val Acc=0.5844  Time=18.7s\n",
            "  Fold 32 Epoch 11/150  Train Loss=0.7171  Val Loss=0.6807  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 12/150  Train Loss=0.7138  Val Loss=0.6837  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 32 Epoch 13/150  Train Loss=0.7121  Val Loss=0.6835  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 14/150  Train Loss=0.7054  Val Loss=0.6817  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 32 Epoch 15/150  Train Loss=0.7096  Val Loss=0.6796  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 16/150  Train Loss=0.7092  Val Loss=0.6821  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 32 Epoch 17/150  Train Loss=0.7075  Val Loss=0.6812  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 18/150  Train Loss=0.7040  Val Loss=0.6795  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 19/150  Train Loss=0.7062  Val Loss=0.6785  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 32 Epoch 20/150  Train Loss=0.7024  Val Loss=0.6782  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 32 Epoch 21/150  Train Loss=0.7074  Val Loss=0.6790  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 32 Epoch 22/150  Train Loss=0.7009  Val Loss=0.6817  Val Acc=0.5891  Time=18.8s\n",
            "  Fold 32 Epoch 23/150  Train Loss=0.7013  Val Loss=0.6721  Val Acc=0.7844  Time=19.0s\n",
            "  Fold 32 Epoch 24/150  Train Loss=0.6829  Val Loss=0.6074  Val Acc=0.7750  Time=18.8s\n",
            "  Fold 32 Epoch 25/150  Train Loss=0.6337  Val Loss=0.5328  Val Acc=0.8125  Time=19.0s\n",
            "  Fold 32 Epoch 26/150  Train Loss=0.5942  Val Loss=0.4688  Val Acc=0.8391  Time=18.9s\n",
            "  Fold 32 Epoch 27/150  Train Loss=0.5686  Val Loss=0.4724  Val Acc=0.8547  Time=19.1s\n",
            "  Fold 32 Epoch 28/150  Train Loss=0.5481  Val Loss=0.3957  Val Acc=0.8844  Time=19.0s\n",
            "  Fold 32 Epoch 29/150  Train Loss=0.5195  Val Loss=0.4801  Val Acc=0.7469  Time=19.0s\n",
            "  Fold 32 Epoch 30/150  Train Loss=0.5209  Val Loss=0.3905  Val Acc=0.8797  Time=18.9s\n",
            "  Fold 32 Epoch 31/150  Train Loss=0.4958  Val Loss=0.3917  Val Acc=0.8781  Time=19.1s\n",
            "  Fold 32 Epoch 32/150  Train Loss=0.4757  Val Loss=0.4455  Val Acc=0.8234  Time=19.0s\n",
            "  Fold 32 Epoch 33/150  Train Loss=0.4671  Val Loss=0.3726  Val Acc=0.8891  Time=19.1s\n",
            "  Fold 32 Epoch 34/150  Train Loss=0.4598  Val Loss=0.3982  Val Acc=0.8656  Time=19.1s\n",
            "  Fold 32 Epoch 35/150  Train Loss=0.4458  Val Loss=0.4070  Val Acc=0.8547  Time=19.1s\n",
            "  Fold 32 Epoch 36/150  Train Loss=0.4379  Val Loss=0.3726  Val Acc=0.8688  Time=18.8s\n",
            "  Fold 32 Epoch 37/150  Train Loss=0.4392  Val Loss=0.3511  Val Acc=0.8969  Time=18.9s\n",
            "  Fold 32 Epoch 38/150  Train Loss=0.4255  Val Loss=0.3540  Val Acc=0.8906  Time=18.8s\n",
            "  Fold 32 Epoch 39/150  Train Loss=0.4254  Val Loss=0.4166  Val Acc=0.8203  Time=18.8s\n",
            "  Fold 32 Epoch 40/150  Train Loss=0.4267  Val Loss=0.3554  Val Acc=0.8844  Time=18.8s\n",
            "  Fold 32 Epoch 41/150  Train Loss=0.4015  Val Loss=0.3451  Val Acc=0.8891  Time=19.0s\n",
            "  Fold 32 Epoch 42/150  Train Loss=0.3827  Val Loss=0.3510  Val Acc=0.8891  Time=18.8s\n",
            "  Fold 32 Epoch 43/150  Train Loss=0.3851  Val Loss=0.3800  Val Acc=0.8547  Time=18.9s\n",
            "  Fold 32 Epoch 44/150  Train Loss=0.3861  Val Loss=0.3470  Val Acc=0.8812  Time=19.0s\n",
            "  Fold 32 Epoch 45/150  Train Loss=0.3659  Val Loss=0.3629  Val Acc=0.8625  Time=18.9s\n",
            "  Fold 32 Epoch 46/150  Train Loss=0.3663  Val Loss=0.3639  Val Acc=0.8578  Time=18.9s\n",
            "  Fold 32 Epoch 47/150  Train Loss=0.3558  Val Loss=0.3954  Val Acc=0.8297  Time=19.0s\n",
            "  Fold 32 Epoch 48/150  Train Loss=0.3665  Val Loss=0.3483  Val Acc=0.8688  Time=18.9s\n",
            "  Fold 32 Epoch 49/150  Train Loss=0.3377  Val Loss=0.3396  Val Acc=0.8797  Time=18.9s\n",
            "  Fold 32 Epoch 50/150  Train Loss=0.3456  Val Loss=0.3340  Val Acc=0.8812  Time=18.9s\n",
            "  Fold 32 Epoch 51/150  Train Loss=0.3409  Val Loss=0.3504  Val Acc=0.8609  Time=18.9s\n",
            "  Fold 32 Epoch 52/150  Train Loss=0.3387  Val Loss=0.3810  Val Acc=0.8297  Time=18.8s\n",
            "  Fold 32 Epoch 53/150  Train Loss=0.3365  Val Loss=0.3366  Val Acc=0.8750  Time=19.0s\n",
            "  Fold 32 Epoch 54/150  Train Loss=0.3140  Val Loss=0.3595  Val Acc=0.8484  Time=18.8s\n",
            "  Fold 32 Epoch 55/150  Train Loss=0.3192  Val Loss=0.3399  Val Acc=0.8703  Time=18.9s\n",
            "  Fold 32 Epoch 56/150  Train Loss=0.3031  Val Loss=0.3629  Val Acc=0.8438  Time=18.8s\n",
            "  Fold 32 Epoch 57/150  Train Loss=0.2960  Val Loss=0.3622  Val Acc=0.8562  Time=18.9s\n",
            "  Fold 32 Epoch 58/150  Train Loss=0.3032  Val Loss=0.3500  Val Acc=0.8703  Time=18.9s\n",
            "  Fold 32 Epoch 59/150  Train Loss=0.3082  Val Loss=0.3804  Val Acc=0.8297  Time=18.9s\n",
            "  Fold 32 Epoch 60/150  Train Loss=0.2893  Val Loss=0.3453  Val Acc=0.8609  Time=18.9s\n",
            "  Fold 32 Epoch 61/150  Train Loss=0.2874  Val Loss=0.3541  Val Acc=0.8531  Time=18.8s\n",
            "  Fold 32 Epoch 62/150  Train Loss=0.2893  Val Loss=0.3556  Val Acc=0.8656  Time=18.8s\n",
            "  Fold 32 Epoch 63/150  Train Loss=0.2841  Val Loss=0.3945  Val Acc=0.8313  Time=19.1s\n",
            "  Fold 32 Epoch 64/150  Train Loss=0.2813  Val Loss=0.3501  Val Acc=0.8609  Time=18.8s\n",
            "  Fold 32 Epoch 65/150  Train Loss=0.2746  Val Loss=0.3292  Val Acc=0.8750  Time=18.9s\n",
            "  Fold 32 Epoch 66/150  Train Loss=0.2896  Val Loss=0.3553  Val Acc=0.8625  Time=18.9s\n",
            "  Fold 32 Epoch 67/150  Train Loss=0.2809  Val Loss=0.3615  Val Acc=0.8500  Time=18.9s\n",
            "  Fold 32 Epoch 68/150  Train Loss=0.2830  Val Loss=0.3760  Val Acc=0.8422  Time=18.9s\n",
            "  Fold 32 Epoch 69/150  Train Loss=0.2550  Val Loss=0.3764  Val Acc=0.8531  Time=19.0s\n",
            "  Fold 32 Epoch 70/150  Train Loss=0.2732  Val Loss=0.3678  Val Acc=0.8500  Time=18.9s\n",
            "  Fold 32 Epoch 71/150  Train Loss=0.2600  Val Loss=0.4099  Val Acc=0.8156  Time=18.9s\n",
            "  Fold 32 Epoch 72/150  Train Loss=0.2599  Val Loss=0.3710  Val Acc=0.8469  Time=19.0s\n",
            "  Fold 32 Epoch 73/150  Train Loss=0.2516  Val Loss=0.3739  Val Acc=0.8391  Time=18.9s\n",
            "  Fold 32 Epoch 74/150  Train Loss=0.2514  Val Loss=0.3465  Val Acc=0.8641  Time=18.7s\n",
            "  Fold 32 Epoch 75/150  Train Loss=0.2445  Val Loss=0.3516  Val Acc=0.8641  Time=18.9s\n",
            "  Fold 32 Epoch 76/150  Train Loss=0.2401  Val Loss=0.3673  Val Acc=0.8562  Time=18.9s\n",
            "  Fold 32 Epoch 77/150  Train Loss=0.2439  Val Loss=0.3353  Val Acc=0.8750  Time=18.8s\n",
            "  Fold 32 Epoch 78/150  Train Loss=0.2315  Val Loss=0.3448  Val Acc=0.8672  Time=18.9s\n",
            "  Fold 32 Epoch 79/150  Train Loss=0.2269  Val Loss=0.3529  Val Acc=0.8609  Time=18.9s\n",
            "  Fold 32 Epoch 80/150  Train Loss=0.2345  Val Loss=0.3452  Val Acc=0.8672  Time=18.8s\n",
            "  → Early stopping at epoch 80 (no val_loss improvement)\n",
            "  Fold 32 Final ACC = 0.1538   (TP=8  TN=0  FP=0  FN=44)\n",
            "\n",
            ">>> Fold 33/65  (leave out sub-033)\n",
            "  Fold 33 Epoch 1/150  Train Loss=0.7176  Val Loss=0.6856  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 33 Epoch 2/150  Train Loss=0.7240  Val Loss=0.6870  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 33 Epoch 3/150  Train Loss=0.7190  Val Loss=0.6905  Val Acc=0.5719  Time=19.0s\n",
            "  Fold 33 Epoch 4/150  Train Loss=0.7160  Val Loss=0.6900  Val Acc=0.5719  Time=18.8s\n",
            "  Fold 33 Epoch 5/150  Train Loss=0.7175  Val Loss=0.6907  Val Acc=0.5547  Time=19.0s\n",
            "  Fold 33 Epoch 6/150  Train Loss=0.7075  Val Loss=0.6908  Val Acc=0.5609  Time=18.9s\n",
            "  Fold 33 Epoch 7/150  Train Loss=0.7062  Val Loss=0.6942  Val Acc=0.4578  Time=19.1s\n",
            "  Fold 33 Epoch 8/150  Train Loss=0.7121  Val Loss=0.6878  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 33 Epoch 9/150  Train Loss=0.7083  Val Loss=0.6838  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 33 Epoch 10/150  Train Loss=0.7028  Val Loss=0.6891  Val Acc=0.5828  Time=18.9s\n",
            "  Fold 33 Epoch 11/150  Train Loss=0.7156  Val Loss=0.6909  Val Acc=0.5750  Time=18.9s\n",
            "  Fold 33 Epoch 12/150  Train Loss=0.7156  Val Loss=0.6962  Val Acc=0.4203  Time=19.0s\n",
            "  Fold 33 Epoch 13/150  Train Loss=0.7099  Val Loss=0.6875  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 33 Epoch 14/150  Train Loss=0.7116  Val Loss=0.6907  Val Acc=0.5781  Time=19.1s\n",
            "  Fold 33 Epoch 15/150  Train Loss=0.7075  Val Loss=0.6934  Val Acc=0.5000  Time=19.0s\n",
            "  Fold 33 Epoch 16/150  Train Loss=0.7100  Val Loss=0.6949  Val Acc=0.4203  Time=18.9s\n",
            "  Fold 33 Epoch 17/150  Train Loss=0.7096  Val Loss=0.6907  Val Acc=0.6062  Time=19.0s\n",
            "  Fold 33 Epoch 18/150  Train Loss=0.7112  Val Loss=0.6883  Val Acc=0.6328  Time=18.9s\n",
            "  Fold 33 Epoch 19/150  Train Loss=0.7018  Val Loss=0.6760  Val Acc=0.5859  Time=18.9s\n",
            "  Fold 33 Epoch 20/150  Train Loss=0.7066  Val Loss=0.6692  Val Acc=0.7906  Time=18.9s\n",
            "  Fold 33 Epoch 21/150  Train Loss=0.6823  Val Loss=0.6287  Val Acc=0.8078  Time=19.0s\n",
            "  Fold 33 Epoch 22/150  Train Loss=0.6500  Val Loss=0.5383  Val Acc=0.8578  Time=18.9s\n",
            "  Fold 33 Epoch 23/150  Train Loss=0.6189  Val Loss=0.4781  Val Acc=0.8422  Time=18.9s\n",
            "  Fold 33 Epoch 24/150  Train Loss=0.5751  Val Loss=0.4669  Val Acc=0.8703  Time=19.0s\n",
            "  Fold 33 Epoch 25/150  Train Loss=0.5531  Val Loss=0.4669  Val Acc=0.8250  Time=18.9s\n",
            "  Fold 33 Epoch 26/150  Train Loss=0.5289  Val Loss=0.4679  Val Acc=0.7734  Time=18.9s\n",
            "  Fold 33 Epoch 27/150  Train Loss=0.5163  Val Loss=0.4400  Val Acc=0.8219  Time=18.9s\n",
            "  Fold 33 Epoch 28/150  Train Loss=0.5007  Val Loss=0.4599  Val Acc=0.7453  Time=19.0s\n",
            "  Fold 33 Epoch 29/150  Train Loss=0.4863  Val Loss=0.3631  Val Acc=0.8969  Time=19.0s\n",
            "  Fold 33 Epoch 30/150  Train Loss=0.4817  Val Loss=0.4455  Val Acc=0.7734  Time=19.0s\n",
            "  Fold 33 Epoch 31/150  Train Loss=0.4695  Val Loss=0.4306  Val Acc=0.7688  Time=18.9s\n",
            "  Fold 33 Epoch 32/150  Train Loss=0.4512  Val Loss=0.3477  Val Acc=0.8891  Time=19.0s\n",
            "  Fold 33 Epoch 33/150  Train Loss=0.4343  Val Loss=0.3389  Val Acc=0.8922  Time=18.9s\n",
            "  Fold 33 Epoch 34/150  Train Loss=0.4271  Val Loss=0.4348  Val Acc=0.7797  Time=18.9s\n",
            "  Fold 33 Epoch 35/150  Train Loss=0.4190  Val Loss=0.4288  Val Acc=0.7828  Time=19.0s\n",
            "  Fold 33 Epoch 36/150  Train Loss=0.4022  Val Loss=0.3645  Val Acc=0.8516  Time=19.0s\n",
            "  Fold 33 Epoch 37/150  Train Loss=0.3962  Val Loss=0.3805  Val Acc=0.8234  Time=18.9s\n",
            "  Fold 33 Epoch 38/150  Train Loss=0.3899  Val Loss=0.4252  Val Acc=0.7781  Time=19.0s\n",
            "  Fold 33 Epoch 39/150  Train Loss=0.3881  Val Loss=0.4035  Val Acc=0.8016  Time=18.9s\n",
            "  Fold 33 Epoch 40/150  Train Loss=0.3772  Val Loss=0.3435  Val Acc=0.8578  Time=19.1s\n",
            "  Fold 33 Epoch 41/150  Train Loss=0.3600  Val Loss=0.4521  Val Acc=0.7625  Time=18.9s\n",
            "  Fold 33 Epoch 42/150  Train Loss=0.3525  Val Loss=0.3673  Val Acc=0.8297  Time=19.3s\n",
            "  Fold 33 Epoch 43/150  Train Loss=0.3466  Val Loss=0.3356  Val Acc=0.8812  Time=19.1s\n",
            "  Fold 33 Epoch 44/150  Train Loss=0.3366  Val Loss=0.3666  Val Acc=0.8359  Time=19.4s\n",
            "  Fold 33 Epoch 45/150  Train Loss=0.3372  Val Loss=0.3696  Val Acc=0.8328  Time=19.4s\n",
            "  Fold 33 Epoch 46/150  Train Loss=0.3398  Val Loss=0.3569  Val Acc=0.8453  Time=19.8s\n",
            "  Fold 33 Epoch 47/150  Train Loss=0.3256  Val Loss=0.3764  Val Acc=0.8266  Time=18.8s\n",
            "  Fold 33 Epoch 48/150  Train Loss=0.3111  Val Loss=0.3427  Val Acc=0.8531  Time=19.1s\n",
            "  Fold 33 Epoch 49/150  Train Loss=0.3251  Val Loss=0.4072  Val Acc=0.7969  Time=19.2s\n",
            "  Fold 33 Epoch 50/150  Train Loss=0.3139  Val Loss=0.3971  Val Acc=0.8078  Time=19.4s\n",
            "  Fold 33 Epoch 51/150  Train Loss=0.2944  Val Loss=0.3516  Val Acc=0.8562  Time=19.8s\n",
            "  Fold 33 Epoch 52/150  Train Loss=0.3012  Val Loss=0.3956  Val Acc=0.8187  Time=20.0s\n",
            "  Fold 33 Epoch 53/150  Train Loss=0.3054  Val Loss=0.3893  Val Acc=0.8266  Time=19.9s\n",
            "  Fold 33 Epoch 54/150  Train Loss=0.2995  Val Loss=0.3425  Val Acc=0.8594  Time=19.8s\n",
            "  Fold 33 Epoch 55/150  Train Loss=0.2900  Val Loss=0.4143  Val Acc=0.8078  Time=20.1s\n",
            "  Fold 33 Epoch 56/150  Train Loss=0.2858  Val Loss=0.3623  Val Acc=0.8453  Time=19.7s\n",
            "  Fold 33 Epoch 57/150  Train Loss=0.2884  Val Loss=0.3506  Val Acc=0.8594  Time=19.1s\n",
            "  Fold 33 Epoch 58/150  Train Loss=0.2726  Val Loss=0.4690  Val Acc=0.7484  Time=18.9s\n",
            "  → Early stopping at epoch 58 (no val_loss improvement)\n",
            "  Fold 33 Final ACC = 1.0000   (TP=52  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 34/65  (leave out sub-034)\n",
            "  Fold 34 Epoch 1/150  Train Loss=0.7356  Val Loss=0.6807  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 34 Epoch 2/150  Train Loss=0.7196  Val Loss=0.6814  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 34 Epoch 3/150  Train Loss=0.7233  Val Loss=0.6827  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 34 Epoch 4/150  Train Loss=0.7147  Val Loss=0.6864  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 34 Epoch 5/150  Train Loss=0.7177  Val Loss=0.6842  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 34 Epoch 6/150  Train Loss=0.7321  Val Loss=0.6897  Val Acc=0.5901  Time=18.9s\n",
            "  Fold 34 Epoch 7/150  Train Loss=0.7170  Val Loss=0.6913  Val Acc=0.6036  Time=18.9s\n",
            "  Fold 34 Epoch 8/150  Train Loss=0.7091  Val Loss=0.6909  Val Acc=0.5916  Time=19.0s\n",
            "  Fold 34 Epoch 9/150  Train Loss=0.7148  Val Loss=0.6913  Val Acc=0.6051  Time=18.9s\n",
            "  Fold 34 Epoch 10/150  Train Loss=0.7123  Val Loss=0.6888  Val Acc=0.6006  Time=18.8s\n",
            "  Fold 34 Epoch 11/150  Train Loss=0.7156  Val Loss=0.6866  Val Acc=0.5916  Time=18.8s\n",
            "  Fold 34 Epoch 12/150  Train Loss=0.7112  Val Loss=0.6918  Val Acc=0.5255  Time=18.8s\n",
            "  Fold 34 Epoch 13/150  Train Loss=0.7117  Val Loss=0.6919  Val Acc=0.4339  Time=18.9s\n",
            "  Fold 34 Epoch 14/150  Train Loss=0.7063  Val Loss=0.6669  Val Acc=0.6321  Time=18.9s\n",
            "  Fold 34 Epoch 15/150  Train Loss=0.6989  Val Loss=0.6362  Val Acc=0.6456  Time=19.0s\n",
            "  Fold 34 Epoch 16/150  Train Loss=0.6683  Val Loss=0.6190  Val Acc=0.7072  Time=18.8s\n",
            "  Fold 34 Epoch 17/150  Train Loss=0.6328  Val Loss=0.5741  Val Acc=0.7282  Time=19.0s\n",
            "  Fold 34 Epoch 18/150  Train Loss=0.6045  Val Loss=0.5651  Val Acc=0.7207  Time=18.9s\n",
            "  Fold 34 Epoch 19/150  Train Loss=0.5766  Val Loss=0.5986  Val Acc=0.6967  Time=19.0s\n",
            "  Fold 34 Epoch 20/150  Train Loss=0.5699  Val Loss=0.5748  Val Acc=0.6982  Time=18.9s\n",
            "  Fold 34 Epoch 21/150  Train Loss=0.5454  Val Loss=0.5543  Val Acc=0.7057  Time=18.9s\n",
            "  Fold 34 Epoch 22/150  Train Loss=0.5350  Val Loss=0.5621  Val Acc=0.7057  Time=18.9s\n",
            "  Fold 34 Epoch 23/150  Train Loss=0.5279  Val Loss=0.5507  Val Acc=0.7087  Time=18.8s\n",
            "  Fold 34 Epoch 24/150  Train Loss=0.5205  Val Loss=0.5225  Val Acc=0.7282  Time=18.8s\n",
            "  Fold 34 Epoch 25/150  Train Loss=0.5098  Val Loss=0.5445  Val Acc=0.6982  Time=19.0s\n",
            "  Fold 34 Epoch 26/150  Train Loss=0.4910  Val Loss=0.5454  Val Acc=0.7057  Time=18.8s\n",
            "  Fold 34 Epoch 27/150  Train Loss=0.4887  Val Loss=0.5542  Val Acc=0.6907  Time=18.9s\n",
            "  Fold 34 Epoch 28/150  Train Loss=0.4688  Val Loss=0.5361  Val Acc=0.7117  Time=19.0s\n",
            "  Fold 34 Epoch 29/150  Train Loss=0.4622  Val Loss=0.5105  Val Acc=0.7462  Time=19.0s\n",
            "  Fold 34 Epoch 30/150  Train Loss=0.4592  Val Loss=0.5091  Val Acc=0.7357  Time=18.9s\n",
            "  Fold 34 Epoch 31/150  Train Loss=0.4494  Val Loss=0.5039  Val Acc=0.7462  Time=19.0s\n",
            "  Fold 34 Epoch 32/150  Train Loss=0.4331  Val Loss=0.4962  Val Acc=0.7523  Time=18.9s\n",
            "  Fold 34 Epoch 33/150  Train Loss=0.4303  Val Loss=0.5044  Val Acc=0.7432  Time=18.9s\n",
            "  Fold 34 Epoch 34/150  Train Loss=0.4238  Val Loss=0.4774  Val Acc=0.7598  Time=18.8s\n",
            "  Fold 34 Epoch 35/150  Train Loss=0.4189  Val Loss=0.4836  Val Acc=0.7462  Time=19.1s\n",
            "  Fold 34 Epoch 36/150  Train Loss=0.3981  Val Loss=0.4808  Val Acc=0.7523  Time=18.8s\n",
            "  Fold 34 Epoch 37/150  Train Loss=0.3872  Val Loss=0.4737  Val Acc=0.7718  Time=19.0s\n",
            "  Fold 34 Epoch 38/150  Train Loss=0.3788  Val Loss=0.4771  Val Acc=0.7613  Time=18.8s\n",
            "  Fold 34 Epoch 39/150  Train Loss=0.3725  Val Loss=0.5220  Val Acc=0.7342  Time=18.8s\n",
            "  Fold 34 Epoch 40/150  Train Loss=0.3704  Val Loss=0.4790  Val Acc=0.7583  Time=18.8s\n",
            "  Fold 34 Epoch 41/150  Train Loss=0.3621  Val Loss=0.4849  Val Acc=0.7538  Time=19.0s\n",
            "  Fold 34 Epoch 42/150  Train Loss=0.3448  Val Loss=0.4699  Val Acc=0.7658  Time=18.9s\n",
            "  Fold 34 Epoch 43/150  Train Loss=0.3517  Val Loss=0.4814  Val Acc=0.7628  Time=18.9s\n",
            "  Fold 34 Epoch 44/150  Train Loss=0.3349  Val Loss=0.4759  Val Acc=0.7673  Time=18.9s\n",
            "  Fold 34 Epoch 45/150  Train Loss=0.3309  Val Loss=0.4656  Val Acc=0.7793  Time=19.0s\n",
            "  Fold 34 Epoch 46/150  Train Loss=0.3371  Val Loss=0.4674  Val Acc=0.7778  Time=18.8s\n",
            "  Fold 34 Epoch 47/150  Train Loss=0.3223  Val Loss=0.4660  Val Acc=0.7763  Time=18.9s\n",
            "  Fold 34 Epoch 48/150  Train Loss=0.3186  Val Loss=0.4732  Val Acc=0.7838  Time=19.1s\n",
            "  Fold 34 Epoch 49/150  Train Loss=0.3135  Val Loss=0.4676  Val Acc=0.7808  Time=18.9s\n",
            "  Fold 34 Epoch 50/150  Train Loss=0.3029  Val Loss=0.4840  Val Acc=0.7823  Time=19.1s\n",
            "  Fold 34 Epoch 51/150  Train Loss=0.3025  Val Loss=0.4937  Val Acc=0.7673  Time=18.9s\n",
            "  Fold 34 Epoch 52/150  Train Loss=0.2908  Val Loss=0.4784  Val Acc=0.7868  Time=18.9s\n",
            "  Fold 34 Epoch 53/150  Train Loss=0.2890  Val Loss=0.4969  Val Acc=0.7763  Time=18.9s\n",
            "  Fold 34 Epoch 54/150  Train Loss=0.2874  Val Loss=0.4761  Val Acc=0.7898  Time=19.2s\n",
            "  Fold 34 Epoch 55/150  Train Loss=0.2721  Val Loss=0.4815  Val Acc=0.7823  Time=19.0s\n",
            "  Fold 34 Epoch 56/150  Train Loss=0.2602  Val Loss=0.4689  Val Acc=0.7883  Time=19.1s\n",
            "  Fold 34 Epoch 57/150  Train Loss=0.2686  Val Loss=0.5219  Val Acc=0.7883  Time=18.8s\n",
            "  Fold 34 Epoch 58/150  Train Loss=0.2601  Val Loss=0.5109  Val Acc=0.7733  Time=19.0s\n",
            "  Fold 34 Epoch 59/150  Train Loss=0.2605  Val Loss=0.4857  Val Acc=0.7853  Time=19.0s\n",
            "  Fold 34 Epoch 60/150  Train Loss=0.2588  Val Loss=0.4961  Val Acc=0.7853  Time=18.9s\n",
            "  → Early stopping at epoch 60 (no val_loss improvement)\n",
            "  Fold 34 Final ACC = 0.9423   (TP=0  TN=49  FP=3  FN=0)\n",
            "\n",
            ">>> Fold 35/65  (leave out sub-035)\n",
            "  Fold 35 Epoch 1/150  Train Loss=0.7255  Val Loss=0.6858  Val Acc=0.5696  Time=19.0s\n",
            "  Fold 35 Epoch 2/150  Train Loss=0.7217  Val Loss=0.6862  Val Acc=0.5696  Time=19.1s\n",
            "  Fold 35 Epoch 3/150  Train Loss=0.7142  Val Loss=0.6868  Val Acc=0.5696  Time=18.8s\n",
            "  Fold 35 Epoch 4/150  Train Loss=0.7199  Val Loss=0.6861  Val Acc=0.5696  Time=19.0s\n",
            "  Fold 35 Epoch 5/150  Train Loss=0.7189  Val Loss=0.6900  Val Acc=0.5696  Time=18.7s\n",
            "  Fold 35 Epoch 6/150  Train Loss=0.7089  Val Loss=0.6903  Val Acc=0.5696  Time=19.1s\n",
            "  Fold 35 Epoch 7/150  Train Loss=0.7124  Val Loss=0.6886  Val Acc=0.5696  Time=18.8s\n",
            "  Fold 35 Epoch 8/150  Train Loss=0.7184  Val Loss=0.6879  Val Acc=0.5696  Time=19.0s\n",
            "  Fold 35 Epoch 9/150  Train Loss=0.7134  Val Loss=0.6863  Val Acc=0.5696  Time=18.9s\n",
            "  Fold 35 Epoch 10/150  Train Loss=0.7132  Val Loss=0.6857  Val Acc=0.5696  Time=19.0s\n",
            "  Fold 35 Epoch 11/150  Train Loss=0.7168  Val Loss=0.6865  Val Acc=0.5696  Time=18.8s\n",
            "  Fold 35 Epoch 12/150  Train Loss=0.7178  Val Loss=0.6853  Val Acc=0.5696  Time=19.0s\n",
            "  Fold 35 Epoch 13/150  Train Loss=0.7138  Val Loss=0.6866  Val Acc=0.5696  Time=19.0s\n",
            "  Fold 35 Epoch 14/150  Train Loss=0.7096  Val Loss=0.6840  Val Acc=0.5696  Time=19.1s\n",
            "  Fold 35 Epoch 15/150  Train Loss=0.7015  Val Loss=0.6852  Val Acc=0.5696  Time=19.1s\n",
            "  Fold 35 Epoch 16/150  Train Loss=0.7093  Val Loss=0.6861  Val Acc=0.5728  Time=18.9s\n",
            "  Fold 35 Epoch 17/150  Train Loss=0.7047  Val Loss=0.6772  Val Acc=0.5728  Time=18.9s\n",
            "  Fold 35 Epoch 18/150  Train Loss=0.6942  Val Loss=0.6559  Val Acc=0.6456  Time=19.0s\n",
            "  Fold 35 Epoch 19/150  Train Loss=0.6608  Val Loss=0.6162  Val Acc=0.6551  Time=18.9s\n",
            "  Fold 35 Epoch 20/150  Train Loss=0.6322  Val Loss=0.5844  Val Acc=0.6962  Time=19.0s\n",
            "  Fold 35 Epoch 21/150  Train Loss=0.5978  Val Loss=0.5368  Val Acc=0.7373  Time=18.9s\n",
            "  Fold 35 Epoch 22/150  Train Loss=0.5513  Val Loss=0.4872  Val Acc=0.7500  Time=19.0s\n",
            "  Fold 35 Epoch 23/150  Train Loss=0.5396  Val Loss=0.4852  Val Acc=0.7547  Time=19.0s\n",
            "  Fold 35 Epoch 24/150  Train Loss=0.5151  Val Loss=0.4758  Val Acc=0.7595  Time=19.0s\n",
            "  Fold 35 Epoch 25/150  Train Loss=0.5001  Val Loss=0.4829  Val Acc=0.7737  Time=19.0s\n",
            "  Fold 35 Epoch 26/150  Train Loss=0.4919  Val Loss=0.4809  Val Acc=0.7706  Time=18.9s\n",
            "  Fold 35 Epoch 27/150  Train Loss=0.4758  Val Loss=0.4871  Val Acc=0.7737  Time=19.0s\n",
            "  Fold 35 Epoch 28/150  Train Loss=0.4701  Val Loss=0.4741  Val Acc=0.7832  Time=19.0s\n",
            "  Fold 35 Epoch 29/150  Train Loss=0.4564  Val Loss=0.4815  Val Acc=0.7801  Time=18.9s\n",
            "  Fold 35 Epoch 30/150  Train Loss=0.4389  Val Loss=0.5238  Val Acc=0.7168  Time=19.1s\n",
            "  Fold 35 Epoch 31/150  Train Loss=0.4294  Val Loss=0.5495  Val Acc=0.7184  Time=18.9s\n",
            "  Fold 35 Epoch 32/150  Train Loss=0.4204  Val Loss=0.5916  Val Acc=0.6709  Time=19.0s\n",
            "  Fold 35 Epoch 33/150  Train Loss=0.4031  Val Loss=0.4867  Val Acc=0.7453  Time=19.0s\n",
            "  Fold 35 Epoch 34/150  Train Loss=0.3914  Val Loss=0.5106  Val Acc=0.7421  Time=19.1s\n",
            "  Fold 35 Epoch 35/150  Train Loss=0.3900  Val Loss=0.5071  Val Acc=0.7421  Time=18.9s\n",
            "  Fold 35 Epoch 36/150  Train Loss=0.3709  Val Loss=0.5206  Val Acc=0.7326  Time=19.2s\n",
            "  Fold 35 Epoch 37/150  Train Loss=0.3582  Val Loss=0.4846  Val Acc=0.7500  Time=18.9s\n",
            "  Fold 35 Epoch 38/150  Train Loss=0.3640  Val Loss=0.5207  Val Acc=0.7278  Time=19.1s\n",
            "  Fold 35 Epoch 39/150  Train Loss=0.3527  Val Loss=0.5084  Val Acc=0.7358  Time=18.9s\n",
            "  Fold 35 Epoch 40/150  Train Loss=0.3427  Val Loss=0.5083  Val Acc=0.7421  Time=19.2s\n",
            "  Fold 35 Epoch 41/150  Train Loss=0.3486  Val Loss=0.5035  Val Acc=0.7579  Time=19.0s\n",
            "  Fold 35 Epoch 42/150  Train Loss=0.3212  Val Loss=0.5148  Val Acc=0.7468  Time=19.1s\n",
            "  Fold 35 Epoch 43/150  Train Loss=0.3269  Val Loss=0.5408  Val Acc=0.7294  Time=18.9s\n",
            "  → Early stopping at epoch 43 (no val_loss improvement)\n",
            "  Fold 35 Final ACC = 0.7451   (TP=0  TN=38  FP=13  FN=0)\n",
            "\n",
            ">>> Fold 36/65  (leave out sub-036)\n",
            "  Fold 36 Epoch 1/150  Train Loss=0.7293  Val Loss=0.6848  Val Acc=0.5792  Time=19.1s\n",
            "  Fold 36 Epoch 2/150  Train Loss=0.7345  Val Loss=0.6839  Val Acc=0.5777  Time=18.8s\n",
            "  Fold 36 Epoch 3/150  Train Loss=0.7333  Val Loss=0.6828  Val Acc=0.5867  Time=19.1s\n",
            "  Fold 36 Epoch 4/150  Train Loss=0.7371  Val Loss=0.6808  Val Acc=0.5897  Time=18.8s\n",
            "  Fold 36 Epoch 5/150  Train Loss=0.7400  Val Loss=0.6802  Val Acc=0.5897  Time=18.9s\n",
            "  Fold 36 Epoch 6/150  Train Loss=0.7260  Val Loss=0.6807  Val Acc=0.5897  Time=18.8s\n",
            "  Fold 36 Epoch 7/150  Train Loss=0.7314  Val Loss=0.6799  Val Acc=0.5897  Time=19.0s\n",
            "  Fold 36 Epoch 8/150  Train Loss=0.7427  Val Loss=0.6805  Val Acc=0.5897  Time=18.9s\n",
            "  Fold 36 Epoch 9/150  Train Loss=0.7225  Val Loss=0.6833  Val Acc=0.5897  Time=19.0s\n",
            "  Fold 36 Epoch 10/150  Train Loss=0.7211  Val Loss=0.6819  Val Acc=0.5897  Time=18.8s\n",
            "  Fold 36 Epoch 11/150  Train Loss=0.7162  Val Loss=0.6795  Val Acc=0.5897  Time=19.0s\n",
            "  Fold 36 Epoch 12/150  Train Loss=0.7200  Val Loss=0.6804  Val Acc=0.5897  Time=18.9s\n",
            "  Fold 36 Epoch 13/150  Train Loss=0.7217  Val Loss=0.6796  Val Acc=0.5897  Time=19.0s\n",
            "  Fold 36 Epoch 14/150  Train Loss=0.7268  Val Loss=0.6907  Val Acc=0.5747  Time=18.8s\n",
            "  Fold 36 Epoch 15/150  Train Loss=0.7205  Val Loss=0.6806  Val Acc=0.6531  Time=18.9s\n",
            "  Fold 36 Epoch 16/150  Train Loss=0.7174  Val Loss=0.6857  Val Acc=0.4721  Time=18.8s\n",
            "  Fold 36 Epoch 17/150  Train Loss=0.7080  Val Loss=0.6816  Val Acc=0.5551  Time=18.9s\n",
            "  Fold 36 Epoch 18/150  Train Loss=0.6836  Val Loss=0.6191  Val Acc=0.6757  Time=18.9s\n",
            "  Fold 36 Epoch 19/150  Train Loss=0.6497  Val Loss=0.5775  Val Acc=0.7059  Time=19.1s\n",
            "  Fold 36 Epoch 20/150  Train Loss=0.5939  Val Loss=0.5773  Val Acc=0.7089  Time=18.8s\n",
            "  Fold 36 Epoch 21/150  Train Loss=0.5731  Val Loss=0.5201  Val Acc=0.7255  Time=18.9s\n",
            "  Fold 36 Epoch 22/150  Train Loss=0.5543  Val Loss=0.5425  Val Acc=0.7255  Time=18.7s\n",
            "  Fold 36 Epoch 23/150  Train Loss=0.5357  Val Loss=0.5227  Val Acc=0.7315  Time=19.0s\n",
            "  Fold 36 Epoch 24/150  Train Loss=0.5266  Val Loss=0.5406  Val Acc=0.7345  Time=18.8s\n",
            "  Fold 36 Epoch 25/150  Train Loss=0.5072  Val Loss=0.5496  Val Acc=0.7210  Time=19.1s\n",
            "  Fold 36 Epoch 26/150  Train Loss=0.4972  Val Loss=0.4679  Val Acc=0.7572  Time=18.9s\n",
            "  Fold 36 Epoch 27/150  Train Loss=0.4822  Val Loss=0.4836  Val Acc=0.7541  Time=19.1s\n",
            "  Fold 36 Epoch 28/150  Train Loss=0.4679  Val Loss=0.4728  Val Acc=0.7632  Time=18.8s\n",
            "  Fold 36 Epoch 29/150  Train Loss=0.4669  Val Loss=0.4776  Val Acc=0.7662  Time=19.1s\n",
            "  Fold 36 Epoch 30/150  Train Loss=0.4507  Val Loss=0.5290  Val Acc=0.7164  Time=18.8s\n",
            "  Fold 36 Epoch 31/150  Train Loss=0.4412  Val Loss=0.4743  Val Acc=0.7753  Time=18.9s\n",
            "  Fold 36 Epoch 32/150  Train Loss=0.4241  Val Loss=0.5274  Val Acc=0.7195  Time=18.8s\n",
            "  Fold 36 Epoch 33/150  Train Loss=0.4174  Val Loss=0.4512  Val Acc=0.7798  Time=19.0s\n",
            "  Fold 36 Epoch 34/150  Train Loss=0.4012  Val Loss=0.4435  Val Acc=0.7843  Time=18.7s\n",
            "  Fold 36 Epoch 35/150  Train Loss=0.3879  Val Loss=0.4569  Val Acc=0.7632  Time=19.0s\n",
            "  Fold 36 Epoch 36/150  Train Loss=0.3874  Val Loss=0.5830  Val Acc=0.6908  Time=18.8s\n",
            "  Fold 36 Epoch 37/150  Train Loss=0.3762  Val Loss=0.4228  Val Acc=0.8024  Time=19.0s\n",
            "  Fold 36 Epoch 38/150  Train Loss=0.3588  Val Loss=0.4361  Val Acc=0.7783  Time=18.8s\n",
            "  Fold 36 Epoch 39/150  Train Loss=0.3607  Val Loss=0.4552  Val Acc=0.7647  Time=19.0s\n",
            "  Fold 36 Epoch 40/150  Train Loss=0.3579  Val Loss=0.4676  Val Acc=0.7526  Time=18.8s\n",
            "  Fold 36 Epoch 41/150  Train Loss=0.3556  Val Loss=0.4633  Val Acc=0.7617  Time=18.9s\n",
            "  Fold 36 Epoch 42/150  Train Loss=0.3391  Val Loss=0.4262  Val Acc=0.7858  Time=18.9s\n",
            "  Fold 36 Epoch 43/150  Train Loss=0.3256  Val Loss=0.4200  Val Acc=0.8009  Time=18.9s\n",
            "  Fold 36 Epoch 44/150  Train Loss=0.3304  Val Loss=0.4160  Val Acc=0.8039  Time=18.8s\n",
            "  Fold 36 Epoch 45/150  Train Loss=0.3303  Val Loss=0.4307  Val Acc=0.7934  Time=19.0s\n",
            "  Fold 36 Epoch 46/150  Train Loss=0.3210  Val Loss=0.4603  Val Acc=0.7572  Time=18.9s\n",
            "  Fold 36 Epoch 47/150  Train Loss=0.3313  Val Loss=0.4193  Val Acc=0.8069  Time=19.0s\n",
            "  Fold 36 Epoch 48/150  Train Loss=0.2992  Val Loss=0.4162  Val Acc=0.8024  Time=18.9s\n",
            "  Fold 36 Epoch 49/150  Train Loss=0.3052  Val Loss=0.4494  Val Acc=0.7722  Time=19.1s\n",
            "  Fold 36 Epoch 50/150  Train Loss=0.2956  Val Loss=0.4096  Val Acc=0.8175  Time=18.8s\n",
            "  Fold 36 Epoch 51/150  Train Loss=0.2985  Val Loss=0.4160  Val Acc=0.8115  Time=19.0s\n",
            "  Fold 36 Epoch 52/150  Train Loss=0.2899  Val Loss=0.4263  Val Acc=0.7964  Time=18.7s\n",
            "  Fold 36 Epoch 53/150  Train Loss=0.2908  Val Loss=0.4182  Val Acc=0.8145  Time=19.0s\n",
            "  Fold 36 Epoch 54/150  Train Loss=0.2761  Val Loss=0.4546  Val Acc=0.7828  Time=19.0s\n",
            "  Fold 36 Epoch 55/150  Train Loss=0.2701  Val Loss=0.4396  Val Acc=0.7934  Time=19.2s\n",
            "  Fold 36 Epoch 56/150  Train Loss=0.2661  Val Loss=0.4435  Val Acc=0.7858  Time=18.9s\n",
            "  Fold 36 Epoch 57/150  Train Loss=0.2655  Val Loss=0.4244  Val Acc=0.8115  Time=19.0s\n",
            "  Fold 36 Epoch 58/150  Train Loss=0.2589  Val Loss=0.4517  Val Acc=0.7843  Time=18.8s\n",
            "  Fold 36 Epoch 59/150  Train Loss=0.2674  Val Loss=0.4431  Val Acc=0.8084  Time=19.0s\n",
            "  Fold 36 Epoch 60/150  Train Loss=0.2632  Val Loss=0.4605  Val Acc=0.7903  Time=18.8s\n",
            "  Fold 36 Epoch 61/150  Train Loss=0.2616  Val Loss=0.5031  Val Acc=0.7632  Time=19.1s\n",
            "  Fold 36 Epoch 62/150  Train Loss=0.2685  Val Loss=0.4524  Val Acc=0.7858  Time=18.7s\n",
            "  Fold 36 Epoch 63/150  Train Loss=0.2531  Val Loss=0.4330  Val Acc=0.8145  Time=19.0s\n",
            "  Fold 36 Epoch 64/150  Train Loss=0.2498  Val Loss=0.4650  Val Acc=0.7903  Time=18.8s\n",
            "  Fold 36 Epoch 65/150  Train Loss=0.2490  Val Loss=0.5197  Val Acc=0.7572  Time=19.1s\n",
            "  → Early stopping at epoch 65 (no val_loss improvement)\n",
            "  Fold 36 Final ACC = 0.6667   (TP=0  TN=34  FP=17  FN=0)\n",
            "\n",
            ">>> Fold 37/65  (leave out sub-037)\n",
            "  Fold 37 Epoch 1/150  Train Loss=0.7210  Val Loss=0.6781  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 37 Epoch 2/150  Train Loss=0.7234  Val Loss=0.6787  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 3/150  Train Loss=0.7189  Val Loss=0.6778  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 4/150  Train Loss=0.7194  Val Loss=0.6780  Val Acc=0.5844  Time=19.1s\n",
            "  Fold 37 Epoch 5/150  Train Loss=0.7133  Val Loss=0.6812  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 37 Epoch 6/150  Train Loss=0.7124  Val Loss=0.6802  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 37 Epoch 7/150  Train Loss=0.7120  Val Loss=0.6784  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 8/150  Train Loss=0.7087  Val Loss=0.6774  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 9/150  Train Loss=0.7049  Val Loss=0.6796  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 10/150  Train Loss=0.7036  Val Loss=0.6788  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 11/150  Train Loss=0.7059  Val Loss=0.6778  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 37 Epoch 12/150  Train Loss=0.7089  Val Loss=0.6775  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 13/150  Train Loss=0.7106  Val Loss=0.6785  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 14/150  Train Loss=0.7014  Val Loss=0.6751  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 15/150  Train Loss=0.7072  Val Loss=0.6777  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 16/150  Train Loss=0.7021  Val Loss=0.6727  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 37 Epoch 17/150  Train Loss=0.7043  Val Loss=0.6693  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 37 Epoch 18/150  Train Loss=0.6958  Val Loss=0.6560  Val Acc=0.6219  Time=18.8s\n",
            "  Fold 37 Epoch 19/150  Train Loss=0.6810  Val Loss=0.6208  Val Acc=0.6734  Time=19.0s\n",
            "  Fold 37 Epoch 20/150  Train Loss=0.6578  Val Loss=0.5335  Val Acc=0.8078  Time=19.0s\n",
            "  Fold 37 Epoch 21/150  Train Loss=0.6153  Val Loss=0.5276  Val Acc=0.8234  Time=18.8s\n",
            "  Fold 37 Epoch 22/150  Train Loss=0.5733  Val Loss=0.4479  Val Acc=0.8234  Time=18.9s\n",
            "  Fold 37 Epoch 23/150  Train Loss=0.5542  Val Loss=0.4450  Val Acc=0.8766  Time=18.9s\n",
            "  Fold 37 Epoch 24/150  Train Loss=0.5452  Val Loss=0.4484  Val Acc=0.8844  Time=19.0s\n",
            "  Fold 37 Epoch 25/150  Train Loss=0.5282  Val Loss=0.4194  Val Acc=0.8875  Time=18.9s\n",
            "  Fold 37 Epoch 26/150  Train Loss=0.5070  Val Loss=0.4401  Val Acc=0.8156  Time=18.9s\n",
            "  Fold 37 Epoch 27/150  Train Loss=0.4975  Val Loss=0.4195  Val Acc=0.8656  Time=18.9s\n",
            "  Fold 37 Epoch 28/150  Train Loss=0.4903  Val Loss=0.4165  Val Acc=0.8578  Time=18.9s\n",
            "  Fold 37 Epoch 29/150  Train Loss=0.4763  Val Loss=0.3785  Val Acc=0.9016  Time=18.9s\n",
            "  Fold 37 Epoch 30/150  Train Loss=0.4602  Val Loss=0.3926  Val Acc=0.8703  Time=18.8s\n",
            "  Fold 37 Epoch 31/150  Train Loss=0.4600  Val Loss=0.4010  Val Acc=0.8484  Time=18.8s\n",
            "  Fold 37 Epoch 32/150  Train Loss=0.4398  Val Loss=0.3409  Val Acc=0.9016  Time=19.0s\n",
            "  Fold 37 Epoch 33/150  Train Loss=0.4269  Val Loss=0.3718  Val Acc=0.8688  Time=18.9s\n",
            "  Fold 37 Epoch 34/150  Train Loss=0.4231  Val Loss=0.4062  Val Acc=0.8156  Time=19.1s\n",
            "  Fold 37 Epoch 35/150  Train Loss=0.4109  Val Loss=0.3271  Val Acc=0.9031  Time=18.9s\n",
            "  Fold 37 Epoch 36/150  Train Loss=0.4144  Val Loss=0.4938  Val Acc=0.6906  Time=18.9s\n",
            "  Fold 37 Epoch 37/150  Train Loss=0.4120  Val Loss=0.3871  Val Acc=0.8234  Time=18.9s\n",
            "  Fold 37 Epoch 38/150  Train Loss=0.3787  Val Loss=0.3972  Val Acc=0.7984  Time=18.9s\n",
            "  Fold 37 Epoch 39/150  Train Loss=0.3828  Val Loss=0.4462  Val Acc=0.7422  Time=18.7s\n",
            "  Fold 37 Epoch 40/150  Train Loss=0.3647  Val Loss=0.3534  Val Acc=0.8516  Time=19.0s\n",
            "  Fold 37 Epoch 41/150  Train Loss=0.3661  Val Loss=0.4712  Val Acc=0.7438  Time=18.9s\n",
            "  Fold 37 Epoch 42/150  Train Loss=0.3533  Val Loss=0.3603  Val Acc=0.8516  Time=19.0s\n",
            "  Fold 37 Epoch 43/150  Train Loss=0.3439  Val Loss=0.5182  Val Acc=0.6922  Time=18.9s\n",
            "  Fold 37 Epoch 44/150  Train Loss=0.3292  Val Loss=0.4456  Val Acc=0.7688  Time=18.9s\n",
            "  Fold 37 Epoch 45/150  Train Loss=0.3258  Val Loss=0.3923  Val Acc=0.7953  Time=18.8s\n",
            "  Fold 37 Epoch 46/150  Train Loss=0.3436  Val Loss=0.4267  Val Acc=0.7750  Time=19.0s\n",
            "  Fold 37 Epoch 47/150  Train Loss=0.3357  Val Loss=0.3841  Val Acc=0.8187  Time=18.9s\n",
            "  Fold 37 Epoch 48/150  Train Loss=0.3226  Val Loss=0.3860  Val Acc=0.8172  Time=19.0s\n",
            "  Fold 37 Epoch 49/150  Train Loss=0.3174  Val Loss=0.4222  Val Acc=0.7844  Time=19.1s\n",
            "  Fold 37 Epoch 50/150  Train Loss=0.3135  Val Loss=0.5275  Val Acc=0.7031  Time=18.8s\n",
            "  → Early stopping at epoch 50 (no val_loss improvement)\n",
            "  Fold 37 Final ACC = 0.1961   (TP=10  TN=0  FP=0  FN=41)\n",
            "\n",
            ">>> Fold 38/65  (leave out sub-038)\n",
            "  Fold 38 Epoch 1/150  Train Loss=0.9523  Val Loss=0.8083  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 38 Epoch 2/150  Train Loss=0.9192  Val Loss=0.8095  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 38 Epoch 3/150  Train Loss=0.9118  Val Loss=0.8299  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 38 Epoch 4/150  Train Loss=0.8962  Val Loss=0.8072  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 38 Epoch 5/150  Train Loss=0.8604  Val Loss=0.7929  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 38 Epoch 6/150  Train Loss=0.8345  Val Loss=0.7934  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 38 Epoch 7/150  Train Loss=0.7942  Val Loss=0.7631  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 38 Epoch 8/150  Train Loss=0.7862  Val Loss=0.7575  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 38 Epoch 9/150  Train Loss=0.7772  Val Loss=0.7275  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 38 Epoch 10/150  Train Loss=0.7585  Val Loss=0.7259  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 38 Epoch 11/150  Train Loss=0.7549  Val Loss=0.7320  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 38 Epoch 12/150  Train Loss=0.7584  Val Loss=0.7261  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 38 Epoch 13/150  Train Loss=0.7518  Val Loss=0.7318  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 38 Epoch 14/150  Train Loss=0.7458  Val Loss=0.7214  Val Acc=0.4156  Time=18.8s\n",
            "  Fold 38 Epoch 15/150  Train Loss=0.7377  Val Loss=0.7040  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 38 Epoch 16/150  Train Loss=0.7330  Val Loss=0.6703  Val Acc=0.4594  Time=18.8s\n",
            "  Fold 38 Epoch 17/150  Train Loss=0.7095  Val Loss=0.6255  Val Acc=0.6344  Time=18.8s\n",
            "  Fold 38 Epoch 18/150  Train Loss=0.6773  Val Loss=0.5454  Val Acc=0.8359  Time=18.9s\n",
            "  Fold 38 Epoch 19/150  Train Loss=0.6256  Val Loss=0.4909  Val Acc=0.8812  Time=18.8s\n",
            "  Fold 38 Epoch 20/150  Train Loss=0.5995  Val Loss=0.4699  Val Acc=0.8578  Time=18.9s\n",
            "  Fold 38 Epoch 21/150  Train Loss=0.5761  Val Loss=0.4693  Val Acc=0.8313  Time=18.9s\n",
            "  Fold 38 Epoch 22/150  Train Loss=0.5632  Val Loss=0.4401  Val Acc=0.8641  Time=18.8s\n",
            "  Fold 38 Epoch 23/150  Train Loss=0.5570  Val Loss=0.4338  Val Acc=0.8859  Time=19.0s\n",
            "  Fold 38 Epoch 24/150  Train Loss=0.5450  Val Loss=0.3964  Val Acc=0.8953  Time=19.0s\n",
            "  Fold 38 Epoch 25/150  Train Loss=0.5489  Val Loss=0.3645  Val Acc=0.8906  Time=18.9s\n",
            "  Fold 38 Epoch 26/150  Train Loss=0.5339  Val Loss=0.3939  Val Acc=0.8922  Time=19.0s\n",
            "  Fold 38 Epoch 27/150  Train Loss=0.5199  Val Loss=0.3795  Val Acc=0.9016  Time=18.9s\n",
            "  Fold 38 Epoch 28/150  Train Loss=0.5107  Val Loss=0.3628  Val Acc=0.9016  Time=18.9s\n",
            "  Fold 38 Epoch 29/150  Train Loss=0.5044  Val Loss=0.3490  Val Acc=0.9016  Time=19.0s\n",
            "  Fold 38 Epoch 30/150  Train Loss=0.4964  Val Loss=0.3515  Val Acc=0.9016  Time=18.9s\n",
            "  Fold 38 Epoch 31/150  Train Loss=0.4694  Val Loss=0.3468  Val Acc=0.8984  Time=18.9s\n",
            "  Fold 38 Epoch 32/150  Train Loss=0.4698  Val Loss=0.3597  Val Acc=0.8953  Time=19.0s\n",
            "  Fold 38 Epoch 33/150  Train Loss=0.4455  Val Loss=0.3404  Val Acc=0.8953  Time=19.0s\n",
            "  Fold 38 Epoch 34/150  Train Loss=0.4413  Val Loss=0.3434  Val Acc=0.8984  Time=18.9s\n",
            "  Fold 38 Epoch 35/150  Train Loss=0.4377  Val Loss=0.3417  Val Acc=0.9031  Time=19.0s\n",
            "  Fold 38 Epoch 36/150  Train Loss=0.4329  Val Loss=0.3240  Val Acc=0.8922  Time=19.0s\n",
            "  Fold 38 Epoch 37/150  Train Loss=0.4077  Val Loss=0.3345  Val Acc=0.9031  Time=18.7s\n",
            "  Fold 38 Epoch 38/150  Train Loss=0.4089  Val Loss=0.3238  Val Acc=0.9078  Time=18.9s\n",
            "  Fold 38 Epoch 39/150  Train Loss=0.3971  Val Loss=0.3281  Val Acc=0.9031  Time=18.9s\n",
            "  Fold 38 Epoch 40/150  Train Loss=0.3943  Val Loss=0.3229  Val Acc=0.8984  Time=18.9s\n",
            "  Fold 38 Epoch 41/150  Train Loss=0.3786  Val Loss=0.3144  Val Acc=0.8922  Time=18.9s\n",
            "  Fold 38 Epoch 42/150  Train Loss=0.3860  Val Loss=0.3458  Val Acc=0.8812  Time=18.9s\n",
            "  Fold 38 Epoch 43/150  Train Loss=0.3641  Val Loss=0.3499  Val Acc=0.8828  Time=18.8s\n",
            "  Fold 38 Epoch 44/150  Train Loss=0.3677  Val Loss=0.3261  Val Acc=0.8969  Time=19.0s\n",
            "  Fold 38 Epoch 45/150  Train Loss=0.3548  Val Loss=0.3461  Val Acc=0.8641  Time=18.9s\n",
            "  Fold 38 Epoch 46/150  Train Loss=0.3556  Val Loss=0.3452  Val Acc=0.8797  Time=18.9s\n",
            "  Fold 38 Epoch 47/150  Train Loss=0.3500  Val Loss=0.3279  Val Acc=0.8984  Time=18.8s\n",
            "  Fold 38 Epoch 48/150  Train Loss=0.3431  Val Loss=0.3353  Val Acc=0.8812  Time=18.8s\n",
            "  Fold 38 Epoch 49/150  Train Loss=0.3444  Val Loss=0.3353  Val Acc=0.8859  Time=18.9s\n",
            "  Fold 38 Epoch 50/150  Train Loss=0.3225  Val Loss=0.3613  Val Acc=0.8750  Time=18.9s\n",
            "  Fold 38 Epoch 51/150  Train Loss=0.3337  Val Loss=0.3331  Val Acc=0.8922  Time=18.7s\n",
            "  Fold 38 Epoch 52/150  Train Loss=0.3280  Val Loss=0.3516  Val Acc=0.8859  Time=18.9s\n",
            "  Fold 38 Epoch 53/150  Train Loss=0.3224  Val Loss=0.3453  Val Acc=0.8641  Time=19.0s\n",
            "  Fold 38 Epoch 54/150  Train Loss=0.3158  Val Loss=0.3817  Val Acc=0.8594  Time=19.0s\n",
            "  Fold 38 Epoch 55/150  Train Loss=0.3108  Val Loss=0.3767  Val Acc=0.8594  Time=19.0s\n",
            "  Fold 38 Epoch 56/150  Train Loss=0.3080  Val Loss=0.3765  Val Acc=0.8688  Time=18.9s\n",
            "  → Early stopping at epoch 56 (no val_loss improvement)\n",
            "  Fold 38 Final ACC = 0.7059   (TP=36  TN=0  FP=0  FN=15)\n",
            "\n",
            ">>> Fold 39/65  (leave out sub-039)\n",
            "  Fold 39 Epoch 1/150  Train Loss=0.7295  Val Loss=0.6894  Val Acc=0.5891  Time=19.0s\n",
            "  Fold 39 Epoch 2/150  Train Loss=0.7192  Val Loss=0.6911  Val Acc=0.5734  Time=18.8s\n",
            "  Fold 39 Epoch 3/150  Train Loss=0.7210  Val Loss=0.6917  Val Acc=0.5641  Time=18.9s\n",
            "  Fold 39 Epoch 4/150  Train Loss=0.7296  Val Loss=0.6946  Val Acc=0.4437  Time=19.0s\n",
            "  Fold 39 Epoch 5/150  Train Loss=0.7206  Val Loss=0.6902  Val Acc=0.5906  Time=19.1s\n",
            "  Fold 39 Epoch 6/150  Train Loss=0.7311  Val Loss=0.6871  Val Acc=0.5844  Time=18.9s\n",
            "  Fold 39 Epoch 7/150  Train Loss=0.7236  Val Loss=0.6889  Val Acc=0.5875  Time=19.0s\n",
            "  Fold 39 Epoch 8/150  Train Loss=0.7178  Val Loss=0.6913  Val Acc=0.5703  Time=19.0s\n",
            "  Fold 39 Epoch 9/150  Train Loss=0.7108  Val Loss=0.6907  Val Acc=0.5797  Time=18.9s\n",
            "  Fold 39 Epoch 10/150  Train Loss=0.7194  Val Loss=0.6964  Val Acc=0.4125  Time=18.9s\n",
            "  Fold 39 Epoch 11/150  Train Loss=0.7218  Val Loss=0.6980  Val Acc=0.4141  Time=19.1s\n",
            "  Fold 39 Epoch 12/150  Train Loss=0.7120  Val Loss=0.6998  Val Acc=0.4156  Time=19.1s\n",
            "  Fold 39 Epoch 13/150  Train Loss=0.7204  Val Loss=0.6842  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 39 Epoch 14/150  Train Loss=0.7156  Val Loss=0.6965  Val Acc=0.4297  Time=18.9s\n",
            "  Fold 39 Epoch 15/150  Train Loss=0.7081  Val Loss=0.6981  Val Acc=0.4156  Time=19.0s\n",
            "  Fold 39 Epoch 16/150  Train Loss=0.7127  Val Loss=0.6925  Val Acc=0.5500  Time=19.0s\n",
            "  Fold 39 Epoch 17/150  Train Loss=0.7096  Val Loss=0.6920  Val Acc=0.5750  Time=19.0s\n",
            "  Fold 39 Epoch 18/150  Train Loss=0.7107  Val Loss=0.6906  Val Acc=0.5844  Time=18.8s\n",
            "  Fold 39 Epoch 19/150  Train Loss=0.7116  Val Loss=0.6924  Val Acc=0.5563  Time=18.9s\n",
            "  Fold 39 Epoch 20/150  Train Loss=0.7062  Val Loss=0.6945  Val Acc=0.4641  Time=18.8s\n",
            "  Fold 39 Epoch 21/150  Train Loss=0.7101  Val Loss=0.6972  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 39 Epoch 22/150  Train Loss=0.7010  Val Loss=0.6983  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 39 Epoch 23/150  Train Loss=0.7089  Val Loss=0.6955  Val Acc=0.4125  Time=19.1s\n",
            "  Fold 39 Epoch 24/150  Train Loss=0.7059  Val Loss=0.7023  Val Acc=0.4156  Time=18.9s\n",
            "  Fold 39 Epoch 25/150  Train Loss=0.7036  Val Loss=0.6958  Val Acc=0.4203  Time=19.0s\n",
            "  Fold 39 Epoch 26/150  Train Loss=0.7047  Val Loss=0.6888  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 39 Epoch 27/150  Train Loss=0.7035  Val Loss=0.6535  Val Acc=0.5844  Time=19.0s\n",
            "  Fold 39 Epoch 28/150  Train Loss=0.6783  Val Loss=0.6219  Val Acc=0.6219  Time=18.8s\n",
            "  Fold 39 Epoch 29/150  Train Loss=0.6384  Val Loss=0.5420  Val Acc=0.7500  Time=19.2s\n",
            "  Fold 39 Epoch 30/150  Train Loss=0.5871  Val Loss=0.4432  Val Acc=0.8641  Time=18.9s\n",
            "  Fold 39 Epoch 31/150  Train Loss=0.5538  Val Loss=0.5515  Val Acc=0.6484  Time=19.0s\n",
            "  Fold 39 Epoch 32/150  Train Loss=0.5375  Val Loss=0.4634  Val Acc=0.7906  Time=18.8s\n",
            "  Fold 39 Epoch 33/150  Train Loss=0.5250  Val Loss=0.4700  Val Acc=0.7656  Time=18.9s\n",
            "  Fold 39 Epoch 34/150  Train Loss=0.5064  Val Loss=0.4309  Val Acc=0.8109  Time=18.7s\n",
            "  Fold 39 Epoch 35/150  Train Loss=0.4914  Val Loss=0.5160  Val Acc=0.6781  Time=19.0s\n",
            "  Fold 39 Epoch 36/150  Train Loss=0.4839  Val Loss=0.4300  Val Acc=0.7937  Time=18.9s\n",
            "  Fold 39 Epoch 37/150  Train Loss=0.4780  Val Loss=0.4374  Val Acc=0.7797  Time=19.0s\n",
            "  Fold 39 Epoch 38/150  Train Loss=0.4629  Val Loss=0.4004  Val Acc=0.8125  Time=18.9s\n",
            "  Fold 39 Epoch 39/150  Train Loss=0.4380  Val Loss=0.4315  Val Acc=0.7828  Time=19.1s\n",
            "  Fold 39 Epoch 40/150  Train Loss=0.4281  Val Loss=0.3572  Val Acc=0.8734  Time=18.9s\n",
            "  Fold 39 Epoch 41/150  Train Loss=0.4235  Val Loss=0.4113  Val Acc=0.8047  Time=19.2s\n",
            "  Fold 39 Epoch 42/150  Train Loss=0.4044  Val Loss=0.3778  Val Acc=0.8328  Time=18.9s\n",
            "  Fold 39 Epoch 43/150  Train Loss=0.3999  Val Loss=0.3543  Val Acc=0.8641  Time=19.0s\n",
            "  Fold 39 Epoch 44/150  Train Loss=0.3952  Val Loss=0.4112  Val Acc=0.7859  Time=18.8s\n",
            "  Fold 39 Epoch 45/150  Train Loss=0.3822  Val Loss=0.3819  Val Acc=0.8141  Time=19.0s\n",
            "  Fold 39 Epoch 46/150  Train Loss=0.3809  Val Loss=0.4077  Val Acc=0.7922  Time=18.9s\n",
            "  Fold 39 Epoch 47/150  Train Loss=0.3593  Val Loss=0.3507  Val Acc=0.8594  Time=19.1s\n",
            "  Fold 39 Epoch 48/150  Train Loss=0.3666  Val Loss=0.3408  Val Acc=0.8578  Time=18.8s\n",
            "  Fold 39 Epoch 49/150  Train Loss=0.3529  Val Loss=0.3459  Val Acc=0.8484  Time=19.1s\n",
            "  Fold 39 Epoch 50/150  Train Loss=0.3376  Val Loss=0.3614  Val Acc=0.8516  Time=18.9s\n",
            "  Fold 39 Epoch 51/150  Train Loss=0.3284  Val Loss=0.3661  Val Acc=0.8453  Time=19.0s\n",
            "  Fold 39 Epoch 52/150  Train Loss=0.3452  Val Loss=0.3708  Val Acc=0.8344  Time=18.8s\n",
            "  Fold 39 Epoch 53/150  Train Loss=0.3205  Val Loss=0.3562  Val Acc=0.8453  Time=19.1s\n",
            "  Fold 39 Epoch 54/150  Train Loss=0.3158  Val Loss=0.4679  Val Acc=0.7625  Time=18.9s\n",
            "  Fold 39 Epoch 55/150  Train Loss=0.3148  Val Loss=0.3626  Val Acc=0.8469  Time=19.1s\n",
            "  Fold 39 Epoch 56/150  Train Loss=0.3160  Val Loss=0.3778  Val Acc=0.8281  Time=18.9s\n",
            "  Fold 39 Epoch 57/150  Train Loss=0.3222  Val Loss=0.3359  Val Acc=0.8719  Time=19.1s\n",
            "  Fold 39 Epoch 58/150  Train Loss=0.3061  Val Loss=0.3481  Val Acc=0.8484  Time=18.7s\n",
            "  Fold 39 Epoch 59/150  Train Loss=0.2981  Val Loss=0.4175  Val Acc=0.8125  Time=19.1s\n",
            "  Fold 39 Epoch 60/150  Train Loss=0.2949  Val Loss=0.3522  Val Acc=0.8484  Time=18.9s\n",
            "  Fold 39 Epoch 61/150  Train Loss=0.2922  Val Loss=0.3683  Val Acc=0.8359  Time=19.2s\n",
            "  Fold 39 Epoch 62/150  Train Loss=0.2721  Val Loss=0.4027  Val Acc=0.8234  Time=19.0s\n",
            "  Fold 39 Epoch 63/150  Train Loss=0.2874  Val Loss=0.3635  Val Acc=0.8438  Time=19.2s\n",
            "  Fold 39 Epoch 64/150  Train Loss=0.2677  Val Loss=0.4307  Val Acc=0.8063  Time=18.9s\n",
            "  Fold 39 Epoch 65/150  Train Loss=0.2780  Val Loss=0.3568  Val Acc=0.8594  Time=19.1s\n",
            "  Fold 39 Epoch 66/150  Train Loss=0.2709  Val Loss=0.3937  Val Acc=0.8250  Time=18.8s\n",
            "  Fold 39 Epoch 67/150  Train Loss=0.2541  Val Loss=0.3695  Val Acc=0.8484  Time=19.1s\n",
            "  Fold 39 Epoch 68/150  Train Loss=0.2641  Val Loss=0.3597  Val Acc=0.8484  Time=18.8s\n",
            "  Fold 39 Epoch 69/150  Train Loss=0.2594  Val Loss=0.3834  Val Acc=0.8266  Time=19.1s\n",
            "  Fold 39 Epoch 70/150  Train Loss=0.2434  Val Loss=0.3657  Val Acc=0.8438  Time=18.9s\n",
            "  Fold 39 Epoch 71/150  Train Loss=0.2578  Val Loss=0.4201  Val Acc=0.8094  Time=19.0s\n",
            "  Fold 39 Epoch 72/150  Train Loss=0.2460  Val Loss=0.4030  Val Acc=0.8203  Time=18.9s\n",
            "  → Early stopping at epoch 72 (no val_loss improvement)\n",
            "  Fold 39 Final ACC = 0.8039   (TP=41  TN=0  FP=0  FN=10)\n",
            "\n",
            ">>> Fold 40/65  (leave out sub-040)\n",
            "  Fold 40 Epoch 1/150  Train Loss=0.7387  Val Loss=0.6864  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 2/150  Train Loss=0.7469  Val Loss=0.6875  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 3/150  Train Loss=0.7394  Val Loss=0.6866  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 4/150  Train Loss=0.7383  Val Loss=0.6878  Val Acc=0.5789  Time=18.7s\n",
            "  Fold 40 Epoch 5/150  Train Loss=0.7405  Val Loss=0.6859  Val Acc=0.5789  Time=19.1s\n",
            "  Fold 40 Epoch 6/150  Train Loss=0.7284  Val Loss=0.6861  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 7/150  Train Loss=0.7392  Val Loss=0.6870  Val Acc=0.5789  Time=19.1s\n",
            "  Fold 40 Epoch 8/150  Train Loss=0.7317  Val Loss=0.6861  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 40 Epoch 9/150  Train Loss=0.7187  Val Loss=0.6865  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 40 Epoch 10/150  Train Loss=0.7199  Val Loss=0.6861  Val Acc=0.5789  Time=18.8s\n",
            "  Fold 40 Epoch 11/150  Train Loss=0.7220  Val Loss=0.6862  Val Acc=0.5789  Time=19.1s\n",
            "  Fold 40 Epoch 12/150  Train Loss=0.7242  Val Loss=0.6902  Val Acc=0.5774  Time=18.9s\n",
            "  Fold 40 Epoch 13/150  Train Loss=0.7245  Val Loss=0.6913  Val Acc=0.5681  Time=18.9s\n",
            "  Fold 40 Epoch 14/150  Train Loss=0.7235  Val Loss=0.6942  Val Acc=0.4180  Time=18.9s\n",
            "  Fold 40 Epoch 15/150  Train Loss=0.7237  Val Loss=0.6884  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 40 Epoch 16/150  Train Loss=0.7259  Val Loss=0.6851  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 17/150  Train Loss=0.7240  Val Loss=0.6854  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 40 Epoch 18/150  Train Loss=0.7115  Val Loss=0.6858  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 19/150  Train Loss=0.7168  Val Loss=0.6821  Val Acc=0.5789  Time=18.9s\n",
            "  Fold 40 Epoch 20/150  Train Loss=0.7189  Val Loss=0.6809  Val Acc=0.5789  Time=19.0s\n",
            "  Fold 40 Epoch 21/150  Train Loss=0.7093  Val Loss=0.6799  Val Acc=0.6192  Time=19.0s\n",
            "  Fold 40 Epoch 22/150  Train Loss=0.7082  Val Loss=0.6713  Val Acc=0.6455  Time=18.8s\n",
            "  Fold 40 Epoch 23/150  Train Loss=0.7110  Val Loss=0.6624  Val Acc=0.8344  Time=19.0s\n",
            "  Fold 40 Epoch 24/150  Train Loss=0.6969  Val Loss=0.6356  Val Acc=0.8313  Time=18.9s\n",
            "  Fold 40 Epoch 25/150  Train Loss=0.6763  Val Loss=0.5457  Val Acc=0.8576  Time=18.9s\n",
            "  Fold 40 Epoch 26/150  Train Loss=0.6322  Val Loss=0.4680  Val Acc=0.8963  Time=18.9s\n",
            "  Fold 40 Epoch 27/150  Train Loss=0.5989  Val Loss=0.4431  Val Acc=0.9087  Time=19.0s\n",
            "  Fold 40 Epoch 28/150  Train Loss=0.5561  Val Loss=0.3573  Val Acc=0.9133  Time=18.9s\n",
            "  Fold 40 Epoch 29/150  Train Loss=0.5782  Val Loss=0.3702  Val Acc=0.9164  Time=19.1s\n",
            "  Fold 40 Epoch 30/150  Train Loss=0.5187  Val Loss=0.3316  Val Acc=0.9257  Time=18.9s\n",
            "  Fold 40 Epoch 31/150  Train Loss=0.5067  Val Loss=0.3327  Val Acc=0.9272  Time=18.8s\n",
            "  Fold 40 Epoch 32/150  Train Loss=0.5049  Val Loss=0.3564  Val Acc=0.8622  Time=18.9s\n",
            "  Fold 40 Epoch 33/150  Train Loss=0.4840  Val Loss=0.3625  Val Acc=0.8638  Time=18.9s\n",
            "  Fold 40 Epoch 34/150  Train Loss=0.4707  Val Loss=0.3999  Val Acc=0.8669  Time=18.9s\n",
            "  Fold 40 Epoch 35/150  Train Loss=0.4605  Val Loss=0.4109  Val Acc=0.7848  Time=18.9s\n",
            "  Fold 40 Epoch 36/150  Train Loss=0.4459  Val Loss=0.3795  Val Acc=0.8514  Time=18.9s\n",
            "  Fold 40 Epoch 37/150  Train Loss=0.4305  Val Loss=0.3282  Val Acc=0.8746  Time=18.9s\n",
            "  Fold 40 Epoch 38/150  Train Loss=0.4230  Val Loss=0.3006  Val Acc=0.9164  Time=18.8s\n",
            "  Fold 40 Epoch 39/150  Train Loss=0.4062  Val Loss=0.3459  Val Acc=0.8885  Time=18.8s\n",
            "  Fold 40 Epoch 40/150  Train Loss=0.3977  Val Loss=0.3275  Val Acc=0.8746  Time=18.8s\n",
            "  Fold 40 Epoch 41/150  Train Loss=0.4021  Val Loss=0.3521  Val Acc=0.8700  Time=18.9s\n",
            "  Fold 40 Epoch 42/150  Train Loss=0.3936  Val Loss=0.3623  Val Acc=0.8266  Time=19.0s\n",
            "  Fold 40 Epoch 43/150  Train Loss=0.3597  Val Loss=0.3709  Val Acc=0.8220  Time=18.9s\n",
            "  Fold 40 Epoch 44/150  Train Loss=0.3673  Val Loss=0.4018  Val Acc=0.8034  Time=18.8s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CB8aDhlnw4QS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}