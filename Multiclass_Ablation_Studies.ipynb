{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyOpu1OYsniGV4xDPpgLUAdn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Hwang/EEG-AD-FTD-Detection/blob/main/Multiclass_Ablation_Studies.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4q0wmSyBzYrM",
        "outputId": "c4500d42-3cc8-4850-8f46-3af4fee13a31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그중에 2025 Lab Research 폴더 안을 확인\n",
        "!ls \"/content/drive/MyDrive/2025_Lab_Research\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6vyhIi6zxjY",
        "outputId": "5338e267-e882-491e-9b98-469ff3e8271a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'AD vs FTD vs CN Binary Classification'   eeg_holdout-4.db\n",
            "'Colab Files'\t\t\t\t  eeg_holdout-5.db\n",
            "'Data Preparation.gdoc'\t\t\t  eeg_holdout-6.db\n",
            " eeg_dataset.py\t\t\t\t  eeg_holdout-7.db\n",
            " EEGformer_model_training.ipynb\t\t  eeg_holdout-8.db\n",
            " eegformer_optuna_cv_3.db\t\t  eeg_holdout-9.db\n",
            " eegformer_optuna_cv_4.db\t\t  eeg_holdout.db\n",
            " eegformer_optuna_cv_5.db\t\t  eeg_holdout_fixed_1.db\n",
            " eeg_grid_search-10.db\t\t\t  eeg_optuna_trial_1.db\n",
            " eeg_grid_search-11.db\t\t\t  eeg_optuna_trial_2.db\n",
            " eeg_grid_search-12.db\t\t\t  eeg_optuna_trial_3.db\n",
            " eeg_grid_search-13.db\t\t\t 'EEG Transformer Architecture.gdoc'\n",
            " eeg_grid_search-14.db\t\t\t 'Lab Info'\n",
            " eeg_grid_search-15.db\t\t\t 'Lab Research Paper Review'\n",
            " eeg_grid_search-16.db\t\t\t 'Meeting Note.gdoc'\n",
            " eeg_grid_search-17.db\t\t\t  model-data\n",
            " eeg_grid_search-18.db\t\t\t  model-data.zip\n",
            " eeg_grid_search-2.db\t\t\t  model_optimized_2.py\n",
            " eeg_grid_search-3.db\t\t\t  model_optimized_3.py\n",
            " eeg_grid_search-4.db\t\t\t  model_optimized_4.py\n",
            " eeg_grid_search-5.db\t\t\t  model_optimized_5.py\n",
            " eeg_grid_search-6.db\t\t\t  model_optimized.py\n",
            " eeg_grid_search-7.db\t\t\t  models_depracated.py\n",
            " eeg_grid_search-8.db\t\t\t  models.py\n",
            " eeg_grid_search-9.db\t\t\t  models_with_dropout_1.py\n",
            " eeg_grid_search.db\t\t\t  models_with_minimal_dropout.py\n",
            " eeg_holdout-1.db\t\t\t  Practice_Note0.ipynb\n",
            " eeg_holdout-2.db\t\t\t  __pycache__\n",
            " eeg_holdout-3.db\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/2025_Lab_Research')"
      ],
      "metadata": {
        "id": "bNkiz54Az227"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on\", DEVICE)  # → “cuda” 가 뜨면 GPU 정상"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xhtrhQ6rz5Ny",
        "outputId": "29ed5126-e405-4be6-da1a-13b343f277e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()  # 첫 실행 시 API 키 입력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hEdGTdBhz7ZY",
        "outputId": "1755dcfb-933f-4661-f545-3038aacd54d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjh8032\u001b[0m (\u001b[33mjh8032-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install wandb\n",
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZJH8hgMRz-Gn",
        "outputId": "82e47912-b66b-4e92-fa97-b1de3a8c7d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search the best hyperparameter using Hold-out set\n",
        "- Use Block = 1\n",
        "- Use Head = [2,3]\n",
        "- Overfitting occurred"
      ],
      "metadata": {
        "id": "L0MSSD_E0m82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4, 1e-3]\n",
        "WD_CHOICES          = [1e-3, 5e-4, 1e-4]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-2\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-2\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-2.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "VGmtO56p2K5z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "38a695c1-e659-4995-cf09-9adfaed76a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:30:35,554] A new study created in RDB with name: eeg_holdout_grid_search-2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_033048-hab1e09a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/hab1e09a' target=\"_blank\">golden-shape-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/hab1e09a' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/hab1e09a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=1.00e-03, wd=1.00e-03, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0711 acc=0.4210 | val_loss=1.0745 acc=0.4317 | time=246.8s\n",
            "Epoch 002 | train_loss=1.0677 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=16.8s\n",
            "Epoch 003 | train_loss=1.0686 acc=0.4311 | val_loss=1.0774 acc=0.4317 | time=17.0s\n",
            "Epoch 004 | train_loss=1.0699 acc=0.4311 | val_loss=1.0864 acc=0.4317 | time=16.9s\n",
            "Epoch 005 | train_loss=1.0675 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.8s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=16.9s\n",
            "Epoch 007 | train_loss=1.0675 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=16.9s\n",
            "Epoch 008 | train_loss=1.0673 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=16.9s\n",
            "Epoch 009 | train_loss=1.0673 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=16.7s\n",
            "Epoch 010 | train_loss=1.0654 acc=0.4311 | val_loss=1.0720 acc=0.4317 | time=17.0s\n",
            "Epoch 011 | train_loss=1.0257 acc=0.5045 | val_loss=1.0469 acc=0.4984 | time=16.7s\n",
            "Epoch 012 | train_loss=0.9431 acc=0.5825 | val_loss=0.9765 acc=0.5652 | time=17.0s\n",
            "Epoch 013 | train_loss=0.9052 acc=0.6031 | val_loss=0.9346 acc=0.5916 | time=17.0s\n",
            "Epoch 014 | train_loss=0.8766 acc=0.6229 | val_loss=0.9194 acc=0.5978 | time=16.9s\n",
            "Epoch 015 | train_loss=0.8465 acc=0.6350 | val_loss=0.9255 acc=0.6242 | time=16.9s\n",
            "Epoch 016 | train_loss=0.8350 acc=0.6384 | val_loss=0.8940 acc=0.6040 | time=16.9s\n",
            "Epoch 017 | train_loss=0.8047 acc=0.6505 | val_loss=0.8786 acc=0.6273 | time=17.1s\n",
            "Epoch 018 | train_loss=0.7783 acc=0.6617 | val_loss=0.8639 acc=0.6320 | time=16.7s\n",
            "Epoch 019 | train_loss=0.7553 acc=0.6831 | val_loss=0.8541 acc=0.6242 | time=17.0s\n",
            "Epoch 020 | train_loss=0.7316 acc=0.6854 | val_loss=0.7876 acc=0.6630 | time=16.8s\n",
            "Epoch 021 | train_loss=0.6882 acc=0.7173 | val_loss=0.8007 acc=0.6630 | time=16.8s\n",
            "Epoch 022 | train_loss=0.6499 acc=0.7367 | val_loss=0.8096 acc=0.6522 | time=16.8s\n",
            "Epoch 023 | train_loss=0.6038 acc=0.7526 | val_loss=0.9048 acc=0.5668 | time=17.0s\n",
            "Epoch 024 | train_loss=0.6211 acc=0.7437 | val_loss=0.8926 acc=0.6661 | time=16.7s\n",
            "Epoch 025 | train_loss=0.5404 acc=0.7786 | val_loss=0.8668 acc=0.6568 | time=16.9s\n",
            "Epoch 026 | train_loss=0.5159 acc=0.7895 | val_loss=0.8155 acc=0.6630 | time=17.1s\n",
            "Epoch 027 | train_loss=0.4185 acc=0.8318 | val_loss=0.9753 acc=0.6848 | time=17.1s\n",
            "Epoch 028 | train_loss=0.3691 acc=0.8497 | val_loss=0.8653 acc=0.6708 | time=17.0s\n",
            "Epoch 029 | train_loss=0.3478 acc=0.8687 | val_loss=1.2742 acc=0.6708 | time=17.0s\n",
            "Epoch 030 | train_loss=0.3195 acc=0.8750 | val_loss=1.1703 acc=0.6786 | time=17.0s\n",
            "★ Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>███████████▇▆▆▆▆▆▅▅▅▄▄▄▄▃▃▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▅▅▆▆▆▆▇▆▇▇▇▅▇▇▇████</td></tr><tr><td>validation_loss</td><td>▅▅▅▅▅▅▅▅▅▅▅▄▃▃▃▃▂▂▂▁▁▁▃▃▂▁▄▂█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.87495</td></tr><tr><td>train_loss</td><td>0.31952</td></tr><tr><td>validation_accuracy</td><td>0.67857</td></tr><tr><td>validation_loss</td><td>1.17028</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">golden-shape-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/hab1e09a' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/hab1e09a</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_033048-hab1e09a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:43:12,928] Trial 0 finished with value: 0.7875745651267824 and parameters: {'lr': 0.001, 'weight_decay': 0.001, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 0 with value: 0.7875745651267824.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_034313-vtalws6w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/vtalws6w' target=\"_blank\">young-glade-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/vtalws6w' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-2/runs/vtalws6w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=1.00e-03, wd=1.00e-04, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0779 acc=0.4062 | val_loss=1.0814 acc=0.4317 | time=18.8s\n",
            "Epoch 002 | train_loss=1.0707 acc=0.4311 | val_loss=1.0787 acc=0.4317 | time=18.4s\n",
            "Epoch 003 | train_loss=1.0680 acc=0.4311 | val_loss=1.0807 acc=0.4317 | time=18.6s\n",
            "Epoch 004 | train_loss=1.0707 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.5s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=18.5s\n",
            "Epoch 006 | train_loss=1.0689 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=18.7s\n",
            "Epoch 007 | train_loss=1.0691 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=18.5s\n",
            "Epoch 008 | train_loss=1.0678 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.6s\n",
            "Epoch 009 | train_loss=1.0676 acc=0.4311 | val_loss=1.0741 acc=0.4317 | time=18.4s\n",
            "Epoch 010 | train_loss=1.0614 acc=0.4447 | val_loss=1.0183 acc=0.5606 | time=18.7s\n",
            "Epoch 011 | train_loss=0.9945 acc=0.5569 | val_loss=0.9825 acc=0.5637 | time=18.5s\n",
            "Epoch 012 | train_loss=0.9538 acc=0.5794 | val_loss=0.9594 acc=0.5792 | time=18.7s\n",
            "Epoch 013 | train_loss=0.9228 acc=0.5946 | val_loss=0.9317 acc=0.5776 | time=18.5s\n",
            "Epoch 014 | train_loss=0.8941 acc=0.6031 | val_loss=0.9762 acc=0.5543 | time=18.6s\n",
            "Epoch 015 | train_loss=0.8811 acc=0.6113 | val_loss=0.9315 acc=0.5932 | time=18.6s\n",
            "Epoch 016 | train_loss=0.8359 acc=0.6280 | val_loss=0.9660 acc=0.6087 | time=18.7s\n",
            "Epoch 017 | train_loss=0.8347 acc=0.6307 | val_loss=0.8804 acc=0.6102 | time=18.7s\n",
            "Epoch 018 | train_loss=0.7836 acc=0.6544 | val_loss=0.8981 acc=0.6242 | time=18.8s\n",
            "Epoch 019 | train_loss=0.7703 acc=0.6734 | val_loss=0.8882 acc=0.6273 | time=18.8s\n",
            "Epoch 020 | train_loss=0.7307 acc=0.6882 | val_loss=0.8838 acc=0.6382 | time=18.6s\n",
            "Epoch 021 | train_loss=0.7103 acc=0.7056 | val_loss=0.8270 acc=0.6398 | time=18.7s\n",
            "Epoch 022 | train_loss=0.7002 acc=0.6986 | val_loss=0.8625 acc=0.6615 | time=18.6s\n",
            "Epoch 023 | train_loss=0.6519 acc=0.7332 | val_loss=0.8004 acc=0.6724 | time=18.8s\n",
            "Epoch 024 | train_loss=0.6080 acc=0.7507 | val_loss=0.8559 acc=0.6460 | time=18.7s\n",
            "Epoch 025 | train_loss=0.6173 acc=0.7452 | val_loss=0.7610 acc=0.6661 | time=18.7s\n",
            "Epoch 026 | train_loss=0.5570 acc=0.7744 | val_loss=0.9209 acc=0.6599 | time=18.8s\n",
            "Epoch 027 | train_loss=0.5283 acc=0.7911 | val_loss=0.7881 acc=0.6366 | time=18.7s\n",
            "Epoch 028 | train_loss=0.5414 acc=0.7845 | val_loss=0.7637 acc=0.6755 | time=18.8s\n",
            "Epoch 029 | train_loss=0.4827 acc=0.8171 | val_loss=0.7850 acc=0.6661 | time=18.8s\n",
            "Epoch 030 | train_loss=0.4634 acc=0.8159 | val_loss=0.7495 acc=0.6460 | time=18.8s\n",
            "Epoch 031 | train_loss=0.4771 acc=0.8093 | val_loss=0.7743 acc=0.6739 | time=18.7s\n",
            "Epoch 032 | train_loss=0.4402 acc=0.8291 | val_loss=0.8906 acc=0.6770 | time=18.9s\n",
            "Epoch 033 | train_loss=0.4163 acc=0.8396 | val_loss=0.8062 acc=0.6661 | time=18.6s\n",
            "Epoch 034 | train_loss=0.4057 acc=0.8377 | val_loss=0.9112 acc=0.6817 | time=18.9s\n",
            "Epoch 035 | train_loss=0.3761 acc=0.8513 | val_loss=0.9601 acc=0.6848 | time=18.6s\n",
            "Epoch 036 | train_loss=0.3465 acc=0.8718 | val_loss=0.9615 acc=0.6304 | time=18.9s\n",
            "Epoch 037 | train_loss=0.3328 acc=0.8722 | val_loss=0.9835 acc=0.6646 | time=18.7s\n",
            "Epoch 038 | train_loss=0.2889 acc=0.8897 | val_loss=1.0933 acc=0.6630 | time=18.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-05-02 03:55:14,332] Trial 1 failed with parameters: {'lr': 0.001, 'weight_decay': 0.0001, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-712a74a241c1>\", line 137, in objective_holdout\n",
            "    loss.backward()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n",
            "    _engine_run_backward(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
            "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-02 03:55:14,335] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-712a74a241c1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_holdout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# grid 크기만큼 자동 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# ─── 결과 출력 ─────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-712a74a241c1>\u001b[0m in \u001b[0;36mobjective_holdout\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    624\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    625\u001b[0m             )\n\u001b[0;32m--> 626\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    627\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    628\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 823\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    824\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    825\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revised the EEGformer model\n",
        "- Additional Dropout in CNNDecoder\n",
        "- Dropout Rate = 0.3"
      ],
      "metadata": {
        "id": "ncpnCmzfKv9h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4, 1e-3]\n",
        "WD_CHOICES          = [1e-3, 5e-4, 1e-4]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-3\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-3\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-3.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "vfkWt39P0O3r",
        "outputId": "cf340e6b-488b-48aa-fa2f-2f52c9c076e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 04:07:56,477] A new study created in RDB with name: eeg_holdout_grid_search-3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_040757-0ofgp2rr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/0ofgp2rr' target=\"_blank\">giddy-lake-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/0ofgp2rr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/0ofgp2rr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=1.00e-03, wd=1.00e-03, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0777 acc=0.4276 | val_loss=1.0760 acc=0.4317 | time=16.8s\n",
            "Epoch 002 | train_loss=1.0727 acc=0.4252 | val_loss=1.0768 acc=0.4317 | time=16.8s\n",
            "Epoch 003 | train_loss=1.0718 acc=0.4299 | val_loss=1.0752 acc=0.4317 | time=16.8s\n",
            "Epoch 004 | train_loss=1.0675 acc=0.4283 | val_loss=1.0759 acc=0.4317 | time=16.9s\n",
            "Epoch 005 | train_loss=1.0698 acc=0.4136 | val_loss=1.0764 acc=0.4317 | time=16.8s\n",
            "Epoch 006 | train_loss=1.0680 acc=0.4307 | val_loss=1.0738 acc=0.4317 | time=17.1s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4283 | val_loss=1.0761 acc=0.4317 | time=16.7s\n",
            "Epoch 008 | train_loss=1.0689 acc=0.4295 | val_loss=1.0740 acc=0.4317 | time=16.7s\n",
            "Epoch 009 | train_loss=1.0636 acc=0.4217 | val_loss=1.0790 acc=0.4317 | time=17.1s\n",
            "Epoch 010 | train_loss=1.0056 acc=0.5379 | val_loss=0.9730 acc=0.5543 | time=16.8s\n",
            "Epoch 011 | train_loss=0.9493 acc=0.5814 | val_loss=0.9982 acc=0.5326 | time=16.8s\n",
            "Epoch 012 | train_loss=0.9224 acc=0.5946 | val_loss=0.9493 acc=0.5466 | time=16.9s\n",
            "Epoch 013 | train_loss=0.8994 acc=0.6132 | val_loss=0.9259 acc=0.5668 | time=17.0s\n",
            "Epoch 014 | train_loss=0.8566 acc=0.6272 | val_loss=0.9058 acc=0.5994 | time=16.7s\n",
            "Epoch 015 | train_loss=0.8046 acc=0.6388 | val_loss=0.9061 acc=0.6102 | time=16.9s\n",
            "Epoch 016 | train_loss=0.7865 acc=0.6594 | val_loss=0.9220 acc=0.5901 | time=16.8s\n",
            "Epoch 017 | train_loss=0.7762 acc=0.6606 | val_loss=0.9312 acc=0.5947 | time=17.0s\n",
            "Epoch 018 | train_loss=0.7432 acc=0.6800 | val_loss=0.9831 acc=0.6149 | time=17.2s\n",
            "Epoch 019 | train_loss=0.7322 acc=0.6917 | val_loss=0.9604 acc=0.6180 | time=16.9s\n",
            "Epoch 020 | train_loss=0.7191 acc=0.6983 | val_loss=1.0555 acc=0.6134 | time=16.9s\n",
            "Epoch 021 | train_loss=0.6848 acc=0.7107 | val_loss=0.9150 acc=0.6071 | time=16.8s\n",
            "Epoch 022 | train_loss=0.6520 acc=0.7297 | val_loss=0.9925 acc=0.6429 | time=17.3s\n",
            "Epoch 023 | train_loss=0.6421 acc=0.7390 | val_loss=1.0082 acc=0.6242 | time=17.1s\n",
            "Epoch 024 | train_loss=0.6284 acc=0.7511 | val_loss=1.0272 acc=0.6102 | time=17.0s\n",
            "★ Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▄▄▅▅▅▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>█████████▇▆▆▅▅▄▃▃▃▃▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▅▄▅▅▇▇▆▆▇▇▇▇█▇▇</td></tr><tr><td>validation_loss</td><td>█████████▄▅▃▂▁▁▂▂▄▃▇▁▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>24</td></tr><tr><td>train_accuracy</td><td>0.75107</td></tr><tr><td>train_loss</td><td>0.62836</td></tr><tr><td>validation_accuracy</td><td>0.61025</td></tr><tr><td>validation_loss</td><td>1.02721</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">giddy-lake-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/0ofgp2rr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/0ofgp2rr</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_040757-0ofgp2rr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 04:14:46,615] Trial 0 finished with value: 0.9058446770622617 and parameters: {'lr': 0.001, 'weight_decay': 0.001, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 0 with value: 0.9058446770622617.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_041446-xpqn4wou</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/xpqn4wou' target=\"_blank\">devout-morning-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/xpqn4wou' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/xpqn4wou</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=1.00e-03, wd=1.00e-04, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0718 acc=0.4299 | val_loss=1.0743 acc=0.4317 | time=18.6s\n",
            "Epoch 002 | train_loss=1.0691 acc=0.4330 | val_loss=1.0817 acc=0.4317 | time=18.4s\n",
            "Epoch 003 | train_loss=1.0666 acc=0.4307 | val_loss=1.0743 acc=0.4317 | time=18.5s\n",
            "Epoch 004 | train_loss=1.0663 acc=0.4311 | val_loss=1.0765 acc=0.4317 | time=18.5s\n",
            "Epoch 005 | train_loss=1.0686 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.5s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=18.4s\n",
            "Epoch 007 | train_loss=1.0684 acc=0.4311 | val_loss=1.0768 acc=0.4317 | time=18.6s\n",
            "Epoch 008 | train_loss=1.0673 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.5s\n",
            "Epoch 009 | train_loss=1.0661 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.6s\n",
            "Epoch 010 | train_loss=1.0662 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=18.5s\n",
            "Epoch 011 | train_loss=1.0689 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=18.6s\n",
            "★ Early stopping at epoch 11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▄▅▅▆▇▇█</td></tr><tr><td>train_accuracy</td><td>▁█▃▄▄▄▄▄▄▄▄</td></tr><tr><td>train_loss</td><td>█▅▂▁▄▂▄▂▁▁▄</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁█▁▃▁▂▃▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>11</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06887</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0743</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-morning-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/xpqn4wou' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/xpqn4wou</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_041446-xpqn4wou/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 04:18:12,383] Trial 1 finished with value: 1.074283242225647 and parameters: {'lr': 0.001, 'weight_decay': 0.0001, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 0.9058446770622617.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_041812-72kqmix4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/72kqmix4' target=\"_blank\">exalted-dawn-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/72kqmix4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/72kqmix4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0721 acc=0.4202 | val_loss=1.0784 acc=0.4317 | time=18.5s\n",
            "Epoch 002 | train_loss=1.0684 acc=0.4237 | val_loss=1.0759 acc=0.4317 | time=18.4s\n",
            "Epoch 003 | train_loss=1.0679 acc=0.4307 | val_loss=1.0744 acc=0.4317 | time=18.5s\n",
            "Epoch 004 | train_loss=1.0677 acc=0.4303 | val_loss=1.0745 acc=0.4317 | time=18.5s\n",
            "Epoch 005 | train_loss=1.0698 acc=0.4307 | val_loss=1.0742 acc=0.4317 | time=18.4s\n",
            "Epoch 006 | train_loss=1.0670 acc=0.4315 | val_loss=1.0744 acc=0.4317 | time=18.5s\n",
            "Epoch 007 | train_loss=1.0683 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=18.6s\n",
            "Epoch 008 | train_loss=1.0684 acc=0.4311 | val_loss=1.0733 acc=0.4317 | time=18.4s\n",
            "Epoch 009 | train_loss=1.0643 acc=0.4326 | val_loss=1.0717 acc=0.4317 | time=18.7s\n",
            "Epoch 010 | train_loss=1.0662 acc=0.4303 | val_loss=1.0735 acc=0.4317 | time=18.5s\n",
            "Epoch 011 | train_loss=1.0234 acc=0.4835 | val_loss=1.0725 acc=0.5512 | time=18.6s\n",
            "Epoch 012 | train_loss=0.9654 acc=0.5732 | val_loss=0.9872 acc=0.5233 | time=18.8s\n",
            "Epoch 013 | train_loss=0.9548 acc=0.5841 | val_loss=0.9675 acc=0.5730 | time=18.6s\n",
            "Epoch 014 | train_loss=0.9180 acc=0.5977 | val_loss=0.9395 acc=0.5699 | time=18.7s\n",
            "Epoch 015 | train_loss=0.8961 acc=0.6132 | val_loss=0.9499 acc=0.5854 | time=18.7s\n",
            "Epoch 016 | train_loss=0.8641 acc=0.6268 | val_loss=0.9299 acc=0.5978 | time=18.6s\n",
            "Epoch 017 | train_loss=0.8453 acc=0.6361 | val_loss=0.9354 acc=0.5963 | time=18.5s\n",
            "Epoch 018 | train_loss=0.8410 acc=0.6295 | val_loss=0.9495 acc=0.6025 | time=18.9s\n",
            "Epoch 019 | train_loss=0.8230 acc=0.6431 | val_loss=0.9300 acc=0.5901 | time=18.5s\n",
            "Epoch 020 | train_loss=0.8013 acc=0.6548 | val_loss=0.9387 acc=0.5994 | time=18.8s\n",
            "Epoch 021 | train_loss=0.7780 acc=0.6645 | val_loss=0.9661 acc=0.6040 | time=18.5s\n",
            "Epoch 022 | train_loss=0.7530 acc=0.6722 | val_loss=1.0006 acc=0.5854 | time=18.9s\n",
            "Epoch 023 | train_loss=0.7113 acc=0.6854 | val_loss=1.0467 acc=0.5730 | time=18.8s\n",
            "Epoch 024 | train_loss=0.6919 acc=0.6932 | val_loss=0.9851 acc=0.5730 | time=18.7s\n",
            "Epoch 025 | train_loss=0.6672 acc=0.7064 | val_loss=1.0483 acc=0.5854 | time=18.7s\n",
            "Epoch 026 | train_loss=0.6408 acc=0.7150 | val_loss=1.0325 acc=0.5637 | time=18.8s\n",
            "★ Early stopping at epoch 26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▅▅▅▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_loss</td><td>██████████▇▆▆▅▅▅▄▄▄▄▃▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▅▇▇▇███▇██▇▇▇▇▆</td></tr><tr><td>validation_loss</td><td>███████████▄▃▁▂▁▁▂▁▁▃▄▇▄▇▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_accuracy</td><td>0.71495</td></tr><tr><td>train_loss</td><td>0.6408</td></tr><tr><td>validation_accuracy</td><td>0.56366</td></tr><tr><td>validation_loss</td><td>1.03248</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">exalted-dawn-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/72kqmix4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/72kqmix4</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_041812-72kqmix4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 04:26:18,665] Trial 2 finished with value: 0.9298576144945054 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 0.9058446770622617.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_042618-5e57psev</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/5e57psev' target=\"_blank\">legendary-sea-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/5e57psev' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/5e57psev</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=1.00e-03, wd=5.00e-04, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0763 acc=0.4089 | val_loss=1.0811 acc=0.4317 | time=18.5s\n",
            "Epoch 002 | train_loss=1.0717 acc=0.4237 | val_loss=1.0799 acc=0.4317 | time=18.4s\n",
            "Epoch 003 | train_loss=1.0723 acc=0.4303 | val_loss=1.0741 acc=0.4317 | time=18.6s\n",
            "Epoch 004 | train_loss=1.0614 acc=0.4501 | val_loss=1.0737 acc=0.4317 | time=18.5s\n",
            "Epoch 005 | train_loss=0.9815 acc=0.5635 | val_loss=0.9694 acc=0.5699 | time=18.8s\n",
            "Epoch 006 | train_loss=0.9453 acc=0.5864 | val_loss=0.9613 acc=0.5481 | time=18.7s\n",
            "Epoch 007 | train_loss=0.9344 acc=0.5880 | val_loss=0.9573 acc=0.5745 | time=19.2s\n",
            "Epoch 008 | train_loss=0.9264 acc=0.5876 | val_loss=0.9573 acc=0.5823 | time=18.8s\n",
            "Epoch 009 | train_loss=0.9008 acc=0.6101 | val_loss=0.9391 acc=0.5683 | time=18.9s\n",
            "Epoch 010 | train_loss=0.9062 acc=0.6016 | val_loss=0.9219 acc=0.5839 | time=18.8s\n",
            "Epoch 011 | train_loss=0.8640 acc=0.6245 | val_loss=0.9408 acc=0.6040 | time=18.9s\n",
            "Epoch 012 | train_loss=0.8514 acc=0.6276 | val_loss=0.9358 acc=0.5854 | time=18.7s\n",
            "Epoch 013 | train_loss=0.8406 acc=0.6342 | val_loss=0.9702 acc=0.5839 | time=18.7s\n",
            "Epoch 014 | train_loss=0.8139 acc=0.6435 | val_loss=0.9677 acc=0.5699 | time=18.9s\n",
            "Epoch 015 | train_loss=0.7907 acc=0.6575 | val_loss=0.9259 acc=0.5699 | time=18.7s\n",
            "Epoch 016 | train_loss=0.8102 acc=0.6427 | val_loss=0.9875 acc=0.5901 | time=19.0s\n",
            "Epoch 017 | train_loss=0.7522 acc=0.6695 | val_loss=0.9877 acc=0.5947 | time=18.9s\n",
            "Epoch 018 | train_loss=0.7042 acc=0.6917 | val_loss=1.0633 acc=0.5699 | time=19.0s\n",
            "Epoch 019 | train_loss=0.6856 acc=0.6893 | val_loss=0.9775 acc=0.5994 | time=18.9s\n",
            "Epoch 020 | train_loss=0.6682 acc=0.7002 | val_loss=1.1259 acc=0.5792 | time=19.0s\n",
            "★ Early stopping at epoch 20\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_loss</td><td>████▆▆▆▅▅▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▇▆▇▇▇▇█▇▇▇▇▇█▇█▇</td></tr><tr><td>validation_loss</td><td>▆▆▆▆▃▂▂▂▂▁▂▁▃▃▁▃▃▆▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>20</td></tr><tr><td>train_accuracy</td><td>0.70019</td></tr><tr><td>train_loss</td><td>0.6682</td></tr><tr><td>validation_accuracy</td><td>0.57919</td></tr><tr><td>validation_loss</td><td>1.12586</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">legendary-sea-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/5e57psev' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/5e57psev</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_042618-5e57psev/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 04:32:36,865] Trial 3 finished with value: 0.9219413456462678 and parameters: {'lr': 0.001, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 0.9058446770622617.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_043237-t1d5m54n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/t1d5m54n' target=\"_blank\">chocolate-frog-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/t1d5m54n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/t1d5m54n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=5.00e-04, wd=1.00e-04, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0698 acc=0.4303 | val_loss=1.0751 acc=0.4317 | time=17.1s\n",
            "Epoch 002 | train_loss=1.0701 acc=0.4307 | val_loss=1.0748 acc=0.4317 | time=17.4s\n",
            "Epoch 003 | train_loss=1.0715 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=17.2s\n",
            "Epoch 004 | train_loss=1.0709 acc=0.4318 | val_loss=1.0752 acc=0.4317 | time=17.3s\n",
            "Epoch 005 | train_loss=1.0664 acc=0.4311 | val_loss=1.0785 acc=0.4317 | time=17.3s\n",
            "Epoch 006 | train_loss=1.0692 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.1s\n",
            "Epoch 007 | train_loss=1.0686 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.2s\n",
            "Epoch 008 | train_loss=1.0689 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.9s\n",
            "Epoch 009 | train_loss=1.0677 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=17.2s\n",
            "Epoch 010 | train_loss=1.0680 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=17.0s\n",
            "Epoch 011 | train_loss=1.0669 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=17.3s\n",
            "Epoch 012 | train_loss=1.0646 acc=0.4334 | val_loss=1.0439 acc=0.4317 | time=17.6s\n",
            "Epoch 013 | train_loss=0.9827 acc=0.5658 | val_loss=0.9730 acc=0.5714 | time=17.1s\n",
            "Epoch 014 | train_loss=0.9350 acc=0.5864 | val_loss=0.9697 acc=0.5528 | time=17.2s\n",
            "Epoch 015 | train_loss=0.8961 acc=0.6062 | val_loss=0.9079 acc=0.5901 | time=16.7s\n",
            "Epoch 016 | train_loss=0.8660 acc=0.6249 | val_loss=0.9148 acc=0.5606 | time=17.1s\n",
            "Epoch 017 | train_loss=0.8369 acc=0.6408 | val_loss=0.9334 acc=0.5730 | time=16.8s\n",
            "Epoch 018 | train_loss=0.8036 acc=0.6447 | val_loss=0.8936 acc=0.6180 | time=16.9s\n",
            "Epoch 019 | train_loss=0.7843 acc=0.6637 | val_loss=0.8611 acc=0.6149 | time=16.9s\n",
            "Epoch 020 | train_loss=0.7644 acc=0.6738 | val_loss=0.8749 acc=0.6102 | time=17.1s\n",
            "Epoch 021 | train_loss=0.7594 acc=0.6955 | val_loss=0.8979 acc=0.6258 | time=16.9s\n",
            "Epoch 022 | train_loss=0.7335 acc=0.6901 | val_loss=0.8700 acc=0.6335 | time=17.1s\n",
            "Epoch 023 | train_loss=0.7040 acc=0.7041 | val_loss=0.8866 acc=0.6398 | time=17.5s\n",
            "Epoch 024 | train_loss=0.6899 acc=0.7161 | val_loss=0.8989 acc=0.6196 | time=17.3s\n",
            "Epoch 025 | train_loss=0.6585 acc=0.7231 | val_loss=0.8607 acc=0.6242 | time=17.1s\n",
            "Epoch 026 | train_loss=0.6603 acc=0.7258 | val_loss=0.8905 acc=0.6289 | time=17.5s\n",
            "Epoch 027 | train_loss=0.6622 acc=0.7153 | val_loss=0.9182 acc=0.6118 | time=17.5s\n",
            "Epoch 028 | train_loss=0.6417 acc=0.7309 | val_loss=0.8713 acc=0.6366 | time=17.3s\n",
            "Epoch 029 | train_loss=0.6122 acc=0.7530 | val_loss=0.9282 acc=0.6242 | time=17.8s\n",
            "Epoch 030 | train_loss=0.5927 acc=0.7619 | val_loss=0.8774 acc=0.6196 | time=17.0s\n",
            "Epoch 031 | train_loss=0.5633 acc=0.7736 | val_loss=0.9007 acc=0.6320 | time=17.3s\n",
            "Epoch 032 | train_loss=0.5080 acc=0.8008 | val_loss=0.9611 acc=0.6273 | time=17.6s\n",
            "Epoch 033 | train_loss=0.4939 acc=0.8023 | val_loss=1.0140 acc=0.6320 | time=17.3s\n",
            "Epoch 034 | train_loss=0.4797 acc=0.8175 | val_loss=0.9936 acc=0.5994 | time=16.9s\n",
            "Epoch 035 | train_loss=0.4575 acc=0.8256 | val_loss=1.0268 acc=0.6289 | time=17.1s\n",
            "★ Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▃▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇████</td></tr><tr><td>train_loss</td><td>████████████▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▆▅▆▅▆▇▇▇███▇▇█▇█▇▇███▇█</td></tr><tr><td>validation_loss</td><td>███████████▇▅▅▃▃▃▂▁▁▂▁▂▂▁▂▃▁▃▂▂▄▆▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_accuracy</td><td>0.82563</td></tr><tr><td>train_loss</td><td>0.45747</td></tr><tr><td>validation_accuracy</td><td>0.62888</td></tr><tr><td>validation_loss</td><td>1.02684</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">chocolate-frog-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/t1d5m54n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/t1d5m54n</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_043237-t1d5m54n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 04:42:41,069] Trial 4 finished with value: 0.8606603259132022 and parameters: {'lr': 0.0005, 'weight_decay': 0.0001, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 4 with value: 0.8606603259132022.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_044241-18geqfqy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/18geqfqy' target=\"_blank\">zany-wind-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/18geqfqy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-3/runs/18geqfqy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=1.00e-03, wd=1.00e-04, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0829 acc=0.4016 | val_loss=1.0754 acc=0.4317 | time=17.1s\n",
            "Epoch 002 | train_loss=1.0687 acc=0.4214 | val_loss=1.0755 acc=0.4317 | time=17.5s\n",
            "Epoch 003 | train_loss=1.0727 acc=0.4190 | val_loss=1.0746 acc=0.4317 | time=17.2s\n",
            "Epoch 004 | train_loss=1.0703 acc=0.4318 | val_loss=1.0745 acc=0.4317 | time=16.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-05-02 04:44:01,082] Trial 5 failed with parameters: {'lr': 0.001, 'weight_decay': 0.0001, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-e29a196a779c>\", line 140, in objective_holdout\n",
            "    tloss    += loss.item()\n",
            "                ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-02 04:44:01,085] Trial 5 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e29a196a779c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_holdout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# grid 크기만큼 자동 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# ─── 결과 출력 ─────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-e29a196a779c>\u001b[0m in \u001b[0;36mobjective_holdout\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r8gJ0Jh40O5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Revised the model due to overfitting\n",
        "- CNNDecoder dropout rate = 0.5\n",
        "- Increase the Weight Decay"
      ],
      "metadata": {
        "id": "7BnShdHrUUUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4, 1e-3]\n",
        "WD_CHOICES          = [5e-3, 5e-4]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [1, 2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-4\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-4\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-4.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3PYt54TJ0O7u",
        "outputId": "d2b95425-491f-471d-f9f3-6c1a3bef32c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 05:02:56,608] A new study created in RDB with name: eeg_holdout_grid_search-4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_050257-lpddtetn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/lpddtetn' target=\"_blank\">comfy-wildflower-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/lpddtetn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/lpddtetn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=1.00e-03, wd=5.00e-03, blocks=1, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.0764 acc=0.4101 | val_loss=1.0752 acc=0.4317 | time=16.6s\n",
            "Epoch 002 | train_loss=1.0723 acc=0.4179 | val_loss=1.0758 acc=0.4317 | time=16.8s\n",
            "Epoch 003 | train_loss=1.0691 acc=0.4287 | val_loss=1.0744 acc=0.4317 | time=16.8s\n",
            "Epoch 004 | train_loss=1.0667 acc=0.4322 | val_loss=1.0742 acc=0.4317 | time=16.5s\n",
            "Epoch 005 | train_loss=1.0709 acc=0.4322 | val_loss=1.0745 acc=0.4317 | time=16.7s\n",
            "Epoch 006 | train_loss=1.0680 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=16.9s\n",
            "Epoch 007 | train_loss=1.0678 acc=0.4307 | val_loss=1.0746 acc=0.4317 | time=16.8s\n",
            "Epoch 008 | train_loss=1.0664 acc=0.4303 | val_loss=1.0743 acc=0.4317 | time=16.5s\n",
            "Epoch 009 | train_loss=1.0674 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.7s\n",
            "Epoch 010 | train_loss=1.0671 acc=0.4311 | val_loss=1.0716 acc=0.4317 | time=16.4s\n",
            "Epoch 011 | train_loss=1.0626 acc=0.4322 | val_loss=1.0571 acc=0.4317 | time=16.7s\n",
            "Epoch 012 | train_loss=1.0415 acc=0.4357 | val_loss=1.0357 acc=0.4581 | time=16.8s\n",
            "Epoch 013 | train_loss=1.0325 acc=0.4567 | val_loss=1.0306 acc=0.4643 | time=16.6s\n",
            "Epoch 014 | train_loss=1.0254 acc=0.4664 | val_loss=1.0205 acc=0.4705 | time=16.6s\n",
            "Epoch 015 | train_loss=0.9952 acc=0.5107 | val_loss=0.9293 acc=0.5901 | time=16.4s\n",
            "Epoch 016 | train_loss=0.9305 acc=0.5946 | val_loss=0.8649 acc=0.6242 | time=16.8s\n",
            "Epoch 017 | train_loss=0.8538 acc=0.6384 | val_loss=0.8226 acc=0.6398 | time=16.6s\n",
            "Epoch 018 | train_loss=0.8218 acc=0.6501 | val_loss=0.8222 acc=0.6413 | time=16.9s\n",
            "Epoch 019 | train_loss=0.7989 acc=0.6579 | val_loss=0.8305 acc=0.6320 | time=16.7s\n",
            "Epoch 020 | train_loss=0.7615 acc=0.6812 | val_loss=0.7647 acc=0.6584 | time=16.8s\n",
            "Epoch 021 | train_loss=0.7446 acc=0.6983 | val_loss=0.7979 acc=0.6599 | time=16.7s\n",
            "Epoch 022 | train_loss=0.7419 acc=0.6944 | val_loss=0.7938 acc=0.6724 | time=16.8s\n",
            "Epoch 023 | train_loss=0.6933 acc=0.7118 | val_loss=0.7356 acc=0.6724 | time=16.8s\n",
            "Epoch 024 | train_loss=0.6728 acc=0.7208 | val_loss=0.7150 acc=0.6615 | time=16.8s\n",
            "Epoch 025 | train_loss=0.6939 acc=0.7188 | val_loss=0.7732 acc=0.6382 | time=17.3s\n",
            "Epoch 026 | train_loss=0.6685 acc=0.7169 | val_loss=0.6825 acc=0.7112 | time=16.8s\n",
            "Epoch 027 | train_loss=0.6411 acc=0.7406 | val_loss=0.7610 acc=0.6801 | time=17.0s\n",
            "Epoch 028 | train_loss=0.6614 acc=0.7289 | val_loss=0.6725 acc=0.7003 | time=16.9s\n",
            "Epoch 029 | train_loss=0.6261 acc=0.7406 | val_loss=0.6824 acc=0.6848 | time=16.9s\n",
            "Epoch 030 | train_loss=0.6137 acc=0.7495 | val_loss=0.7723 acc=0.6739 | time=16.7s\n",
            "Epoch 031 | train_loss=0.6329 acc=0.7402 | val_loss=0.6640 acc=0.7112 | time=16.6s\n",
            "Epoch 032 | train_loss=0.5917 acc=0.7534 | val_loss=0.6984 acc=0.7096 | time=16.8s\n",
            "Epoch 033 | train_loss=0.5954 acc=0.7557 | val_loss=0.7135 acc=0.6801 | time=16.4s\n",
            "Epoch 034 | train_loss=0.6027 acc=0.7511 | val_loss=0.6646 acc=0.6879 | time=16.6s\n",
            "Epoch 035 | train_loss=0.5923 acc=0.7507 | val_loss=0.6386 acc=0.7174 | time=17.1s\n",
            "Epoch 036 | train_loss=0.5678 acc=0.7616 | val_loss=0.6576 acc=0.7019 | time=16.9s\n",
            "Epoch 037 | train_loss=0.5676 acc=0.7561 | val_loss=0.6449 acc=0.7096 | time=16.6s\n",
            "Epoch 038 | train_loss=0.5667 acc=0.7682 | val_loss=0.6773 acc=0.6957 | time=16.9s\n",
            "Epoch 039 | train_loss=0.5363 acc=0.7759 | val_loss=0.6773 acc=0.7081 | time=16.9s\n",
            "Epoch 040 | train_loss=0.5556 acc=0.7666 | val_loss=0.6686 acc=0.7252 | time=16.6s\n",
            "Epoch 041 | train_loss=0.5430 acc=0.7732 | val_loss=0.6499 acc=0.7050 | time=16.8s\n",
            "Epoch 042 | train_loss=0.5101 acc=0.7950 | val_loss=0.7122 acc=0.7236 | time=17.0s\n",
            "Epoch 043 | train_loss=0.5033 acc=0.7934 | val_loss=0.6564 acc=0.7205 | time=16.8s\n",
            "Epoch 044 | train_loss=0.4804 acc=0.8054 | val_loss=0.6899 acc=0.7050 | time=16.6s\n",
            "Epoch 045 | train_loss=0.4823 acc=0.7988 | val_loss=0.6884 acc=0.7220 | time=16.8s\n",
            "★ Early stopping at epoch 45\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▂▃▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇████</td></tr><tr><td>train_loss</td><td>███████████▇▇▇▆▅▅▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▂▂▅▆▆▆▆▆▇▇▆▆█▇▇▇██▇▇██▇██████</td></tr><tr><td>validation_loss</td><td>██████████▇▇▇▆▅▄▄▃▄▃▃▂▃▂▂▂▃▁▂▂▁▁▁▂▂▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>45</td></tr><tr><td>train_accuracy</td><td>0.79883</td></tr><tr><td>train_loss</td><td>0.48234</td></tr><tr><td>validation_accuracy</td><td>0.72205</td></tr><tr><td>validation_loss</td><td>0.68842</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comfy-wildflower-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/lpddtetn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/lpddtetn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_050257-lpddtetn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 05:15:34,806] Trial 0 finished with value: 0.6386472043536958 and parameters: {'lr': 0.001, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 1, 'num_segments': 5}. Best is trial 0 with value: 0.6386472043536958.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_051534-x6af54ie</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/x6af54ie' target=\"_blank\">cool-smoke-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/x6af54ie' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-4/runs/x6af54ie</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=1.00e-03, wd=5.00e-04, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0796 acc=0.4132 | val_loss=1.0757 acc=0.4317 | time=18.5s\n",
            "Epoch 002 | train_loss=1.0757 acc=0.4140 | val_loss=1.0742 acc=0.4317 | time=18.6s\n",
            "Epoch 003 | train_loss=1.0726 acc=0.4194 | val_loss=1.0750 acc=0.4317 | time=18.5s\n",
            "Epoch 004 | train_loss=1.0690 acc=0.4210 | val_loss=1.0739 acc=0.4317 | time=18.9s\n",
            "Epoch 005 | train_loss=1.0596 acc=0.4501 | val_loss=1.0207 acc=0.5668 | time=18.7s\n",
            "Epoch 006 | train_loss=1.0027 acc=0.5468 | val_loss=1.0058 acc=0.5621 | time=18.7s\n",
            "Epoch 007 | train_loss=0.9713 acc=0.5728 | val_loss=0.9684 acc=0.5264 | time=18.6s\n",
            "Epoch 008 | train_loss=0.9407 acc=0.5856 | val_loss=0.9816 acc=0.5792 | time=18.6s\n",
            "Epoch 009 | train_loss=0.9147 acc=0.6031 | val_loss=0.9445 acc=0.5870 | time=18.7s\n",
            "Epoch 010 | train_loss=0.8854 acc=0.6136 | val_loss=0.9190 acc=0.5839 | time=18.7s\n",
            "Epoch 011 | train_loss=0.8837 acc=0.6113 | val_loss=0.9402 acc=0.5839 | time=18.7s\n",
            "Epoch 012 | train_loss=0.8561 acc=0.6338 | val_loss=0.9063 acc=0.6134 | time=18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-05-02 05:19:21,532] Trial 1 failed with parameters: {'lr': 0.001, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-b80845e0b137>\", line 140, in objective_holdout\n",
            "    tloss    += loss.item()\n",
            "                ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-02 05:19:21,535] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-b80845e0b137>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_holdout\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# grid 크기만큼 자동 실행\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m     \u001b[0;31m# ─── 결과 출력 ─────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-b80845e0b137>\u001b[0m in \u001b[0;36mobjective_holdout\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m             \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m             \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4, 1e-3]\n",
        "WD_CHOICES          = [5e-3, 5e-4]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [1, 2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-4\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-4\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-4.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "q44kbaGd7uAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stronger Weight Decay\n",
        "- Block = 1, Head = 1\n"
      ],
      "metadata": {
        "id": "p7gJaV_db40h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [1e-3]\n",
        "WD_CHOICES          = [1e-2]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [1]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-5\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-5\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-5.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5dxb74Hhb4Lm",
        "outputId": "0a3a7832-2641-4b47-e35c-afafae08a13b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 05:23:41,610] A new study created in RDB with name: eeg_holdout_grid_search-5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_052342-brzksog2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5/runs/brzksog2' target=\"_blank\">trim-violet-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5/runs/brzksog2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5/runs/brzksog2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=1.00e-03, wd=1.00e-02, blocks=1, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.0783 acc=0.4070 | val_loss=1.0755 acc=0.4317 | time=16.7s\n",
            "Epoch 002 | train_loss=1.0727 acc=0.4190 | val_loss=1.0761 acc=0.4317 | time=16.6s\n",
            "Epoch 003 | train_loss=1.0711 acc=0.4249 | val_loss=1.0742 acc=0.4317 | time=16.5s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4338 | val_loss=1.0744 acc=0.4317 | time=16.5s\n",
            "Epoch 005 | train_loss=1.0714 acc=0.4307 | val_loss=1.0764 acc=0.4317 | time=16.6s\n",
            "Epoch 006 | train_loss=1.0672 acc=0.4307 | val_loss=1.0705 acc=0.4317 | time=16.7s\n",
            "Epoch 007 | train_loss=1.0533 acc=0.4598 | val_loss=1.0750 acc=0.4317 | time=17.0s\n",
            "Epoch 008 | train_loss=1.0687 acc=0.4280 | val_loss=1.0671 acc=0.4317 | time=16.6s\n",
            "Epoch 009 | train_loss=1.0299 acc=0.4955 | val_loss=1.0729 acc=0.3416 | time=16.4s\n",
            "Epoch 010 | train_loss=1.0026 acc=0.5472 | val_loss=1.0091 acc=0.5435 | time=16.6s\n",
            "Epoch 011 | train_loss=0.9708 acc=0.5693 | val_loss=0.9890 acc=0.5512 | time=16.8s\n",
            "Epoch 012 | train_loss=0.9875 acc=0.5565 | val_loss=0.9802 acc=0.5373 | time=16.6s\n",
            "Epoch 013 | train_loss=0.9337 acc=0.5953 | val_loss=0.9837 acc=0.5792 | time=16.6s\n",
            "Epoch 014 | train_loss=0.9513 acc=0.5852 | val_loss=1.0081 acc=0.5823 | time=16.6s\n",
            "Epoch 015 | train_loss=0.8952 acc=0.6144 | val_loss=0.9354 acc=0.5807 | time=16.5s\n",
            "Epoch 016 | train_loss=0.8622 acc=0.6311 | val_loss=0.9070 acc=0.5932 | time=16.4s\n",
            "Epoch 017 | train_loss=0.8333 acc=0.6388 | val_loss=0.9382 acc=0.5714 | time=16.6s\n",
            "Epoch 018 | train_loss=0.8118 acc=0.6559 | val_loss=0.9217 acc=0.5947 | time=16.7s\n",
            "Epoch 019 | train_loss=0.8136 acc=0.6497 | val_loss=0.9035 acc=0.6211 | time=16.7s\n",
            "Epoch 020 | train_loss=0.7434 acc=0.6831 | val_loss=0.9543 acc=0.5885 | time=16.6s\n",
            "Epoch 021 | train_loss=0.6958 acc=0.6901 | val_loss=1.0063 acc=0.5543 | time=16.9s\n",
            "Epoch 022 | train_loss=0.6384 acc=0.7130 | val_loss=0.9254 acc=0.6273 | time=16.5s\n",
            "Epoch 023 | train_loss=0.5826 acc=0.7417 | val_loss=0.8518 acc=0.6537 | time=16.6s\n",
            "Epoch 024 | train_loss=0.5407 acc=0.7685 | val_loss=0.8665 acc=0.6444 | time=16.8s\n",
            "Epoch 025 | train_loss=0.5871 acc=0.7522 | val_loss=1.0992 acc=0.5932 | time=16.5s\n",
            "Epoch 026 | train_loss=0.7734 acc=0.6656 | val_loss=1.0555 acc=0.5481 | time=16.6s\n",
            "Epoch 027 | train_loss=0.6372 acc=0.7262 | val_loss=1.0348 acc=0.6102 | time=16.5s\n",
            "Epoch 028 | train_loss=0.5800 acc=0.7600 | val_loss=0.9410 acc=0.6444 | time=16.9s\n",
            "Epoch 029 | train_loss=0.4631 acc=0.8054 | val_loss=1.1245 acc=0.5901 | time=16.5s\n",
            "Epoch 030 | train_loss=0.3963 acc=0.8454 | val_loss=1.2812 acc=0.6335 | time=16.5s\n",
            "Epoch 031 | train_loss=0.3429 acc=0.8683 | val_loss=1.3378 acc=0.6289 | time=17.1s\n",
            "Epoch 032 | train_loss=0.3178 acc=0.8753 | val_loss=1.4671 acc=0.6289 | time=16.8s\n",
            "Epoch 033 | train_loss=0.2723 acc=0.8955 | val_loss=1.5263 acc=0.6180 | time=17.0s\n",
            "★ Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▁▂▃▃▃▄▄▄▄▄▅▄▅▅▅▆▆▆▅▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▇▆▆▆▆▆▅▅▄▄▃▄▅▄▄▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▃▃▃▁▆▆▅▆▆▆▇▆▇▇▇▆▇██▇▆▇█▇█▇▇▇</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▂▂▃▂▁▁▄▃▃▂▄▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_accuracy</td><td>0.89553</td></tr><tr><td>train_loss</td><td>0.27233</td></tr><tr><td>validation_accuracy</td><td>0.61801</td></tr><tr><td>validation_loss</td><td>1.52626</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trim-violet-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5/runs/brzksog2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5/runs/brzksog2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_052342-brzksog2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 05:32:55,611] Trial 0 finished with value: 0.851824280761537 and parameters: {'lr': 0.001, 'weight_decay': 0.01, 'num_blocks': 1, 'num_heads': 1, 'num_segments': 5}. Best is trial 0 with value: 0.851824280761537.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.851824\n",
            "best_train_loss     = 0.582561\n",
            "best_train_accuracy = 0.7417\n",
            "best_val_accuracy   = 0.6537\n",
            "best params:\n",
            "  lr: 0.001\n",
            "  weight_decay: 0.01\n",
            "  num_blocks: 1\n",
            "  num_heads: 1\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Wdig-a2b4Nt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Even though setting stronger weight decay, the model overfitting\n",
        "- This is because I didn't change the learning rate according to the weight decay. The learning rate should be addressed to decreased as the weight decay set high"
      ],
      "metadata": {
        "id": "DZD3z3-Yg3NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [1e-2]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [1]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-6\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-6\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-6.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "If6k0Eg3b4Pt",
        "outputId": "424f12e8-4d89-47b5-b8d8-2eab3177371f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 05:43:13,744] A new study created in RDB with name: eeg_holdout_grid_search-6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_054313-6rghopf3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6/runs/6rghopf3' target=\"_blank\">deft-deluge-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6/runs/6rghopf3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6/runs/6rghopf3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=1.00e-02, blocks=1, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.0805 acc=0.3907 | val_loss=1.0744 acc=0.4317 | time=16.8s\n",
            "Epoch 002 | train_loss=1.0749 acc=0.4117 | val_loss=1.0745 acc=0.4317 | time=16.5s\n",
            "Epoch 003 | train_loss=1.0699 acc=0.4252 | val_loss=1.0763 acc=0.4317 | time=16.9s\n",
            "Epoch 004 | train_loss=1.0696 acc=0.4210 | val_loss=1.0765 acc=0.4317 | time=16.8s\n",
            "Epoch 005 | train_loss=1.0736 acc=0.4307 | val_loss=1.0746 acc=0.4317 | time=16.6s\n",
            "Epoch 006 | train_loss=1.0723 acc=0.4183 | val_loss=1.0760 acc=0.4317 | time=16.7s\n",
            "Epoch 007 | train_loss=1.0714 acc=0.4315 | val_loss=1.0738 acc=0.4317 | time=16.6s\n",
            "Epoch 008 | train_loss=1.0642 acc=0.4384 | val_loss=1.0558 acc=0.4317 | time=16.7s\n",
            "Epoch 009 | train_loss=1.0025 acc=0.5483 | val_loss=0.9978 acc=0.5062 | time=16.7s\n",
            "Epoch 010 | train_loss=0.9708 acc=0.5771 | val_loss=0.9783 acc=0.5233 | time=16.4s\n",
            "Epoch 011 | train_loss=0.9411 acc=0.5899 | val_loss=0.9799 acc=0.5730 | time=16.8s\n",
            "Epoch 012 | train_loss=0.9236 acc=0.6004 | val_loss=0.9462 acc=0.5870 | time=16.8s\n",
            "Epoch 013 | train_loss=0.9022 acc=0.6113 | val_loss=0.9447 acc=0.5963 | time=16.7s\n",
            "Epoch 014 | train_loss=0.8735 acc=0.6206 | val_loss=0.9225 acc=0.5994 | time=17.1s\n",
            "Epoch 015 | train_loss=0.8410 acc=0.6346 | val_loss=0.8983 acc=0.6071 | time=16.8s\n",
            "Epoch 016 | train_loss=0.8068 acc=0.6489 | val_loss=0.9366 acc=0.6304 | time=16.6s\n",
            "Epoch 017 | train_loss=0.7697 acc=0.6691 | val_loss=0.8484 acc=0.6599 | time=16.6s\n",
            "Epoch 018 | train_loss=0.7278 acc=0.6975 | val_loss=0.8362 acc=0.6522 | time=16.8s\n",
            "Epoch 019 | train_loss=0.6810 acc=0.7173 | val_loss=0.9780 acc=0.6429 | time=16.8s\n",
            "Epoch 020 | train_loss=0.6499 acc=0.7398 | val_loss=0.8489 acc=0.6677 | time=16.5s\n",
            "Epoch 021 | train_loss=0.6315 acc=0.7491 | val_loss=0.7673 acc=0.6708 | time=16.9s\n",
            "Epoch 022 | train_loss=0.5692 acc=0.7717 | val_loss=0.7523 acc=0.6988 | time=16.4s\n",
            "Epoch 023 | train_loss=0.4934 acc=0.8062 | val_loss=0.8860 acc=0.6957 | time=16.4s\n",
            "Epoch 024 | train_loss=0.4674 acc=0.8194 | val_loss=0.8552 acc=0.7096 | time=16.4s\n",
            "Epoch 025 | train_loss=0.4557 acc=0.8183 | val_loss=0.8389 acc=0.7065 | time=16.8s\n",
            "Epoch 026 | train_loss=0.3898 acc=0.8544 | val_loss=0.8189 acc=0.6646 | time=16.6s\n",
            "Epoch 027 | train_loss=0.3543 acc=0.8641 | val_loss=0.9097 acc=0.7003 | time=16.6s\n",
            "Epoch 028 | train_loss=0.3913 acc=0.8575 | val_loss=0.9877 acc=0.6522 | time=16.6s\n",
            "Epoch 029 | train_loss=0.2877 acc=0.8986 | val_loss=0.9491 acc=0.7081 | time=16.5s\n",
            "Epoch 030 | train_loss=0.2544 acc=0.9103 | val_loss=1.0206 acc=0.6988 | time=16.8s\n",
            "Epoch 031 | train_loss=0.2248 acc=0.9250 | val_loss=1.0914 acc=0.7050 | time=16.8s\n",
            "Epoch 032 | train_loss=0.2232 acc=0.9243 | val_loss=1.1249 acc=0.7112 | time=16.7s\n",
            "★ Early stopping at epoch 32\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▁▂▂▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>████████▇▇▇▇▇▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▃▃▅▅▅▅▅▆▇▇▆▇▇████▇█▇████</td></tr><tr><td>validation_loss</td><td>▇▇▇▇▇▇▇▇▆▅▅▅▅▄▄▄▃▃▅▃▁▁▄▃▃▂▄▅▅▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>train_accuracy</td><td>0.92427</td></tr><tr><td>train_loss</td><td>0.22318</td></tr><tr><td>validation_accuracy</td><td>0.71118</td></tr><tr><td>validation_loss</td><td>1.12488</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deft-deluge-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6/runs/6rghopf3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6/runs/6rghopf3</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_054313-6rghopf3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 05:52:09,596] Trial 0 finished with value: 0.7523488317217145 and parameters: {'lr': 0.0005, 'weight_decay': 0.01, 'num_blocks': 1, 'num_heads': 1, 'num_segments': 5}. Best is trial 0 with value: 0.7523488317217145.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.752349\n",
            "best_train_loss     = 0.569233\n",
            "best_train_accuracy = 0.7717\n",
            "best_val_accuracy   = 0.6988\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.01\n",
            "  num_blocks: 1\n",
            "  num_heads: 1\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5EnAUU0Ib4Rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add drop out in transformer model\n",
        "- After Depthwise Conv(ODCM)\n",
        "- After TransformerBlock Attention output (before residual connection)\n",
        "- After Token Embedding (RTM/STM/TTM)\n",
        "- After CNNDecoder Conv layers\n"
      ],
      "metadata": {
        "id": "PCppn6y9p58C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-3, 5e-4, 5e-5]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [1]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-7\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-7\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-7.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "81PH28EJjr61",
        "outputId": "41c3414b-3bfc-4fcb-b3e6-4cb368e42c32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 06:28:51,249] A new study created in RDB with name: eeg_holdout_grid_search-7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_062851-mv5ia0j6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/mv5ia0j6' target=\"_blank\">divine-star-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/mv5ia0j6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/mv5ia0j6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-05, blocks=1, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.0794 acc=0.4113 | val_loss=1.0761 acc=0.4317 | time=17.1s\n",
            "Epoch 002 | train_loss=1.0762 acc=0.4070 | val_loss=1.0751 acc=0.4317 | time=17.0s\n",
            "Epoch 003 | train_loss=1.0725 acc=0.4171 | val_loss=1.0743 acc=0.4317 | time=16.7s\n",
            "Epoch 004 | train_loss=1.0737 acc=0.4272 | val_loss=1.0743 acc=0.4317 | time=16.8s\n",
            "Epoch 005 | train_loss=1.0670 acc=0.4338 | val_loss=1.0746 acc=0.4317 | time=16.7s\n",
            "Epoch 006 | train_loss=1.0716 acc=0.4322 | val_loss=1.0743 acc=0.4317 | time=17.0s\n",
            "Epoch 007 | train_loss=1.0687 acc=0.4303 | val_loss=1.0755 acc=0.4317 | time=16.5s\n",
            "Epoch 008 | train_loss=1.0709 acc=0.4315 | val_loss=1.0745 acc=0.4317 | time=16.9s\n",
            "Epoch 009 | train_loss=1.0701 acc=0.4315 | val_loss=1.0768 acc=0.4317 | time=16.6s\n",
            "Epoch 010 | train_loss=1.0700 acc=0.4307 | val_loss=1.0745 acc=0.4317 | time=16.6s\n",
            "Epoch 011 | train_loss=1.0698 acc=0.4307 | val_loss=1.0742 acc=0.4317 | time=16.7s\n",
            "Epoch 012 | train_loss=1.0664 acc=0.4307 | val_loss=1.0743 acc=0.4317 | time=16.7s\n",
            "Epoch 013 | train_loss=1.0679 acc=0.4315 | val_loss=1.0746 acc=0.4317 | time=17.0s\n",
            "Epoch 014 | train_loss=1.0716 acc=0.4315 | val_loss=1.0746 acc=0.4317 | time=16.9s\n",
            "Epoch 015 | train_loss=1.0704 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=17.2s\n",
            "Epoch 016 | train_loss=1.0680 acc=0.4315 | val_loss=1.0731 acc=0.4317 | time=17.2s\n",
            "Epoch 017 | train_loss=1.0712 acc=0.4245 | val_loss=1.0742 acc=0.4317 | time=17.2s\n",
            "Epoch 018 | train_loss=1.0667 acc=0.4291 | val_loss=1.0753 acc=0.4317 | time=16.9s\n",
            "Epoch 019 | train_loss=1.0694 acc=0.4322 | val_loss=1.0742 acc=0.4317 | time=17.1s\n",
            "Epoch 020 | train_loss=1.0662 acc=0.4303 | val_loss=1.0767 acc=0.4317 | time=16.9s\n",
            "Epoch 021 | train_loss=1.0691 acc=0.4318 | val_loss=1.0740 acc=0.4317 | time=17.2s\n",
            "Epoch 022 | train_loss=1.0696 acc=0.4322 | val_loss=1.0747 acc=0.4317 | time=16.7s\n",
            "Epoch 023 | train_loss=1.0685 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.8s\n",
            "Epoch 024 | train_loss=1.0679 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.8s\n",
            "Epoch 025 | train_loss=1.0656 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=16.8s\n",
            "Epoch 026 | train_loss=1.0676 acc=0.4311 | val_loss=1.0740 acc=0.4317 | time=16.9s\n",
            "★ Early stopping at epoch 26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▂▁▄▆██▇▇▇▇▇▇▇▇▇▇▆▇█▇▇█▇▇▇▇</td></tr><tr><td>train_loss</td><td>█▆▄▅▂▄▃▄▃▃▃▁▂▄▃▂▄▂▃▁▃▃▂▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▇▅▃▃▄▃▅▄█▄▃▃▄▄▃▁▃▅▃█▃▄▃▃▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06758</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07404</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-star-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/mv5ia0j6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/mv5ia0j6</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_062851-mv5ia0j6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 06:36:12,736] Trial 0 finished with value: 1.0731083296594166 and parameters: {'lr': 0.0005, 'weight_decay': 5e-05, 'num_blocks': 1, 'num_heads': 1, 'num_segments': 5}. Best is trial 0 with value: 1.0731083296594166.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_063612-b14spiag</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/b14spiag' target=\"_blank\">deft-deluge-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/b14spiag' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/b14spiag</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=1, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.0815 acc=0.3860 | val_loss=1.0746 acc=0.4317 | time=17.2s\n",
            "Epoch 002 | train_loss=1.0773 acc=0.3992 | val_loss=1.0749 acc=0.4317 | time=17.3s\n",
            "Epoch 003 | train_loss=1.0770 acc=0.4000 | val_loss=1.0753 acc=0.4317 | time=16.8s\n",
            "Epoch 004 | train_loss=1.0728 acc=0.4171 | val_loss=1.0754 acc=0.4317 | time=17.4s\n",
            "Epoch 005 | train_loss=1.0720 acc=0.4163 | val_loss=1.0745 acc=0.4317 | time=17.7s\n",
            "Epoch 006 | train_loss=1.0694 acc=0.4268 | val_loss=1.0744 acc=0.4317 | time=17.5s\n",
            "Epoch 007 | train_loss=1.0726 acc=0.4171 | val_loss=1.0744 acc=0.4317 | time=17.3s\n",
            "Epoch 008 | train_loss=1.0718 acc=0.4264 | val_loss=1.0747 acc=0.4317 | time=17.1s\n",
            "Epoch 009 | train_loss=1.0654 acc=0.4497 | val_loss=1.0609 acc=0.4596 | time=17.3s\n",
            "Epoch 010 | train_loss=1.0103 acc=0.5414 | val_loss=0.9821 acc=0.5714 | time=17.1s\n",
            "Epoch 011 | train_loss=0.9592 acc=0.5790 | val_loss=0.9560 acc=0.5652 | time=17.3s\n",
            "Epoch 012 | train_loss=0.9336 acc=0.5981 | val_loss=0.9424 acc=0.5745 | time=17.1s\n",
            "Epoch 013 | train_loss=0.9044 acc=0.6085 | val_loss=0.9564 acc=0.5839 | time=16.9s\n",
            "Epoch 014 | train_loss=0.8706 acc=0.6198 | val_loss=0.9345 acc=0.5839 | time=17.1s\n",
            "Epoch 015 | train_loss=0.8455 acc=0.6326 | val_loss=0.9222 acc=0.5932 | time=16.9s\n",
            "Epoch 016 | train_loss=0.8069 acc=0.6419 | val_loss=0.9467 acc=0.5947 | time=17.0s\n",
            "Epoch 017 | train_loss=0.7739 acc=0.6583 | val_loss=0.9397 acc=0.5885 | time=16.9s\n",
            "Epoch 018 | train_loss=0.7542 acc=0.6645 | val_loss=0.9704 acc=0.6071 | time=16.7s\n",
            "Epoch 019 | train_loss=0.7370 acc=0.6645 | val_loss=1.0071 acc=0.6196 | time=17.1s\n",
            "Epoch 020 | train_loss=0.6961 acc=0.6882 | val_loss=1.0336 acc=0.6227 | time=16.7s\n",
            "Epoch 021 | train_loss=0.6944 acc=0.6862 | val_loss=0.9713 acc=0.5621 | time=16.8s\n",
            "Epoch 022 | train_loss=0.6230 acc=0.7278 | val_loss=1.1064 acc=0.6149 | time=17.2s\n",
            "Epoch 023 | train_loss=0.6230 acc=0.7231 | val_loss=1.0963 acc=0.6025 | time=16.6s\n",
            "Epoch 024 | train_loss=0.5905 acc=0.7367 | val_loss=1.1465 acc=0.6180 | time=16.6s\n",
            "Epoch 025 | train_loss=0.5729 acc=0.7553 | val_loss=1.2013 acc=0.6071 | time=16.8s\n",
            "★ Early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▂▂▂▂▄▅▅▅▅▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█████████▇▆▆▆▅▅▄▄▃▃▃▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▂▆▆▆▇▇▇▇▇▇██▆█▇█▇</td></tr><tr><td>validation_loss</td><td>▅▅▅▅▅▅▅▅▄▃▂▂▂▁▁▂▁▂▃▄▂▆▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_accuracy</td><td>0.75534</td></tr><tr><td>train_loss</td><td>0.57294</td></tr><tr><td>validation_accuracy</td><td>0.60714</td></tr><tr><td>validation_loss</td><td>1.20127</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deft-deluge-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/b14spiag' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/b14spiag</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_063612-b14spiag/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 06:43:21,365] Trial 1 finished with value: 0.9222010232153393 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 1, 'num_segments': 5}. Best is trial 1 with value: 0.9222010232153393.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_064321-wg1ay5jm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/wg1ay5jm' target=\"_blank\">dashing-brook-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/wg1ay5jm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/wg1ay5jm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.0883 acc=0.3938 | val_loss=1.0766 acc=0.4317 | time=16.6s\n",
            "Epoch 002 | train_loss=1.0721 acc=0.4369 | val_loss=1.0758 acc=0.4317 | time=16.8s\n",
            "Epoch 003 | train_loss=1.0739 acc=0.4159 | val_loss=1.0742 acc=0.4317 | time=16.8s\n",
            "Epoch 004 | train_loss=1.0728 acc=0.4229 | val_loss=1.0748 acc=0.4317 | time=16.6s\n",
            "Epoch 005 | train_loss=1.0689 acc=0.4295 | val_loss=1.0751 acc=0.4317 | time=16.7s\n",
            "Epoch 006 | train_loss=1.0692 acc=0.4322 | val_loss=1.0774 acc=0.4317 | time=16.9s\n",
            "Epoch 007 | train_loss=1.0713 acc=0.4307 | val_loss=1.0742 acc=0.4317 | time=16.9s\n",
            "Epoch 008 | train_loss=1.0713 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.8s\n",
            "Epoch 009 | train_loss=1.0664 acc=0.4326 | val_loss=1.0754 acc=0.4317 | time=17.0s\n",
            "Epoch 010 | train_loss=1.0663 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.3s\n",
            "Epoch 011 | train_loss=1.0685 acc=0.4318 | val_loss=1.0742 acc=0.4317 | time=17.1s\n",
            "Epoch 012 | train_loss=1.0667 acc=0.4307 | val_loss=1.0697 acc=0.4317 | time=17.2s\n",
            "Epoch 013 | train_loss=1.0439 acc=0.4819 | val_loss=0.9995 acc=0.5575 | time=17.2s\n",
            "Epoch 014 | train_loss=0.9747 acc=0.5697 | val_loss=0.9560 acc=0.5761 | time=17.0s\n",
            "Epoch 015 | train_loss=0.9550 acc=0.5922 | val_loss=0.9537 acc=0.5699 | time=17.1s\n",
            "Epoch 016 | train_loss=0.9047 acc=0.6155 | val_loss=0.9195 acc=0.5807 | time=17.0s\n",
            "Epoch 017 | train_loss=0.8781 acc=0.6326 | val_loss=0.8979 acc=0.6087 | time=17.3s\n",
            "Epoch 018 | train_loss=0.8414 acc=0.6404 | val_loss=0.8818 acc=0.6025 | time=16.8s\n",
            "Epoch 019 | train_loss=0.8243 acc=0.6524 | val_loss=0.8451 acc=0.6258 | time=16.9s\n",
            "Epoch 020 | train_loss=0.7730 acc=0.6715 | val_loss=0.8380 acc=0.6320 | time=16.9s\n",
            "Epoch 021 | train_loss=0.7326 acc=0.6909 | val_loss=0.8886 acc=0.6149 | time=16.6s\n",
            "Epoch 022 | train_loss=0.7406 acc=0.6866 | val_loss=0.8191 acc=0.6506 | time=17.0s\n",
            "Epoch 023 | train_loss=0.6919 acc=0.7099 | val_loss=0.8194 acc=0.6522 | time=16.6s\n",
            "Epoch 024 | train_loss=0.6699 acc=0.7247 | val_loss=0.8171 acc=0.6553 | time=16.7s\n",
            "Epoch 025 | train_loss=0.6430 acc=0.7297 | val_loss=0.8358 acc=0.6289 | time=17.0s\n",
            "Epoch 026 | train_loss=0.6108 acc=0.7472 | val_loss=0.8492 acc=0.6522 | time=16.8s\n",
            "Epoch 027 | train_loss=0.5672 acc=0.7612 | val_loss=0.8944 acc=0.6724 | time=16.6s\n",
            "Epoch 028 | train_loss=0.5655 acc=0.7573 | val_loss=0.8470 acc=0.6646 | time=16.9s\n",
            "Epoch 029 | train_loss=0.5217 acc=0.7887 | val_loss=0.8761 acc=0.6801 | time=16.8s\n",
            "Epoch 030 | train_loss=0.5084 acc=0.7965 | val_loss=0.9158 acc=0.6630 | time=16.6s\n",
            "Epoch 031 | train_loss=0.4394 acc=0.8287 | val_loss=1.0433 acc=0.6708 | time=16.6s\n",
            "Epoch 032 | train_loss=0.4172 acc=0.8330 | val_loss=0.9377 acc=0.6724 | time=16.9s\n",
            "Epoch 033 | train_loss=0.3897 acc=0.8427 | val_loss=1.0033 acc=0.6770 | time=17.1s\n",
            "Epoch 034 | train_loss=0.3828 acc=0.8517 | val_loss=1.0587 acc=0.6817 | time=16.8s\n",
            "★ Early stopping at epoch 34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▂▂▂▂▂▂▂▂▂▄▄▄▅▅▅▅▆▅▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▆▆▆▅▅▄▅▄▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▆▆▆▇▆▇▇▇▇▇███▇████</td></tr><tr><td>validation_loss</td><td>████████████▆▅▅▄▃▃▂▂▃▁▁▁▂▂▃▂▃▄▇▄▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_accuracy</td><td>0.85165</td></tr><tr><td>train_loss</td><td>0.3828</td></tr><tr><td>validation_accuracy</td><td>0.68168</td></tr><tr><td>validation_loss</td><td>1.05871</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dashing-brook-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/wg1ay5jm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7/runs/wg1ay5jm</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-7</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_064321-wg1ay5jm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 06:52:57,979] Trial 2 finished with value: 0.8170908462433588 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 1, 'num_segments': 5}. Best is trial 2 with value: 0.8170908462433588.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.817091\n",
            "best_train_loss     = 0.669932\n",
            "best_train_accuracy = 0.7247\n",
            "best_val_accuracy   = 0.6553\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.005\n",
            "  num_blocks: 1\n",
            "  num_heads: 1\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-4]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3, 4]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-8\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-8\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-8.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lZZwSZB3jr9F",
        "outputId": "b9cd6fe2-d9d0-4dbb-b996-de8cad5214cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 07:10:43,774] A new study created in RDB with name: eeg_holdout_grid_search-8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_071043-6muzs132</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/6muzs132' target=\"_blank\">rich-paper-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/6muzs132' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/6muzs132</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=1, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0736 acc=0.4175 | val_loss=1.0744 acc=0.4317 | time=21.9s\n",
            "Epoch 002 | train_loss=1.0740 acc=0.4047 | val_loss=1.0769 acc=0.4317 | time=21.7s\n",
            "Epoch 003 | train_loss=1.0730 acc=0.4175 | val_loss=1.0748 acc=0.4317 | time=21.6s\n",
            "Epoch 004 | train_loss=1.0718 acc=0.4291 | val_loss=1.0749 acc=0.4317 | time=21.6s\n",
            "Epoch 005 | train_loss=1.0680 acc=0.4260 | val_loss=1.0744 acc=0.4317 | time=21.7s\n",
            "Epoch 006 | train_loss=1.0693 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=21.4s\n",
            "Epoch 007 | train_loss=1.0675 acc=0.4280 | val_loss=1.0751 acc=0.4317 | time=21.8s\n",
            "Epoch 008 | train_loss=1.0679 acc=0.4318 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 009 | train_loss=1.0686 acc=0.4291 | val_loss=1.0748 acc=0.4317 | time=21.7s\n",
            "Epoch 010 | train_loss=1.0699 acc=0.4315 | val_loss=1.0742 acc=0.4317 | time=21.9s\n",
            "Epoch 011 | train_loss=1.0688 acc=0.4303 | val_loss=1.0748 acc=0.4317 | time=21.6s\n",
            "Epoch 012 | train_loss=1.0688 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=21.7s\n",
            "Epoch 013 | train_loss=1.0688 acc=0.4322 | val_loss=1.0742 acc=0.4317 | time=21.9s\n",
            "Epoch 014 | train_loss=1.0666 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=21.8s\n",
            "Epoch 015 | train_loss=1.0665 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=21.7s\n",
            "Epoch 016 | train_loss=1.0677 acc=0.4307 | val_loss=1.0745 acc=0.4317 | time=21.6s\n",
            "Epoch 017 | train_loss=1.0677 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 018 | train_loss=1.0696 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.6s\n",
            "Epoch 019 | train_loss=1.0658 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=21.5s\n",
            "Epoch 020 | train_loss=1.0710 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 021 | train_loss=1.0688 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 022 | train_loss=1.0678 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 023 | train_loss=1.0659 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=21.8s\n",
            "★ Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▄▁▄▇▆█▇█▇██████████████</td></tr><tr><td>train_loss</td><td>██▇▆▃▄▂▃▃▅▄▄▄▂▂▃▃▄▁▅▄▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂█▃▃▁▂▃▁▂▁▃▁▁▃▂▂▁▁▂▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06586</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07439</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-paper-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/6muzs132' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/6muzs132</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_071043-6muzs132/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 07:19:04,836] Trial 0 finished with value: 1.074195731253851 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5}. Best is trial 0 with value: 1.074195731253851.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_071904-e7fmq45f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/e7fmq45f' target=\"_blank\">royal-river-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/e7fmq45f' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/e7fmq45f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0770 acc=0.4027 | val_loss=1.0778 acc=0.4317 | time=18.5s\n",
            "Epoch 002 | train_loss=1.0740 acc=0.4175 | val_loss=1.0748 acc=0.4317 | time=18.8s\n",
            "Epoch 003 | train_loss=1.0700 acc=0.4264 | val_loss=1.0751 acc=0.4317 | time=18.7s\n",
            "Epoch 004 | train_loss=1.0710 acc=0.4241 | val_loss=1.0743 acc=0.4317 | time=18.8s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4276 | val_loss=1.0743 acc=0.4317 | time=18.5s\n",
            "Epoch 006 | train_loss=1.0714 acc=0.4256 | val_loss=1.0742 acc=0.4317 | time=18.5s\n",
            "Epoch 007 | train_loss=1.0697 acc=0.4315 | val_loss=1.0742 acc=0.4317 | time=18.5s\n",
            "Epoch 008 | train_loss=1.0687 acc=0.4318 | val_loss=1.0753 acc=0.4317 | time=18.5s\n",
            "Epoch 009 | train_loss=1.0681 acc=0.4303 | val_loss=1.0748 acc=0.4317 | time=18.7s\n",
            "Epoch 010 | train_loss=1.0681 acc=0.4303 | val_loss=1.0744 acc=0.4317 | time=18.6s\n",
            "Epoch 011 | train_loss=1.0683 acc=0.4287 | val_loss=1.0745 acc=0.4317 | time=18.7s\n",
            "Epoch 012 | train_loss=1.0691 acc=0.4307 | val_loss=1.0744 acc=0.4317 | time=18.6s\n",
            "Epoch 013 | train_loss=1.0691 acc=0.4303 | val_loss=1.0743 acc=0.4317 | time=18.8s\n",
            "Epoch 014 | train_loss=1.0664 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.4s\n",
            "Epoch 015 | train_loss=1.0693 acc=0.4307 | val_loss=1.0743 acc=0.4317 | time=18.7s\n",
            "Epoch 016 | train_loss=1.0670 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.6s\n",
            "★ Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▅▇▆▇▇████▇█████</td></tr><tr><td>train_loss</td><td>█▆▃▄▃▄▃▂▂▂▂▃▃▁▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▃▁▁▁▁▃▂▁▂▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06698</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07439</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">royal-river-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/e7fmq45f' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/e7fmq45f</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_071904-e7fmq45f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 07:24:04,792] Trial 1 finished with value: 1.074192376363845 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 1 with value: 1.074192376363845.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_072404-7hdjuwxy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/7hdjuwxy' target=\"_blank\">dauntless-wave-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/7hdjuwxy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/7hdjuwxy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0759 acc=0.4198 | val_loss=1.0793 acc=0.4317 | time=17.2s\n",
            "Epoch 002 | train_loss=1.0741 acc=0.4276 | val_loss=1.0761 acc=0.4317 | time=17.3s\n",
            "Epoch 003 | train_loss=1.0706 acc=0.4299 | val_loss=1.0745 acc=0.4317 | time=17.0s\n",
            "Epoch 004 | train_loss=1.0700 acc=0.4315 | val_loss=1.0754 acc=0.4317 | time=17.1s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4280 | val_loss=1.0751 acc=0.4317 | time=17.1s\n",
            "Epoch 006 | train_loss=1.0681 acc=0.4268 | val_loss=1.0752 acc=0.4317 | time=16.9s\n",
            "Epoch 007 | train_loss=1.0683 acc=0.4315 | val_loss=1.0754 acc=0.4317 | time=17.1s\n",
            "Epoch 008 | train_loss=1.0698 acc=0.4307 | val_loss=1.0744 acc=0.4317 | time=17.3s\n",
            "Epoch 009 | train_loss=1.0683 acc=0.4311 | val_loss=1.0741 acc=0.4317 | time=16.9s\n",
            "Epoch 010 | train_loss=1.0654 acc=0.4353 | val_loss=1.0624 acc=0.4317 | time=17.2s\n",
            "Epoch 011 | train_loss=1.0033 acc=0.5565 | val_loss=0.9699 acc=0.5668 | time=17.2s\n",
            "Epoch 012 | train_loss=0.9549 acc=0.5763 | val_loss=0.9492 acc=0.5466 | time=17.0s\n",
            "Epoch 013 | train_loss=0.9374 acc=0.5845 | val_loss=0.9214 acc=0.5854 | time=17.1s\n",
            "Epoch 014 | train_loss=0.9106 acc=0.5992 | val_loss=0.9225 acc=0.5776 | time=17.3s\n",
            "Epoch 015 | train_loss=0.8781 acc=0.6148 | val_loss=0.8939 acc=0.5854 | time=17.3s\n",
            "Epoch 016 | train_loss=0.8679 acc=0.6252 | val_loss=0.8764 acc=0.6025 | time=17.1s\n",
            "Epoch 017 | train_loss=0.8049 acc=0.6517 | val_loss=0.8550 acc=0.6025 | time=17.0s\n",
            "Epoch 018 | train_loss=0.7766 acc=0.6621 | val_loss=0.9233 acc=0.6211 | time=17.0s\n",
            "Epoch 019 | train_loss=0.7473 acc=0.6850 | val_loss=0.9038 acc=0.6211 | time=17.2s\n",
            "Epoch 020 | train_loss=0.7265 acc=0.6940 | val_loss=0.8608 acc=0.6180 | time=17.0s\n",
            "Epoch 021 | train_loss=0.7120 acc=0.6847 | val_loss=0.8666 acc=0.6118 | time=16.9s\n",
            "Epoch 022 | train_loss=0.6861 acc=0.6971 | val_loss=0.8757 acc=0.6196 | time=17.0s\n",
            "Epoch 023 | train_loss=0.6628 acc=0.7126 | val_loss=0.9037 acc=0.6134 | time=17.3s\n",
            "Epoch 024 | train_loss=0.6273 acc=0.7285 | val_loss=0.8976 acc=0.6180 | time=17.0s\n",
            "Epoch 025 | train_loss=0.6067 acc=0.7348 | val_loss=0.8900 acc=0.6149 | time=17.2s\n",
            "Epoch 026 | train_loss=0.5810 acc=0.7480 | val_loss=0.8846 acc=0.6056 | time=17.1s\n",
            "Epoch 027 | train_loss=0.5900 acc=0.7480 | val_loss=0.9405 acc=0.6102 | time=16.9s\n",
            "★ Early stopping at epoch 27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▄▄▅▅▅▅▆▆▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>██████████▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▅▇▆▇▇▇████████▇█</td></tr><tr><td>validation_loss</td><td>█████████▇▅▄▃▃▂▂▁▃▃▁▁▂▃▂▂▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_accuracy</td><td>0.74796</td></tr><tr><td>train_loss</td><td>0.59003</td></tr><tr><td>validation_accuracy</td><td>0.61025</td></tr><tr><td>validation_loss</td><td>0.94048</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dauntless-wave-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/7hdjuwxy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8/runs/7hdjuwxy</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-8</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_072404-7hdjuwxy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 07:31:48,919] Trial 2 finished with value: 0.8549597462018331 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 2 with value: 0.8549597462018331.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.854960\n",
            "best_train_loss     = 0.804907\n",
            "best_train_accuracy = 0.6517\n",
            "best_val_accuracy   = 0.6025\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.0005\n",
            "  num_blocks: 1\n",
            "  num_heads: 2\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-3]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3, 4]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-9\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-9\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-9.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RMjpXtXWjr_f",
        "outputId": "4ff4476d-5afd-4ed1-d672-06ae96401668"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-02 07:46:37,065] A new study created in RDB with name: eeg_holdout_grid_search-9\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_074637-63q5m6fa</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/63q5m6fa' target=\"_blank\">usual-violet-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/63q5m6fa' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/63q5m6fa</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0747 acc=0.4252 | val_loss=1.0825 acc=0.4317 | time=21.8s\n",
            "Epoch 002 | train_loss=1.0718 acc=0.4283 | val_loss=1.0744 acc=0.4317 | time=21.7s\n",
            "Epoch 003 | train_loss=1.0690 acc=0.4353 | val_loss=1.0752 acc=0.4317 | time=21.8s\n",
            "Epoch 004 | train_loss=1.0689 acc=0.4350 | val_loss=1.0756 acc=0.4317 | time=21.7s\n",
            "Epoch 005 | train_loss=1.0711 acc=0.4338 | val_loss=1.0742 acc=0.4317 | time=21.7s\n",
            "Epoch 006 | train_loss=1.0685 acc=0.4280 | val_loss=1.0744 acc=0.4317 | time=21.6s\n",
            "Epoch 007 | train_loss=1.0698 acc=0.4299 | val_loss=1.0746 acc=0.4317 | time=21.7s\n",
            "Epoch 008 | train_loss=1.0661 acc=0.4315 | val_loss=1.0746 acc=0.4317 | time=21.7s\n",
            "Epoch 009 | train_loss=1.0704 acc=0.4315 | val_loss=1.0743 acc=0.4317 | time=21.6s\n",
            "Epoch 010 | train_loss=1.0673 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=21.7s\n",
            "Epoch 011 | train_loss=1.0680 acc=0.4307 | val_loss=1.0742 acc=0.4317 | time=22.0s\n",
            "Epoch 012 | train_loss=1.0673 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.6s\n",
            "Epoch 013 | train_loss=1.0669 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 014 | train_loss=1.0685 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=21.6s\n",
            "Epoch 015 | train_loss=1.0676 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=21.7s\n",
            "Epoch 016 | train_loss=1.0679 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=21.7s\n",
            "Epoch 017 | train_loss=1.0667 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=21.7s\n",
            "Epoch 018 | train_loss=1.0663 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=21.7s\n",
            "Epoch 019 | train_loss=1.0672 acc=0.4311 | val_loss=1.0739 acc=0.4317 | time=21.7s\n",
            "Epoch 020 | train_loss=1.0678 acc=0.4315 | val_loss=1.0745 acc=0.4317 | time=21.7s\n",
            "Epoch 021 | train_loss=1.0664 acc=0.4303 | val_loss=1.0746 acc=0.4317 | time=21.8s\n",
            "Epoch 022 | train_loss=1.0679 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=21.6s\n",
            "Epoch 023 | train_loss=1.0668 acc=0.4311 | val_loss=1.0739 acc=0.4317 | time=21.7s\n",
            "Epoch 024 | train_loss=1.0613 acc=0.4330 | val_loss=1.0619 acc=0.4317 | time=21.7s\n",
            "Epoch 025 | train_loss=1.0077 acc=0.5239 | val_loss=0.9701 acc=0.5730 | time=21.7s\n",
            "Epoch 026 | train_loss=0.9738 acc=0.5720 | val_loss=0.9674 acc=0.5668 | time=21.6s\n",
            "Epoch 027 | train_loss=0.9514 acc=0.5817 | val_loss=0.9528 acc=0.5714 | time=21.6s\n",
            "Epoch 028 | train_loss=0.9484 acc=0.5845 | val_loss=0.9568 acc=0.5699 | time=21.7s\n",
            "Epoch 029 | train_loss=0.9358 acc=0.5911 | val_loss=0.9591 acc=0.5745 | time=21.6s\n",
            "Epoch 030 | train_loss=0.9326 acc=0.5918 | val_loss=0.9489 acc=0.5745 | time=21.7s\n",
            "Epoch 031 | train_loss=0.9310 acc=0.5876 | val_loss=0.9501 acc=0.5730 | time=21.8s\n",
            "Epoch 032 | train_loss=0.9170 acc=0.5965 | val_loss=0.9477 acc=0.5761 | time=21.5s\n",
            "Epoch 033 | train_loss=0.9013 acc=0.6113 | val_loss=0.9335 acc=0.5932 | time=21.7s\n",
            "Epoch 034 | train_loss=0.8866 acc=0.6221 | val_loss=0.9426 acc=0.5901 | time=21.8s\n",
            "Epoch 035 | train_loss=0.8596 acc=0.6249 | val_loss=0.9178 acc=0.6071 | time=21.8s\n",
            "Epoch 036 | train_loss=0.8519 acc=0.6311 | val_loss=0.9159 acc=0.6118 | time=21.7s\n",
            "Epoch 037 | train_loss=0.8313 acc=0.6396 | val_loss=0.9576 acc=0.5947 | time=21.6s\n",
            "Epoch 038 | train_loss=0.8100 acc=0.6439 | val_loss=0.8887 acc=0.6242 | time=21.7s\n",
            "Epoch 039 | train_loss=0.8154 acc=0.6466 | val_loss=0.8864 acc=0.6149 | time=21.7s\n",
            "Epoch 040 | train_loss=0.7705 acc=0.6652 | val_loss=0.8672 acc=0.6180 | time=21.7s\n",
            "Epoch 041 | train_loss=0.7514 acc=0.6680 | val_loss=0.8593 acc=0.6304 | time=21.9s\n",
            "Epoch 042 | train_loss=0.7330 acc=0.6823 | val_loss=0.8346 acc=0.6289 | time=21.4s\n",
            "Epoch 043 | train_loss=0.7082 acc=0.6928 | val_loss=0.8840 acc=0.6227 | time=21.7s\n",
            "Epoch 044 | train_loss=0.6726 acc=0.7068 | val_loss=0.8740 acc=0.6491 | time=21.6s\n",
            "Epoch 045 | train_loss=0.6678 acc=0.7083 | val_loss=0.9907 acc=0.6258 | time=21.7s\n",
            "Epoch 046 | train_loss=0.6507 acc=0.7192 | val_loss=0.8970 acc=0.6661 | time=21.6s\n",
            "Epoch 047 | train_loss=0.6056 acc=0.7608 | val_loss=0.8294 acc=0.6724 | time=21.6s\n",
            "Epoch 048 | train_loss=0.5850 acc=0.7616 | val_loss=1.1485 acc=0.6491 | time=21.8s\n",
            "Epoch 049 | train_loss=0.5805 acc=0.7705 | val_loss=0.8033 acc=0.6739 | time=21.6s\n",
            "Epoch 050 | train_loss=0.5320 acc=0.7876 | val_loss=0.8072 acc=0.6988 | time=21.5s\n",
            "Epoch 051 | train_loss=0.5007 acc=0.7973 | val_loss=0.7783 acc=0.7081 | time=21.7s\n",
            "Epoch 052 | train_loss=0.4702 acc=0.8085 | val_loss=0.8756 acc=0.6863 | time=21.7s\n",
            "Epoch 053 | train_loss=0.5055 acc=0.7852 | val_loss=0.7734 acc=0.6941 | time=21.6s\n",
            "Epoch 054 | train_loss=0.4548 acc=0.8050 | val_loss=0.9861 acc=0.6786 | time=21.7s\n",
            "Epoch 055 | train_loss=0.4353 acc=0.8186 | val_loss=0.9007 acc=0.7158 | time=21.6s\n",
            "Epoch 056 | train_loss=0.3990 acc=0.8342 | val_loss=0.8175 acc=0.6972 | time=21.7s\n",
            "Epoch 057 | train_loss=0.3843 acc=0.8528 | val_loss=0.8196 acc=0.6739 | time=21.7s\n",
            "Epoch 058 | train_loss=0.3643 acc=0.8501 | val_loss=1.1802 acc=0.6863 | time=21.8s\n",
            "Epoch 059 | train_loss=0.3589 acc=0.8617 | val_loss=0.8458 acc=0.7034 | time=21.7s\n",
            "Epoch 060 | train_loss=0.2984 acc=0.8913 | val_loss=0.9472 acc=0.7112 | time=21.6s\n",
            "Epoch 061 | train_loss=0.2711 acc=0.9021 | val_loss=0.8940 acc=0.7345 | time=21.8s\n",
            "Epoch 062 | train_loss=0.2462 acc=0.9087 | val_loss=0.8881 acc=0.7081 | time=21.6s\n",
            "Epoch 063 | train_loss=0.2377 acc=0.9150 | val_loss=1.0963 acc=0.7174 | time=21.6s\n",
            "★ Early stopping at epoch 63\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▄▄▄▄▅▅▅▆▆▆▆▆▇▇███</td></tr><tr><td>train_loss</td><td>█████████████████▇▇▇▇▇▇▆▆▆▅▅▅▅▅▄▄▄▄▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▅▄▅▅▅▆▆▆▆▆▇▇▇█▇▇██▇█</td></tr><tr><td>validation_loss</td><td>▆▆▆▆▆▆▆▆▆▆▆▆▆▆▆▄▄▄▄▄▃▃▃▃▃▂▃▃▅▃▂▁▃▁▅▂█▂▄▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>63</td></tr><tr><td>train_accuracy</td><td>0.91495</td></tr><tr><td>train_loss</td><td>0.23775</td></tr><tr><td>validation_accuracy</td><td>0.71739</td></tr><tr><td>validation_loss</td><td>1.09627</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">usual-violet-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/63q5m6fa' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/63q5m6fa</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_074637-63q5m6fa/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-02 08:09:26,020] Trial 0 finished with value: 0.7734349171320597 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5}. Best is trial 0 with value: 0.7734349171320597.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_080926-ximdyapw</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/ximdyapw' target=\"_blank\">fluent-butterfly-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/ximdyapw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/ximdyapw</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0745 acc=0.4249 | val_loss=1.0752 acc=0.4317 | time=18.8s\n",
            "Epoch 002 | train_loss=1.0752 acc=0.4214 | val_loss=1.0750 acc=0.4317 | time=18.5s\n",
            "Epoch 003 | train_loss=1.0721 acc=0.4283 | val_loss=1.0749 acc=0.4317 | time=18.7s\n",
            "Epoch 004 | train_loss=1.0711 acc=0.4249 | val_loss=1.0751 acc=0.4317 | time=18.6s\n",
            "Epoch 005 | train_loss=1.0708 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.8s\n",
            "Epoch 006 | train_loss=1.0687 acc=0.4330 | val_loss=1.0757 acc=0.4317 | time=18.6s\n",
            "Epoch 007 | train_loss=1.0699 acc=0.4268 | val_loss=1.0743 acc=0.4317 | time=18.7s\n",
            "Epoch 008 | train_loss=1.0691 acc=0.4256 | val_loss=1.0781 acc=0.4317 | time=18.6s\n",
            "Epoch 009 | train_loss=1.0703 acc=0.4303 | val_loss=1.0740 acc=0.4317 | time=18.6s\n",
            "Epoch 010 | train_loss=1.0674 acc=0.4334 | val_loss=1.0746 acc=0.4317 | time=18.8s\n",
            "Epoch 011 | train_loss=1.0630 acc=0.4365 | val_loss=1.0561 acc=0.5543 | time=18.7s\n",
            "Epoch 012 | train_loss=0.9997 acc=0.5581 | val_loss=0.9775 acc=0.5683 | time=18.7s\n",
            "Epoch 013 | train_loss=0.9494 acc=0.5814 | val_loss=0.9750 acc=0.5419 | time=18.6s\n",
            "Epoch 014 | train_loss=0.9319 acc=0.5965 | val_loss=0.9403 acc=0.5839 | time=18.8s\n",
            "Epoch 015 | train_loss=0.8969 acc=0.6190 | val_loss=0.9118 acc=0.6071 | time=18.5s\n",
            "Epoch 016 | train_loss=0.8836 acc=0.6217 | val_loss=0.9063 acc=0.5978 | time=18.6s\n",
            "Epoch 017 | train_loss=0.8653 acc=0.6237 | val_loss=0.9112 acc=0.6134 | time=18.7s\n",
            "Epoch 018 | train_loss=0.8186 acc=0.6400 | val_loss=0.8729 acc=0.6165 | time=18.6s\n",
            "Epoch 019 | train_loss=0.8067 acc=0.6485 | val_loss=0.9288 acc=0.5823 | time=18.6s\n",
            "Epoch 020 | train_loss=0.7783 acc=0.6633 | val_loss=0.8359 acc=0.6196 | time=18.7s\n",
            "Epoch 021 | train_loss=0.7418 acc=0.6924 | val_loss=0.8239 acc=0.6289 | time=18.8s\n",
            "Epoch 022 | train_loss=0.6988 acc=0.6951 | val_loss=0.8376 acc=0.6180 | time=18.7s\n",
            "Epoch 023 | train_loss=0.6900 acc=0.7134 | val_loss=0.8157 acc=0.6522 | time=18.7s\n",
            "Epoch 024 | train_loss=0.6526 acc=0.7285 | val_loss=0.8591 acc=0.6320 | time=18.6s\n",
            "Epoch 025 | train_loss=0.5988 acc=0.7445 | val_loss=0.8755 acc=0.6568 | time=18.7s\n",
            "Epoch 026 | train_loss=0.5899 acc=0.7565 | val_loss=0.7855 acc=0.6615 | time=18.6s\n",
            "Epoch 027 | train_loss=0.5417 acc=0.7728 | val_loss=0.9271 acc=0.6366 | time=18.9s\n",
            "Epoch 028 | train_loss=0.5012 acc=0.7992 | val_loss=0.7810 acc=0.6491 | time=18.7s\n",
            "Epoch 029 | train_loss=0.4728 acc=0.8179 | val_loss=1.2075 acc=0.6568 | time=18.8s\n",
            "Epoch 030 | train_loss=0.4680 acc=0.8190 | val_loss=1.5382 acc=0.6444 | time=18.7s\n",
            "Epoch 031 | train_loss=0.3980 acc=0.8369 | val_loss=1.0578 acc=0.6553 | time=18.8s\n",
            "Epoch 032 | train_loss=0.3795 acc=0.8532 | val_loss=1.2336 acc=0.6537 | time=18.8s\n",
            "Epoch 033 | train_loss=0.3616 acc=0.8614 | val_loss=1.0947 acc=0.6320 | time=18.8s\n",
            "Epoch 034 | train_loss=0.2992 acc=0.8839 | val_loss=1.1842 acc=0.6351 | time=18.6s\n",
            "Epoch 035 | train_loss=0.2582 acc=0.9052 | val_loss=1.2934 acc=0.6537 | time=18.7s\n",
            "Epoch 036 | train_loss=0.2069 acc=0.9250 | val_loss=1.4466 acc=0.6724 | time=18.7s\n",
            "Epoch 037 | train_loss=0.2143 acc=0.9243 | val_loss=1.5645 acc=0.6630 | time=18.7s\n",
            "Epoch 038 | train_loss=0.1639 acc=0.9386 | val_loss=1.8001 acc=0.6491 | time=18.7s\n",
            "★ Early stopping at epoch 38\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇████</td></tr><tr><td>train_loss</td><td>███████████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▅▅▄▅▆▆▆▆▅▆▇▆▇▇██▇▇█▇█▇▇▇▇██▇</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▂▂▁▂▁▄▆▃▄▃▄▅▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_accuracy</td><td>0.93864</td></tr><tr><td>train_loss</td><td>0.16386</td></tr><tr><td>validation_accuracy</td><td>0.64907</td></tr><tr><td>validation_loss</td><td>1.80009</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fluent-butterfly-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/ximdyapw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/ximdyapw</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_080926-ximdyapw/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-02 08:21:18,935] Trial 1 finished with value: 0.7810127351965223 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 0.7734349171320597.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_082119-wqjw6szb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/wqjw6szb' target=\"_blank\">devout-thunder-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/wqjw6szb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/wqjw6szb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0776 acc=0.3895 | val_loss=1.0771 acc=0.4317 | time=17.3s\n",
            "Epoch 002 | train_loss=1.0716 acc=0.4260 | val_loss=1.0753 acc=0.4317 | time=17.4s\n",
            "Epoch 003 | train_loss=1.0715 acc=0.4241 | val_loss=1.0757 acc=0.4317 | time=17.3s\n",
            "Epoch 004 | train_loss=1.0728 acc=0.4190 | val_loss=1.0754 acc=0.4317 | time=17.1s\n",
            "Epoch 005 | train_loss=1.0718 acc=0.4225 | val_loss=1.0742 acc=0.4317 | time=17.0s\n",
            "Epoch 006 | train_loss=1.0711 acc=0.4233 | val_loss=1.0742 acc=0.4317 | time=17.5s\n",
            "Epoch 007 | train_loss=1.0675 acc=0.4307 | val_loss=1.0739 acc=0.4317 | time=17.2s\n",
            "Epoch 008 | train_loss=1.0704 acc=0.4225 | val_loss=1.0780 acc=0.4317 | time=17.1s\n",
            "Epoch 009 | train_loss=1.0705 acc=0.4276 | val_loss=1.0719 acc=0.4317 | time=17.2s\n",
            "Epoch 010 | train_loss=1.0512 acc=0.4505 | val_loss=1.0504 acc=0.4984 | time=17.1s\n",
            "Epoch 011 | train_loss=0.9871 acc=0.5534 | val_loss=0.9778 acc=0.5683 | time=17.0s\n",
            "Epoch 012 | train_loss=0.9660 acc=0.5748 | val_loss=0.9552 acc=0.5683 | time=17.3s\n",
            "Epoch 013 | train_loss=0.9346 acc=0.5880 | val_loss=0.9572 acc=0.5543 | time=16.8s\n",
            "Epoch 014 | train_loss=0.9238 acc=0.5973 | val_loss=0.9509 acc=0.5916 | time=17.1s\n",
            "Epoch 015 | train_loss=0.8897 acc=0.6140 | val_loss=0.9187 acc=0.5839 | time=17.3s\n",
            "Epoch 016 | train_loss=0.8607 acc=0.6276 | val_loss=0.9355 acc=0.6071 | time=16.8s\n",
            "Epoch 017 | train_loss=0.8208 acc=0.6431 | val_loss=0.9029 acc=0.6118 | time=17.1s\n",
            "Epoch 018 | train_loss=0.8108 acc=0.6400 | val_loss=0.9171 acc=0.6071 | time=17.1s\n",
            "Epoch 019 | train_loss=0.7827 acc=0.6470 | val_loss=0.8866 acc=0.5994 | time=17.4s\n",
            "Epoch 020 | train_loss=0.7682 acc=0.6548 | val_loss=0.8837 acc=0.6009 | time=17.3s\n",
            "Epoch 021 | train_loss=0.7372 acc=0.6695 | val_loss=0.9139 acc=0.6134 | time=17.1s\n",
            "Epoch 022 | train_loss=0.7030 acc=0.6777 | val_loss=0.8971 acc=0.6289 | time=17.1s\n",
            "Epoch 023 | train_loss=0.6803 acc=0.6878 | val_loss=0.9671 acc=0.6304 | time=17.3s\n",
            "Epoch 024 | train_loss=0.6411 acc=0.7118 | val_loss=0.9249 acc=0.6413 | time=17.2s\n",
            "Epoch 025 | train_loss=0.6023 acc=0.7297 | val_loss=0.9198 acc=0.6460 | time=17.3s\n",
            "Epoch 026 | train_loss=0.5579 acc=0.7542 | val_loss=0.8947 acc=0.6460 | time=17.4s\n",
            "Epoch 027 | train_loss=0.4995 acc=0.7779 | val_loss=1.0342 acc=0.6724 | time=17.1s\n",
            "Epoch 028 | train_loss=0.4794 acc=0.7895 | val_loss=0.9695 acc=0.6568 | time=17.0s\n",
            "Epoch 029 | train_loss=0.4517 acc=0.8050 | val_loss=0.9263 acc=0.6739 | time=17.2s\n",
            "Epoch 030 | train_loss=0.4494 acc=0.7973 | val_loss=0.9694 acc=0.6755 | time=16.9s\n",
            "★ Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▂▁▂▂▂▂▂▂▄▄▄▅▅▅▅▅▅▅▆▆▆▆▇▇████</td></tr><tr><td>train_loss</td><td>██████████▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▃▅▅▅▆▅▆▆▆▆▆▆▇▇▇▇▇█▇██</td></tr><tr><td>validation_loss</td><td>█████████▇▄▄▄▃▂▃▂▂▁▁▂▁▄▂▂▁▆▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.79728</td></tr><tr><td>train_loss</td><td>0.44936</td></tr><tr><td>validation_accuracy</td><td>0.67547</td></tr><tr><td>validation_loss</td><td>0.96939</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-thunder-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/wqjw6szb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9/runs/wqjw6szb</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_082119-wqjw6szb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 08:30:01,256] Trial 2 finished with value: 0.8837438935325259 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 0 with value: 0.7734349171320597.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.773435\n",
            "best_train_loss     = 0.505497\n",
            "best_train_accuracy = 0.7852\n",
            "best_val_accuracy   = 0.6941\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.005\n",
            "  num_blocks: 1\n",
            "  num_heads: 4\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-2, 5e-3]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-10\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-10\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-10.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "oXXMYTw9jsBu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "fabd7333-0cfc-4448-9ca2-b006ac4754f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 09:27:06,085] A new study created in RDB with name: eeg_holdout_grid_search-10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_092706-6yp0mlxx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6yp0mlxx' target=\"_blank\">olive-resonance-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6yp0mlxx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6yp0mlxx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-02, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1809 acc=0.3662 | val_loss=1.0806 acc=0.4317 | time=19.8s\n",
            "Epoch 002 | train_loss=1.1256 acc=0.3895 | val_loss=1.0773 acc=0.4317 | time=19.4s\n",
            "Epoch 003 | train_loss=1.1110 acc=0.3880 | val_loss=1.0810 acc=0.4317 | time=19.7s\n",
            "Epoch 004 | train_loss=1.0968 acc=0.4062 | val_loss=1.0821 acc=0.4317 | time=19.5s\n",
            "Epoch 005 | train_loss=1.0939 acc=0.3973 | val_loss=1.0792 acc=0.4317 | time=19.7s\n",
            "Epoch 006 | train_loss=1.0886 acc=0.4058 | val_loss=1.0796 acc=0.4317 | time=19.6s\n",
            "Epoch 007 | train_loss=1.0853 acc=0.4066 | val_loss=1.0749 acc=0.4317 | time=19.6s\n",
            "Epoch 008 | train_loss=1.0801 acc=0.4093 | val_loss=1.0729 acc=0.4317 | time=19.6s\n",
            "Epoch 009 | train_loss=1.0767 acc=0.4078 | val_loss=1.0753 acc=0.4317 | time=19.5s\n",
            "Epoch 010 | train_loss=1.0774 acc=0.4082 | val_loss=1.0632 acc=0.4317 | time=19.5s\n",
            "Epoch 011 | train_loss=1.0674 acc=0.4299 | val_loss=1.0396 acc=0.5450 | time=19.6s\n",
            "Epoch 012 | train_loss=1.0350 acc=0.4882 | val_loss=1.0220 acc=0.4845 | time=19.6s\n",
            "Epoch 013 | train_loss=0.9640 acc=0.5515 | val_loss=0.9692 acc=0.5233 | time=19.7s\n",
            "Epoch 014 | train_loss=0.9312 acc=0.5705 | val_loss=0.8765 acc=0.6211 | time=19.8s\n",
            "Epoch 015 | train_loss=0.8915 acc=0.5841 | val_loss=0.8130 acc=0.6693 | time=19.5s\n",
            "Epoch 016 | train_loss=0.8813 acc=0.5961 | val_loss=0.8113 acc=0.6553 | time=19.8s\n",
            "Epoch 017 | train_loss=0.8513 acc=0.5887 | val_loss=0.7715 acc=0.6801 | time=19.4s\n",
            "Epoch 018 | train_loss=0.8379 acc=0.6023 | val_loss=0.7959 acc=0.6770 | time=19.6s\n",
            "Epoch 019 | train_loss=0.8155 acc=0.6326 | val_loss=0.7249 acc=0.6910 | time=19.6s\n",
            "Epoch 020 | train_loss=0.7861 acc=0.6318 | val_loss=0.7592 acc=0.6863 | time=19.7s\n",
            "Epoch 021 | train_loss=0.7656 acc=0.6567 | val_loss=0.7303 acc=0.7360 | time=19.6s\n",
            "Epoch 022 | train_loss=0.7719 acc=0.6330 | val_loss=0.7352 acc=0.7345 | time=19.7s\n",
            "Epoch 023 | train_loss=0.7518 acc=0.6474 | val_loss=0.6729 acc=0.7081 | time=19.5s\n",
            "Epoch 024 | train_loss=0.7460 acc=0.6454 | val_loss=0.8162 acc=0.7019 | time=19.6s\n",
            "Epoch 025 | train_loss=0.7362 acc=0.6629 | val_loss=0.8913 acc=0.6429 | time=19.5s\n",
            "Epoch 026 | train_loss=0.7144 acc=0.6703 | val_loss=0.7394 acc=0.6832 | time=19.6s\n",
            "Epoch 027 | train_loss=0.7102 acc=0.6715 | val_loss=0.7656 acc=0.7081 | time=19.5s\n",
            "Epoch 028 | train_loss=0.6629 acc=0.6932 | val_loss=0.8189 acc=0.6988 | time=19.5s\n",
            "Epoch 029 | train_loss=0.6770 acc=0.7103 | val_loss=1.0335 acc=0.6568 | time=19.5s\n",
            "Epoch 030 | train_loss=0.6531 acc=0.6967 | val_loss=0.7993 acc=0.6351 | time=19.6s\n",
            "Epoch 031 | train_loss=0.6502 acc=0.7091 | val_loss=0.9228 acc=0.6646 | time=19.6s\n",
            "Epoch 032 | train_loss=0.6237 acc=0.7227 | val_loss=0.8547 acc=0.6755 | time=19.8s\n",
            "Epoch 033 | train_loss=0.6239 acc=0.7134 | val_loss=0.7882 acc=0.6460 | time=19.8s\n",
            "★ Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▅▅▅▆▅▆▆▆▇▆▇▆▇▇▇▇█▇███</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▇▆▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▄▂▃▅▆▆▇▇▇▇██▇▇▆▇▇▇▆▆▆▇▆</td></tr><tr><td>validation_loss</td><td>██████████▇▇▆▄▃▃▃▃▂▂▂▂▁▃▅▂▃▃▇▃▅▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_accuracy</td><td>0.7134</td></tr><tr><td>train_loss</td><td>0.6239</td></tr><tr><td>validation_accuracy</td><td>0.64596</td></tr><tr><td>validation_loss</td><td>0.78822</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-resonance-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6yp0mlxx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6yp0mlxx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_092706-6yp0mlxx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 09:37:55,864] Trial 0 finished with value: 0.672920666989826 and parameters: {'lr': 0.0005, 'weight_decay': 0.05, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 0.672920666989826.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_093756-6kv4txmh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6kv4txmh' target=\"_blank\">daily-dragon-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6kv4txmh' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6kv4txmh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1699 acc=0.3674 | val_loss=1.0817 acc=0.4317 | time=19.7s\n",
            "Epoch 002 | train_loss=1.1243 acc=0.3860 | val_loss=1.0750 acc=0.4317 | time=19.7s\n",
            "Epoch 003 | train_loss=1.1024 acc=0.3868 | val_loss=1.0765 acc=0.4317 | time=19.7s\n",
            "Epoch 004 | train_loss=1.0980 acc=0.3872 | val_loss=1.0717 acc=0.4317 | time=19.8s\n",
            "Epoch 005 | train_loss=1.0787 acc=0.4179 | val_loss=1.0380 acc=0.5217 | time=19.7s\n",
            "Epoch 006 | train_loss=1.0567 acc=0.4489 | val_loss=0.9920 acc=0.5870 | time=19.8s\n",
            "Epoch 007 | train_loss=1.0064 acc=0.5126 | val_loss=0.9131 acc=0.6025 | time=19.6s\n",
            "Epoch 008 | train_loss=0.9598 acc=0.5546 | val_loss=0.8720 acc=0.6382 | time=19.6s\n",
            "Epoch 009 | train_loss=0.9216 acc=0.5732 | val_loss=0.8418 acc=0.6708 | time=19.6s\n",
            "Epoch 010 | train_loss=0.9104 acc=0.5930 | val_loss=0.8247 acc=0.6661 | time=19.7s\n",
            "Epoch 011 | train_loss=0.8814 acc=0.5860 | val_loss=0.7986 acc=0.6957 | time=19.5s\n",
            "Epoch 012 | train_loss=0.8566 acc=0.6058 | val_loss=0.7543 acc=0.7220 | time=19.7s\n",
            "Epoch 013 | train_loss=0.8449 acc=0.6128 | val_loss=0.7462 acc=0.7174 | time=19.6s\n",
            "Epoch 014 | train_loss=0.8066 acc=0.6318 | val_loss=0.7458 acc=0.7360 | time=19.7s\n",
            "Epoch 015 | train_loss=0.7978 acc=0.6342 | val_loss=0.8010 acc=0.6366 | time=19.8s\n",
            "Epoch 016 | train_loss=0.7878 acc=0.6264 | val_loss=0.7227 acc=0.7112 | time=19.5s\n",
            "Epoch 017 | train_loss=0.7742 acc=0.6377 | val_loss=0.7698 acc=0.6724 | time=19.7s\n",
            "Epoch 018 | train_loss=0.7762 acc=0.6404 | val_loss=0.7497 acc=0.7034 | time=19.4s\n",
            "Epoch 019 | train_loss=0.7585 acc=0.6497 | val_loss=0.7307 acc=0.6770 | time=19.5s\n",
            "Epoch 020 | train_loss=0.7401 acc=0.6548 | val_loss=0.7290 acc=0.6879 | time=19.6s\n",
            "Epoch 021 | train_loss=0.7350 acc=0.6606 | val_loss=0.7651 acc=0.6615 | time=19.5s\n",
            "Epoch 022 | train_loss=0.7087 acc=0.6703 | val_loss=0.7713 acc=0.6335 | time=19.4s\n",
            "Epoch 023 | train_loss=0.6887 acc=0.6757 | val_loss=0.7206 acc=0.6677 | time=19.7s\n",
            "Epoch 024 | train_loss=0.6904 acc=0.6920 | val_loss=0.6840 acc=0.6848 | time=19.3s\n",
            "Epoch 025 | train_loss=0.6605 acc=0.6901 | val_loss=0.7252 acc=0.6677 | time=19.4s\n",
            "Epoch 026 | train_loss=0.6752 acc=0.6885 | val_loss=0.7134 acc=0.7127 | time=19.4s\n",
            "Epoch 027 | train_loss=0.6741 acc=0.6862 | val_loss=0.6642 acc=0.7003 | time=19.4s\n",
            "Epoch 028 | train_loss=0.6229 acc=0.7118 | val_loss=0.7095 acc=0.7112 | time=19.4s\n",
            "Epoch 029 | train_loss=0.6155 acc=0.7142 | val_loss=0.7503 acc=0.6972 | time=19.4s\n",
            "Epoch 030 | train_loss=0.6335 acc=0.7045 | val_loss=0.7833 acc=0.6444 | time=19.5s\n",
            "Epoch 031 | train_loss=0.6284 acc=0.7161 | val_loss=0.8469 acc=0.6786 | time=19.4s\n",
            "Epoch 032 | train_loss=0.6200 acc=0.7223 | val_loss=0.7179 acc=0.7034 | time=19.5s\n",
            "Epoch 033 | train_loss=0.6293 acc=0.7103 | val_loss=0.7872 acc=0.7065 | time=19.5s\n",
            "Epoch 034 | train_loss=0.5905 acc=0.7247 | val_loss=0.7095 acc=0.7252 | time=19.5s\n",
            "Epoch 035 | train_loss=0.5758 acc=0.7379 | val_loss=0.7248 acc=0.7096 | time=19.4s\n",
            "Epoch 036 | train_loss=0.5638 acc=0.7499 | val_loss=0.7544 acc=0.7034 | time=19.6s\n",
            "Epoch 037 | train_loss=0.5626 acc=0.7421 | val_loss=0.7662 acc=0.7081 | time=19.4s\n",
            "★ Early stopping at epoch 37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▂▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▃▅▅▆▇▆▇███▆▇▇▇▇▇▆▆▆▇▆▇▇▇▇▆▇▇▇█▇▇▇</td></tr><tr><td>validation_loss</td><td>████▇▆▅▄▄▄▃▃▂▂▃▂▃▂▂▂▃▃▂▁▂▂▁▂▂▃▄▂▃▂▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_accuracy</td><td>0.74214</td></tr><tr><td>train_loss</td><td>0.5626</td></tr><tr><td>validation_accuracy</td><td>0.70807</td></tr><tr><td>validation_loss</td><td>0.76616</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">daily-dragon-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6kv4txmh' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/6kv4txmh</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_093756-6kv4txmh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 09:50:02,248] Trial 1 finished with value: 0.6642108644757952 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 1 with value: 0.6642108644757952.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_095002-owje7peq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/owje7peq' target=\"_blank\">fiery-wildflower-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/owje7peq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/owje7peq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1348 acc=0.3786 | val_loss=1.0839 acc=0.4317 | time=17.5s\n",
            "Epoch 002 | train_loss=1.1228 acc=0.4039 | val_loss=1.0790 acc=0.4317 | time=17.6s\n",
            "Epoch 003 | train_loss=1.1083 acc=0.3996 | val_loss=1.0784 acc=0.4317 | time=18.0s\n",
            "Epoch 004 | train_loss=1.1000 acc=0.3918 | val_loss=1.0705 acc=0.5233 | time=17.7s\n",
            "Epoch 005 | train_loss=1.0829 acc=0.4190 | val_loss=1.0476 acc=0.4876 | time=17.2s\n",
            "Epoch 006 | train_loss=1.0358 acc=0.4812 | val_loss=0.9707 acc=0.5807 | time=17.3s\n",
            "Epoch 007 | train_loss=1.0025 acc=0.5200 | val_loss=0.8998 acc=0.6180 | time=17.3s\n",
            "Epoch 008 | train_loss=0.9618 acc=0.5612 | val_loss=0.9030 acc=0.6149 | time=17.2s\n",
            "Epoch 009 | train_loss=0.9150 acc=0.5670 | val_loss=0.8771 acc=0.6227 | time=17.4s\n",
            "Epoch 010 | train_loss=0.9103 acc=0.5709 | val_loss=0.8627 acc=0.6102 | time=17.5s\n",
            "Epoch 011 | train_loss=0.8859 acc=0.5880 | val_loss=0.8849 acc=0.5761 | time=17.3s\n",
            "Epoch 012 | train_loss=0.8494 acc=0.5988 | val_loss=0.8235 acc=0.6708 | time=17.4s\n",
            "Epoch 013 | train_loss=0.8465 acc=0.5926 | val_loss=0.8105 acc=0.6366 | time=17.4s\n",
            "Epoch 014 | train_loss=0.7985 acc=0.6229 | val_loss=0.8127 acc=0.6165 | time=17.3s\n",
            "Epoch 015 | train_loss=0.8115 acc=0.6093 | val_loss=0.7700 acc=0.6537 | time=17.5s\n",
            "Epoch 016 | train_loss=0.7862 acc=0.6287 | val_loss=0.7074 acc=0.7050 | time=17.2s\n",
            "Epoch 017 | train_loss=0.7661 acc=0.6412 | val_loss=0.8012 acc=0.6398 | time=17.4s\n",
            "Epoch 018 | train_loss=0.7506 acc=0.6353 | val_loss=0.7488 acc=0.6599 | time=17.5s\n",
            "Epoch 019 | train_loss=0.7437 acc=0.6509 | val_loss=0.7818 acc=0.6615 | time=17.0s\n",
            "Epoch 020 | train_loss=0.7550 acc=0.6357 | val_loss=1.0279 acc=0.6258 | time=17.3s\n",
            "Epoch 021 | train_loss=0.6961 acc=0.6676 | val_loss=0.8349 acc=0.6475 | time=17.4s\n",
            "Epoch 022 | train_loss=0.7238 acc=0.6637 | val_loss=0.9907 acc=0.5388 | time=17.2s\n",
            "Epoch 023 | train_loss=0.7040 acc=0.6699 | val_loss=0.9449 acc=0.6537 | time=17.2s\n",
            "Epoch 024 | train_loss=0.6558 acc=0.6971 | val_loss=0.9138 acc=0.6537 | time=17.6s\n",
            "Epoch 025 | train_loss=0.6663 acc=0.6870 | val_loss=0.9322 acc=0.6289 | time=17.1s\n",
            "Epoch 026 | train_loss=0.6553 acc=0.7006 | val_loss=0.9705 acc=0.6444 | time=17.3s\n",
            "★ Early stopping at epoch 26\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▁▁▂▃▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>███▇▇▇▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▃▂▅▆▆▆▆▅▇▆▆▇█▆▇▇▆▇▄▇▇▆▆</td></tr><tr><td>validation_loss</td><td>████▇▆▅▅▄▄▄▃▃▃▂▁▃▂▂▇▃▆▅▅▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>train_accuracy</td><td>0.70058</td></tr><tr><td>train_loss</td><td>0.65527</td></tr><tr><td>validation_accuracy</td><td>0.64441</td></tr><tr><td>validation_loss</td><td>0.97048</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fiery-wildflower-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/owje7peq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/owje7peq</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_095002-owje7peq/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 09:57:36,608] Trial 2 finished with value: 0.7074131766955057 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 1 with value: 0.6642108644757952.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_095736-0msgh13i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/0msgh13i' target=\"_blank\">smooth-sea-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/0msgh13i' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/0msgh13i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.00e-04, wd=5.00e-02, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1706 acc=0.3825 | val_loss=1.0724 acc=0.4317 | time=17.3s\n",
            "Epoch 002 | train_loss=1.1569 acc=0.3581 | val_loss=1.0840 acc=0.4767 | time=17.5s\n",
            "Epoch 003 | train_loss=1.1227 acc=0.3977 | val_loss=1.0793 acc=0.4317 | time=17.3s\n",
            "Epoch 004 | train_loss=1.1076 acc=0.3872 | val_loss=1.0732 acc=0.4317 | time=17.4s\n",
            "Epoch 005 | train_loss=1.0906 acc=0.3977 | val_loss=1.0726 acc=0.4317 | time=17.3s\n",
            "Epoch 006 | train_loss=1.0905 acc=0.3984 | val_loss=1.0611 acc=0.4581 | time=17.5s\n",
            "Epoch 007 | train_loss=1.0593 acc=0.4505 | val_loss=1.0015 acc=0.5870 | time=17.0s\n",
            "Epoch 008 | train_loss=0.9978 acc=0.5227 | val_loss=0.9774 acc=0.6009 | time=17.7s\n",
            "Epoch 009 | train_loss=0.9568 acc=0.5550 | val_loss=0.8785 acc=0.6289 | time=17.4s\n",
            "Epoch 010 | train_loss=0.9202 acc=0.5685 | val_loss=0.8944 acc=0.6382 | time=17.2s\n",
            "Epoch 011 | train_loss=0.9090 acc=0.5724 | val_loss=0.8257 acc=0.6366 | time=17.1s\n",
            "Epoch 012 | train_loss=0.8649 acc=0.5930 | val_loss=0.8309 acc=0.6134 | time=17.6s\n",
            "Epoch 013 | train_loss=0.8455 acc=0.5860 | val_loss=0.9548 acc=0.5342 | time=17.3s\n",
            "Epoch 014 | train_loss=0.8438 acc=0.5891 | val_loss=0.8586 acc=0.6087 | time=17.3s\n",
            "Epoch 015 | train_loss=0.8196 acc=0.5899 | val_loss=0.7748 acc=0.6475 | time=17.4s\n",
            "Epoch 016 | train_loss=0.7987 acc=0.6070 | val_loss=0.7480 acc=0.6630 | time=17.2s\n",
            "Epoch 017 | train_loss=0.8099 acc=0.5922 | val_loss=0.7471 acc=0.6615 | time=17.4s\n",
            "Epoch 018 | train_loss=0.7645 acc=0.6179 | val_loss=0.8908 acc=0.5947 | time=17.3s\n",
            "Epoch 019 | train_loss=0.7476 acc=0.6346 | val_loss=0.7359 acc=0.6739 | time=17.6s\n",
            "Epoch 020 | train_loss=0.7566 acc=0.6287 | val_loss=0.7910 acc=0.6553 | time=17.8s\n",
            "Epoch 021 | train_loss=0.7537 acc=0.6322 | val_loss=0.7258 acc=0.6894 | time=18.1s\n",
            "Epoch 022 | train_loss=0.7321 acc=0.6357 | val_loss=0.7539 acc=0.6677 | time=17.2s\n",
            "Epoch 023 | train_loss=0.7301 acc=0.6478 | val_loss=0.9632 acc=0.6149 | time=17.5s\n",
            "Epoch 024 | train_loss=0.7022 acc=0.6532 | val_loss=0.7949 acc=0.6786 | time=17.8s\n",
            "Epoch 025 | train_loss=0.7239 acc=0.6540 | val_loss=0.7706 acc=0.6708 | time=17.6s\n",
            "Epoch 026 | train_loss=0.7090 acc=0.6621 | val_loss=0.8468 acc=0.6770 | time=17.7s\n",
            "Epoch 027 | train_loss=0.6863 acc=0.6753 | val_loss=0.7243 acc=0.6925 | time=16.9s\n",
            "Epoch 028 | train_loss=0.6613 acc=0.6734 | val_loss=0.8718 acc=0.6661 | time=17.3s\n",
            "Epoch 029 | train_loss=0.6370 acc=0.6835 | val_loss=1.0178 acc=0.6630 | time=18.0s\n",
            "Epoch 030 | train_loss=0.6639 acc=0.6971 | val_loss=0.9711 acc=0.5575 | time=17.7s\n",
            "Epoch 031 | train_loss=0.6623 acc=0.6761 | val_loss=1.0319 acc=0.6957 | time=17.2s\n",
            "Epoch 032 | train_loss=0.6539 acc=0.6854 | val_loss=1.0628 acc=0.6351 | time=17.3s\n",
            "Epoch 033 | train_loss=0.6619 acc=0.6843 | val_loss=1.1917 acc=0.6025 | time=17.3s\n",
            "Epoch 034 | train_loss=0.6146 acc=0.7153 | val_loss=1.0128 acc=0.6382 | time=17.1s\n",
            "Epoch 035 | train_loss=0.6008 acc=0.7052 | val_loss=1.2895 acc=0.5637 | time=17.3s\n",
            "Epoch 036 | train_loss=0.5971 acc=0.7173 | val_loss=1.0853 acc=0.6537 | time=17.3s\n",
            "Epoch 037 | train_loss=0.5715 acc=0.7340 | val_loss=1.0165 acc=0.6661 | time=17.4s\n",
            "★ Early stopping at epoch 37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▂▂▃▄▅▅▅▅▅▅▅▆▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇█▇██</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▆▆▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▁▁▂▅▅▆▆▆▆▄▆▇▇▇▅▇▇█▇▆█▇██▇▇▄█▆▆▆▅▇▇</td></tr><tr><td>validation_loss</td><td>▅▅▅▅▅▅▄▄▃▃▂▂▄▃▂▁▁▃▁▂▁▁▄▂▂▃▁▃▅▄▅▅▇▅█▅▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_accuracy</td><td>0.73398</td></tr><tr><td>train_loss</td><td>0.5715</td></tr><tr><td>validation_accuracy</td><td>0.66615</td></tr><tr><td>validation_loss</td><td>1.01655</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smooth-sea-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/0msgh13i' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10/runs/0msgh13i</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_095736-0msgh13i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 10:08:24,202] Trial 3 finished with value: 0.7243047186306545 and parameters: {'lr': 0.0005, 'weight_decay': 0.05, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 1 with value: 0.6642108644757952.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.664211\n",
            "best_train_loss     = 0.674144\n",
            "best_train_accuracy = 0.6862\n",
            "best_val_accuracy   = 0.7003\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.005\n",
            "  num_blocks: 1\n",
            "  num_heads: 3\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J1zDMypA8Am7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_2 import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-2, 5e-3]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-11\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-11\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-11.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "WddVY89H8Ao8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "aee16fe2-0be4-4f0e-a2c6-9bad1cd83a2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 10:54:18,347] Using an existing study with name 'eeg_holdout_grid_search-11' instead of creating a new one.\n",
            "[W 2025-05-02 10:54:18,405] `GridSampler` is re-evaluating a configuration because the grid has been exhausted. This may happen due to a timing issue during distributed optimization or when re-running optimizations on already finished studies.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_105418-1cz1qa1m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11/runs/1cz1qa1m' target=\"_blank\">floral-star-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11/runs/1cz1qa1m' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11/runs/1cz1qa1m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=5.00e-04, wd=5.00e-02, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1692 acc=0.3297 | val_loss=1.0784 acc=0.4317 | time=17.3s\n",
            "Epoch 002 | train_loss=1.1136 acc=0.3748 | val_loss=1.0748 acc=0.4317 | time=17.2s\n",
            "Epoch 003 | train_loss=1.0828 acc=0.4202 | val_loss=1.0749 acc=0.4317 | time=17.0s\n",
            "Epoch 004 | train_loss=1.0910 acc=0.4016 | val_loss=1.0742 acc=0.4317 | time=17.4s\n",
            "Epoch 005 | train_loss=1.0854 acc=0.4155 | val_loss=1.0745 acc=0.4317 | time=17.6s\n",
            "Epoch 006 | train_loss=1.0799 acc=0.4120 | val_loss=1.0742 acc=0.4317 | time=17.0s\n",
            "Epoch 007 | train_loss=1.0787 acc=0.4117 | val_loss=1.0740 acc=0.4317 | time=17.1s\n",
            "Epoch 008 | train_loss=1.0791 acc=0.4163 | val_loss=1.0740 acc=0.4317 | time=17.1s\n",
            "Epoch 009 | train_loss=1.0709 acc=0.4186 | val_loss=1.0744 acc=0.4317 | time=17.0s\n",
            "Epoch 010 | train_loss=1.0924 acc=0.3973 | val_loss=1.0781 acc=0.4317 | time=17.0s\n",
            "Epoch 011 | train_loss=1.0766 acc=0.4183 | val_loss=1.0779 acc=0.4317 | time=16.8s\n",
            "Epoch 012 | train_loss=1.0802 acc=0.4085 | val_loss=1.0762 acc=0.4317 | time=17.0s\n",
            "Epoch 013 | train_loss=1.0799 acc=0.4105 | val_loss=1.0737 acc=0.4317 | time=17.2s\n",
            "Epoch 014 | train_loss=1.0808 acc=0.4058 | val_loss=1.0739 acc=0.4317 | time=17.0s\n",
            "Epoch 015 | train_loss=1.0735 acc=0.4186 | val_loss=1.0738 acc=0.4317 | time=17.0s\n",
            "Epoch 016 | train_loss=1.0753 acc=0.4159 | val_loss=1.0731 acc=0.4317 | time=16.9s\n",
            "Epoch 017 | train_loss=1.0699 acc=0.4225 | val_loss=1.0803 acc=0.4317 | time=17.2s\n",
            "Epoch 018 | train_loss=1.0691 acc=0.4287 | val_loss=1.0745 acc=0.4317 | time=17.1s\n",
            "Epoch 019 | train_loss=1.0712 acc=0.4252 | val_loss=1.0747 acc=0.4317 | time=17.0s\n",
            "Epoch 020 | train_loss=1.0690 acc=0.4287 | val_loss=1.0702 acc=0.4317 | time=17.1s\n",
            "Epoch 021 | train_loss=1.0723 acc=0.4268 | val_loss=1.0744 acc=0.4317 | time=17.3s\n",
            "Epoch 022 | train_loss=1.0700 acc=0.4272 | val_loss=1.0738 acc=0.4317 | time=17.0s\n",
            "Epoch 023 | train_loss=1.0684 acc=0.4280 | val_loss=1.0737 acc=0.4317 | time=17.2s\n",
            "Epoch 024 | train_loss=1.0674 acc=0.4307 | val_loss=1.0697 acc=0.4317 | time=17.4s\n",
            "Epoch 025 | train_loss=1.0583 acc=0.4303 | val_loss=1.0485 acc=0.4317 | time=17.0s\n",
            "Epoch 026 | train_loss=1.0387 acc=0.4369 | val_loss=1.0444 acc=0.4317 | time=16.9s\n",
            "Epoch 027 | train_loss=1.0281 acc=0.4610 | val_loss=1.0308 acc=0.4798 | time=17.4s\n",
            "Epoch 028 | train_loss=1.0115 acc=0.4878 | val_loss=1.0172 acc=0.5994 | time=16.9s\n",
            "Epoch 029 | train_loss=0.9830 acc=0.5254 | val_loss=1.0280 acc=0.5000 | time=17.1s\n",
            "Epoch 030 | train_loss=0.9667 acc=0.5441 | val_loss=1.0062 acc=0.6025 | time=17.1s\n",
            "Epoch 031 | train_loss=0.9451 acc=0.5627 | val_loss=0.9584 acc=0.6242 | time=17.0s\n",
            "Epoch 032 | train_loss=0.9446 acc=0.5604 | val_loss=0.9523 acc=0.6009 | time=17.0s\n",
            "Epoch 033 | train_loss=0.9196 acc=0.5744 | val_loss=0.9216 acc=0.6537 | time=17.1s\n",
            "Epoch 034 | train_loss=0.8974 acc=0.5880 | val_loss=0.9708 acc=0.6382 | time=17.2s\n",
            "Epoch 035 | train_loss=0.8746 acc=0.5814 | val_loss=0.9232 acc=0.6382 | time=17.1s\n",
            "Epoch 036 | train_loss=0.8699 acc=0.5841 | val_loss=0.9913 acc=0.4596 | time=17.1s\n",
            "Epoch 037 | train_loss=0.8700 acc=0.5810 | val_loss=0.8955 acc=0.6475 | time=17.0s\n",
            "Epoch 038 | train_loss=0.8533 acc=0.5833 | val_loss=0.8372 acc=0.6661 | time=17.1s\n",
            "Epoch 039 | train_loss=0.8422 acc=0.5926 | val_loss=0.8356 acc=0.6398 | time=17.3s\n",
            "Epoch 040 | train_loss=0.8342 acc=0.5895 | val_loss=0.7877 acc=0.6817 | time=17.2s\n",
            "Epoch 041 | train_loss=0.8298 acc=0.6062 | val_loss=0.8394 acc=0.6025 | time=16.9s\n",
            "Epoch 042 | train_loss=0.8113 acc=0.5996 | val_loss=0.9568 acc=0.5217 | time=17.2s\n",
            "Epoch 043 | train_loss=0.8129 acc=0.6054 | val_loss=0.8588 acc=0.6398 | time=17.4s\n",
            "Epoch 044 | train_loss=0.7910 acc=0.6124 | val_loss=0.7908 acc=0.6661 | time=16.9s\n",
            "Epoch 045 | train_loss=0.8133 acc=0.5992 | val_loss=0.8018 acc=0.6537 | time=17.1s\n",
            "Epoch 046 | train_loss=0.7655 acc=0.6276 | val_loss=0.8953 acc=0.6273 | time=17.1s\n",
            "Epoch 047 | train_loss=0.7953 acc=0.6093 | val_loss=0.7751 acc=0.6739 | time=17.1s\n",
            "Epoch 048 | train_loss=0.7492 acc=0.6268 | val_loss=0.8754 acc=0.6165 | time=17.1s\n",
            "Epoch 049 | train_loss=0.7418 acc=0.6241 | val_loss=0.8325 acc=0.6646 | time=17.0s\n",
            "Epoch 050 | train_loss=0.7479 acc=0.6221 | val_loss=0.8113 acc=0.6537 | time=16.9s\n",
            "Epoch 051 | train_loss=0.7225 acc=0.6303 | val_loss=0.8024 acc=0.6661 | time=17.2s\n",
            "Epoch 052 | train_loss=0.7350 acc=0.6287 | val_loss=0.8557 acc=0.6366 | time=17.2s\n",
            "Epoch 053 | train_loss=0.7191 acc=0.6322 | val_loss=0.8683 acc=0.6491 | time=16.8s\n",
            "Epoch 054 | train_loss=0.7198 acc=0.6315 | val_loss=0.8356 acc=0.6646 | time=17.1s\n",
            "Epoch 055 | train_loss=0.7117 acc=0.6268 | val_loss=0.8304 acc=0.6382 | time=16.8s\n",
            "Epoch 056 | train_loss=0.7139 acc=0.6295 | val_loss=0.8349 acc=0.6366 | time=16.9s\n",
            "Epoch 057 | train_loss=0.7093 acc=0.6346 | val_loss=0.8258 acc=0.6444 | time=17.3s\n",
            "★ Early stopping at epoch 57\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▅▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆▆▆▆▆▆▅▅▅▄▄▃▃▃▃▃▂▃▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▆▆▇▇▇▇██▆▄▇▆█▆▇▇▇█▇▇</td></tr><tr><td>validation_loss</td><td>███████████████████▇▇▆▅▅▄▄▆▄▂▁▅▃▂▁▃▂▂▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>57</td></tr><tr><td>train_accuracy</td><td>0.63456</td></tr><tr><td>train_loss</td><td>0.70932</td></tr><tr><td>validation_accuracy</td><td>0.64441</td></tr><tr><td>validation_loss</td><td>0.82578</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">floral-star-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11/runs/1cz1qa1m' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11/runs/1cz1qa1m</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-11</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_105418-1cz1qa1m/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 11:10:38,217] Trial 4 finished with value: 0.7751180188996452 and parameters: {'lr': 0.0005, 'weight_decay': 0.05, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 4 with value: 0.7751180188996452.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.775118\n",
            "best_train_loss     = 0.795321\n",
            "best_train_accuracy = 0.6093\n",
            "best_val_accuracy   = 0.6739\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.05\n",
            "  num_blocks: 1\n",
            "  num_heads: 2\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_2 import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-2, 5e-3]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3, 4]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-16\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-16\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-16.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "2WxE9eFQ8BSX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a3312c2e-0752-47bc-caed-451396dfd205"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 11:56:30,533] A new study created in RDB with name: eeg_holdout_grid_search-16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_115645-vdixhlyx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/vdixhlyx' target=\"_blank\">deep-frost-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/vdixhlyx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/vdixhlyx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.1536 acc=0.3783 | val_loss=1.0772 acc=0.4317 | time=242.5s\n",
            "Epoch 002 | train_loss=1.1305 acc=0.3829 | val_loss=1.0801 acc=0.4317 | time=21.3s\n",
            "Epoch 003 | train_loss=1.1068 acc=0.3899 | val_loss=1.0782 acc=0.4317 | time=21.3s\n",
            "Epoch 004 | train_loss=1.0975 acc=0.4004 | val_loss=1.0767 acc=0.4317 | time=21.1s\n",
            "Epoch 005 | train_loss=1.0883 acc=0.4082 | val_loss=1.0784 acc=0.4317 | time=21.1s\n",
            "Epoch 006 | train_loss=1.0875 acc=0.4093 | val_loss=1.0786 acc=0.4317 | time=21.3s\n",
            "Epoch 007 | train_loss=1.0885 acc=0.4132 | val_loss=1.0779 acc=0.4317 | time=21.1s\n",
            "Epoch 008 | train_loss=1.0757 acc=0.4167 | val_loss=1.0757 acc=0.4317 | time=21.2s\n",
            "Epoch 009 | train_loss=1.0830 acc=0.4132 | val_loss=1.0702 acc=0.4317 | time=21.1s\n",
            "Epoch 010 | train_loss=1.0785 acc=0.4237 | val_loss=1.0759 acc=0.4317 | time=21.1s\n",
            "Epoch 011 | train_loss=1.0736 acc=0.4210 | val_loss=1.0739 acc=0.4317 | time=21.2s\n",
            "Epoch 012 | train_loss=1.0777 acc=0.4155 | val_loss=1.0736 acc=0.4317 | time=21.1s\n",
            "Epoch 013 | train_loss=1.0698 acc=0.4268 | val_loss=1.0745 acc=0.4317 | time=21.0s\n",
            "Epoch 014 | train_loss=1.0723 acc=0.4252 | val_loss=1.0742 acc=0.4317 | time=21.3s\n",
            "Epoch 015 | train_loss=1.0751 acc=0.4245 | val_loss=1.0740 acc=0.4317 | time=21.1s\n",
            "Epoch 016 | train_loss=1.0682 acc=0.4322 | val_loss=1.0737 acc=0.4317 | time=21.3s\n",
            "Epoch 017 | train_loss=1.0680 acc=0.4268 | val_loss=1.0743 acc=0.4317 | time=21.3s\n",
            "Epoch 018 | train_loss=1.0676 acc=0.4276 | val_loss=1.0736 acc=0.4317 | time=21.1s\n",
            "Epoch 019 | train_loss=1.0678 acc=0.4291 | val_loss=1.0713 acc=0.4317 | time=21.1s\n",
            "★ Early stopping at epoch 19\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▃▄▅▅▆▆▆▇▇▆▇▇▇█▇▇█</td></tr><tr><td>train_loss</td><td>█▆▄▃▃▃▃▂▂▂▁▂▁▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▆█▇▆▇▇▆▅▁▅▄▃▄▄▄▃▄▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.42913</td></tr><tr><td>train_loss</td><td>1.0678</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07131</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep-frost-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/vdixhlyx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/vdixhlyx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_115645-vdixhlyx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 12:07:15,745] Trial 0 finished with value: 1.0702224600882757 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5}. Best is trial 0 with value: 1.0702224600882757.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_120715-zldnzsvv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/zldnzsvv' target=\"_blank\">logical-breeze-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/zldnzsvv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/zldnzsvv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-02, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1664 acc=0.3903 | val_loss=1.0771 acc=0.4317 | time=18.2s\n",
            "Epoch 002 | train_loss=1.1262 acc=0.3895 | val_loss=1.0758 acc=0.4317 | time=18.1s\n",
            "Epoch 003 | train_loss=1.1338 acc=0.3860 | val_loss=1.0729 acc=0.4317 | time=18.0s\n",
            "Epoch 004 | train_loss=1.1194 acc=0.3837 | val_loss=1.0957 acc=0.4317 | time=18.3s\n",
            "Epoch 005 | train_loss=1.1117 acc=0.3942 | val_loss=1.0761 acc=0.4317 | time=18.0s\n",
            "Epoch 006 | train_loss=1.0970 acc=0.4136 | val_loss=1.0777 acc=0.4317 | time=18.1s\n",
            "Epoch 007 | train_loss=1.0934 acc=0.3950 | val_loss=1.0851 acc=0.4317 | time=18.0s\n",
            "Epoch 008 | train_loss=1.0803 acc=0.4245 | val_loss=1.0821 acc=0.4317 | time=18.1s\n",
            "Epoch 009 | train_loss=1.0826 acc=0.4120 | val_loss=1.0854 acc=0.4317 | time=18.3s\n",
            "Epoch 010 | train_loss=1.0828 acc=0.4070 | val_loss=1.0827 acc=0.4317 | time=18.1s\n",
            "Epoch 011 | train_loss=1.0770 acc=0.4206 | val_loss=1.0773 acc=0.4317 | time=18.2s\n",
            "Epoch 012 | train_loss=1.0706 acc=0.4443 | val_loss=1.0527 acc=0.5248 | time=18.1s\n",
            "Epoch 013 | train_loss=1.0642 acc=0.4520 | val_loss=1.0473 acc=0.5450 | time=18.2s\n",
            "Epoch 014 | train_loss=1.0567 acc=0.4649 | val_loss=1.0049 acc=0.5745 | time=18.2s\n",
            "Epoch 015 | train_loss=1.0313 acc=0.4936 | val_loss=1.0058 acc=0.5792 | time=18.1s\n",
            "Epoch 016 | train_loss=1.0219 acc=0.4924 | val_loss=0.9387 acc=0.5932 | time=18.3s\n",
            "Epoch 017 | train_loss=1.0025 acc=0.5153 | val_loss=1.0014 acc=0.6071 | time=18.0s\n",
            "Epoch 018 | train_loss=0.9875 acc=0.5449 | val_loss=0.9808 acc=0.6273 | time=18.2s\n",
            "Epoch 019 | train_loss=0.9841 acc=0.5441 | val_loss=0.9628 acc=0.6180 | time=18.2s\n",
            "Epoch 020 | train_loss=0.9625 acc=0.5480 | val_loss=0.9568 acc=0.6040 | time=18.1s\n",
            "Epoch 021 | train_loss=0.9314 acc=0.5720 | val_loss=0.9600 acc=0.5683 | time=18.3s\n",
            "Epoch 022 | train_loss=0.9282 acc=0.5689 | val_loss=0.9371 acc=0.5947 | time=18.0s\n",
            "Epoch 023 | train_loss=0.9053 acc=0.5767 | val_loss=0.8625 acc=0.6196 | time=18.2s\n",
            "Epoch 024 | train_loss=0.8794 acc=0.5891 | val_loss=0.8463 acc=0.6444 | time=18.1s\n",
            "Epoch 025 | train_loss=0.8816 acc=0.5891 | val_loss=0.8662 acc=0.6460 | time=18.1s\n",
            "Epoch 026 | train_loss=0.8674 acc=0.5876 | val_loss=0.8601 acc=0.6118 | time=18.2s\n",
            "Epoch 027 | train_loss=0.8646 acc=0.5911 | val_loss=0.8109 acc=0.6444 | time=18.0s\n",
            "Epoch 028 | train_loss=0.8508 acc=0.5942 | val_loss=0.8078 acc=0.6460 | time=18.3s\n",
            "Epoch 029 | train_loss=0.8445 acc=0.6008 | val_loss=0.8831 acc=0.5901 | time=18.0s\n",
            "Epoch 030 | train_loss=0.8444 acc=0.5883 | val_loss=0.8207 acc=0.6630 | time=18.2s\n",
            "Epoch 031 | train_loss=0.8319 acc=0.5996 | val_loss=0.8206 acc=0.6273 | time=18.2s\n",
            "Epoch 032 | train_loss=0.7984 acc=0.6066 | val_loss=0.7866 acc=0.6398 | time=18.0s\n",
            "Epoch 033 | train_loss=0.8007 acc=0.6031 | val_loss=0.7836 acc=0.6646 | time=18.3s\n",
            "Epoch 034 | train_loss=0.8046 acc=0.6054 | val_loss=0.7982 acc=0.6444 | time=18.1s\n",
            "Epoch 035 | train_loss=0.7849 acc=0.6047 | val_loss=0.7697 acc=0.6242 | time=18.1s\n",
            "Epoch 036 | train_loss=0.7845 acc=0.6190 | val_loss=0.7830 acc=0.6522 | time=18.1s\n",
            "Epoch 037 | train_loss=0.7741 acc=0.6264 | val_loss=0.7792 acc=0.6242 | time=18.2s\n",
            "Epoch 038 | train_loss=0.7843 acc=0.6136 | val_loss=0.7754 acc=0.6413 | time=18.2s\n",
            "Epoch 039 | train_loss=0.7495 acc=0.6264 | val_loss=0.7841 acc=0.6134 | time=17.8s\n",
            "Epoch 040 | train_loss=0.7763 acc=0.6252 | val_loss=0.7500 acc=0.6568 | time=18.4s\n",
            "Epoch 041 | train_loss=0.7481 acc=0.6346 | val_loss=0.7867 acc=0.6211 | time=18.1s\n",
            "Epoch 042 | train_loss=0.7639 acc=0.6167 | val_loss=0.7589 acc=0.6584 | time=18.1s\n",
            "Epoch 043 | train_loss=0.7429 acc=0.6330 | val_loss=0.7679 acc=0.6537 | time=18.1s\n",
            "Epoch 044 | train_loss=0.7255 acc=0.6548 | val_loss=0.7546 acc=0.6584 | time=18.2s\n",
            "Epoch 045 | train_loss=0.7384 acc=0.6439 | val_loss=0.8102 acc=0.6196 | time=18.2s\n",
            "Epoch 046 | train_loss=0.7468 acc=0.6419 | val_loss=0.7716 acc=0.6739 | time=18.1s\n",
            "Epoch 047 | train_loss=0.7226 acc=0.6478 | val_loss=0.7588 acc=0.6879 | time=18.3s\n",
            "Epoch 048 | train_loss=0.7120 acc=0.6571 | val_loss=0.7757 acc=0.6630 | time=18.1s\n",
            "Epoch 049 | train_loss=0.7229 acc=0.6563 | val_loss=0.7754 acc=0.6475 | time=18.0s\n",
            "Epoch 050 | train_loss=0.7170 acc=0.6513 | val_loss=0.7604 acc=0.6801 | time=18.1s\n",
            "★ Early stopping at epoch 50\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▂▂▂▂▃▃▄▄▄▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▄▅▅▆▆▆▅▅▆▇▇▇▇▅▇▆▇▇▆▇▆▆▇▆▇▇▆██▇█</td></tr><tr><td>validation_loss</td><td>█████████▇▆▆▅▆▆▅▅▅▃▃▃▂▂▄▂▂▂▁▂▂▂▁▂▁▁▂▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_accuracy</td><td>0.65126</td></tr><tr><td>train_loss</td><td>0.71699</td></tr><tr><td>validation_accuracy</td><td>0.68012</td></tr><tr><td>validation_loss</td><td>0.7604</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">logical-breeze-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/zldnzsvv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/zldnzsvv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_120715-zldnzsvv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 12:22:25,554] Trial 1 finished with value: 0.7499674530256362 and parameters: {'lr': 0.0005, 'weight_decay': 0.05, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 1 with value: 0.7499674530256362.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_122225-a01zkjwg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/a01zkjwg' target=\"_blank\">atomic-puddle-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/a01zkjwg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/a01zkjwg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.2016 acc=0.3250 | val_loss=1.0832 acc=0.3416 | time=17.0s\n",
            "Epoch 002 | train_loss=1.1391 acc=0.3740 | val_loss=1.0831 acc=0.4317 | time=17.1s\n",
            "Epoch 003 | train_loss=1.1150 acc=0.3903 | val_loss=1.0805 acc=0.3416 | time=17.0s\n",
            "Epoch 004 | train_loss=1.1150 acc=0.3841 | val_loss=1.0790 acc=0.4317 | time=16.7s\n",
            "Epoch 005 | train_loss=1.1010 acc=0.3992 | val_loss=1.0783 acc=0.4317 | time=16.9s\n",
            "Epoch 006 | train_loss=1.0900 acc=0.3992 | val_loss=1.0753 acc=0.4317 | time=16.9s\n",
            "Epoch 007 | train_loss=1.0879 acc=0.4171 | val_loss=1.0770 acc=0.4317 | time=17.3s\n",
            "Epoch 008 | train_loss=1.0843 acc=0.4058 | val_loss=1.0749 acc=0.4317 | time=17.0s\n",
            "Epoch 009 | train_loss=1.0745 acc=0.4295 | val_loss=1.0738 acc=0.4317 | time=17.2s\n",
            "Epoch 010 | train_loss=1.0767 acc=0.4128 | val_loss=1.0728 acc=0.4317 | time=17.1s\n",
            "Epoch 011 | train_loss=1.0813 acc=0.4144 | val_loss=1.0702 acc=0.4317 | time=17.0s\n",
            "Epoch 012 | train_loss=1.0622 acc=0.4454 | val_loss=1.0612 acc=0.4612 | time=17.1s\n",
            "Epoch 013 | train_loss=1.0538 acc=0.4718 | val_loss=1.0010 acc=0.5435 | time=17.1s\n",
            "Epoch 014 | train_loss=1.0169 acc=0.5146 | val_loss=1.0101 acc=0.6009 | time=16.9s\n",
            "Epoch 015 | train_loss=1.0042 acc=0.5200 | val_loss=0.9581 acc=0.6102 | time=16.9s\n",
            "Epoch 016 | train_loss=0.9812 acc=0.5390 | val_loss=0.9433 acc=0.5807 | time=16.9s\n",
            "Epoch 017 | train_loss=0.9757 acc=0.5336 | val_loss=0.8766 acc=0.6289 | time=17.0s\n",
            "Epoch 018 | train_loss=0.9489 acc=0.5456 | val_loss=0.8839 acc=0.6444 | time=17.2s\n",
            "Epoch 019 | train_loss=0.9425 acc=0.5569 | val_loss=0.8998 acc=0.6382 | time=16.9s\n",
            "Epoch 020 | train_loss=0.9222 acc=0.5654 | val_loss=0.8252 acc=0.6444 | time=16.8s\n",
            "Epoch 021 | train_loss=0.9228 acc=0.5682 | val_loss=0.8531 acc=0.6211 | time=17.3s\n",
            "Epoch 022 | train_loss=0.8896 acc=0.5825 | val_loss=0.7830 acc=0.6568 | time=17.0s\n",
            "Epoch 023 | train_loss=0.8708 acc=0.5798 | val_loss=0.8243 acc=0.6475 | time=17.0s\n",
            "Epoch 024 | train_loss=0.8639 acc=0.5856 | val_loss=0.8028 acc=0.6553 | time=17.0s\n",
            "Epoch 025 | train_loss=0.8498 acc=0.6047 | val_loss=0.7995 acc=0.6553 | time=17.1s\n",
            "Epoch 026 | train_loss=0.8330 acc=0.5934 | val_loss=0.8916 acc=0.5854 | time=17.1s\n",
            "Epoch 027 | train_loss=0.8550 acc=0.5883 | val_loss=0.7702 acc=0.6630 | time=16.9s\n",
            "Epoch 028 | train_loss=0.8282 acc=0.5907 | val_loss=0.8342 acc=0.6320 | time=17.1s\n",
            "Epoch 029 | train_loss=0.8183 acc=0.5981 | val_loss=0.7592 acc=0.6429 | time=17.0s\n",
            "Epoch 030 | train_loss=0.7900 acc=0.6132 | val_loss=0.8285 acc=0.6258 | time=17.1s\n",
            "Epoch 031 | train_loss=0.8130 acc=0.6194 | val_loss=0.7909 acc=0.6382 | time=16.9s\n",
            "Epoch 032 | train_loss=0.8286 acc=0.6054 | val_loss=0.7946 acc=0.6335 | time=17.0s\n",
            "Epoch 033 | train_loss=0.7933 acc=0.6183 | val_loss=0.7719 acc=0.6475 | time=17.1s\n",
            "Epoch 034 | train_loss=0.7968 acc=0.6082 | val_loss=0.8053 acc=0.6242 | time=16.7s\n",
            "Epoch 035 | train_loss=0.7779 acc=0.6050 | val_loss=0.8110 acc=0.6553 | time=16.8s\n",
            "Epoch 036 | train_loss=0.7799 acc=0.6093 | val_loss=0.7561 acc=0.6491 | time=17.0s\n",
            "Epoch 037 | train_loss=0.7733 acc=0.6163 | val_loss=0.7712 acc=0.6413 | time=17.0s\n",
            "Epoch 038 | train_loss=0.7479 acc=0.6268 | val_loss=0.9138 acc=0.5621 | time=16.6s\n",
            "Epoch 039 | train_loss=0.7515 acc=0.6268 | val_loss=0.7610 acc=0.6475 | time=17.4s\n",
            "Epoch 040 | train_loss=0.7368 acc=0.6353 | val_loss=0.8158 acc=0.6180 | time=17.1s\n",
            "Epoch 041 | train_loss=0.7376 acc=0.6276 | val_loss=0.8378 acc=0.5916 | time=16.9s\n",
            "Epoch 042 | train_loss=0.7205 acc=0.6361 | val_loss=0.8734 acc=0.6134 | time=17.3s\n",
            "Epoch 043 | train_loss=0.7015 acc=0.6392 | val_loss=0.7771 acc=0.6475 | time=17.0s\n",
            "Epoch 044 | train_loss=0.6993 acc=0.6450 | val_loss=0.7669 acc=0.6444 | time=16.8s\n",
            "Epoch 045 | train_loss=0.7337 acc=0.6377 | val_loss=0.8332 acc=0.6320 | time=17.2s\n",
            "Epoch 046 | train_loss=0.7094 acc=0.6427 | val_loss=0.9096 acc=0.5683 | time=16.9s\n",
            "★ Early stopping at epoch 46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▃▃▃▃▃▃▄▄▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▆▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▁▃▃▃▃▃▃▃▄▅▇▇▇█▇█▇███▆█▇█▇▇█▇████▇▆▇██▇</td></tr><tr><td>validation_loss</td><td>███████████▆▆▅▄▄▄▂▃▂▂▂▄▁▃▁▃▂▁▂▂▁▁▁▂▃▄▁▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>46</td></tr><tr><td>train_accuracy</td><td>0.64272</td></tr><tr><td>train_loss</td><td>0.70944</td></tr><tr><td>validation_accuracy</td><td>0.56832</td></tr><tr><td>validation_loss</td><td>0.90963</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">atomic-puddle-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/a01zkjwg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/a01zkjwg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_122225-a01zkjwg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 12:35:30,751] Trial 2 finished with value: 0.7560889039720807 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 1 with value: 0.7499674530256362.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_123530-nizesbyk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/nizesbyk' target=\"_blank\">ethereal-sky-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/nizesbyk' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/nizesbyk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=1, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1793 acc=0.3600 | val_loss=1.0827 acc=0.3416 | time=18.1s\n",
            "Epoch 002 | train_loss=1.1415 acc=0.3767 | val_loss=1.0948 acc=0.3416 | time=18.1s\n",
            "Epoch 003 | train_loss=1.1208 acc=0.3751 | val_loss=1.0772 acc=0.3416 | time=18.1s\n",
            "Epoch 004 | train_loss=1.1144 acc=0.3837 | val_loss=1.0887 acc=0.3416 | time=18.2s\n",
            "Epoch 005 | train_loss=1.1074 acc=0.3938 | val_loss=1.0837 acc=0.3416 | time=18.2s\n",
            "Epoch 006 | train_loss=1.0895 acc=0.4148 | val_loss=1.0789 acc=0.4317 | time=18.3s\n",
            "Epoch 007 | train_loss=1.0934 acc=0.4117 | val_loss=1.0722 acc=0.4332 | time=18.2s\n",
            "Epoch 008 | train_loss=1.0885 acc=0.4070 | val_loss=1.0745 acc=0.4317 | time=18.0s\n",
            "Epoch 009 | train_loss=1.0720 acc=0.4194 | val_loss=1.0560 acc=0.5202 | time=18.3s\n",
            "Epoch 010 | train_loss=1.0823 acc=0.4221 | val_loss=1.0732 acc=0.4053 | time=18.1s\n",
            "Epoch 011 | train_loss=1.0627 acc=0.4482 | val_loss=1.0232 acc=0.5621 | time=18.3s\n",
            "Epoch 012 | train_loss=1.0192 acc=0.4979 | val_loss=1.0446 acc=0.6025 | time=18.1s\n",
            "Epoch 013 | train_loss=0.9778 acc=0.5340 | val_loss=0.9500 acc=0.6149 | time=18.1s\n",
            "Epoch 014 | train_loss=0.9855 acc=0.5332 | val_loss=0.9523 acc=0.5901 | time=18.1s\n",
            "Epoch 015 | train_loss=0.9533 acc=0.5402 | val_loss=0.9044 acc=0.6242 | time=18.1s\n",
            "Epoch 016 | train_loss=0.9585 acc=0.5480 | val_loss=0.9164 acc=0.6366 | time=18.2s\n",
            "Epoch 017 | train_loss=0.9295 acc=0.5732 | val_loss=0.8859 acc=0.6398 | time=18.1s\n",
            "Epoch 018 | train_loss=0.9179 acc=0.5790 | val_loss=0.9356 acc=0.6040 | time=18.5s\n",
            "Epoch 019 | train_loss=0.9045 acc=0.5717 | val_loss=0.9220 acc=0.6211 | time=18.3s\n",
            "Epoch 020 | train_loss=0.8728 acc=0.5942 | val_loss=0.9446 acc=0.5916 | time=18.2s\n",
            "Epoch 021 | train_loss=0.8581 acc=0.5899 | val_loss=0.8245 acc=0.6599 | time=18.3s\n",
            "Epoch 022 | train_loss=0.8537 acc=0.5872 | val_loss=0.8651 acc=0.6165 | time=18.2s\n",
            "Epoch 023 | train_loss=0.8386 acc=0.6016 | val_loss=0.7627 acc=0.7003 | time=18.5s\n",
            "Epoch 024 | train_loss=0.8007 acc=0.6190 | val_loss=0.8014 acc=0.6537 | time=18.2s\n",
            "Epoch 025 | train_loss=0.8445 acc=0.6019 | val_loss=0.7548 acc=0.6817 | time=18.1s\n",
            "Epoch 026 | train_loss=0.7890 acc=0.6369 | val_loss=0.7844 acc=0.6817 | time=18.2s\n",
            "Epoch 027 | train_loss=0.7981 acc=0.6276 | val_loss=0.8171 acc=0.6786 | time=18.1s\n",
            "Epoch 028 | train_loss=0.7805 acc=0.6291 | val_loss=0.8311 acc=0.6366 | time=18.4s\n",
            "Epoch 029 | train_loss=0.7581 acc=0.6260 | val_loss=0.7972 acc=0.6537 | time=18.1s\n",
            "Epoch 030 | train_loss=0.7807 acc=0.6233 | val_loss=0.7400 acc=0.6848 | time=18.3s\n",
            "Epoch 031 | train_loss=0.7479 acc=0.6412 | val_loss=0.8899 acc=0.6491 | time=18.1s\n",
            "Epoch 032 | train_loss=0.7281 acc=0.6505 | val_loss=0.8102 acc=0.6584 | time=18.2s\n",
            "Epoch 033 | train_loss=0.7402 acc=0.6435 | val_loss=0.7726 acc=0.6273 | time=18.3s\n",
            "Epoch 034 | train_loss=0.7423 acc=0.6482 | val_loss=0.9614 acc=0.5839 | time=18.2s\n",
            "Epoch 035 | train_loss=0.7103 acc=0.6509 | val_loss=0.7206 acc=0.6786 | time=18.4s\n",
            "Epoch 036 | train_loss=0.7194 acc=0.6575 | val_loss=0.8519 acc=0.6056 | time=18.2s\n",
            "Epoch 037 | train_loss=0.7067 acc=0.6551 | val_loss=0.7926 acc=0.6832 | time=18.3s\n",
            "Epoch 038 | train_loss=0.7057 acc=0.6517 | val_loss=1.0460 acc=0.6227 | time=18.2s\n",
            "Epoch 039 | train_loss=0.6908 acc=0.6602 | val_loss=1.1462 acc=0.5093 | time=18.2s\n",
            "Epoch 040 | train_loss=0.6822 acc=0.6726 | val_loss=0.8084 acc=0.6568 | time=18.3s\n",
            "Epoch 041 | train_loss=0.6846 acc=0.6695 | val_loss=1.2975 acc=0.5590 | time=18.3s\n",
            "Epoch 042 | train_loss=0.6528 acc=0.6854 | val_loss=0.9086 acc=0.6475 | time=18.2s\n",
            "Epoch 043 | train_loss=0.6697 acc=0.6769 | val_loss=0.9250 acc=0.6366 | time=18.1s\n",
            "Epoch 044 | train_loss=0.6639 acc=0.6831 | val_loss=1.0397 acc=0.6134 | time=18.1s\n",
            "Epoch 045 | train_loss=0.6449 acc=0.6862 | val_loss=0.8612 acc=0.6429 | time=18.5s\n",
            "★ Early stopping at epoch 45\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▂▂▂▂▃▄▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▆▆▅▅▅▅▅▄▄▄▄▄▃▄▃▃▂▃▂▂▂▂▂▂▂▂▁▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▃▃▃▂▅▆▆▆▇▇▇▆▆▇▆█▇██▇▇█▇▇▇▆██▆▄▇▅▇▇▇</td></tr><tr><td>validation_loss</td><td>▅▆▅▅▅▅▅▅▅▅▅▄▄▃▃▃▃▄▂▃▂▂▁▂▂▂▁▃▂▂▄▁▂▅▆▂█▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>45</td></tr><tr><td>train_accuracy</td><td>0.68621</td></tr><tr><td>train_loss</td><td>0.64485</td></tr><tr><td>validation_accuracy</td><td>0.64286</td></tr><tr><td>validation_loss</td><td>0.86116</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ethereal-sky-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/nizesbyk' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/nizesbyk</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_123530-nizesbyk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 12:49:13,239] Trial 3 finished with value: 0.7206419763110933 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5}. Best is trial 3 with value: 0.7206419763110933.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_124913-z2z4r3op</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/z2z4r3op' target=\"_blank\">vocal-cherry-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/z2z4r3op' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/z2z4r3op</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=5.00e-04, wd=5.00e-02, blocks=1, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1965 acc=0.3169 | val_loss=1.0846 acc=0.4317 | time=17.4s\n",
            "Epoch 002 | train_loss=1.1646 acc=0.3196 | val_loss=1.0996 acc=0.2236 | time=17.7s\n",
            "Epoch 003 | train_loss=1.1237 acc=0.3565 | val_loss=1.0868 acc=0.4317 | time=17.7s\n",
            "Epoch 004 | train_loss=1.1024 acc=0.3833 | val_loss=1.0875 acc=0.4317 | time=17.4s\n",
            "Epoch 005 | train_loss=1.0874 acc=0.4074 | val_loss=1.0760 acc=0.4317 | time=17.4s\n",
            "Epoch 006 | train_loss=1.0795 acc=0.4186 | val_loss=1.0225 acc=0.5730 | time=17.6s\n",
            "Epoch 007 | train_loss=1.0320 acc=0.4885 | val_loss=1.0213 acc=0.6025 | time=17.4s\n",
            "Epoch 008 | train_loss=1.0016 acc=0.5231 | val_loss=0.9806 acc=0.6227 | time=17.4s\n",
            "Epoch 009 | train_loss=0.9736 acc=0.5503 | val_loss=0.9631 acc=0.6196 | time=17.1s\n",
            "Epoch 010 | train_loss=0.9499 acc=0.5631 | val_loss=0.8848 acc=0.6211 | time=17.6s\n",
            "Epoch 011 | train_loss=0.9267 acc=0.5845 | val_loss=0.8956 acc=0.6273 | time=17.6s\n",
            "Epoch 012 | train_loss=0.9178 acc=0.5872 | val_loss=0.8703 acc=0.6304 | time=17.4s\n",
            "Epoch 013 | train_loss=0.8870 acc=0.5852 | val_loss=0.8841 acc=0.5994 | time=17.8s\n",
            "Epoch 014 | train_loss=0.8824 acc=0.5988 | val_loss=0.8773 acc=0.6537 | time=17.5s\n",
            "Epoch 015 | train_loss=0.8594 acc=0.5930 | val_loss=0.8789 acc=0.6118 | time=17.7s\n",
            "Epoch 016 | train_loss=0.8526 acc=0.5918 | val_loss=0.8758 acc=0.6553 | time=17.7s\n",
            "Epoch 017 | train_loss=0.8368 acc=0.5992 | val_loss=0.7753 acc=0.6460 | time=17.6s\n",
            "Epoch 018 | train_loss=0.8209 acc=0.6019 | val_loss=0.8273 acc=0.6289 | time=17.3s\n",
            "Epoch 019 | train_loss=0.8047 acc=0.6031 | val_loss=0.7871 acc=0.6273 | time=17.4s\n",
            "Epoch 020 | train_loss=0.7962 acc=0.6074 | val_loss=0.7899 acc=0.6304 | time=17.0s\n",
            "Epoch 021 | train_loss=0.7772 acc=0.6140 | val_loss=0.8282 acc=0.6149 | time=17.0s\n",
            "Epoch 022 | train_loss=0.7799 acc=0.6148 | val_loss=0.7945 acc=0.6258 | time=17.7s\n",
            "Epoch 023 | train_loss=0.7871 acc=0.6171 | val_loss=0.7504 acc=0.6537 | time=17.3s\n",
            "Epoch 024 | train_loss=0.7683 acc=0.6221 | val_loss=0.8117 acc=0.6196 | time=17.5s\n",
            "Epoch 025 | train_loss=0.7480 acc=0.6369 | val_loss=0.8174 acc=0.6537 | time=17.8s\n",
            "Epoch 026 | train_loss=0.7446 acc=0.6260 | val_loss=0.7608 acc=0.6863 | time=17.7s\n",
            "Epoch 027 | train_loss=0.7572 acc=0.6400 | val_loss=0.7887 acc=0.7034 | time=18.5s\n",
            "Epoch 028 | train_loss=0.7546 acc=0.6287 | val_loss=0.7392 acc=0.6615 | time=17.6s\n",
            "Epoch 029 | train_loss=0.7613 acc=0.6229 | val_loss=0.7986 acc=0.6724 | time=17.4s\n",
            "Epoch 030 | train_loss=0.7459 acc=0.6233 | val_loss=0.7122 acc=0.7034 | time=17.8s\n",
            "Epoch 031 | train_loss=0.7265 acc=0.6536 | val_loss=0.7729 acc=0.6925 | time=17.5s\n",
            "Epoch 032 | train_loss=0.6932 acc=0.6489 | val_loss=0.8483 acc=0.6801 | time=17.8s\n",
            "Epoch 033 | train_loss=0.7230 acc=0.6520 | val_loss=0.9899 acc=0.6149 | time=17.6s\n",
            "Epoch 034 | train_loss=0.7253 acc=0.6540 | val_loss=0.8655 acc=0.6724 | time=17.6s\n",
            "Epoch 035 | train_loss=0.7151 acc=0.6652 | val_loss=0.8983 acc=0.6413 | time=17.9s\n",
            "Epoch 036 | train_loss=0.7159 acc=0.6513 | val_loss=0.8410 acc=0.6506 | time=17.6s\n",
            "Epoch 037 | train_loss=0.6713 acc=0.6827 | val_loss=0.9311 acc=0.6413 | time=17.6s\n",
            "Epoch 038 | train_loss=0.6513 acc=0.6913 | val_loss=0.9823 acc=0.6413 | time=17.6s\n",
            "Epoch 039 | train_loss=0.6385 acc=0.6971 | val_loss=0.8983 acc=0.6398 | time=17.6s\n",
            "Epoch 040 | train_loss=0.6597 acc=0.6816 | val_loss=0.8354 acc=0.6444 | time=17.4s\n",
            "★ Early stopping at epoch 40\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▃▃▄▅▅▆▆▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▄▁▄▄▄▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇██▇████▇█▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>█████▇▇▆▆▄▄▄▄▄▄▄▂▃▂▂▃▂▂▃▃▂▂▁▃▁▂▃▆▄▄▃▅▆▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_accuracy</td><td>0.68155</td></tr><tr><td>train_loss</td><td>0.65972</td></tr><tr><td>validation_accuracy</td><td>0.64441</td></tr><tr><td>validation_loss</td><td>0.83544</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vocal-cherry-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/z2z4r3op' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/z2z4r3op</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_124913-z2z4r3op/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 13:00:58,342] Trial 4 finished with value: 0.7121514422552926 and parameters: {'lr': 0.0005, 'weight_decay': 0.05, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5}. Best is trial 4 with value: 0.7121514422552926.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_130058-r4vlbmux</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/r4vlbmux' target=\"_blank\">stilted-frost-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/r4vlbmux' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/r4vlbmux</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=5.00e-04, wd=5.00e-02, blocks=1, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.1755 acc=0.3425 | val_loss=1.0770 acc=0.4317 | time=21.2s\n",
            "Epoch 002 | train_loss=1.1183 acc=0.3918 | val_loss=1.0792 acc=0.4317 | time=21.3s\n",
            "Epoch 003 | train_loss=1.1112 acc=0.3833 | val_loss=1.0755 acc=0.4317 | time=21.6s\n",
            "Epoch 004 | train_loss=1.1088 acc=0.3779 | val_loss=1.0753 acc=0.4317 | time=21.2s\n",
            "Epoch 005 | train_loss=1.0924 acc=0.3988 | val_loss=1.0726 acc=0.4317 | time=21.2s\n",
            "Epoch 006 | train_loss=1.0923 acc=0.4113 | val_loss=1.0724 acc=0.4317 | time=21.4s\n",
            "Epoch 007 | train_loss=1.0778 acc=0.4047 | val_loss=1.0761 acc=0.4317 | time=21.3s\n",
            "Epoch 008 | train_loss=1.0834 acc=0.3911 | val_loss=1.0779 acc=0.4317 | time=21.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▆▅▅▇█▇▆</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▆█▄▄▁▁▅▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>8</td></tr><tr><td>train_accuracy</td><td>0.39107</td></tr><tr><td>train_loss</td><td>1.08343</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07786</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stilted-frost-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/r4vlbmux' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16/runs/r4vlbmux</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-16</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_130058-r4vlbmux/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 13:04:11,959] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 5 pruned at epoch 9\n",
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.712151\n",
            "best_train_loss     = 0.745850\n",
            "best_train_accuracy = 0.6233\n",
            "best_val_accuracy   = 0.7034\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.05\n",
            "  num_blocks: 1\n",
            "  num_heads: 2\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "metadata": {
        "id": "GpLtCSzZ8BT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_3 import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-3, 5e-4]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [2]\n",
        "NUM_HEAD_CHOICES    = [1, 2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 10\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-17\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-17\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-17.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v4ZFr_2azP4P",
        "outputId": "ed1b4b4e-7bd5-4c9b-9f67-6aca71fd391a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 13:28:55,853] A new study created in RDB with name: eeg_holdout_grid_search-17\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_132856-fug5wovm</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/fug5wovm' target=\"_blank\">laced-aardvark-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/fug5wovm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/fug5wovm</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1312 acc=0.3802 | val_loss=1.0756 acc=0.4317 | time=29.7s\n",
            "Epoch 002 | train_loss=1.1196 acc=0.3802 | val_loss=1.0803 acc=0.4317 | time=29.5s\n",
            "Epoch 003 | train_loss=1.1044 acc=0.3946 | val_loss=1.0756 acc=0.4317 | time=29.2s\n",
            "Epoch 004 | train_loss=1.0924 acc=0.3977 | val_loss=1.0836 acc=0.4317 | time=29.2s\n",
            "Epoch 005 | train_loss=1.0939 acc=0.3860 | val_loss=1.0761 acc=0.4317 | time=29.5s\n",
            "Epoch 006 | train_loss=1.0760 acc=0.4155 | val_loss=1.0739 acc=0.4317 | time=29.2s\n",
            "Epoch 007 | train_loss=1.0944 acc=0.4082 | val_loss=1.0833 acc=0.4317 | time=29.3s\n",
            "Epoch 008 | train_loss=1.0808 acc=0.4035 | val_loss=1.0827 acc=0.4317 | time=29.4s\n",
            "Epoch 009 | train_loss=1.0862 acc=0.4101 | val_loss=1.0833 acc=0.4317 | time=29.2s\n",
            "Epoch 010 | train_loss=1.0798 acc=0.4249 | val_loss=1.0863 acc=0.4317 | time=29.1s\n",
            "Epoch 011 | train_loss=1.0787 acc=0.4074 | val_loss=1.0821 acc=0.4317 | time=29.2s\n",
            "Epoch 012 | train_loss=1.0686 acc=0.4326 | val_loss=1.1182 acc=0.4317 | time=29.3s\n",
            "Epoch 013 | train_loss=1.0703 acc=0.4229 | val_loss=1.0813 acc=0.4317 | time=29.1s\n",
            "Epoch 014 | train_loss=1.0783 acc=0.4280 | val_loss=1.0862 acc=0.4317 | time=29.2s\n",
            "Epoch 015 | train_loss=1.0747 acc=0.4280 | val_loss=1.0791 acc=0.4317 | time=29.3s\n",
            "Epoch 016 | train_loss=1.0747 acc=0.4268 | val_loss=1.0761 acc=0.4317 | time=29.2s\n",
            "★ Early stopping at epoch 16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▃▃▂▆▅▄▅▇▅█▇▇▇▇</td></tr><tr><td>train_loss</td><td>█▇▅▄▄▂▄▂▃▂▂▁▁▂▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▂▁▃▁▁▂▂▂▃▂█▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>train_accuracy</td><td>0.4268</td></tr><tr><td>train_loss</td><td>1.07471</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07606</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">laced-aardvark-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/fug5wovm' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/fug5wovm</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_132856-fug5wovm/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 13:36:48,896] Trial 0 finished with value: 1.0738752626237416 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 1.0738752626237416.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_133649-pu46kt5c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/pu46kt5c' target=\"_blank\">wobbly-armadillo-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/pu46kt5c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/pu46kt5c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=2, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1864 acc=0.3511 | val_loss=1.0804 acc=0.4317 | time=23.7s\n",
            "Epoch 002 | train_loss=1.1254 acc=0.3860 | val_loss=1.0826 acc=0.4317 | time=23.8s\n",
            "Epoch 003 | train_loss=1.1194 acc=0.3950 | val_loss=1.0746 acc=0.4317 | time=23.9s\n",
            "Epoch 004 | train_loss=1.1071 acc=0.4000 | val_loss=1.0723 acc=0.4317 | time=24.0s\n",
            "Epoch 005 | train_loss=1.0927 acc=0.4109 | val_loss=1.0715 acc=0.4519 | time=23.9s\n",
            "Epoch 006 | train_loss=1.0487 acc=0.4792 | val_loss=0.9982 acc=0.6025 | time=23.9s\n",
            "Epoch 007 | train_loss=1.0306 acc=0.5056 | val_loss=0.9829 acc=0.5994 | time=23.8s\n",
            "Epoch 008 | train_loss=0.9799 acc=0.5460 | val_loss=0.9550 acc=0.6087 | time=23.8s\n",
            "Epoch 009 | train_loss=0.9331 acc=0.5837 | val_loss=0.9110 acc=0.6320 | time=23.8s\n",
            "Epoch 010 | train_loss=0.9065 acc=0.5860 | val_loss=0.8887 acc=0.6335 | time=23.7s\n",
            "Epoch 011 | train_loss=0.8735 acc=0.6019 | val_loss=0.8471 acc=0.6522 | time=23.8s\n",
            "Epoch 012 | train_loss=0.8611 acc=0.6008 | val_loss=0.8970 acc=0.5295 | time=23.9s\n",
            "Epoch 013 | train_loss=0.8111 acc=0.6396 | val_loss=0.8146 acc=0.7096 | time=23.9s\n",
            "Epoch 014 | train_loss=0.8100 acc=0.6369 | val_loss=0.8141 acc=0.6382 | time=23.8s\n",
            "Epoch 015 | train_loss=0.7693 acc=0.6517 | val_loss=0.8310 acc=0.6832 | time=23.8s\n",
            "Epoch 016 | train_loss=0.7405 acc=0.6536 | val_loss=0.8122 acc=0.6491 | time=23.8s\n",
            "Epoch 017 | train_loss=0.7252 acc=0.6769 | val_loss=0.7163 acc=0.6988 | time=23.7s\n",
            "Epoch 018 | train_loss=0.7133 acc=0.6734 | val_loss=0.8624 acc=0.6413 | time=23.8s\n",
            "Epoch 019 | train_loss=0.7051 acc=0.6715 | val_loss=0.8163 acc=0.6786 | time=23.9s\n",
            "Epoch 020 | train_loss=0.6764 acc=0.6878 | val_loss=0.7539 acc=0.6832 | time=24.1s\n",
            "Epoch 021 | train_loss=0.6584 acc=0.6850 | val_loss=0.7966 acc=0.6786 | time=23.8s\n",
            "Epoch 022 | train_loss=0.6541 acc=0.6986 | val_loss=0.8573 acc=0.6522 | time=23.9s\n",
            "Epoch 023 | train_loss=0.6477 acc=0.7052 | val_loss=0.7616 acc=0.6661 | time=23.7s\n",
            "Epoch 024 | train_loss=0.5859 acc=0.7398 | val_loss=0.7435 acc=0.6755 | time=23.7s\n",
            "Epoch 025 | train_loss=0.6002 acc=0.7200 | val_loss=0.7588 acc=0.6708 | time=23.7s\n",
            "Epoch 026 | train_loss=0.5894 acc=0.7324 | val_loss=0.7683 acc=0.6149 | time=23.8s\n",
            "Epoch 027 | train_loss=0.5678 acc=0.7441 | val_loss=0.7829 acc=0.6429 | time=23.9s\n",
            "★ Early stopping at epoch 27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▂▃▄▄▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▆▆▆▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▂▅▅▅▆▆▇▃█▆▇▆█▆▇▇▇▇▇▇▇▆▆</td></tr><tr><td>validation_loss</td><td>█████▆▆▆▅▄▃▄▃▃▃▃▁▄▃▂▃▄▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>27</td></tr><tr><td>train_accuracy</td><td>0.74408</td></tr><tr><td>train_loss</td><td>0.5678</td></tr><tr><td>validation_accuracy</td><td>0.64286</td></tr><tr><td>validation_loss</td><td>0.78289</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wobbly-armadillo-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/pu46kt5c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/pu46kt5c</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_133649-pu46kt5c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 13:47:34,427] Trial 1 finished with value: 0.7162879818961734 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 2, 'num_heads': 2, 'num_segments': 5}. Best is trial 1 with value: 0.7162879818961734.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_134734-j10ld35w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/j10ld35w' target=\"_blank\">fanciful-yogurt-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/j10ld35w' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/j10ld35w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=2, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.1393 acc=0.3588 | val_loss=1.0758 acc=0.4317 | time=18.1s\n",
            "Epoch 002 | train_loss=1.1282 acc=0.3767 | val_loss=1.0789 acc=0.4317 | time=18.3s\n",
            "Epoch 003 | train_loss=1.1026 acc=0.4019 | val_loss=1.0744 acc=0.4317 | time=18.3s\n",
            "Epoch 004 | train_loss=1.1037 acc=0.3771 | val_loss=1.0746 acc=0.4317 | time=18.2s\n",
            "Epoch 005 | train_loss=1.0851 acc=0.4117 | val_loss=1.0744 acc=0.4317 | time=18.1s\n",
            "Epoch 006 | train_loss=1.0782 acc=0.4019 | val_loss=1.0839 acc=0.3416 | time=18.1s\n",
            "Epoch 007 | train_loss=1.0734 acc=0.4427 | val_loss=1.0739 acc=0.3385 | time=18.3s\n",
            "Epoch 008 | train_loss=1.0245 acc=0.5091 | val_loss=0.9524 acc=0.6009 | time=18.1s\n",
            "Epoch 009 | train_loss=0.9687 acc=0.5515 | val_loss=0.9500 acc=0.6056 | time=18.2s\n",
            "Epoch 010 | train_loss=0.9320 acc=0.5829 | val_loss=0.9923 acc=0.5186 | time=18.2s\n",
            "Epoch 011 | train_loss=0.9243 acc=0.5860 | val_loss=0.8931 acc=0.6413 | time=18.0s\n",
            "Epoch 012 | train_loss=0.8735 acc=0.6136 | val_loss=0.8561 acc=0.6568 | time=18.2s\n",
            "Epoch 013 | train_loss=0.8448 acc=0.6151 | val_loss=0.9392 acc=0.4907 | time=18.1s\n",
            "Epoch 014 | train_loss=0.8033 acc=0.6318 | val_loss=0.7787 acc=0.6910 | time=18.3s\n",
            "Epoch 015 | train_loss=0.7931 acc=0.6419 | val_loss=0.8139 acc=0.6553 | time=18.3s\n",
            "Epoch 016 | train_loss=0.7794 acc=0.6551 | val_loss=0.7917 acc=0.6584 | time=18.2s\n",
            "Epoch 017 | train_loss=0.7490 acc=0.6672 | val_loss=0.7881 acc=0.6770 | time=18.2s\n",
            "Epoch 018 | train_loss=0.7423 acc=0.6617 | val_loss=0.7412 acc=0.7112 | time=18.3s\n",
            "Epoch 019 | train_loss=0.7208 acc=0.6687 | val_loss=0.7657 acc=0.6832 | time=18.3s\n",
            "Epoch 020 | train_loss=0.7056 acc=0.6695 | val_loss=0.7601 acc=0.6724 | time=18.2s\n",
            "Epoch 021 | train_loss=0.6842 acc=0.6909 | val_loss=0.7128 acc=0.7174 | time=18.3s\n",
            "Epoch 022 | train_loss=0.6831 acc=0.6905 | val_loss=0.6708 acc=0.7158 | time=18.1s\n",
            "Epoch 023 | train_loss=0.6498 acc=0.7087 | val_loss=0.6301 acc=0.7376 | time=18.2s\n",
            "Epoch 024 | train_loss=0.6477 acc=0.7107 | val_loss=0.6780 acc=0.7189 | time=18.2s\n",
            "Epoch 025 | train_loss=0.6403 acc=0.7083 | val_loss=0.8498 acc=0.6366 | time=18.1s\n",
            "Epoch 026 | train_loss=0.6383 acc=0.7041 | val_loss=0.8595 acc=0.6599 | time=18.3s\n",
            "Epoch 027 | train_loss=0.6079 acc=0.7328 | val_loss=0.6384 acc=0.7484 | time=18.2s\n",
            "Epoch 028 | train_loss=0.6067 acc=0.7274 | val_loss=0.7952 acc=0.6196 | time=18.2s\n",
            "Epoch 029 | train_loss=0.5972 acc=0.7371 | val_loss=0.7521 acc=0.6444 | time=18.2s\n",
            "Epoch 030 | train_loss=0.5771 acc=0.7375 | val_loss=0.6730 acc=0.6848 | time=18.1s\n",
            "Epoch 031 | train_loss=0.5519 acc=0.7577 | val_loss=0.7127 acc=0.6988 | time=18.2s\n",
            "Epoch 032 | train_loss=0.5194 acc=0.7825 | val_loss=0.7205 acc=0.6941 | time=18.1s\n",
            "Epoch 033 | train_loss=0.5323 acc=0.7670 | val_loss=0.7053 acc=0.7065 | time=18.2s\n",
            "★ Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▂▂▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▆▆▆▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▁▁▅▆▄▆▆▄▇▆▆▇▇▇▇▇▇█▇▆▆█▆▆▇▇▇▇</td></tr><tr><td>validation_loss</td><td>███████▆▆▇▅▄▆▃▄▃▃▃▃▃▂▂▁▂▄▅▁▄▃▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_accuracy</td><td>0.76699</td></tr><tr><td>train_loss</td><td>0.53228</td></tr><tr><td>validation_accuracy</td><td>0.70652</td></tr><tr><td>validation_loss</td><td>0.70533</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fanciful-yogurt-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/j10ld35w' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/j10ld35w</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_134734-j10ld35w/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 13:57:37,244] Trial 2 finished with value: 0.6300574328218188 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 2, 'num_heads': 1, 'num_segments': 5}. Best is trial 2 with value: 0.6300574328218188.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_135737-52jp4tm2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/52jp4tm2' target=\"_blank\">faithful-haze-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/52jp4tm2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/52jp4tm2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=2, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1651 acc=0.3417 | val_loss=1.0988 acc=0.4317 | time=23.9s\n",
            "Epoch 002 | train_loss=1.1195 acc=0.3724 | val_loss=1.0813 acc=0.4317 | time=23.9s\n",
            "Epoch 003 | train_loss=1.1053 acc=0.3926 | val_loss=1.0727 acc=0.4317 | time=23.9s\n",
            "Epoch 004 | train_loss=1.0687 acc=0.4252 | val_loss=1.0897 acc=0.4317 | time=23.8s\n",
            "Epoch 005 | train_loss=1.0437 acc=0.4800 | val_loss=0.9976 acc=0.5963 | time=23.8s\n",
            "Epoch 006 | train_loss=0.9677 acc=0.5592 | val_loss=0.9756 acc=0.5730 | time=23.8s\n",
            "Epoch 007 | train_loss=0.9268 acc=0.5829 | val_loss=0.8813 acc=0.6366 | time=23.9s\n",
            "Epoch 008 | train_loss=0.9037 acc=0.5981 | val_loss=0.9349 acc=0.5730 | time=23.9s\n",
            "Epoch 009 | train_loss=0.8949 acc=0.6008 | val_loss=0.8561 acc=0.6289 | time=23.8s\n",
            "Epoch 010 | train_loss=0.8759 acc=0.6202 | val_loss=0.8534 acc=0.6087 | time=23.9s\n",
            "Epoch 011 | train_loss=0.8398 acc=0.6229 | val_loss=0.8263 acc=0.6211 | time=23.8s\n",
            "Epoch 012 | train_loss=0.8133 acc=0.6268 | val_loss=0.7760 acc=0.6988 | time=23.8s\n",
            "Epoch 013 | train_loss=0.7854 acc=0.6439 | val_loss=0.7856 acc=0.6925 | time=23.7s\n",
            "Epoch 014 | train_loss=0.7641 acc=0.6513 | val_loss=0.8599 acc=0.6258 | time=23.7s\n",
            "Epoch 015 | train_loss=0.7294 acc=0.6672 | val_loss=0.7546 acc=0.6863 | time=23.8s\n",
            "Epoch 016 | train_loss=0.7010 acc=0.6819 | val_loss=0.6724 acc=0.7143 | time=24.0s\n",
            "Epoch 017 | train_loss=0.7327 acc=0.6633 | val_loss=0.7401 acc=0.6522 | time=23.8s\n",
            "Epoch 018 | train_loss=0.6948 acc=0.6769 | val_loss=0.7490 acc=0.6708 | time=23.8s\n",
            "Epoch 019 | train_loss=0.6807 acc=0.6878 | val_loss=0.6645 acc=0.7360 | time=23.9s\n",
            "Epoch 020 | train_loss=0.6817 acc=0.6944 | val_loss=0.7636 acc=0.6910 | time=23.8s\n",
            "Epoch 021 | train_loss=0.6521 acc=0.6967 | val_loss=0.6670 acc=0.7034 | time=23.7s\n",
            "Epoch 022 | train_loss=0.6383 acc=0.7134 | val_loss=0.8124 acc=0.6460 | time=23.7s\n",
            "Epoch 023 | train_loss=0.6278 acc=0.7122 | val_loss=0.7218 acc=0.6724 | time=23.9s\n",
            "Epoch 024 | train_loss=0.6095 acc=0.7157 | val_loss=0.7025 acc=0.6693 | time=23.9s\n",
            "Epoch 025 | train_loss=0.6096 acc=0.7293 | val_loss=0.7122 acc=0.6988 | time=23.8s\n",
            "Epoch 026 | train_loss=0.5886 acc=0.7282 | val_loss=0.7240 acc=0.7034 | time=23.8s\n",
            "Epoch 027 | train_loss=0.5417 acc=0.7584 | val_loss=0.8390 acc=0.6584 | time=23.7s\n",
            "Epoch 028 | train_loss=0.5417 acc=0.7561 | val_loss=0.7678 acc=0.6801 | time=23.8s\n",
            "Epoch 029 | train_loss=0.5249 acc=0.7654 | val_loss=0.8237 acc=0.6739 | time=23.8s\n",
            "★ Early stopping at epoch 29\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▃▅▅▅▅▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▅▄▆▄▆▅▅▇▇▅▇█▆▇█▇▇▆▇▆▇▇▆▇▇</td></tr><tr><td>validation_loss</td><td>████▆▆▄▅▄▄▄▃▃▄▂▁▂▂▁▃▁▃▂▂▂▂▄▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>29</td></tr><tr><td>train_accuracy</td><td>0.76544</td></tr><tr><td>train_loss</td><td>0.52494</td></tr><tr><td>validation_accuracy</td><td>0.67391</td></tr><tr><td>validation_loss</td><td>0.82371</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">faithful-haze-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/52jp4tm2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/52jp4tm2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_135737-52jp4tm2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 14:09:10,844] Trial 3 finished with value: 0.6644889655567351 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 2, 'num_heads': 2, 'num_segments': 5}. Best is trial 2 with value: 0.6300574328218188.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_140910-vrgg7gn4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/vrgg7gn4' target=\"_blank\">good-blaze-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/vrgg7gn4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/vrgg7gn4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=2, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.1563 acc=0.3864 | val_loss=1.0747 acc=0.4317 | time=18.2s\n",
            "Epoch 002 | train_loss=1.1275 acc=0.3868 | val_loss=1.0750 acc=0.4317 | time=18.2s\n",
            "Epoch 003 | train_loss=1.0930 acc=0.4035 | val_loss=1.0744 acc=0.4317 | time=18.1s\n",
            "Epoch 004 | train_loss=1.1034 acc=0.3915 | val_loss=1.0863 acc=0.4317 | time=18.3s\n",
            "Epoch 005 | train_loss=1.0922 acc=0.4066 | val_loss=1.0807 acc=0.4317 | time=18.1s\n",
            "Epoch 006 | train_loss=1.0965 acc=0.4078 | val_loss=1.0798 acc=0.4317 | time=18.3s\n",
            "Epoch 007 | train_loss=1.0952 acc=0.4074 | val_loss=1.0850 acc=0.4317 | time=18.2s\n",
            "Epoch 008 | train_loss=1.0870 acc=0.4039 | val_loss=1.0876 acc=0.4317 | time=18.0s\n",
            "Epoch 009 | train_loss=1.0877 acc=0.4078 | val_loss=1.0774 acc=0.4317 | time=18.1s\n",
            "Epoch 010 | train_loss=1.0931 acc=0.3829 | val_loss=1.0773 acc=0.4317 | time=18.0s\n",
            "Epoch 011 | train_loss=1.0815 acc=0.4276 | val_loss=1.0759 acc=0.4317 | time=18.1s\n",
            "Epoch 012 | train_loss=1.0792 acc=0.4105 | val_loss=1.0753 acc=0.4317 | time=18.2s\n",
            "Epoch 013 | train_loss=1.0811 acc=0.4144 | val_loss=1.0751 acc=0.4317 | time=18.2s\n",
            "★ Early stopping at epoch 13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_accuracy</td><td>▂▂▄▂▅▅▅▄▅▁█▅▆</td></tr><tr><td>train_loss</td><td>█▅▂▃▂▃▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▁▁▇▄▄▇█▃▃▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_accuracy</td><td>0.41437</td></tr><tr><td>train_loss</td><td>1.08113</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07513</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-blaze-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/vrgg7gn4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/vrgg7gn4</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_140910-vrgg7gn4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 14:13:09,071] Trial 4 finished with value: 1.0744036038716633 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 2, 'num_heads': 1, 'num_segments': 5}. Best is trial 2 with value: 0.6300574328218188.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_141309-0o27fhmu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/0o27fhmu' target=\"_blank\">pleasant-sea-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/0o27fhmu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/0o27fhmu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1470 acc=0.3720 | val_loss=1.0757 acc=0.4317 | time=29.3s\n",
            "Epoch 002 | train_loss=1.1211 acc=0.3899 | val_loss=1.0831 acc=0.4596 | time=29.3s\n",
            "Epoch 003 | train_loss=1.1192 acc=0.3852 | val_loss=1.0760 acc=0.4317 | time=29.3s\n",
            "Epoch 004 | train_loss=1.0912 acc=0.3946 | val_loss=1.0752 acc=0.5326 | time=29.3s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4326 | val_loss=1.0450 acc=0.5186 | time=29.2s\n",
            "Epoch 006 | train_loss=1.0316 acc=0.4862 | val_loss=1.0473 acc=0.5171 | time=29.3s\n",
            "Epoch 007 | train_loss=0.9751 acc=0.5282 | val_loss=0.9226 acc=0.6242 | time=29.5s\n",
            "Epoch 008 | train_loss=0.9431 acc=0.5635 | val_loss=0.8909 acc=0.6273 | time=29.2s\n",
            "Epoch 009 | train_loss=0.8889 acc=0.5934 | val_loss=0.8421 acc=0.6304 | time=29.2s\n",
            "Epoch 010 | train_loss=0.8760 acc=0.6124 | val_loss=0.7831 acc=0.6599 | time=29.4s\n",
            "Epoch 011 | train_loss=0.8420 acc=0.6163 | val_loss=0.8462 acc=0.6599 | time=29.2s\n",
            "Epoch 012 | train_loss=0.8078 acc=0.6295 | val_loss=0.7651 acc=0.6351 | time=29.3s\n",
            "Epoch 013 | train_loss=0.8138 acc=0.6361 | val_loss=0.7665 acc=0.6584 | time=29.4s\n",
            "Epoch 014 | train_loss=0.7940 acc=0.6470 | val_loss=0.8119 acc=0.6289 | time=29.3s\n",
            "Epoch 015 | train_loss=0.7748 acc=0.6485 | val_loss=0.7749 acc=0.7127 | time=29.3s\n",
            "Epoch 016 | train_loss=0.7244 acc=0.6905 | val_loss=0.7189 acc=0.7034 | time=29.3s\n",
            "Epoch 017 | train_loss=0.7067 acc=0.6850 | val_loss=0.6827 acc=0.7174 | time=29.5s\n",
            "Epoch 018 | train_loss=0.7001 acc=0.6932 | val_loss=0.6873 acc=0.7034 | time=29.3s\n",
            "Epoch 019 | train_loss=0.6931 acc=0.6963 | val_loss=0.8402 acc=0.5699 | time=29.2s\n",
            "Epoch 020 | train_loss=0.6815 acc=0.6990 | val_loss=0.7540 acc=0.6817 | time=29.4s\n",
            "Epoch 021 | train_loss=0.6570 acc=0.7095 | val_loss=0.6330 acc=0.7220 | time=29.4s\n",
            "Epoch 022 | train_loss=0.6691 acc=0.7080 | val_loss=0.7145 acc=0.6972 | time=29.2s\n",
            "Epoch 023 | train_loss=0.6412 acc=0.7138 | val_loss=0.7611 acc=0.6165 | time=29.6s\n",
            "Epoch 024 | train_loss=0.6206 acc=0.7351 | val_loss=0.6406 acc=0.7158 | time=29.2s\n",
            "Epoch 025 | train_loss=0.6040 acc=0.7414 | val_loss=0.6787 acc=0.6925 | time=29.2s\n",
            "Epoch 026 | train_loss=0.5857 acc=0.7425 | val_loss=0.7036 acc=0.7345 | time=29.3s\n",
            "Epoch 027 | train_loss=0.5571 acc=0.7623 | val_loss=0.7899 acc=0.6444 | time=29.3s\n",
            "Epoch 028 | train_loss=0.5267 acc=0.7674 | val_loss=0.7190 acc=0.7189 | time=29.2s\n",
            "Epoch 029 | train_loss=0.5240 acc=0.7658 | val_loss=0.6873 acc=0.7096 | time=29.3s\n",
            "Epoch 030 | train_loss=0.5164 acc=0.7829 | val_loss=0.8199 acc=0.6863 | time=29.2s\n",
            "Epoch 031 | train_loss=0.4959 acc=0.7849 | val_loss=0.7791 acc=0.6848 | time=29.2s\n",
            "★ Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▃▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>███▇▇▇▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▁▃▃▃▅▆▆▆▆▆▆▆▇▇█▇▄▇█▇▅█▇█▆█▇▇▇</td></tr><tr><td>validation_loss</td><td>████▇▇▆▅▄▃▄▃▃▄▃▂▂▂▄▃▁▂▃▁▂▂▃▂▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>31</td></tr><tr><td>train_accuracy</td><td>0.78485</td></tr><tr><td>train_loss</td><td>0.4959</td></tr><tr><td>validation_accuracy</td><td>0.68478</td></tr><tr><td>validation_loss</td><td>0.77909</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">pleasant-sea-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/0o27fhmu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17/runs/0o27fhmu</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-17</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_141309-0o27fhmu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 14:28:20,094] Trial 5 finished with value: 0.6330114475318364 and parameters: {'lr': 0.0005, 'weight_decay': 0.005, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5}. Best is trial 2 with value: 0.6300574328218188.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.630057\n",
            "best_train_loss     = 0.649820\n",
            "best_train_accuracy = 0.7087\n",
            "best_val_accuracy   = 0.7376\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.0005\n",
            "  num_blocks: 2\n",
            "  num_heads: 1\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R4bQs-D8LWB4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Blocks = 2 -> 3 으로 표현력 늘려보기\n",
        "- Random Seed 바꿔보기\n",
        "- 추후에 여기서 잘 나온 거 기반으로 5 Fold Cross Validation 돌려보기"
      ],
      "metadata": {
        "id": "tpr4FvjcNLmm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_3 import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [5e-4]\n",
        "WD_CHOICES          = [5e-3, 5e-4, 5e-6]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [3]\n",
        "NUM_HEAD_CHOICES    = [1, 2, 3]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 15\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search-18\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search-18\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search-18.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WdmoA5kHm82M",
        "outputId": "fb05ae0d-5591-4cfe-a178-a07a329f0ab1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 03:41:53,234] A new study created in RDB with name: eeg_holdout_grid_search-18\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_034201-e00rhzwr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/e00rhzwr' target=\"_blank\">divine-sun-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/e00rhzwr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/e00rhzwr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=3, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1592 acc=0.3616 | val_loss=1.0927 acc=0.4317 | time=183.0s\n",
            "Epoch 002 | train_loss=1.1299 acc=0.3751 | val_loss=1.0919 acc=0.4317 | time=40.4s\n",
            "Epoch 003 | train_loss=1.1114 acc=0.3817 | val_loss=1.0768 acc=0.4317 | time=40.6s\n",
            "Epoch 004 | train_loss=1.0910 acc=0.4062 | val_loss=1.0754 acc=0.4317 | time=40.4s\n",
            "Epoch 005 | train_loss=1.0962 acc=0.3977 | val_loss=1.0743 acc=0.4317 | time=40.4s\n",
            "Epoch 006 | train_loss=1.0975 acc=0.4004 | val_loss=1.0736 acc=0.4317 | time=40.3s\n",
            "Epoch 007 | train_loss=1.0898 acc=0.4012 | val_loss=1.0775 acc=0.4317 | time=40.4s\n",
            "Epoch 008 | train_loss=1.0764 acc=0.4066 | val_loss=1.0763 acc=0.4317 | time=40.6s\n",
            "Epoch 009 | train_loss=1.0783 acc=0.4198 | val_loss=1.0746 acc=0.4317 | time=40.3s\n",
            "Epoch 010 | train_loss=1.0646 acc=0.4272 | val_loss=1.0789 acc=0.4317 | time=40.3s\n",
            "Epoch 011 | train_loss=1.0739 acc=0.4167 | val_loss=1.0636 acc=0.4317 | time=40.3s\n",
            "Epoch 012 | train_loss=1.0606 acc=0.4489 | val_loss=1.0098 acc=0.5652 | time=40.4s\n",
            "Epoch 013 | train_loss=1.0205 acc=0.5146 | val_loss=1.0022 acc=0.6118 | time=40.4s\n",
            "Epoch 014 | train_loss=0.9893 acc=0.5406 | val_loss=0.9783 acc=0.5481 | time=40.4s\n",
            "Epoch 015 | train_loss=0.9446 acc=0.5546 | val_loss=0.8919 acc=0.6211 | time=40.3s\n",
            "Epoch 016 | train_loss=0.9129 acc=0.5864 | val_loss=0.9398 acc=0.5637 | time=40.2s\n",
            "Epoch 017 | train_loss=0.9020 acc=0.5880 | val_loss=0.8695 acc=0.6382 | time=40.5s\n",
            "Epoch 018 | train_loss=0.8729 acc=0.6012 | val_loss=0.8814 acc=0.5823 | time=40.5s\n",
            "Epoch 019 | train_loss=0.8446 acc=0.6136 | val_loss=0.8251 acc=0.6304 | time=40.3s\n",
            "Epoch 020 | train_loss=0.8220 acc=0.6315 | val_loss=0.8669 acc=0.6118 | time=40.3s\n",
            "Epoch 021 | train_loss=0.7825 acc=0.6412 | val_loss=0.7793 acc=0.6755 | time=40.2s\n",
            "Epoch 022 | train_loss=0.7598 acc=0.6489 | val_loss=0.9193 acc=0.5512 | time=40.5s\n",
            "Epoch 023 | train_loss=0.7502 acc=0.6718 | val_loss=0.8106 acc=0.6211 | time=40.4s\n",
            "Epoch 024 | train_loss=0.7458 acc=0.6730 | val_loss=0.8772 acc=0.5901 | time=40.4s\n",
            "Epoch 025 | train_loss=0.7090 acc=0.6874 | val_loss=0.7861 acc=0.6087 | time=40.4s\n",
            "Epoch 026 | train_loss=0.6906 acc=0.6990 | val_loss=0.7685 acc=0.6506 | time=40.3s\n",
            "Epoch 027 | train_loss=0.6678 acc=0.7130 | val_loss=0.9151 acc=0.6211 | time=40.5s\n",
            "Epoch 028 | train_loss=0.6515 acc=0.7146 | val_loss=0.7072 acc=0.7034 | time=40.4s\n",
            "Epoch 029 | train_loss=0.6647 acc=0.7049 | val_loss=0.7977 acc=0.6568 | time=40.5s\n",
            "Epoch 030 | train_loss=0.6162 acc=0.7258 | val_loss=0.7670 acc=0.6972 | time=40.3s\n",
            "Epoch 031 | train_loss=0.6368 acc=0.7212 | val_loss=0.7666 acc=0.6646 | time=40.4s\n",
            "Epoch 032 | train_loss=0.6152 acc=0.7379 | val_loss=0.7186 acc=0.6817 | time=40.6s\n",
            "Epoch 033 | train_loss=0.6139 acc=0.7371 | val_loss=0.8128 acc=0.6770 | time=40.4s\n",
            "Epoch 034 | train_loss=0.5850 acc=0.7414 | val_loss=0.8628 acc=0.6180 | time=40.3s\n",
            "Epoch 035 | train_loss=0.5518 acc=0.7643 | val_loss=0.9303 acc=0.6553 | time=40.4s\n",
            "Epoch 036 | train_loss=0.5338 acc=0.7650 | val_loss=0.7859 acc=0.6677 | time=40.5s\n",
            "Epoch 037 | train_loss=0.5315 acc=0.7693 | val_loss=0.8730 acc=0.6661 | time=40.6s\n",
            "Epoch 038 | train_loss=0.5220 acc=0.7658 | val_loss=0.9890 acc=0.6398 | time=40.4s\n",
            "Epoch 039 | train_loss=0.4844 acc=0.7915 | val_loss=1.0375 acc=0.6149 | time=40.3s\n",
            "Epoch 040 | train_loss=0.5105 acc=0.7829 | val_loss=1.0165 acc=0.6351 | time=40.3s\n",
            "Epoch 041 | train_loss=0.4794 acc=0.7899 | val_loss=0.8820 acc=0.6568 | time=40.5s\n",
            "Epoch 042 | train_loss=0.4667 acc=0.7907 | val_loss=0.9872 acc=0.6366 | time=40.6s\n",
            "Epoch 043 | train_loss=0.4812 acc=0.7961 | val_loss=1.0729 acc=0.5932 | time=40.3s\n",
            "★ Early stopping at epoch 43\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▂▂▂▂▂▂▂▃▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▇▇▇▇▆▅▅▅▅▅▄▄▄▄▃▃▃▃▂▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▆▆▄▆▅▆▆▇▄▆▅▆▇▆▇█▇█▇▆▇▇▇▆▆▆▇▅</td></tr><tr><td>validation_loss</td><td>██████████▇▆▆▄▅▄▄▃▄▂▅▃▄▂▂▅▂▂▂▁▃▄▅▂▄▆▇▇▄█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>43</td></tr><tr><td>train_accuracy</td><td>0.79612</td></tr><tr><td>train_loss</td><td>0.48118</td></tr><tr><td>validation_accuracy</td><td>0.59317</td></tr><tr><td>validation_loss</td><td>1.07291</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-sun-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/e00rhzwr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/e00rhzwr</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_034201-e00rhzwr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 04:13:30,024] Trial 0 finished with value: 0.7071966386976696 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 3, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 0.7071966386976696.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_041330-tyh3aa6l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/tyh3aa6l' target=\"_blank\">azure-tree-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/tyh3aa6l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/tyh3aa6l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=5.00e-04, wd=5.00e-06, blocks=3, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.1435 acc=0.3577 | val_loss=1.0758 acc=0.4317 | time=23.6s\n",
            "Epoch 002 | train_loss=1.1207 acc=0.3802 | val_loss=1.0770 acc=0.4317 | time=23.8s\n",
            "Epoch 003 | train_loss=1.1043 acc=0.3965 | val_loss=1.0782 acc=0.4317 | time=23.8s\n",
            "Epoch 004 | train_loss=1.0955 acc=0.3814 | val_loss=1.0771 acc=0.4317 | time=23.8s\n",
            "Epoch 005 | train_loss=1.0822 acc=0.4194 | val_loss=1.0687 acc=0.5124 | time=23.8s\n",
            "Epoch 006 | train_loss=1.1063 acc=0.3810 | val_loss=1.0939 acc=0.4317 | time=23.7s\n",
            "Epoch 007 | train_loss=1.1042 acc=0.3891 | val_loss=1.0661 acc=0.4317 | time=23.7s\n",
            "Epoch 008 | train_loss=1.0684 acc=0.4315 | val_loss=1.0526 acc=0.5916 | time=23.6s\n",
            "Epoch 009 | train_loss=1.0186 acc=0.5029 | val_loss=1.0184 acc=0.5668 | time=23.7s\n",
            "Epoch 010 | train_loss=1.0062 acc=0.5118 | val_loss=1.0571 acc=0.3727 | time=23.7s\n",
            "Epoch 011 | train_loss=0.9728 acc=0.5530 | val_loss=1.0631 acc=0.3478 | time=23.9s\n",
            "Epoch 012 | train_loss=0.9612 acc=0.5584 | val_loss=0.9198 acc=0.6134 | time=23.7s\n",
            "Epoch 013 | train_loss=0.9378 acc=0.5751 | val_loss=0.9203 acc=0.6398 | time=23.7s\n",
            "Epoch 014 | train_loss=0.9099 acc=0.5817 | val_loss=0.8790 acc=0.6506 | time=23.8s\n",
            "Epoch 015 | train_loss=0.8921 acc=0.5934 | val_loss=0.8807 acc=0.6630 | time=23.8s\n",
            "Epoch 016 | train_loss=0.8851 acc=0.6019 | val_loss=0.8451 acc=0.6879 | time=23.7s\n",
            "Epoch 017 | train_loss=0.8567 acc=0.6264 | val_loss=0.8332 acc=0.6910 | time=23.7s\n",
            "Epoch 018 | train_loss=0.8239 acc=0.6392 | val_loss=0.8633 acc=0.6755 | time=23.9s\n",
            "Epoch 019 | train_loss=0.8068 acc=0.6443 | val_loss=0.8599 acc=0.6242 | time=23.8s\n",
            "Epoch 020 | train_loss=0.8245 acc=0.6350 | val_loss=0.8416 acc=0.6429 | time=23.7s\n",
            "Epoch 021 | train_loss=0.7692 acc=0.6536 | val_loss=0.7298 acc=0.6832 | time=23.7s\n",
            "Epoch 022 | train_loss=0.7918 acc=0.6435 | val_loss=0.7240 acc=0.7050 | time=23.6s\n",
            "Epoch 023 | train_loss=0.7541 acc=0.6625 | val_loss=0.7803 acc=0.6755 | time=23.7s\n",
            "Epoch 024 | train_loss=0.7194 acc=0.6734 | val_loss=0.7049 acc=0.6925 | time=23.6s\n",
            "Epoch 025 | train_loss=0.7331 acc=0.6796 | val_loss=0.8875 acc=0.5280 | time=23.7s\n",
            "Epoch 026 | train_loss=0.7240 acc=0.6718 | val_loss=0.7162 acc=0.6941 | time=23.8s\n",
            "Epoch 027 | train_loss=0.7126 acc=0.6761 | val_loss=0.6983 acc=0.7127 | time=23.7s\n",
            "Epoch 028 | train_loss=0.6945 acc=0.6928 | val_loss=0.7577 acc=0.6925 | time=23.8s\n",
            "Epoch 029 | train_loss=0.6691 acc=0.7010 | val_loss=0.7451 acc=0.6646 | time=23.7s\n",
            "Epoch 030 | train_loss=0.6708 acc=0.7072 | val_loss=0.7344 acc=0.6925 | time=23.5s\n",
            "Epoch 031 | train_loss=0.6455 acc=0.7181 | val_loss=0.7233 acc=0.6708 | time=23.6s\n",
            "Epoch 032 | train_loss=0.6297 acc=0.7130 | val_loss=0.7145 acc=0.7065 | time=23.8s\n",
            "Epoch 033 | train_loss=0.6530 acc=0.7138 | val_loss=0.8240 acc=0.5963 | time=24.0s\n",
            "Epoch 034 | train_loss=0.6015 acc=0.7417 | val_loss=0.7200 acc=0.6646 | time=23.7s\n",
            "Epoch 035 | train_loss=0.5880 acc=0.7351 | val_loss=0.7686 acc=0.6553 | time=23.6s\n",
            "Epoch 036 | train_loss=0.5657 acc=0.7417 | val_loss=0.7310 acc=0.6894 | time=23.6s\n",
            "Epoch 037 | train_loss=0.5530 acc=0.7701 | val_loss=0.9106 acc=0.6289 | time=23.7s\n",
            "Epoch 038 | train_loss=0.5396 acc=0.7592 | val_loss=0.8556 acc=0.6320 | time=23.7s\n",
            "Epoch 039 | train_loss=0.5408 acc=0.7701 | val_loss=0.7578 acc=0.6630 | time=23.7s\n",
            "Epoch 040 | train_loss=0.5009 acc=0.7786 | val_loss=0.9518 acc=0.5807 | time=23.8s\n",
            "Epoch 041 | train_loss=0.5072 acc=0.7856 | val_loss=0.8064 acc=0.6506 | time=23.8s\n",
            "Epoch 042 | train_loss=0.4963 acc=0.7984 | val_loss=0.9641 acc=0.5761 | time=23.8s\n",
            "★ Early stopping at epoch 42\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▂▁▁▂▃▃▄▄▄▅▅▅▅▅▆▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇███</td></tr><tr><td>train_loss</td><td>███▇▇██▇▇▇▆▆▆▅▅▅▅▅▄▅▄▄▃▄▃▃▃▃▃▃▂▃▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▄▃▃▆▅▁▁▆▇▇▇██▇▆▇█▇█▄███▇█▇█▆▇▇█▆▆▇▅▅</td></tr><tr><td>validation_loss</td><td>███████▇▇▇▇▅▅▄▄▄▃▄▄▄▁▂▁▄▁▁▂▂▂▁▁▃▁▂▂▅▄▂▅▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>42</td></tr><tr><td>train_accuracy</td><td>0.79845</td></tr><tr><td>train_loss</td><td>0.49634</td></tr><tr><td>validation_accuracy</td><td>0.57609</td></tr><tr><td>validation_loss</td><td>0.96411</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">azure-tree-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/tyh3aa6l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/tyh3aa6l</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_041330-tyh3aa6l/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 04:30:09,750] Trial 1 finished with value: 0.6982603782699222 and parameters: {'lr': 0.0005, 'weight_decay': 5e-06, 'num_blocks': 3, 'num_heads': 1, 'num_segments': 5}. Best is trial 1 with value: 0.6982603782699222.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_043009-690qyyh6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/690qyyh6' target=\"_blank\">stellar-universe-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/690qyyh6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/690qyyh6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=3, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.1314 acc=0.3584 | val_loss=1.0756 acc=0.4317 | time=23.9s\n",
            "Epoch 002 | train_loss=1.1067 acc=0.3810 | val_loss=1.0760 acc=0.4317 | time=23.7s\n",
            "Epoch 003 | train_loss=1.0949 acc=0.3946 | val_loss=1.0763 acc=0.4317 | time=23.6s\n",
            "Epoch 004 | train_loss=1.0905 acc=0.3942 | val_loss=1.0749 acc=0.4317 | time=23.8s\n",
            "Epoch 005 | train_loss=1.0794 acc=0.4190 | val_loss=1.0689 acc=0.4317 | time=23.7s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4342 | val_loss=1.0983 acc=0.3416 | time=23.6s\n",
            "Epoch 007 | train_loss=1.0226 acc=0.4983 | val_loss=0.9701 acc=0.6056 | time=23.7s\n",
            "Epoch 008 | train_loss=0.9678 acc=0.5464 | val_loss=0.9184 acc=0.5901 | time=24.0s\n",
            "Epoch 009 | train_loss=0.9223 acc=0.5783 | val_loss=0.9658 acc=0.5124 | time=23.7s\n",
            "Epoch 010 | train_loss=0.9115 acc=0.5814 | val_loss=0.9145 acc=0.6273 | time=23.6s\n",
            "Epoch 011 | train_loss=0.8918 acc=0.5996 | val_loss=0.8426 acc=0.6398 | time=23.7s\n",
            "Epoch 012 | train_loss=0.8564 acc=0.6054 | val_loss=0.8898 acc=0.5730 | time=23.7s\n",
            "Epoch 013 | train_loss=0.8606 acc=0.6082 | val_loss=0.8657 acc=0.6351 | time=23.7s\n",
            "Epoch 014 | train_loss=0.8236 acc=0.6280 | val_loss=0.8210 acc=0.6413 | time=23.8s\n",
            "Epoch 015 | train_loss=0.7795 acc=0.6536 | val_loss=0.7798 acc=0.6739 | time=23.8s\n",
            "Epoch 016 | train_loss=0.7661 acc=0.6571 | val_loss=0.8516 acc=0.6460 | time=23.9s\n",
            "Epoch 017 | train_loss=0.7675 acc=0.6548 | val_loss=0.8057 acc=0.6941 | time=23.8s\n",
            "Epoch 018 | train_loss=0.7218 acc=0.6703 | val_loss=0.7546 acc=0.6925 | time=23.7s\n",
            "Epoch 019 | train_loss=0.7137 acc=0.6839 | val_loss=0.7714 acc=0.6879 | time=23.7s\n",
            "Epoch 020 | train_loss=0.7124 acc=0.6827 | val_loss=0.8183 acc=0.6444 | time=23.7s\n",
            "Epoch 021 | train_loss=0.7099 acc=0.6800 | val_loss=0.7223 acc=0.6879 | time=23.6s\n",
            "Epoch 022 | train_loss=0.6988 acc=0.6913 | val_loss=0.7519 acc=0.6739 | time=23.8s\n",
            "Epoch 023 | train_loss=0.6570 acc=0.7006 | val_loss=0.7502 acc=0.7158 | time=23.9s\n",
            "Epoch 024 | train_loss=0.6860 acc=0.6924 | val_loss=0.7407 acc=0.6832 | time=23.8s\n",
            "Epoch 025 | train_loss=0.6194 acc=0.7301 | val_loss=0.8299 acc=0.6724 | time=23.7s\n",
            "Epoch 026 | train_loss=0.6230 acc=0.7258 | val_loss=0.8155 acc=0.6708 | time=23.7s\n",
            "Epoch 027 | train_loss=0.6036 acc=0.7425 | val_loss=0.8879 acc=0.6460 | time=23.7s\n",
            "Epoch 028 | train_loss=0.5874 acc=0.7379 | val_loss=0.8365 acc=0.6537 | time=23.8s\n",
            "Epoch 029 | train_loss=0.5724 acc=0.7464 | val_loss=0.7469 acc=0.6957 | time=23.7s\n",
            "Epoch 030 | train_loss=0.5454 acc=0.7581 | val_loss=0.8660 acc=0.6475 | time=23.8s\n",
            "Epoch 031 | train_loss=0.5477 acc=0.7507 | val_loss=1.0685 acc=0.6087 | time=23.8s\n",
            "Epoch 032 | train_loss=0.5474 acc=0.7592 | val_loss=1.0507 acc=0.6149 | time=23.8s\n",
            "Epoch 033 | train_loss=0.5197 acc=0.7627 | val_loss=1.1829 acc=0.5559 | time=23.7s\n",
            "Epoch 034 | train_loss=0.4805 acc=0.7864 | val_loss=0.8912 acc=0.6491 | time=23.7s\n",
            "Epoch 035 | train_loss=0.4979 acc=0.7852 | val_loss=0.9595 acc=0.6227 | time=23.5s\n",
            "Epoch 036 | train_loss=0.4789 acc=0.7856 | val_loss=0.8512 acc=0.6506 | time=23.7s\n",
            "★ Early stopping at epoch 36\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▂▂▃▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇█▇█████</td></tr><tr><td>train_loss</td><td>████▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▃▃▃▃▃▁▆▆▄▆▇▅▆▇▇▇██▇▇▇▇█▇▇▇▇▇█▇▆▆▅▇▆▇</td></tr><tr><td>validation_loss</td><td>▆▆▆▆▆▇▅▄▅▄▃▄▃▃▂▃▂▁▂▂▁▁▁▁▃▂▄▃▁▃▆▆█▄▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>36</td></tr><tr><td>train_accuracy</td><td>0.78563</td></tr><tr><td>train_loss</td><td>0.47894</td></tr><tr><td>validation_accuracy</td><td>0.65062</td></tr><tr><td>validation_loss</td><td>0.85122</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-universe-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/690qyyh6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/690qyyh6</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_043009-690qyyh6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 04:44:27,409] Trial 2 finished with value: 0.7222818647112165 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 3, 'num_heads': 1, 'num_segments': 5}. Best is trial 1 with value: 0.6982603782699222.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_044427-5fb6i6h5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/5fb6i6h5' target=\"_blank\">stellar-violet-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/5fb6i6h5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/5fb6i6h5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=3, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1429 acc=0.3588 | val_loss=1.0759 acc=0.4317 | time=32.4s\n",
            "Epoch 002 | train_loss=1.1154 acc=0.3907 | val_loss=1.0803 acc=0.4317 | time=32.2s\n",
            "Epoch 003 | train_loss=1.1014 acc=0.3907 | val_loss=1.0754 acc=0.4317 | time=32.2s\n",
            "Epoch 004 | train_loss=1.0963 acc=0.3833 | val_loss=1.0744 acc=0.4317 | time=32.3s\n",
            "Epoch 005 | train_loss=1.0938 acc=0.4000 | val_loss=1.0750 acc=0.4317 | time=32.3s\n",
            "Epoch 006 | train_loss=1.0822 acc=0.4194 | val_loss=1.0758 acc=0.4317 | time=32.3s\n",
            "Epoch 007 | train_loss=1.0751 acc=0.4120 | val_loss=1.0732 acc=0.4317 | time=32.1s\n",
            "Epoch 008 | train_loss=1.0720 acc=0.4249 | val_loss=1.0656 acc=0.4317 | time=32.4s\n",
            "Epoch 009 | train_loss=1.0596 acc=0.4528 | val_loss=1.0664 acc=0.4441 | time=32.2s\n",
            "Epoch 010 | train_loss=1.0010 acc=0.5247 | val_loss=0.9649 acc=0.6009 | time=32.4s\n",
            "Epoch 011 | train_loss=0.9427 acc=0.5740 | val_loss=0.8823 acc=0.6211 | time=32.3s\n",
            "Epoch 012 | train_loss=0.9020 acc=0.6000 | val_loss=0.8593 acc=0.6599 | time=32.4s\n",
            "Epoch 013 | train_loss=0.8837 acc=0.6043 | val_loss=0.8161 acc=0.6646 | time=32.2s\n",
            "Epoch 014 | train_loss=0.8400 acc=0.6268 | val_loss=0.7832 acc=0.6957 | time=32.4s\n",
            "Epoch 015 | train_loss=0.8175 acc=0.6377 | val_loss=0.8737 acc=0.6227 | time=32.2s\n",
            "Epoch 016 | train_loss=0.8127 acc=0.6280 | val_loss=0.7751 acc=0.6832 | time=32.4s\n",
            "Epoch 017 | train_loss=0.7910 acc=0.6338 | val_loss=0.7737 acc=0.7019 | time=32.4s\n",
            "Epoch 018 | train_loss=0.7688 acc=0.6524 | val_loss=0.7384 acc=0.6957 | time=32.3s\n",
            "Epoch 019 | train_loss=0.7333 acc=0.6699 | val_loss=0.7431 acc=0.6832 | time=32.4s\n",
            "Epoch 020 | train_loss=0.7177 acc=0.6823 | val_loss=0.7448 acc=0.6630 | time=32.2s\n",
            "Epoch 021 | train_loss=0.7100 acc=0.6773 | val_loss=0.7234 acc=0.6972 | time=32.4s\n",
            "Epoch 022 | train_loss=0.6940 acc=0.6905 | val_loss=0.7606 acc=0.6894 | time=32.2s\n",
            "Epoch 023 | train_loss=0.6979 acc=0.6823 | val_loss=0.8195 acc=0.6102 | time=32.3s\n",
            "Epoch 024 | train_loss=0.6754 acc=0.6862 | val_loss=0.8899 acc=0.5901 | time=32.3s\n",
            "Epoch 025 | train_loss=0.6878 acc=0.7002 | val_loss=0.7509 acc=0.6615 | time=32.3s\n",
            "Epoch 026 | train_loss=0.6421 acc=0.7107 | val_loss=0.7527 acc=0.6879 | time=32.4s\n",
            "Epoch 027 | train_loss=0.6589 acc=0.7080 | val_loss=0.6678 acc=0.7298 | time=32.4s\n",
            "Epoch 028 | train_loss=0.6536 acc=0.7103 | val_loss=0.7244 acc=0.6693 | time=32.6s\n",
            "Epoch 029 | train_loss=0.6289 acc=0.7200 | val_loss=0.7480 acc=0.6134 | time=32.5s\n",
            "Epoch 030 | train_loss=0.6167 acc=0.7289 | val_loss=0.7460 acc=0.6786 | time=32.3s\n",
            "Epoch 031 | train_loss=0.5885 acc=0.7293 | val_loss=0.6913 acc=0.7050 | time=32.1s\n",
            "Epoch 032 | train_loss=0.5991 acc=0.7332 | val_loss=0.6957 acc=0.6506 | time=32.4s\n",
            "Epoch 033 | train_loss=0.6015 acc=0.7355 | val_loss=0.7550 acc=0.6040 | time=32.2s\n",
            "Epoch 034 | train_loss=0.5457 acc=0.7647 | val_loss=0.6476 acc=0.7205 | time=32.3s\n",
            "Epoch 035 | train_loss=0.5551 acc=0.7534 | val_loss=0.6630 acc=0.7003 | time=32.4s\n",
            "Epoch 036 | train_loss=0.5311 acc=0.7697 | val_loss=0.6246 acc=0.7345 | time=32.3s\n",
            "Epoch 037 | train_loss=0.5083 acc=0.7891 | val_loss=0.6936 acc=0.7484 | time=32.6s\n",
            "Epoch 038 | train_loss=0.5140 acc=0.7775 | val_loss=0.6203 acc=0.7422 | time=32.2s\n",
            "Epoch 039 | train_loss=0.4795 acc=0.7833 | val_loss=0.6183 acc=0.7500 | time=32.4s\n",
            "Epoch 040 | train_loss=0.5037 acc=0.7802 | val_loss=0.8364 acc=0.6755 | time=32.2s\n",
            "Epoch 041 | train_loss=0.4625 acc=0.7988 | val_loss=0.7193 acc=0.7127 | time=32.4s\n",
            "Epoch 042 | train_loss=0.4455 acc=0.8082 | val_loss=0.7997 acc=0.6693 | time=32.2s\n",
            "Epoch 043 | train_loss=0.4545 acc=0.8039 | val_loss=0.7115 acc=0.7127 | time=32.4s\n",
            "Epoch 044 | train_loss=0.4600 acc=0.8012 | val_loss=0.7937 acc=0.7236 | time=32.3s\n",
            "Epoch 045 | train_loss=0.4640 acc=0.7981 | val_loss=0.6934 acc=0.7360 | time=32.3s\n",
            "Epoch 046 | train_loss=0.4340 acc=0.8031 | val_loss=0.7106 acc=0.7484 | time=32.5s\n",
            "Epoch 047 | train_loss=0.4223 acc=0.8074 | val_loss=0.6309 acc=0.7609 | time=32.3s\n",
            "Epoch 048 | train_loss=0.4439 acc=0.8004 | val_loss=0.7316 acc=0.7345 | time=32.3s\n",
            "Epoch 049 | train_loss=0.4262 acc=0.8183 | val_loss=0.8378 acc=0.7220 | time=32.2s\n",
            "Epoch 050 | train_loss=0.4238 acc=0.8155 | val_loss=0.7311 acc=0.7469 | time=32.3s\n",
            "Epoch 051 | train_loss=0.3948 acc=0.8303 | val_loss=0.6632 acc=0.7453 | time=32.2s\n",
            "Epoch 052 | train_loss=0.3810 acc=0.8311 | val_loss=0.7010 acc=0.7500 | time=32.3s\n",
            "Epoch 053 | train_loss=0.3835 acc=0.8315 | val_loss=0.7517 acc=0.7329 | time=32.5s\n",
            "Epoch 054 | train_loss=0.3821 acc=0.8291 | val_loss=0.7483 acc=0.7236 | time=32.3s\n",
            "★ Early stopping at epoch 54\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▂▃▅▅▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇███████████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▅▅▆▇▇▇▇▇▇▅▄▆▅▆▇▆▅▇███▆▆▇▇██▇███▇</td></tr><tr><td>validation_loss</td><td>███████▅▅▄▃▃▃▃▃▄▅▃▃▂▃▃▂▂▃▂▁▂▁▁▃▄▂▂▁▄▃▂▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>54</td></tr><tr><td>train_accuracy</td><td>0.82913</td></tr><tr><td>train_loss</td><td>0.38215</td></tr><tr><td>validation_accuracy</td><td>0.7236</td></tr><tr><td>validation_loss</td><td>0.74834</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-violet-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/5fb6i6h5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/5fb6i6h5</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_044427-5fb6i6h5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 05:13:36,413] Trial 3 finished with value: 0.6183182043688638 and parameters: {'lr': 0.0005, 'weight_decay': 0.0005, 'num_blocks': 3, 'num_heads': 2, 'num_segments': 5}. Best is trial 3 with value: 0.6183182043688638.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_051336-lwvurprg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/lwvurprg' target=\"_blank\">wild-dream-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/lwvurprg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/lwvurprg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=5.00e-04, wd=5.00e-06, blocks=3, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1454 acc=0.3522 | val_loss=1.0839 acc=0.4317 | time=40.5s\n",
            "Epoch 002 | train_loss=1.1230 acc=0.3887 | val_loss=1.0779 acc=0.4317 | time=40.6s\n",
            "Epoch 003 | train_loss=1.1002 acc=0.3930 | val_loss=1.0815 acc=0.4317 | time=40.4s\n",
            "Epoch 004 | train_loss=1.0935 acc=0.3922 | val_loss=1.0751 acc=0.4317 | time=40.3s\n",
            "Epoch 005 | train_loss=1.0865 acc=0.4109 | val_loss=1.0790 acc=0.4317 | time=40.2s\n",
            "Epoch 006 | train_loss=1.1033 acc=0.3771 | val_loss=1.0743 acc=0.4317 | time=40.3s\n",
            "Epoch 007 | train_loss=1.0907 acc=0.3981 | val_loss=1.0752 acc=0.4317 | time=40.6s\n",
            "Epoch 008 | train_loss=1.0781 acc=0.4252 | val_loss=1.0762 acc=0.4317 | time=40.4s\n",
            "Epoch 009 | train_loss=1.0822 acc=0.4012 | val_loss=1.0753 acc=0.4317 | time=40.3s\n",
            "Epoch 010 | train_loss=1.0776 acc=0.4128 | val_loss=1.0754 acc=0.4317 | time=40.3s\n",
            "Epoch 011 | train_loss=1.0686 acc=0.4326 | val_loss=1.0734 acc=0.4317 | time=40.5s\n",
            "Epoch 012 | train_loss=1.0736 acc=0.4206 | val_loss=1.0743 acc=0.4317 | time=40.7s\n",
            "Epoch 013 | train_loss=1.0726 acc=0.4280 | val_loss=1.0737 acc=0.4317 | time=40.5s\n",
            "Epoch 014 | train_loss=1.0681 acc=0.4299 | val_loss=1.0659 acc=0.4317 | time=40.5s\n",
            "Epoch 015 | train_loss=1.0541 acc=0.4621 | val_loss=1.0274 acc=0.5652 | time=40.4s\n",
            "Epoch 016 | train_loss=1.0044 acc=0.5247 | val_loss=0.9612 acc=0.6040 | time=40.5s\n",
            "Epoch 017 | train_loss=0.9472 acc=0.5748 | val_loss=0.8962 acc=0.6304 | time=40.7s\n",
            "Epoch 018 | train_loss=0.9158 acc=0.5868 | val_loss=0.9255 acc=0.6227 | time=40.4s\n",
            "Epoch 019 | train_loss=0.8965 acc=0.5965 | val_loss=0.8099 acc=0.6398 | time=40.4s\n",
            "Epoch 020 | train_loss=0.8412 acc=0.6105 | val_loss=0.8501 acc=0.5916 | time=40.3s\n",
            "Epoch 021 | train_loss=0.8486 acc=0.6050 | val_loss=0.8119 acc=0.6553 | time=40.6s\n",
            "Epoch 022 | train_loss=0.8117 acc=0.6217 | val_loss=0.7663 acc=0.6444 | time=40.5s\n",
            "Epoch 023 | train_loss=0.7776 acc=0.6280 | val_loss=0.7417 acc=0.6398 | time=40.5s\n",
            "Epoch 024 | train_loss=0.7850 acc=0.6210 | val_loss=0.7414 acc=0.6724 | time=40.4s\n",
            "Epoch 025 | train_loss=0.7760 acc=0.6357 | val_loss=0.8655 acc=0.6118 | time=40.4s\n",
            "Epoch 026 | train_loss=0.7704 acc=0.6307 | val_loss=0.6975 acc=0.6724 | time=40.6s\n",
            "Epoch 027 | train_loss=0.7599 acc=0.6229 | val_loss=0.7669 acc=0.6537 | time=40.5s\n",
            "Epoch 028 | train_loss=0.7323 acc=0.6435 | val_loss=0.7575 acc=0.6615 | time=40.4s\n",
            "Epoch 029 | train_loss=0.7401 acc=0.6361 | val_loss=0.6824 acc=0.6724 | time=40.3s\n",
            "Epoch 030 | train_loss=0.7469 acc=0.6365 | val_loss=0.7686 acc=0.6506 | time=40.4s\n",
            "Epoch 031 | train_loss=0.7075 acc=0.6602 | val_loss=0.7243 acc=0.6366 | time=40.6s\n",
            "Epoch 032 | train_loss=0.7095 acc=0.6707 | val_loss=0.7658 acc=0.6801 | time=40.3s\n",
            "Epoch 033 | train_loss=0.6955 acc=0.6656 | val_loss=0.7860 acc=0.6832 | time=40.3s\n",
            "Epoch 034 | train_loss=0.6588 acc=0.6878 | val_loss=0.7071 acc=0.6848 | time=40.2s\n",
            "Epoch 035 | train_loss=0.6442 acc=0.6854 | val_loss=0.8081 acc=0.6724 | time=40.5s\n",
            "Epoch 036 | train_loss=0.6198 acc=0.7064 | val_loss=0.7591 acc=0.6848 | time=40.6s\n",
            "Epoch 037 | train_loss=0.6186 acc=0.6893 | val_loss=0.7531 acc=0.6786 | time=40.4s\n",
            "Epoch 038 | train_loss=0.6015 acc=0.6998 | val_loss=0.7652 acc=0.6599 | time=40.4s\n",
            "Epoch 039 | train_loss=0.5950 acc=0.7134 | val_loss=0.9595 acc=0.6040 | time=40.4s\n",
            "Epoch 040 | train_loss=0.5947 acc=0.7130 | val_loss=0.8929 acc=0.6460 | time=40.5s\n",
            "Epoch 041 | train_loss=0.5834 acc=0.7250 | val_loss=0.8715 acc=0.6382 | time=40.5s\n",
            "Epoch 042 | train_loss=0.5685 acc=0.7340 | val_loss=0.8322 acc=0.6118 | time=40.4s\n",
            "Epoch 043 | train_loss=0.5786 acc=0.7328 | val_loss=0.9650 acc=0.5870 | time=40.3s\n",
            "Epoch 044 | train_loss=0.5805 acc=0.7231 | val_loss=0.9221 acc=0.6304 | time=40.3s\n",
            "★ Early stopping at epoch 44\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▂▁▂▂▂▂▂▂▂▃▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▇▇▇▇▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▅▆▆▆▇▅▇▇█▆█▇▇█▇▇█████▇▆▇▇▆▆</td></tr><tr><td>validation_loss</td><td>█████████████▇▆▅▅▃▄▃▂▂▄▁▂▂▁▃▂▂▁▃▂▂▂▆▅▄▄▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>44</td></tr><tr><td>train_accuracy</td><td>0.72311</td></tr><tr><td>train_loss</td><td>0.58053</td></tr><tr><td>validation_accuracy</td><td>0.63043</td></tr><tr><td>validation_loss</td><td>0.92208</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">wild-dream-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/lwvurprg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/lwvurprg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_051336-lwvurprg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 05:43:19,069] Trial 4 finished with value: 0.6824441452821096 and parameters: {'lr': 0.0005, 'weight_decay': 5e-06, 'num_blocks': 3, 'num_heads': 3, 'num_segments': 5}. Best is trial 3 with value: 0.6183182043688638.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_054319-g4lc76pt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/g4lc76pt' target=\"_blank\">celestial-meadow-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/g4lc76pt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/g4lc76pt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=3, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.1323 acc=0.3697 | val_loss=1.0850 acc=0.3416 | time=40.5s\n",
            "Epoch 002 | train_loss=1.1073 acc=0.3961 | val_loss=1.0774 acc=0.4161 | time=40.3s\n",
            "Epoch 003 | train_loss=1.0941 acc=0.3992 | val_loss=1.0921 acc=0.3416 | time=40.3s\n",
            "Epoch 004 | train_loss=1.0960 acc=0.3845 | val_loss=1.0745 acc=0.4317 | time=40.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁▇█▅</td></tr><tr><td>train_loss</td><td>█▃▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▇▁█</td></tr><tr><td>validation_loss</td><td>▅▂█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.38447</td></tr><tr><td>train_loss</td><td>1.09599</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07447</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">celestial-meadow-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/g4lc76pt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/g4lc76pt</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_054319-g4lc76pt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 05:46:42,962] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 5 pruned at epoch 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_054643-h0dtwub9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/h0dtwub9' target=\"_blank\">rich-shadow-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/h0dtwub9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/h0dtwub9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 6 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=3, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1348 acc=0.3612 | val_loss=1.0933 acc=0.4317 | time=32.3s\n",
            "Epoch 002 | train_loss=1.1161 acc=0.3728 | val_loss=1.0793 acc=0.4317 | time=32.2s\n",
            "Epoch 003 | train_loss=1.1098 acc=0.3926 | val_loss=1.0765 acc=0.4317 | time=32.2s\n",
            "Epoch 004 | train_loss=1.0956 acc=0.4058 | val_loss=1.0754 acc=0.4317 | time=32.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁▃▆█</td></tr><tr><td>train_loss</td><td>█▅▄▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.40583</td></tr><tr><td>train_loss</td><td>1.09557</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07542</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-shadow-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/h0dtwub9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/h0dtwub9</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_054643-h0dtwub9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 05:49:26,015] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 6 pruned at epoch 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_054926-1a704lek</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/1a704lek' target=\"_blank\">comic-lion-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/1a704lek' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/1a704lek</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 7 =====\n",
            " lr=5.00e-04, wd=5.00e-03, blocks=3, heads=1, segs=5\n",
            "Epoch 001 | train_loss=1.1422 acc=0.3724 | val_loss=1.0795 acc=0.4317 | time=23.9s\n",
            "Epoch 002 | train_loss=1.1130 acc=0.3903 | val_loss=1.0757 acc=0.4317 | time=23.7s\n",
            "Epoch 003 | train_loss=1.1079 acc=0.3876 | val_loss=1.0774 acc=0.4317 | time=23.7s\n",
            "Epoch 004 | train_loss=1.0945 acc=0.3814 | val_loss=1.0768 acc=0.4317 | time=23.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁█▇▄</td></tr><tr><td>train_loss</td><td>█▄▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▁▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.38136</td></tr><tr><td>train_loss</td><td>1.09445</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07683</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">comic-lion-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/1a704lek' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/1a704lek</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_054926-1a704lek/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 05:51:26,440] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 7 pruned at epoch 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_055126-bs5nzm1t</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/bs5nzm1t' target=\"_blank\">fine-pine-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/bs5nzm1t' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/bs5nzm1t</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 8 =====\n",
            " lr=5.00e-04, wd=5.00e-06, blocks=3, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.1411 acc=0.3487 | val_loss=1.1201 acc=0.3416 | time=32.5s\n",
            "Epoch 002 | train_loss=1.1171 acc=0.3619 | val_loss=1.0757 acc=0.4317 | time=32.3s\n",
            "Epoch 003 | train_loss=1.1008 acc=0.3895 | val_loss=1.0791 acc=0.3416 | time=32.5s\n",
            "Epoch 004 | train_loss=1.0933 acc=0.3845 | val_loss=1.0762 acc=0.4317 | time=32.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁▃█▇</td></tr><tr><td>train_loss</td><td>█▄▂▁</td></tr><tr><td>validation_accuracy</td><td>▁█▁█</td></tr><tr><td>validation_loss</td><td>█▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.38447</td></tr><tr><td>train_loss</td><td>1.0933</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0762</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-pine-9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/bs5nzm1t' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18/runs/bs5nzm1t</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search-18</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_055126-bs5nzm1t/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-03 05:54:10,560] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 8 pruned at epoch 5\n",
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.618318\n",
            "best_train_loss     = 0.479521\n",
            "best_train_accuracy = 0.7833\n",
            "best_val_accuracy   = 0.7500\n",
            "best params:\n",
            "  lr: 0.0005\n",
            "  weight_decay: 0.0005\n",
            "  num_blocks: 3\n",
            "  num_heads: 2\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K fold Cross Validation Test using best hyperparameter\n",
        "- Block=1, Head=3\n",
        "- ReduceLRPlateu --> Not stable"
      ],
      "metadata": {
        "id": "Aap23Rkf2EEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_3 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 5e-2\n",
        "NUM_FILTERS = 120\n",
        "NUM_BLOCKS = 1\n",
        "NUM_HEADS = 3\n",
        "NUM_SEGMENTS = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler parameters ─────────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA = 0.5\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        # wandb run 시작\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-2\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 / 옵티마이저 / 손실함수\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=FIXED_GAMMA,\n",
        "            patience=FIXED_STEP_SIZE,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            # — 터미널 출력 —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # — wandb 로깅 —\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # best 갱신\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_val_acc    = val_acc\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "\n",
        "        # Fold 종료 시 summary 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        # cleanup\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 로깅 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    # 별도 W&B run 으로 평균 지표 기록\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-2\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # ─── 5-Fold CV Summary 터미널 출력 ────────────────────────────\n",
        "    print(\"\\n===== 5-Fold CV Summary =====\")\n",
        "    for i, res in enumerate(fold_results, 1):\n",
        "        print(\n",
        "            f\" Fold {i:2d}: \"\n",
        "            f\"train_loss = {res['train_loss']:.4f}, \"\n",
        "            f\"train_acc = {res['train_acc']:.4f}, \"\n",
        "            f\"val_loss = {res['val_loss']:.4f}, \"\n",
        "            f\"val_acc = {res['val_acc']:.4f}\"\n",
        "        )\n",
        "    print(\n",
        "        f\"\\n Average: \"\n",
        "        f\"train_loss = {avg['train_loss']:.4f}, \"\n",
        "        f\"train_acc = {avg['train_acc']:.4f}, \"\n",
        "        f\"val_loss = {avg['val_loss']:.4f}, \"\n",
        "        f\"val_acc = {avg['val_acc']:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "x_WMrAdr2LVP",
        "outputId": "1525be1f-cf23-4776-df4d-38347acf49ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_073023-1q2kcwlg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/1q2kcwlg' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/1q2kcwlg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/1q2kcwlg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1964 acc=0.3340 | val_loss=1.0918 acc=0.3587 | time=18.6s\n",
            "Epoch 002 | train_loss=1.1114 acc=0.3643 | val_loss=1.1037 acc=0.4006 | time=18.2s\n",
            "Epoch 003 | train_loss=1.1302 acc=0.3662 | val_loss=1.0936 acc=0.4022 | time=18.0s\n",
            "Epoch 004 | train_loss=1.1028 acc=0.3887 | val_loss=1.0953 acc=0.4286 | time=18.1s\n",
            "Epoch 005 | train_loss=1.0821 acc=0.4074 | val_loss=1.0885 acc=0.5078 | time=18.3s\n",
            "Epoch 006 | train_loss=1.0702 acc=0.4400 | val_loss=1.6323 acc=0.3587 | time=18.1s\n",
            "Epoch 007 | train_loss=1.0474 acc=0.4695 | val_loss=1.0324 acc=0.5233 | time=18.1s\n",
            "Epoch 008 | train_loss=0.9879 acc=0.5480 | val_loss=0.9448 acc=0.6040 | time=18.2s\n",
            "Epoch 009 | train_loss=0.9583 acc=0.5705 | val_loss=0.9453 acc=0.6320 | time=18.1s\n",
            "Epoch 010 | train_loss=0.9459 acc=0.5693 | val_loss=0.9224 acc=0.6227 | time=18.2s\n",
            "Epoch 011 | train_loss=0.9050 acc=0.5957 | val_loss=0.8497 acc=0.6398 | time=18.0s\n",
            "Epoch 012 | train_loss=0.8867 acc=0.6054 | val_loss=0.9269 acc=0.5885 | time=18.2s\n",
            "Epoch 013 | train_loss=0.8769 acc=0.6000 | val_loss=0.8323 acc=0.6537 | time=18.2s\n",
            "Epoch 014 | train_loss=0.8499 acc=0.6256 | val_loss=0.8866 acc=0.6351 | time=18.0s\n",
            "Epoch 015 | train_loss=0.8369 acc=0.6202 | val_loss=0.7923 acc=0.6584 | time=18.2s\n",
            "Epoch 016 | train_loss=0.8194 acc=0.6338 | val_loss=0.8011 acc=0.6677 | time=18.1s\n",
            "Epoch 017 | train_loss=0.7984 acc=0.6419 | val_loss=0.8075 acc=0.6661 | time=18.3s\n",
            "Epoch 018 | train_loss=0.7890 acc=0.6377 | val_loss=0.7441 acc=0.7034 | time=18.0s\n",
            "Epoch 019 | train_loss=0.7445 acc=0.6559 | val_loss=0.7606 acc=0.6910 | time=18.1s\n",
            "Epoch 020 | train_loss=0.7337 acc=0.6563 | val_loss=0.9418 acc=0.6413 | time=18.1s\n",
            "Epoch 021 | train_loss=0.7074 acc=0.6707 | val_loss=0.7922 acc=0.6832 | time=18.1s\n",
            "Epoch 022 | train_loss=0.6740 acc=0.7014 | val_loss=0.7208 acc=0.6925 | time=18.4s\n",
            "Epoch 023 | train_loss=0.6795 acc=0.6905 | val_loss=0.7100 acc=0.7112 | time=18.1s\n",
            "Epoch 024 | train_loss=0.6760 acc=0.6796 | val_loss=0.9139 acc=0.6242 | time=18.1s\n",
            "Epoch 025 | train_loss=0.6362 acc=0.6971 | val_loss=0.7572 acc=0.6848 | time=18.2s\n",
            "Epoch 026 | train_loss=0.6530 acc=0.7095 | val_loss=0.7510 acc=0.6863 | time=18.1s\n",
            "Epoch 027 | train_loss=0.6200 acc=0.7099 | val_loss=0.8504 acc=0.6584 | time=18.2s\n",
            "Epoch 028 | train_loss=0.5922 acc=0.7344 | val_loss=0.9184 acc=0.6724 | time=18.1s\n",
            "Epoch 029 | train_loss=0.5975 acc=0.7390 | val_loss=1.0659 acc=0.6056 | time=18.2s\n",
            "Epoch 030 | train_loss=0.5523 acc=0.7429 | val_loss=0.7309 acc=0.7050 | time=18.2s\n",
            "Epoch 031 | train_loss=0.5138 acc=0.7732 | val_loss=0.7313 acc=0.7158 | time=18.2s\n",
            "Epoch 032 | train_loss=0.5449 acc=0.7499 | val_loss=0.9006 acc=0.7003 | time=18.2s\n",
            "Epoch 033 | train_loss=0.5047 acc=0.7755 | val_loss=0.9074 acc=0.6910 | time=18.0s\n",
            "Epoch 034 | train_loss=0.5027 acc=0.7713 | val_loss=0.8415 acc=0.7205 | time=18.2s\n",
            "Epoch 035 | train_loss=0.4971 acc=0.7670 | val_loss=0.7431 acc=0.7205 | time=18.1s\n",
            "Epoch 036 | train_loss=0.4719 acc=0.7852 | val_loss=0.9595 acc=0.6599 | time=18.1s\n",
            "Epoch 037 | train_loss=0.4548 acc=0.8008 | val_loss=0.9584 acc=0.6724 | time=18.3s\n",
            "Epoch 038 | train_loss=0.4523 acc=0.7977 | val_loss=1.0273 acc=0.6677 | time=18.1s\n",
            "Epoch 039 | train_loss=0.4507 acc=0.7965 | val_loss=0.9175 acc=0.6879 | time=18.2s\n",
            "Epoch 040 | train_loss=0.4427 acc=0.8089 | val_loss=1.0481 acc=0.6522 | time=18.2s\n",
            "Epoch 041 | train_loss=0.4346 acc=0.8070 | val_loss=1.0213 acc=0.6553 | time=18.1s\n",
            "Epoch 042 | train_loss=0.4301 acc=0.8047 | val_loss=0.9753 acc=0.6817 | time=18.2s\n",
            "Epoch 043 | train_loss=0.4149 acc=0.8175 | val_loss=0.8493 acc=0.7003 | time=18.1s\n",
            "Epoch 044 | train_loss=0.4226 acc=0.8012 | val_loss=0.9335 acc=0.6879 | time=18.3s\n",
            "Epoch 045 | train_loss=0.4118 acc=0.8140 | val_loss=0.9680 acc=0.6755 | time=18.1s\n",
            "Epoch 046 | train_loss=0.4186 acc=0.8190 | val_loss=1.0361 acc=0.6661 | time=18.0s\n",
            "Epoch 047 | train_loss=0.4147 acc=0.8101 | val_loss=0.9718 acc=0.6708 | time=18.2s\n",
            "Epoch 048 | train_loss=0.3884 acc=0.8295 | val_loss=1.0071 acc=0.6693 | time=18.1s\n",
            "Epoch 049 | train_loss=0.4018 acc=0.8225 | val_loss=1.0827 acc=0.6693 | time=18.2s\n",
            "Epoch 050 | train_loss=0.3906 acc=0.8264 | val_loss=0.9875 acc=0.6848 | time=18.2s\n",
            "Epoch 051 | train_loss=0.4046 acc=0.8155 | val_loss=1.0794 acc=0.6832 | time=18.0s\n",
            "Epoch 052 | train_loss=0.3993 acc=0.8245 | val_loss=1.0734 acc=0.6770 | time=18.2s\n",
            "Epoch 053 | train_loss=0.4035 acc=0.8210 | val_loss=1.0140 acc=0.6801 | time=18.1s\n",
            "Epoch 054 | train_loss=0.3830 acc=0.8221 | val_loss=1.0109 acc=0.6770 | time=18.3s\n",
            "Epoch 055 | train_loss=0.3829 acc=0.8412 | val_loss=0.9817 acc=0.6863 | time=18.1s\n",
            "Epoch 056 | train_loss=0.3811 acc=0.8342 | val_loss=1.0272 acc=0.6739 | time=18.1s\n",
            "Epoch 057 | train_loss=0.3744 acc=0.8334 | val_loss=1.0731 acc=0.6646 | time=18.2s\n",
            "Epoch 058 | train_loss=0.3818 acc=0.8283 | val_loss=1.0353 acc=0.6817 | time=18.1s\n",
            "Epoch 059 | train_loss=0.3751 acc=0.8287 | val_loss=1.0613 acc=0.6770 | time=18.2s\n",
            "Epoch 060 | train_loss=0.3889 acc=0.8190 | val_loss=1.0360 acc=0.6786 | time=18.1s\n",
            "Epoch 061 | train_loss=0.3968 acc=0.8175 | val_loss=1.0142 acc=0.6801 | time=18.1s\n",
            "Epoch 062 | train_loss=0.3669 acc=0.8287 | val_loss=0.9886 acc=0.6755 | time=18.1s\n",
            "Epoch 063 | train_loss=0.3655 acc=0.8326 | val_loss=1.0288 acc=0.6739 | time=18.0s\n",
            "Epoch 064 | train_loss=0.3559 acc=0.8458 | val_loss=1.0277 acc=0.6832 | time=18.3s\n",
            "Epoch 065 | train_loss=0.3891 acc=0.8179 | val_loss=1.0547 acc=0.6755 | time=18.1s\n",
            "Epoch 066 | train_loss=0.3768 acc=0.8249 | val_loss=1.0725 acc=0.6786 | time=18.1s\n",
            "Epoch 067 | train_loss=0.3840 acc=0.8388 | val_loss=1.0611 acc=0.6879 | time=18.1s\n",
            "Epoch 068 | train_loss=0.3888 acc=0.8190 | val_loss=1.0749 acc=0.6755 | time=18.1s\n",
            "Epoch 069 | train_loss=0.3834 acc=0.8311 | val_loss=1.0544 acc=0.6708 | time=18.3s\n",
            "Epoch 070 | train_loss=0.3971 acc=0.8318 | val_loss=1.0837 acc=0.6739 | time=17.9s\n",
            "Epoch 071 | train_loss=0.3758 acc=0.8350 | val_loss=1.0730 acc=0.6724 | time=18.1s\n",
            "Epoch 072 | train_loss=0.3751 acc=0.8264 | val_loss=1.0760 acc=0.6724 | time=18.1s\n",
            "Epoch 073 | train_loss=0.3828 acc=0.8210 | val_loss=1.0650 acc=0.6801 | time=18.0s\n",
            "Epoch 074 | train_loss=0.3766 acc=0.8315 | val_loss=1.0815 acc=0.6724 | time=18.3s\n",
            "Epoch 075 | train_loss=0.3845 acc=0.8214 | val_loss=1.0789 acc=0.6755 | time=18.0s\n",
            "Epoch 076 | train_loss=0.3999 acc=0.8175 | val_loss=1.0228 acc=0.6755 | time=18.3s\n",
            "Epoch 077 | train_loss=0.3655 acc=0.8295 | val_loss=1.0214 acc=0.6879 | time=18.1s\n",
            "Epoch 078 | train_loss=0.3707 acc=0.8264 | val_loss=1.0272 acc=0.6801 | time=18.0s\n",
            "Epoch 079 | train_loss=0.3792 acc=0.8245 | val_loss=1.0620 acc=0.6770 | time=18.2s\n",
            "Epoch 080 | train_loss=0.3744 acc=0.8291 | val_loss=1.0523 acc=0.6817 | time=17.9s\n",
            "Epoch 081 | train_loss=0.3767 acc=0.8326 | val_loss=1.0506 acc=0.6817 | time=18.3s\n",
            "Epoch 082 | train_loss=0.3773 acc=0.8299 | val_loss=1.0538 acc=0.6894 | time=18.0s\n",
            "Epoch 083 | train_loss=0.3883 acc=0.8256 | val_loss=1.0309 acc=0.6832 | time=18.2s\n",
            "Epoch 084 | train_loss=0.3763 acc=0.8256 | val_loss=1.0591 acc=0.6770 | time=18.2s\n",
            "Epoch 085 | train_loss=0.3822 acc=0.8256 | val_loss=1.0176 acc=0.6832 | time=18.1s\n",
            "Epoch 086 | train_loss=0.3939 acc=0.8179 | val_loss=1.1039 acc=0.6739 | time=18.3s\n",
            "Epoch 087 | train_loss=0.3786 acc=0.8283 | val_loss=1.0391 acc=0.6755 | time=18.0s\n",
            "Epoch 088 | train_loss=0.4029 acc=0.8210 | val_loss=1.0605 acc=0.6770 | time=18.4s\n",
            "Epoch 089 | train_loss=0.3789 acc=0.8241 | val_loss=1.0350 acc=0.6739 | time=18.1s\n",
            "Epoch 090 | train_loss=0.3864 acc=0.8256 | val_loss=1.0515 acc=0.6786 | time=18.0s\n",
            "Epoch 091 | train_loss=0.3855 acc=0.8322 | val_loss=1.0425 acc=0.6863 | time=18.3s\n",
            "Epoch 092 | train_loss=0.3919 acc=0.8260 | val_loss=1.0627 acc=0.6879 | time=18.1s\n",
            "Epoch 093 | train_loss=0.3991 acc=0.8221 | val_loss=1.0695 acc=0.6801 | time=18.3s\n",
            "Epoch 094 | train_loss=0.3941 acc=0.8260 | val_loss=1.0404 acc=0.6724 | time=18.0s\n",
            "Epoch 095 | train_loss=0.3806 acc=0.8221 | val_loss=1.0623 acc=0.6786 | time=18.0s\n",
            "Epoch 096 | train_loss=0.3949 acc=0.8206 | val_loss=1.0791 acc=0.6739 | time=18.2s\n",
            "Epoch 097 | train_loss=0.3755 acc=0.8280 | val_loss=1.0574 acc=0.6786 | time=18.1s\n",
            "Epoch 098 | train_loss=0.3965 acc=0.8233 | val_loss=1.0406 acc=0.6755 | time=18.3s\n",
            "Epoch 099 | train_loss=0.3974 acc=0.8237 | val_loss=1.0470 acc=0.6863 | time=18.1s\n",
            "Epoch 100 | train_loss=0.3656 acc=0.8381 | val_loss=1.0550 acc=0.6770 | time=18.1s\n",
            "Fold 1 best_train_loss=0.6795, best_train_acc=0.6905, best_val_loss=0.7100, best_val_acc=0.7112\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▂▄▄▅▅▅▅▆▅▆▆▇▇▇███▇████████████████████</td></tr><tr><td>train_loss</td><td>██▇▇▆▅▅▄▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▂▂▁▆▅▇█▇▇▇▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>▄▄█▃▃▂▂▂▁▁▂▁▁▁▃▂▃▄▃▃▄▃▃▃▃▄▄▄▄▃▃▄▄▃▃▃▄▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.69049</td></tr><tr><td>best_train_loss</td><td>0.67951</td></tr><tr><td>best_val_accuracy</td><td>0.71118</td></tr><tr><td>best_val_loss</td><td>0.71002</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>0.83806</td></tr><tr><td>train_loss</td><td>0.36556</td></tr><tr><td>validation_accuracy</td><td>0.67702</td></tr><tr><td>validation_loss</td><td>1.05496</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/1q2kcwlg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/1q2kcwlg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_073023-1q2kcwlg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_080040-ky2vn06m</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/ky2vn06m' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/ky2vn06m' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-2/runs/ky2vn06m</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1512 acc=0.3751 | val_loss=1.0792 acc=0.4255 | time=18.1s\n",
            "Epoch 002 | train_loss=1.1054 acc=0.3903 | val_loss=1.0804 acc=0.4255 | time=18.0s\n",
            "Epoch 003 | train_loss=1.0996 acc=0.3957 | val_loss=1.0763 acc=0.4255 | time=18.2s\n",
            "Epoch 004 | train_loss=1.0974 acc=0.3996 | val_loss=1.0657 acc=0.5031 | time=18.1s\n",
            "Epoch 005 | train_loss=1.0610 acc=0.4520 | val_loss=0.9951 acc=0.5621 | time=18.2s\n",
            "Epoch 006 | train_loss=0.9868 acc=0.5363 | val_loss=0.9157 acc=0.6351 | time=18.1s\n",
            "Epoch 007 | train_loss=0.9582 acc=0.5608 | val_loss=0.9044 acc=0.6537 | time=18.0s\n",
            "Epoch 008 | train_loss=0.9286 acc=0.5783 | val_loss=0.8829 acc=0.6242 | time=18.2s\n",
            "Epoch 009 | train_loss=0.8926 acc=0.5849 | val_loss=0.8285 acc=0.6786 | time=18.1s\n",
            "Epoch 010 | train_loss=0.8613 acc=0.5981 | val_loss=0.8492 acc=0.6925 | time=18.3s\n",
            "Epoch 011 | train_loss=0.8509 acc=0.6058 | val_loss=0.8419 acc=0.6335 | time=18.0s\n",
            "Epoch 012 | train_loss=0.8210 acc=0.6249 | val_loss=0.7974 acc=0.6910 | time=18.3s\n",
            "Epoch 013 | train_loss=0.7972 acc=0.6326 | val_loss=0.8391 acc=0.6957 | time=18.2s\n",
            "Epoch 014 | train_loss=0.7551 acc=0.6474 | val_loss=0.7868 acc=0.6894 | time=18.2s\n",
            "Epoch 015 | train_loss=0.7386 acc=0.6819 | val_loss=0.7870 acc=0.7034 | time=18.3s\n",
            "Epoch 016 | train_loss=0.7501 acc=0.6536 | val_loss=0.7589 acc=0.7050 | time=18.0s\n",
            "Epoch 017 | train_loss=0.7317 acc=0.6548 | val_loss=0.7892 acc=0.6848 | time=18.1s\n",
            "Epoch 018 | train_loss=0.7100 acc=0.6808 | val_loss=0.7126 acc=0.7236 | time=18.2s\n",
            "Epoch 019 | train_loss=0.6806 acc=0.6850 | val_loss=0.8197 acc=0.6693 | time=18.1s\n",
            "Epoch 020 | train_loss=0.7028 acc=0.6847 | val_loss=1.0220 acc=0.4845 | time=18.3s\n",
            "Epoch 021 | train_loss=0.6564 acc=0.7056 | val_loss=0.7480 acc=0.7220 | time=18.1s\n",
            "Epoch 022 | train_loss=0.6586 acc=0.7134 | val_loss=1.0532 acc=0.5994 | time=18.2s\n",
            "Epoch 023 | train_loss=0.6526 acc=0.7103 | val_loss=0.7667 acc=0.7329 | time=18.2s\n",
            "Epoch 024 | train_loss=0.6386 acc=0.7208 | val_loss=1.0413 acc=0.5792 | time=18.1s\n",
            "Epoch 025 | train_loss=0.6048 acc=0.7239 | val_loss=0.8602 acc=0.6817 | time=18.2s\n",
            "Epoch 026 | train_loss=0.6011 acc=0.7386 | val_loss=0.9380 acc=0.6398 | time=18.1s\n",
            "Epoch 027 | train_loss=0.5924 acc=0.7417 | val_loss=0.9752 acc=0.6522 | time=18.3s\n",
            "Epoch 028 | train_loss=0.5514 acc=0.7499 | val_loss=0.8140 acc=0.6848 | time=18.3s\n",
            "Epoch 029 | train_loss=0.5435 acc=0.7596 | val_loss=1.0082 acc=0.6320 | time=18.6s\n",
            "Epoch 030 | train_loss=0.5469 acc=0.7487 | val_loss=0.8066 acc=0.7189 | time=18.5s\n",
            "Epoch 031 | train_loss=0.5320 acc=0.7678 | val_loss=1.0256 acc=0.6506 | time=18.0s\n",
            "Epoch 032 | train_loss=0.5204 acc=0.7643 | val_loss=1.2222 acc=0.5730 | time=18.3s\n",
            "Epoch 033 | train_loss=0.4998 acc=0.7817 | val_loss=1.1284 acc=0.5963 | time=18.1s\n",
            "Epoch 034 | train_loss=0.4866 acc=0.7849 | val_loss=1.1507 acc=0.6071 | time=18.0s\n",
            "Epoch 035 | train_loss=0.4846 acc=0.7829 | val_loss=1.1218 acc=0.6444 | time=18.2s\n",
            "Epoch 036 | train_loss=0.4715 acc=0.7891 | val_loss=1.0797 acc=0.6242 | time=18.0s\n",
            "Epoch 037 | train_loss=0.4789 acc=0.7930 | val_loss=1.1385 acc=0.6242 | time=18.1s\n",
            "Epoch 038 | train_loss=0.4611 acc=0.8016 | val_loss=1.1942 acc=0.6040 | time=18.1s\n",
            "Epoch 039 | train_loss=0.4536 acc=0.8008 | val_loss=1.0919 acc=0.6506 | time=18.3s\n",
            "Epoch 040 | train_loss=0.4619 acc=0.7981 | val_loss=1.2432 acc=0.5947 | time=18.2s\n",
            "Epoch 041 | train_loss=0.4517 acc=0.7930 | val_loss=1.1932 acc=0.6071 | time=18.2s\n",
            "Epoch 042 | train_loss=0.4464 acc=0.8082 | val_loss=1.0559 acc=0.6553 | time=18.3s\n",
            "Epoch 043 | train_loss=0.4431 acc=0.7996 | val_loss=1.0358 acc=0.6584 | time=18.1s\n",
            "Epoch 044 | train_loss=0.4511 acc=0.7984 | val_loss=1.1998 acc=0.6304 | time=18.3s\n",
            "Epoch 045 | train_loss=0.4471 acc=0.8113 | val_loss=1.1726 acc=0.6149 | time=18.2s\n",
            "Epoch 046 | train_loss=0.4489 acc=0.7934 | val_loss=1.1121 acc=0.6413 | time=18.2s\n",
            "Epoch 047 | train_loss=0.4286 acc=0.8105 | val_loss=1.0886 acc=0.6398 | time=18.2s\n",
            "Epoch 048 | train_loss=0.4492 acc=0.8019 | val_loss=1.1244 acc=0.6211 | time=18.2s\n",
            "Epoch 049 | train_loss=0.4361 acc=0.8066 | val_loss=1.1671 acc=0.6040 | time=18.3s\n",
            "Epoch 050 | train_loss=0.4243 acc=0.8105 | val_loss=1.1845 acc=0.6056 | time=18.1s\n",
            "Epoch 051 | train_loss=0.4481 acc=0.7996 | val_loss=1.1926 acc=0.5978 | time=18.2s\n",
            "Epoch 052 | train_loss=0.4602 acc=0.7946 | val_loss=1.1770 acc=0.6165 | time=18.1s\n",
            "Epoch 053 | train_loss=0.4251 acc=0.8117 | val_loss=1.1893 acc=0.5885 | time=18.1s\n",
            "Epoch 054 | train_loss=0.4291 acc=0.8109 | val_loss=1.2474 acc=0.6118 | time=18.3s\n",
            "Epoch 055 | train_loss=0.4395 acc=0.8023 | val_loss=1.2304 acc=0.6009 | time=18.1s\n",
            "Epoch 056 | train_loss=0.4388 acc=0.7965 | val_loss=1.2008 acc=0.5947 | time=18.2s\n",
            "Epoch 057 | train_loss=0.4368 acc=0.8058 | val_loss=1.2940 acc=0.5854 | time=18.2s\n",
            "Epoch 058 | train_loss=0.4341 acc=0.8105 | val_loss=1.2381 acc=0.5994 | time=18.0s\n",
            "Epoch 059 | train_loss=0.4357 acc=0.8105 | val_loss=1.2267 acc=0.5994 | time=18.2s\n",
            "Epoch 060 | train_loss=0.4351 acc=0.8082 | val_loss=1.2175 acc=0.6025 | time=18.0s\n",
            "Epoch 061 | train_loss=0.4231 acc=0.8171 | val_loss=1.1656 acc=0.6118 | time=18.2s\n",
            "Epoch 062 | train_loss=0.4138 acc=0.8256 | val_loss=1.1998 acc=0.6025 | time=18.1s\n",
            "Epoch 063 | train_loss=0.4349 acc=0.8016 | val_loss=1.1982 acc=0.6087 | time=17.9s\n",
            "Epoch 064 | train_loss=0.4243 acc=0.8054 | val_loss=1.2054 acc=0.6242 | time=18.2s\n",
            "Epoch 065 | train_loss=0.4453 acc=0.8097 | val_loss=1.1271 acc=0.6351 | time=18.1s\n",
            "Epoch 066 | train_loss=0.4376 acc=0.8089 | val_loss=1.1570 acc=0.6273 | time=18.2s\n",
            "Epoch 067 | train_loss=0.4224 acc=0.8167 | val_loss=1.1543 acc=0.6227 | time=18.1s\n",
            "Epoch 068 | train_loss=0.4279 acc=0.8101 | val_loss=1.1712 acc=0.6382 | time=18.2s\n",
            "Epoch 069 | train_loss=0.4208 acc=0.8074 | val_loss=1.1347 acc=0.6382 | time=18.2s\n",
            "Epoch 070 | train_loss=0.4082 acc=0.8194 | val_loss=1.1916 acc=0.6242 | time=18.1s\n",
            "Epoch 071 | train_loss=0.4182 acc=0.8186 | val_loss=1.1871 acc=0.6180 | time=18.3s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e3dcf4c3c761>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-e3dcf4c3c761>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Learning Rate Scheduler 바꾸기\n",
        "- ReduceLRPlateu -> CosineAnnealingWarmupRestarts"
      ],
      "metadata": {
        "id": "HD0rvNpjMS5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_3 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 5e-2\n",
        "NUM_FILTERS = 120\n",
        "NUM_BLOCKS = 1\n",
        "NUM_HEADS = 3\n",
        "NUM_SEGMENTS = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler parameters ─────────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA = 0.5\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        # wandb run 시작\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-3\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 / 옵티마이저 / 손실함수\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=10,\n",
        "            T_mult=2,\n",
        "            eta_min=1e-6\n",
        "            )\n",
        "\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            # — 터미널 출력 —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # — wandb 로깅 —\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step()\n",
        "\n",
        "            # best 갱신\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_val_acc    = val_acc\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "\n",
        "        # Fold 종료 시 summary 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        # cleanup\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 로깅 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    # 별도 W&B run 으로 평균 지표 기록\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-3\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # ─── 5-Fold CV Summary 터미널 출력 ────────────────────────────\n",
        "    print(\"\\n===== 5-Fold CV Summary =====\")\n",
        "    for i, res in enumerate(fold_results, 1):\n",
        "        print(\n",
        "            f\" Fold {i:2d}: \"\n",
        "            f\"train_loss = {res['train_loss']:.4f}, \"\n",
        "            f\"train_acc = {res['train_acc']:.4f}, \"\n",
        "            f\"val_loss = {res['val_loss']:.4f}, \"\n",
        "            f\"val_acc = {res['val_acc']:.4f}\"\n",
        "        )\n",
        "    print(\n",
        "        f\"\\n Average: \"\n",
        "        f\"train_loss = {avg['train_loss']:.4f}, \"\n",
        "        f\"train_acc = {avg['train_acc']:.4f}, \"\n",
        "        f\"val_loss = {avg['val_loss']:.4f}, \"\n",
        "        f\"val_acc = {avg['val_acc']:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HodTFIJeCzkA",
        "outputId": "4bbac6ad-a27d-40e3-dec5-3915ac29e283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_083554-2l470698</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/2l470698' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/2l470698' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/2l470698</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1428 acc=0.3771 | val_loss=1.0937 acc=0.3587 | time=18.6s\n",
            "Epoch 002 | train_loss=1.1296 acc=0.3755 | val_loss=1.0822 acc=0.4783 | time=18.3s\n",
            "Epoch 003 | train_loss=1.0983 acc=0.3930 | val_loss=1.0740 acc=0.4161 | time=18.2s\n",
            "Epoch 004 | train_loss=1.0355 acc=0.4901 | val_loss=0.9692 acc=0.5901 | time=18.1s\n",
            "Epoch 005 | train_loss=0.9776 acc=0.5518 | val_loss=0.9922 acc=0.6227 | time=18.2s\n",
            "Epoch 006 | train_loss=0.9668 acc=0.5569 | val_loss=0.9329 acc=0.5730 | time=18.2s\n",
            "Epoch 007 | train_loss=0.9282 acc=0.5876 | val_loss=0.9263 acc=0.6289 | time=18.4s\n",
            "Epoch 008 | train_loss=0.9201 acc=0.5852 | val_loss=0.8918 acc=0.6382 | time=18.0s\n",
            "Epoch 009 | train_loss=0.9010 acc=0.6050 | val_loss=0.8902 acc=0.6382 | time=18.1s\n",
            "Epoch 010 | train_loss=0.9084 acc=0.6039 | val_loss=0.8965 acc=0.6320 | time=18.1s\n",
            "Epoch 011 | train_loss=0.9142 acc=0.5872 | val_loss=0.9102 acc=0.6553 | time=18.1s\n",
            "Epoch 012 | train_loss=0.8876 acc=0.6070 | val_loss=0.8223 acc=0.6460 | time=18.4s\n",
            "Epoch 013 | train_loss=0.8745 acc=0.6012 | val_loss=0.8226 acc=0.6475 | time=18.1s\n",
            "Epoch 014 | train_loss=0.8374 acc=0.6318 | val_loss=0.8689 acc=0.6165 | time=18.4s\n",
            "Epoch 015 | train_loss=0.8099 acc=0.6272 | val_loss=0.7459 acc=0.6786 | time=18.2s\n",
            "Epoch 016 | train_loss=0.7736 acc=0.6493 | val_loss=0.8413 acc=0.6366 | time=18.1s\n",
            "Epoch 017 | train_loss=0.7720 acc=0.6447 | val_loss=0.7533 acc=0.7050 | time=18.3s\n",
            "Epoch 018 | train_loss=0.7351 acc=0.6800 | val_loss=0.7493 acc=0.7143 | time=18.1s\n",
            "Epoch 019 | train_loss=0.7162 acc=0.6680 | val_loss=0.6988 acc=0.7360 | time=18.2s\n",
            "Epoch 020 | train_loss=0.7016 acc=0.6850 | val_loss=0.7655 acc=0.6693 | time=18.2s\n",
            "Epoch 021 | train_loss=0.6893 acc=0.7033 | val_loss=0.6960 acc=0.7236 | time=18.2s\n",
            "Epoch 022 | train_loss=0.6670 acc=0.7118 | val_loss=0.7116 acc=0.7407 | time=18.2s\n",
            "Epoch 023 | train_loss=0.6412 acc=0.7181 | val_loss=0.7508 acc=0.7298 | time=18.1s\n",
            "Epoch 024 | train_loss=0.6189 acc=0.7278 | val_loss=0.7829 acc=0.7252 | time=18.4s\n",
            "Epoch 025 | train_loss=0.6353 acc=0.7146 | val_loss=0.7641 acc=0.7081 | time=18.1s\n",
            "Epoch 026 | train_loss=0.6118 acc=0.7266 | val_loss=0.8103 acc=0.7220 | time=18.2s\n",
            "Epoch 027 | train_loss=0.6033 acc=0.7305 | val_loss=0.8175 acc=0.7360 | time=18.1s\n",
            "Epoch 028 | train_loss=0.6174 acc=0.7200 | val_loss=0.8270 acc=0.7189 | time=18.0s\n",
            "Epoch 029 | train_loss=0.6054 acc=0.7309 | val_loss=0.7862 acc=0.7360 | time=18.3s\n",
            "Epoch 030 | train_loss=0.5930 acc=0.7476 | val_loss=0.7987 acc=0.7329 | time=18.1s\n",
            "Epoch 031 | train_loss=0.6659 acc=0.7060 | val_loss=0.6985 acc=0.7345 | time=18.3s\n",
            "Epoch 032 | train_loss=0.6647 acc=0.7111 | val_loss=0.6810 acc=0.6724 | time=18.3s\n",
            "Epoch 033 | train_loss=0.6562 acc=0.7006 | val_loss=0.8842 acc=0.6056 | time=18.1s\n",
            "Epoch 034 | train_loss=0.6620 acc=0.6990 | val_loss=0.6841 acc=0.7236 | time=18.4s\n",
            "Epoch 035 | train_loss=0.6475 acc=0.7142 | val_loss=0.8755 acc=0.6677 | time=18.2s\n",
            "Epoch 036 | train_loss=0.6182 acc=0.7328 | val_loss=0.9153 acc=0.6646 | time=18.4s\n",
            "Epoch 037 | train_loss=0.5876 acc=0.7468 | val_loss=0.8873 acc=0.7189 | time=18.1s\n",
            "Epoch 038 | train_loss=0.5996 acc=0.7332 | val_loss=0.8945 acc=0.6196 | time=18.2s\n",
            "Epoch 039 | train_loss=0.5817 acc=0.7487 | val_loss=0.7520 acc=0.6770 | time=18.1s\n",
            "Epoch 040 | train_loss=0.5546 acc=0.7612 | val_loss=0.9295 acc=0.6537 | time=18.0s\n",
            "Epoch 041 | train_loss=0.5256 acc=0.7713 | val_loss=1.1748 acc=0.5761 | time=18.4s\n",
            "Epoch 042 | train_loss=0.5337 acc=0.7825 | val_loss=0.6801 acc=0.6941 | time=18.2s\n",
            "Epoch 043 | train_loss=0.5382 acc=0.7837 | val_loss=0.8990 acc=0.7081 | time=18.3s\n",
            "Epoch 044 | train_loss=0.5362 acc=0.7689 | val_loss=0.7574 acc=0.7189 | time=18.1s\n",
            "Epoch 045 | train_loss=0.5112 acc=0.7880 | val_loss=0.9658 acc=0.6366 | time=18.0s\n",
            "Epoch 046 | train_loss=0.5287 acc=0.7744 | val_loss=0.9945 acc=0.6320 | time=18.2s\n",
            "Epoch 047 | train_loss=0.4719 acc=0.8066 | val_loss=0.8922 acc=0.6568 | time=18.1s\n",
            "Epoch 048 | train_loss=0.4449 acc=0.8163 | val_loss=1.0318 acc=0.6304 | time=18.1s\n",
            "Epoch 049 | train_loss=0.4264 acc=0.8280 | val_loss=0.9811 acc=0.6677 | time=18.1s\n",
            "Epoch 050 | train_loss=0.4327 acc=0.8148 | val_loss=1.0994 acc=0.6661 | time=18.2s\n",
            "Epoch 051 | train_loss=0.4296 acc=0.8113 | val_loss=0.9536 acc=0.7345 | time=18.3s\n",
            "Epoch 052 | train_loss=0.4255 acc=0.8008 | val_loss=1.0594 acc=0.6755 | time=18.1s\n",
            "Epoch 053 | train_loss=0.3985 acc=0.8307 | val_loss=1.0308 acc=0.6786 | time=18.3s\n",
            "Epoch 054 | train_loss=0.3997 acc=0.8214 | val_loss=1.0547 acc=0.6894 | time=18.2s\n",
            "Epoch 055 | train_loss=0.3939 acc=0.8272 | val_loss=1.1254 acc=0.6724 | time=18.3s\n",
            "Epoch 056 | train_loss=0.4077 acc=0.8237 | val_loss=1.3477 acc=0.6165 | time=18.1s\n",
            "Epoch 057 | train_loss=0.3663 acc=0.8435 | val_loss=1.1324 acc=0.6770 | time=18.0s\n",
            "Epoch 058 | train_loss=0.3623 acc=0.8447 | val_loss=1.2460 acc=0.6537 | time=18.3s\n",
            "Epoch 059 | train_loss=0.3669 acc=0.8462 | val_loss=1.0843 acc=0.6988 | time=18.1s\n",
            "Epoch 060 | train_loss=0.3840 acc=0.8381 | val_loss=1.2017 acc=0.6599 | time=18.2s\n",
            "Epoch 061 | train_loss=0.3569 acc=0.8466 | val_loss=1.1931 acc=0.6630 | time=18.1s\n",
            "Epoch 062 | train_loss=0.3361 acc=0.8517 | val_loss=1.2007 acc=0.6568 | time=18.1s\n",
            "Epoch 063 | train_loss=0.3590 acc=0.8416 | val_loss=1.3058 acc=0.6320 | time=18.3s\n",
            "Epoch 064 | train_loss=0.3612 acc=0.8412 | val_loss=1.2327 acc=0.6366 | time=18.1s\n",
            "Epoch 065 | train_loss=0.3444 acc=0.8528 | val_loss=1.2875 acc=0.6273 | time=18.3s\n",
            "Epoch 066 | train_loss=0.3445 acc=0.8435 | val_loss=1.2498 acc=0.6537 | time=17.9s\n",
            "Epoch 067 | train_loss=0.3381 acc=0.8517 | val_loss=1.2160 acc=0.6475 | time=18.1s\n",
            "Epoch 068 | train_loss=0.3478 acc=0.8423 | val_loss=1.1833 acc=0.6630 | time=18.2s\n",
            "Epoch 069 | train_loss=0.3260 acc=0.8505 | val_loss=1.1602 acc=0.6568 | time=18.1s\n",
            "Epoch 070 | train_loss=0.3667 acc=0.8322 | val_loss=1.2537 acc=0.6460 | time=18.5s\n",
            "Epoch 071 | train_loss=0.4148 acc=0.8233 | val_loss=1.1934 acc=0.6522 | time=18.2s\n",
            "Epoch 072 | train_loss=0.4624 acc=0.8124 | val_loss=1.3189 acc=0.6071 | time=18.3s\n",
            "Epoch 073 | train_loss=0.4719 acc=0.7965 | val_loss=1.2078 acc=0.6584 | time=18.2s\n",
            "Epoch 074 | train_loss=0.4113 acc=0.8225 | val_loss=1.0721 acc=0.6242 | time=18.1s\n",
            "Epoch 075 | train_loss=0.4119 acc=0.8326 | val_loss=1.2884 acc=0.6506 | time=18.3s\n",
            "Epoch 076 | train_loss=0.3973 acc=0.8350 | val_loss=1.4879 acc=0.6335 | time=18.1s\n",
            "Epoch 077 | train_loss=0.4422 acc=0.8105 | val_loss=0.9043 acc=0.7267 | time=18.3s\n",
            "Epoch 078 | train_loss=0.4256 acc=0.8117 | val_loss=1.2334 acc=0.5761 | time=18.0s\n",
            "Epoch 079 | train_loss=0.3961 acc=0.8295 | val_loss=1.4717 acc=0.5839 | time=18.2s\n",
            "Epoch 080 | train_loss=0.4009 acc=0.8283 | val_loss=1.2341 acc=0.6537 | time=18.3s\n",
            "Epoch 081 | train_loss=0.3830 acc=0.8283 | val_loss=1.1844 acc=0.6832 | time=18.1s\n",
            "Epoch 082 | train_loss=0.3720 acc=0.8404 | val_loss=1.4495 acc=0.6196 | time=18.4s\n",
            "Epoch 083 | train_loss=0.3660 acc=0.8283 | val_loss=1.1871 acc=0.6863 | time=18.0s\n",
            "Epoch 084 | train_loss=0.3972 acc=0.8256 | val_loss=1.3110 acc=0.6615 | time=18.3s\n",
            "Epoch 085 | train_loss=0.3803 acc=0.8295 | val_loss=1.1681 acc=0.6677 | time=18.2s\n",
            "Epoch 086 | train_loss=0.3653 acc=0.8350 | val_loss=1.2779 acc=0.6366 | time=18.1s\n",
            "Epoch 087 | train_loss=0.3893 acc=0.8256 | val_loss=1.4334 acc=0.6677 | time=18.2s\n",
            "Epoch 088 | train_loss=0.3807 acc=0.8338 | val_loss=1.4535 acc=0.5807 | time=18.1s\n",
            "Epoch 089 | train_loss=0.3848 acc=0.8171 | val_loss=1.5768 acc=0.6040 | time=18.3s\n",
            "Epoch 090 | train_loss=0.3568 acc=0.8342 | val_loss=1.4395 acc=0.6848 | time=18.1s\n",
            "Epoch 091 | train_loss=0.3506 acc=0.8381 | val_loss=1.6191 acc=0.6304 | time=18.1s\n",
            "Epoch 092 | train_loss=0.3648 acc=0.8264 | val_loss=1.4075 acc=0.6506 | time=18.2s\n",
            "Epoch 093 | train_loss=0.3668 acc=0.8245 | val_loss=1.3470 acc=0.6568 | time=18.0s\n",
            "Epoch 094 | train_loss=0.3441 acc=0.8458 | val_loss=1.4345 acc=0.6537 | time=18.3s\n",
            "Epoch 095 | train_loss=0.3494 acc=0.8439 | val_loss=1.9093 acc=0.5823 | time=18.1s\n",
            "Epoch 096 | train_loss=0.3668 acc=0.8392 | val_loss=1.4137 acc=0.6522 | time=18.2s\n",
            "Epoch 097 | train_loss=0.3412 acc=0.8431 | val_loss=1.6386 acc=0.6553 | time=18.1s\n",
            "Epoch 098 | train_loss=0.3160 acc=0.8610 | val_loss=1.5466 acc=0.6506 | time=18.1s\n",
            "Epoch 099 | train_loss=0.2927 acc=0.8583 | val_loss=1.7243 acc=0.6398 | time=18.3s\n",
            "Epoch 100 | train_loss=0.3427 acc=0.8396 | val_loss=1.5928 acc=0.5932 | time=18.1s\n",
            "Fold 1 best_train_loss=0.5337, best_train_acc=0.7825, best_val_loss=0.6801, best_val_acc=0.6941\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇████████▇█▇████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▆▅▅▄▄▃▄▃▄▄▄▃▃▃▃▃▂▂▂▁▁▁▁▁▁▁▂▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▃▂▅▆▆▇▆▇████▆██▅▇▆▇▇█▇▆▇▇▆▇▇▆▆▆▇▇▆▅▆▇▆▆</td></tr><tr><td>validation_loss</td><td>▃▃▃▂▂▁▁▁▁▂▂▁▂▂▂▂▁▃▂▃▂▃▃▄▄▄▄▄▄▄▄▅▄▅▅▆▅█▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.78252</td></tr><tr><td>best_train_loss</td><td>0.53374</td></tr><tr><td>best_val_accuracy</td><td>0.6941</td></tr><tr><td>best_val_loss</td><td>0.68011</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>0.83961</td></tr><tr><td>train_loss</td><td>0.34271</td></tr><tr><td>validation_accuracy</td><td>0.59317</td></tr><tr><td>validation_loss</td><td>1.59282</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/2l470698' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/2l470698</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_083554-2l470698/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_090617-sx5kq4wg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/sx5kq4wg' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/sx5kq4wg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-3/runs/sx5kq4wg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1421 acc=0.3460 | val_loss=1.0796 acc=0.4255 | time=18.2s\n",
            "Epoch 002 | train_loss=1.0977 acc=0.3926 | val_loss=1.0727 acc=0.4255 | time=18.1s\n",
            "Epoch 003 | train_loss=1.0634 acc=0.4447 | val_loss=1.0168 acc=0.4891 | time=18.3s\n",
            "Epoch 004 | train_loss=1.0039 acc=0.5076 | val_loss=0.9399 acc=0.6242 | time=18.1s\n",
            "Epoch 005 | train_loss=0.9778 acc=0.5348 | val_loss=0.9044 acc=0.6553 | time=18.1s\n",
            "Epoch 006 | train_loss=0.9542 acc=0.5569 | val_loss=0.8612 acc=0.6755 | time=18.2s\n",
            "Epoch 007 | train_loss=0.9248 acc=0.5860 | val_loss=0.8604 acc=0.6848 | time=18.0s\n",
            "Epoch 008 | train_loss=0.8801 acc=0.6101 | val_loss=0.8552 acc=0.7003 | time=18.3s\n",
            "Epoch 009 | train_loss=0.8657 acc=0.6245 | val_loss=0.8288 acc=0.6988 | time=18.2s\n",
            "Epoch 010 | train_loss=0.8589 acc=0.6280 | val_loss=0.8281 acc=0.7081 | time=18.1s\n",
            "Epoch 011 | train_loss=0.8787 acc=0.6066 | val_loss=0.8907 acc=0.5807 | time=18.1s\n",
            "Epoch 012 | train_loss=0.8753 acc=0.6085 | val_loss=0.7857 acc=0.6801 | time=18.0s\n",
            "Epoch 013 | train_loss=0.8295 acc=0.6326 | val_loss=0.7701 acc=0.7112 | time=18.3s\n",
            "Epoch 014 | train_loss=0.8071 acc=0.6404 | val_loss=0.7507 acc=0.7267 | time=18.1s\n",
            "Epoch 015 | train_loss=0.8055 acc=0.6555 | val_loss=0.7402 acc=0.7081 | time=18.3s\n",
            "Epoch 016 | train_loss=0.7604 acc=0.6583 | val_loss=0.8292 acc=0.6242 | time=18.1s\n",
            "Epoch 017 | train_loss=0.7436 acc=0.6637 | val_loss=0.8260 acc=0.6475 | time=18.0s\n",
            "Epoch 018 | train_loss=0.7266 acc=0.6847 | val_loss=0.7013 acc=0.7267 | time=18.2s\n",
            "Epoch 019 | train_loss=0.7192 acc=0.6870 | val_loss=0.7195 acc=0.7174 | time=18.0s\n",
            "Epoch 020 | train_loss=0.6836 acc=0.7083 | val_loss=0.7107 acc=0.7205 | time=18.3s\n",
            "Epoch 021 | train_loss=0.6798 acc=0.7045 | val_loss=0.6842 acc=0.7360 | time=18.1s\n",
            "Epoch 022 | train_loss=0.6533 acc=0.7181 | val_loss=0.7030 acc=0.7298 | time=18.0s\n",
            "Epoch 023 | train_loss=0.6281 acc=0.7278 | val_loss=0.6789 acc=0.7484 | time=18.2s\n",
            "Epoch 024 | train_loss=0.6135 acc=0.7410 | val_loss=0.6684 acc=0.7593 | time=18.1s\n",
            "Epoch 025 | train_loss=0.6030 acc=0.7449 | val_loss=0.6709 acc=0.7376 | time=18.3s\n",
            "Epoch 026 | train_loss=0.5989 acc=0.7449 | val_loss=0.6519 acc=0.7609 | time=18.1s\n",
            "Epoch 027 | train_loss=0.5839 acc=0.7476 | val_loss=0.6600 acc=0.7531 | time=18.2s\n",
            "Epoch 028 | train_loss=0.5738 acc=0.7650 | val_loss=0.6505 acc=0.7547 | time=18.2s\n",
            "Epoch 029 | train_loss=0.5661 acc=0.7584 | val_loss=0.6507 acc=0.7547 | time=18.0s\n",
            "Epoch 030 | train_loss=0.5924 acc=0.7406 | val_loss=0.6590 acc=0.7547 | time=18.3s\n",
            "Epoch 031 | train_loss=0.6379 acc=0.7196 | val_loss=0.7048 acc=0.7298 | time=18.2s\n",
            "Epoch 032 | train_loss=0.6444 acc=0.7208 | val_loss=0.7570 acc=0.6925 | time=18.1s\n",
            "Epoch 033 | train_loss=0.6377 acc=0.7243 | val_loss=0.7287 acc=0.6972 | time=18.2s\n",
            "Epoch 034 | train_loss=0.6192 acc=0.7282 | val_loss=0.7716 acc=0.6925 | time=18.1s\n",
            "Epoch 035 | train_loss=0.6029 acc=0.7421 | val_loss=0.7540 acc=0.6925 | time=18.2s\n",
            "Epoch 036 | train_loss=0.6124 acc=0.7398 | val_loss=0.6818 acc=0.7376 | time=18.0s\n",
            "Epoch 037 | train_loss=0.5584 acc=0.7495 | val_loss=0.6530 acc=0.7516 | time=18.2s\n",
            "Epoch 038 | train_loss=0.5903 acc=0.7383 | val_loss=0.7626 acc=0.6708 | time=18.2s\n",
            "Epoch 039 | train_loss=0.5742 acc=0.7604 | val_loss=0.6787 acc=0.7360 | time=18.2s\n",
            "Epoch 040 | train_loss=0.5493 acc=0.7608 | val_loss=0.7215 acc=0.7376 | time=18.3s\n",
            "Epoch 041 | train_loss=0.5087 acc=0.7806 | val_loss=0.7181 acc=0.7329 | time=18.2s\n",
            "Epoch 042 | train_loss=0.5276 acc=0.7748 | val_loss=0.7159 acc=0.7500 | time=18.3s\n",
            "Epoch 043 | train_loss=0.5096 acc=0.7798 | val_loss=0.7440 acc=0.7267 | time=18.0s\n",
            "Epoch 044 | train_loss=0.5160 acc=0.7790 | val_loss=0.7528 acc=0.7516 | time=18.3s\n",
            "Epoch 045 | train_loss=0.4801 acc=0.7996 | val_loss=0.7647 acc=0.6925 | time=18.2s\n",
            "Epoch 046 | train_loss=0.4531 acc=0.8047 | val_loss=0.8011 acc=0.6863 | time=18.0s\n",
            "Epoch 047 | train_loss=0.4638 acc=0.7957 | val_loss=0.7570 acc=0.7112 | time=18.2s\n",
            "Epoch 048 | train_loss=0.4335 acc=0.8140 | val_loss=0.8882 acc=0.7065 | time=18.0s\n",
            "Epoch 049 | train_loss=0.4332 acc=0.8163 | val_loss=0.7576 acc=0.7220 | time=18.2s\n",
            "Epoch 050 | train_loss=0.4172 acc=0.8163 | val_loss=0.7790 acc=0.7220 | time=18.2s\n",
            "Epoch 051 | train_loss=0.4264 acc=0.8179 | val_loss=0.8525 acc=0.7143 | time=18.2s\n",
            "Epoch 052 | train_loss=0.4118 acc=0.8198 | val_loss=0.8470 acc=0.7314 | time=18.3s\n",
            "Epoch 053 | train_loss=0.4056 acc=0.8295 | val_loss=0.8622 acc=0.7174 | time=18.2s\n",
            "Epoch 054 | train_loss=0.3919 acc=0.8318 | val_loss=0.8774 acc=0.7174 | time=18.2s\n",
            "Epoch 055 | train_loss=0.3893 acc=0.8342 | val_loss=0.8865 acc=0.7096 | time=18.1s\n",
            "Epoch 056 | train_loss=0.3634 acc=0.8423 | val_loss=0.8534 acc=0.7220 | time=18.1s\n",
            "Epoch 057 | train_loss=0.3746 acc=0.8396 | val_loss=1.1219 acc=0.6366 | time=18.3s\n",
            "Epoch 058 | train_loss=0.3556 acc=0.8474 | val_loss=1.0621 acc=0.6599 | time=18.0s\n",
            "Epoch 059 | train_loss=0.3553 acc=0.8528 | val_loss=1.0039 acc=0.6755 | time=18.2s\n",
            "Epoch 060 | train_loss=0.3702 acc=0.8357 | val_loss=1.0934 acc=0.6739 | time=18.1s\n",
            "Epoch 061 | train_loss=0.3452 acc=0.8450 | val_loss=1.0378 acc=0.6677 | time=18.1s\n",
            "Epoch 062 | train_loss=0.3560 acc=0.8478 | val_loss=0.9470 acc=0.7034 | time=18.3s\n",
            "Epoch 063 | train_loss=0.3599 acc=0.8454 | val_loss=0.9993 acc=0.6832 | time=18.0s\n",
            "Epoch 064 | train_loss=0.3386 acc=0.8497 | val_loss=1.0290 acc=0.6755 | time=18.4s\n",
            "Epoch 065 | train_loss=0.3500 acc=0.8443 | val_loss=1.0197 acc=0.6755 | time=18.1s\n",
            "Epoch 066 | train_loss=0.3407 acc=0.8505 | val_loss=0.9818 acc=0.6832 | time=18.2s\n",
            "Epoch 067 | train_loss=0.3343 acc=0.8532 | val_loss=1.0416 acc=0.6801 | time=18.1s\n",
            "Epoch 068 | train_loss=0.3546 acc=0.8454 | val_loss=1.0088 acc=0.6863 | time=18.2s\n",
            "Epoch 069 | train_loss=0.3210 acc=0.8614 | val_loss=1.0496 acc=0.6755 | time=18.2s\n",
            "Epoch 070 | train_loss=0.3319 acc=0.8540 | val_loss=1.0213 acc=0.6817 | time=18.0s\n",
            "Epoch 071 | train_loss=0.4148 acc=0.8151 | val_loss=0.9195 acc=0.6693 | time=18.2s\n",
            "Epoch 072 | train_loss=0.4625 acc=0.7973 | val_loss=0.9048 acc=0.6972 | time=18.3s\n",
            "Epoch 073 | train_loss=0.4877 acc=0.7899 | val_loss=1.0339 acc=0.7019 | time=18.1s\n",
            "Epoch 074 | train_loss=0.4203 acc=0.8307 | val_loss=1.0124 acc=0.6646 | time=18.2s\n",
            "Epoch 075 | train_loss=0.4219 acc=0.8167 | val_loss=0.9080 acc=0.7174 | time=17.9s\n",
            "Epoch 076 | train_loss=0.4064 acc=0.8276 | val_loss=0.8949 acc=0.7096 | time=18.2s\n",
            "Epoch 077 | train_loss=0.4216 acc=0.8268 | val_loss=0.9980 acc=0.6879 | time=18.1s\n",
            "Epoch 078 | train_loss=0.4205 acc=0.8194 | val_loss=0.9413 acc=0.7345 | time=18.1s\n",
            "Epoch 079 | train_loss=0.4288 acc=0.8206 | val_loss=1.1346 acc=0.6786 | time=18.2s\n",
            "Epoch 080 | train_loss=0.3874 acc=0.8369 | val_loss=0.9628 acc=0.6910 | time=18.0s\n",
            "Epoch 081 | train_loss=0.4116 acc=0.8268 | val_loss=1.3626 acc=0.5978 | time=18.3s\n",
            "Epoch 082 | train_loss=0.3768 acc=0.8361 | val_loss=1.0360 acc=0.6801 | time=18.1s\n",
            "Epoch 083 | train_loss=0.3976 acc=0.8272 | val_loss=0.9945 acc=0.7081 | time=18.1s\n",
            "Epoch 084 | train_loss=0.3783 acc=0.8388 | val_loss=1.0850 acc=0.7081 | time=18.0s\n",
            "Epoch 085 | train_loss=0.3599 acc=0.8388 | val_loss=1.3556 acc=0.6615 | time=18.1s\n",
            "Epoch 086 | train_loss=0.3501 acc=0.8447 | val_loss=1.2383 acc=0.6568 | time=18.2s\n",
            "Epoch 087 | train_loss=0.3390 acc=0.8485 | val_loss=1.1214 acc=0.6972 | time=18.0s\n",
            "Epoch 088 | train_loss=0.3481 acc=0.8470 | val_loss=1.0769 acc=0.6801 | time=18.1s\n",
            "Epoch 089 | train_loss=0.3815 acc=0.8350 | val_loss=1.2094 acc=0.6149 | time=18.0s\n",
            "Epoch 090 | train_loss=0.3627 acc=0.8318 | val_loss=1.1453 acc=0.6801 | time=18.0s\n",
            "Epoch 091 | train_loss=0.3469 acc=0.8450 | val_loss=1.2490 acc=0.6056 | time=18.4s\n",
            "Epoch 092 | train_loss=0.3488 acc=0.8443 | val_loss=1.1240 acc=0.6941 | time=18.1s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-9d7e3dd003dc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-9d7e3dd003dc>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m                 \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Epoch 021 | train_loss=0.6985 acc=0.6928 | val_loss=0.6842 acc=0.7469 | time=18.1s"
      ],
      "metadata": {
        "id": "9y4sS0_jSZ6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ReducePlateuLR Step Size -> 10,  gamma factor-> 0.8\n",
        "### Model Revision\n",
        "- CNNDecoder CNN Layer N1 = 16, N2= 32 revision\n",
        "- CNNDecoder dropout rate increase to 0.5\n",
        "- Transformer Block Encoder  dropout rate = 0.2\n",
        "- Weight Decay to be strong 5e-2 -> 7e-2\n",
        "- LR gamma factor 0.5 -> 0.7 revision\n",
        "- Number of segments = 5 -> 3: prevent overfitting and focus on Global Context"
      ],
      "metadata": {
        "id": "TXtvP5rQ9dwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_4 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 5e-4\n",
        "NUM_FILTERS = 120\n",
        "NUM_BLOCKS = 1\n",
        "NUM_HEADS = 3\n",
        "NUM_SEGMENTS = 3\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler parameters ─────────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA = 0.7\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        # wandb run 시작\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-4\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 / 옵티마이저 / 손실함수\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=FIXED_GAMMA,\n",
        "            patience=FIXED_STEP_SIZE,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            # — 터미널 출력 —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # — wandb 로깅 —\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # best 갱신\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_val_acc    = val_acc\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "\n",
        "        # Fold 종료 시 summary 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        # cleanup\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 로깅 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    # 별도 W&B run 으로 평균 지표 기록\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-4\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # ─── 5-Fold CV Summary 터미널 출력 ────────────────────────────\n",
        "    print(\"\\n===== 5-Fold CV Summary =====\")\n",
        "    for i, res in enumerate(fold_results, 1):\n",
        "        print(\n",
        "            f\" Fold {i:2d}: \"\n",
        "            f\"train_loss = {res['train_loss']:.4f}, \"\n",
        "            f\"train_acc = {res['train_acc']:.4f}, \"\n",
        "            f\"val_loss = {res['val_loss']:.4f}, \"\n",
        "            f\"val_acc = {res['val_acc']:.4f}\"\n",
        "        )\n",
        "    print(\n",
        "        f\"\\n Average: \"\n",
        "        f\"train_loss = {avg['train_loss']:.4f}, \"\n",
        "        f\"train_acc = {avg['train_acc']:.4f}, \"\n",
        "        f\"val_loss = {avg['val_loss']:.4f}, \"\n",
        "        f\"val_acc = {avg['val_acc']:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "IU__c9nec2YO",
        "outputId": "fbafd75b-4221-43c4-f4dc-d39e6eb0f911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_214334-sm7mst20</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/sm7mst20' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/sm7mst20' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/sm7mst20</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1635 acc=0.3173 | val_loss=1.0962 acc=0.3587 | time=192.9s\n",
            "Epoch 002 | train_loss=1.1223 acc=0.3577 | val_loss=1.0910 acc=0.4006 | time=19.0s\n",
            "Epoch 003 | train_loss=1.0954 acc=0.3957 | val_loss=1.0888 acc=0.4006 | time=18.7s\n",
            "Epoch 004 | train_loss=1.0854 acc=0.4221 | val_loss=1.0899 acc=0.4006 | time=19.0s\n",
            "Epoch 005 | train_loss=1.0866 acc=0.4058 | val_loss=1.0899 acc=0.4006 | time=19.0s\n",
            "Epoch 006 | train_loss=1.0859 acc=0.4066 | val_loss=1.0885 acc=0.4006 | time=18.5s\n",
            "Epoch 007 | train_loss=1.0766 acc=0.4194 | val_loss=1.0895 acc=0.4006 | time=19.0s\n",
            "Epoch 008 | train_loss=1.0763 acc=0.4276 | val_loss=1.0882 acc=0.4006 | time=18.8s\n",
            "Epoch 009 | train_loss=1.0770 acc=0.4175 | val_loss=1.0856 acc=0.4006 | time=19.3s\n",
            "Epoch 010 | train_loss=1.0738 acc=0.4287 | val_loss=1.0855 acc=0.4006 | time=19.1s\n",
            "Epoch 011 | train_loss=1.0485 acc=0.4532 | val_loss=1.0444 acc=0.5994 | time=19.3s\n",
            "Epoch 012 | train_loss=0.9995 acc=0.5243 | val_loss=0.9694 acc=0.5745 | time=19.2s\n",
            "Epoch 013 | train_loss=0.9356 acc=0.5728 | val_loss=0.9228 acc=0.5963 | time=19.5s\n",
            "Epoch 014 | train_loss=0.8884 acc=0.6082 | val_loss=0.8570 acc=0.6755 | time=19.3s\n",
            "Epoch 015 | train_loss=0.8530 acc=0.6078 | val_loss=0.8159 acc=0.6661 | time=19.6s\n",
            "Epoch 016 | train_loss=0.8028 acc=0.6381 | val_loss=0.7657 acc=0.6832 | time=19.2s\n",
            "Epoch 017 | train_loss=0.7847 acc=0.6474 | val_loss=0.7796 acc=0.6972 | time=19.6s\n",
            "Epoch 018 | train_loss=0.7780 acc=0.6575 | val_loss=0.7253 acc=0.7034 | time=19.1s\n",
            "Epoch 019 | train_loss=0.7416 acc=0.6680 | val_loss=0.8026 acc=0.6739 | time=19.5s\n",
            "Epoch 020 | train_loss=0.7112 acc=0.6711 | val_loss=0.7491 acc=0.6848 | time=18.6s\n",
            "Epoch 021 | train_loss=0.6985 acc=0.6928 | val_loss=0.6842 acc=0.7469 | time=18.1s\n",
            "Epoch 022 | train_loss=0.6811 acc=0.6835 | val_loss=0.6851 acc=0.7283 | time=18.2s\n",
            "Epoch 023 | train_loss=0.6651 acc=0.6975 | val_loss=0.7434 acc=0.6972 | time=18.6s\n",
            "Epoch 024 | train_loss=0.6450 acc=0.7068 | val_loss=0.7109 acc=0.7329 | time=19.0s\n",
            "Epoch 025 | train_loss=0.6471 acc=0.7126 | val_loss=0.7453 acc=0.6739 | time=18.4s\n",
            "Epoch 026 | train_loss=0.6454 acc=0.7107 | val_loss=0.7254 acc=0.6988 | time=18.9s\n",
            "Epoch 027 | train_loss=0.6306 acc=0.7165 | val_loss=0.7651 acc=0.6910 | time=18.5s\n",
            "Epoch 028 | train_loss=0.5924 acc=0.7247 | val_loss=0.7697 acc=0.6335 | time=18.7s\n",
            "Epoch 029 | train_loss=0.5645 acc=0.7390 | val_loss=0.8463 acc=0.6832 | time=18.4s\n",
            "Epoch 030 | train_loss=0.5768 acc=0.7355 | val_loss=0.7438 acc=0.6444 | time=18.5s\n",
            "Epoch 031 | train_loss=0.5572 acc=0.7375 | val_loss=0.8447 acc=0.6677 | time=18.5s\n",
            "Epoch 032 | train_loss=0.5636 acc=0.7433 | val_loss=0.9694 acc=0.6724 | time=18.4s\n",
            "Epoch 033 | train_loss=0.5330 acc=0.7744 | val_loss=0.9143 acc=0.6739 | time=18.7s\n",
            "Epoch 034 | train_loss=0.5425 acc=0.7612 | val_loss=0.8433 acc=0.7081 | time=18.6s\n",
            "Epoch 035 | train_loss=0.5168 acc=0.7689 | val_loss=0.8639 acc=0.6755 | time=18.6s\n",
            "Epoch 036 | train_loss=0.5288 acc=0.7720 | val_loss=0.7953 acc=0.6863 | time=18.6s\n",
            "Epoch 037 | train_loss=0.4996 acc=0.7798 | val_loss=0.8993 acc=0.6615 | time=18.5s\n",
            "Epoch 038 | train_loss=0.4879 acc=0.7911 | val_loss=0.8717 acc=0.6739 | time=18.7s\n",
            "Epoch 039 | train_loss=0.4963 acc=0.7981 | val_loss=0.8435 acc=0.6770 | time=18.4s\n",
            "Epoch 040 | train_loss=0.4941 acc=0.7922 | val_loss=0.8849 acc=0.6491 | time=18.8s\n",
            "Epoch 041 | train_loss=0.4997 acc=0.7763 | val_loss=0.9900 acc=0.6770 | time=18.3s\n",
            "Epoch 042 | train_loss=0.4574 acc=0.8093 | val_loss=0.9421 acc=0.6708 | time=18.8s\n",
            "Epoch 043 | train_loss=0.4409 acc=0.8136 | val_loss=0.9737 acc=0.6739 | time=18.4s\n",
            "Epoch 044 | train_loss=0.4453 acc=0.8190 | val_loss=1.0269 acc=0.6568 | time=18.4s\n",
            "Epoch 045 | train_loss=0.4495 acc=0.8097 | val_loss=1.0089 acc=0.6444 | time=18.5s\n",
            "Epoch 046 | train_loss=0.4411 acc=0.8043 | val_loss=0.9913 acc=0.6786 | time=18.5s\n",
            "Epoch 047 | train_loss=0.4284 acc=0.8159 | val_loss=1.0245 acc=0.6755 | time=18.6s\n",
            "Epoch 048 | train_loss=0.4547 acc=0.7957 | val_loss=0.8917 acc=0.6755 | time=18.5s\n",
            "Epoch 049 | train_loss=0.4298 acc=0.8050 | val_loss=1.0098 acc=0.6661 | time=18.9s\n",
            "Epoch 050 | train_loss=0.4318 acc=0.8175 | val_loss=1.1350 acc=0.6568 | time=18.6s\n",
            "Epoch 051 | train_loss=0.4403 acc=0.8140 | val_loss=0.9522 acc=0.6770 | time=18.6s\n",
            "Epoch 052 | train_loss=0.4061 acc=0.8260 | val_loss=1.0244 acc=0.6677 | time=18.4s\n",
            "Epoch 053 | train_loss=0.4235 acc=0.8117 | val_loss=1.0587 acc=0.6693 | time=18.4s\n",
            "Epoch 054 | train_loss=0.3994 acc=0.8237 | val_loss=1.0165 acc=0.6817 | time=18.6s\n",
            "Epoch 055 | train_loss=0.4081 acc=0.8202 | val_loss=0.9868 acc=0.6755 | time=18.3s\n",
            "Epoch 056 | train_loss=0.4291 acc=0.8093 | val_loss=0.9946 acc=0.6739 | time=18.8s\n",
            "Epoch 057 | train_loss=0.4044 acc=0.8237 | val_loss=1.1067 acc=0.6522 | time=18.4s\n",
            "Epoch 058 | train_loss=0.3933 acc=0.8249 | val_loss=1.0297 acc=0.6770 | time=18.6s\n",
            "Epoch 059 | train_loss=0.4178 acc=0.8140 | val_loss=1.0943 acc=0.6708 | time=18.4s\n",
            "Epoch 060 | train_loss=0.4072 acc=0.8179 | val_loss=1.0673 acc=0.6584 | time=18.1s\n",
            "Epoch 061 | train_loss=0.3845 acc=0.8307 | val_loss=1.0897 acc=0.6770 | time=18.6s\n",
            "Epoch 062 | train_loss=0.3929 acc=0.8233 | val_loss=1.0265 acc=0.6708 | time=18.3s\n",
            "Epoch 063 | train_loss=0.3810 acc=0.8318 | val_loss=1.1094 acc=0.6817 | time=18.6s\n",
            "Epoch 064 | train_loss=0.4024 acc=0.8202 | val_loss=1.1322 acc=0.6801 | time=18.4s\n",
            "Epoch 065 | train_loss=0.3701 acc=0.8326 | val_loss=1.0159 acc=0.6755 | time=19.0s\n",
            "Epoch 066 | train_loss=0.3840 acc=0.8338 | val_loss=1.0421 acc=0.6817 | time=18.4s\n",
            "Epoch 067 | train_loss=0.3779 acc=0.8381 | val_loss=1.0072 acc=0.6863 | time=18.3s\n",
            "Epoch 068 | train_loss=0.3914 acc=0.8190 | val_loss=1.1536 acc=0.6786 | time=18.5s\n",
            "Epoch 069 | train_loss=0.3742 acc=0.8381 | val_loss=1.1212 acc=0.6630 | time=18.5s\n",
            "Epoch 070 | train_loss=0.3707 acc=0.8311 | val_loss=1.0501 acc=0.6786 | time=18.7s\n",
            "Epoch 071 | train_loss=0.3789 acc=0.8276 | val_loss=1.0677 acc=0.6817 | time=18.4s\n",
            "Epoch 072 | train_loss=0.3979 acc=0.8311 | val_loss=1.0854 acc=0.6708 | time=18.8s\n",
            "Epoch 073 | train_loss=0.3857 acc=0.8260 | val_loss=1.0347 acc=0.6925 | time=18.4s\n",
            "Epoch 074 | train_loss=0.3744 acc=0.8268 | val_loss=1.1085 acc=0.6677 | time=18.6s\n",
            "Epoch 075 | train_loss=0.3839 acc=0.8233 | val_loss=1.0246 acc=0.6786 | time=18.5s\n",
            "Epoch 076 | train_loss=0.3635 acc=0.8427 | val_loss=1.0097 acc=0.6848 | time=18.4s\n",
            "Epoch 077 | train_loss=0.3832 acc=0.8252 | val_loss=1.0959 acc=0.6677 | time=18.5s\n",
            "Epoch 078 | train_loss=0.3626 acc=0.8373 | val_loss=1.1013 acc=0.6615 | time=18.6s\n",
            "Epoch 079 | train_loss=0.3492 acc=0.8361 | val_loss=1.0702 acc=0.6770 | time=18.8s\n",
            "Epoch 080 | train_loss=0.3805 acc=0.8330 | val_loss=1.1930 acc=0.6506 | time=18.6s\n",
            "Epoch 081 | train_loss=0.3726 acc=0.8264 | val_loss=1.0054 acc=0.6863 | time=19.0s\n",
            "Epoch 082 | train_loss=0.3679 acc=0.8330 | val_loss=1.0310 acc=0.6770 | time=18.7s\n",
            "Epoch 083 | train_loss=0.3733 acc=0.8283 | val_loss=1.0270 acc=0.6755 | time=18.6s\n",
            "Epoch 084 | train_loss=0.3915 acc=0.8276 | val_loss=1.1880 acc=0.6584 | time=18.5s\n",
            "Epoch 085 | train_loss=0.3686 acc=0.8369 | val_loss=1.0963 acc=0.6739 | time=19.0s\n",
            "Epoch 086 | train_loss=0.3678 acc=0.8295 | val_loss=1.1191 acc=0.6755 | time=18.7s\n",
            "Epoch 087 | train_loss=0.3650 acc=0.8342 | val_loss=1.1219 acc=0.6739 | time=18.4s\n",
            "Epoch 088 | train_loss=0.3678 acc=0.8326 | val_loss=1.1583 acc=0.6755 | time=18.8s\n",
            "Epoch 089 | train_loss=0.3945 acc=0.8315 | val_loss=1.1136 acc=0.6786 | time=18.5s\n",
            "Epoch 090 | train_loss=0.3470 acc=0.8474 | val_loss=1.0938 acc=0.6786 | time=18.7s\n",
            "Epoch 091 | train_loss=0.3841 acc=0.8377 | val_loss=1.1470 acc=0.6801 | time=18.5s\n",
            "Epoch 092 | train_loss=0.3627 acc=0.8400 | val_loss=1.1184 acc=0.6801 | time=18.5s\n",
            "Epoch 093 | train_loss=0.3772 acc=0.8291 | val_loss=1.0868 acc=0.6848 | time=18.5s\n",
            "Epoch 094 | train_loss=0.3686 acc=0.8365 | val_loss=1.0746 acc=0.6786 | time=18.4s\n",
            "Epoch 095 | train_loss=0.3616 acc=0.8346 | val_loss=1.1117 acc=0.6817 | time=18.5s\n",
            "Epoch 096 | train_loss=0.3561 acc=0.8350 | val_loss=1.0958 acc=0.6786 | time=18.4s\n",
            "Epoch 097 | train_loss=0.3414 acc=0.8482 | val_loss=1.1343 acc=0.6708 | time=18.6s\n",
            "Epoch 098 | train_loss=0.3926 acc=0.8260 | val_loss=1.1360 acc=0.6677 | time=18.5s\n",
            "Epoch 099 | train_loss=0.3702 acc=0.8353 | val_loss=1.1182 acc=0.6739 | time=18.7s\n",
            "Epoch 100 | train_loss=0.3513 acc=0.8408 | val_loss=1.1369 acc=0.6677 | time=18.5s\n",
            "Fold 1 best_train_loss=0.6985, best_train_acc=0.6928, best_val_loss=0.6842, best_val_acc=0.7469\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▃▄▅▅▆▆▆▆▆▆▇▇▇▇▇██▇██████████████████</td></tr><tr><td>train_loss</td><td>█████▆▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▆▇█▇█▇█▆▇▇▇█▇▇▇█▇█▇▇▇▇██▇██▇▇▇█▇▇███▇</td></tr><tr><td>validation_loss</td><td>▆▆▆▆▆▆▆▅▄▃▂▁▂▁▁▃▅▂▆▃▄▅▆▅▅▆▆▆▆▆▅▇▇█▆▇▇▇▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.69282</td></tr><tr><td>best_train_loss</td><td>0.69853</td></tr><tr><td>best_val_accuracy</td><td>0.74689</td></tr><tr><td>best_val_loss</td><td>0.68421</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>0.84078</td></tr><tr><td>train_loss</td><td>0.35132</td></tr><tr><td>validation_accuracy</td><td>0.6677</td></tr><tr><td>validation_loss</td><td>1.13687</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/sm7mst20' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/sm7mst20</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_214334-sm7mst20/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_221741-a2hjrm8o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/a2hjrm8o' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/a2hjrm8o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-4/runs/a2hjrm8o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1601 acc=0.3748 | val_loss=1.0952 acc=0.3665 | time=18.6s\n",
            "Epoch 002 | train_loss=1.1454 acc=0.3697 | val_loss=1.0762 acc=0.4270 | time=18.5s\n",
            "Epoch 003 | train_loss=1.1187 acc=0.3856 | val_loss=1.0760 acc=0.4255 | time=18.4s\n",
            "Epoch 004 | train_loss=1.1237 acc=0.3779 | val_loss=1.0739 acc=0.5326 | time=18.8s\n",
            "Epoch 005 | train_loss=1.1111 acc=0.3860 | val_loss=1.0736 acc=0.4752 | time=18.4s\n",
            "Epoch 006 | train_loss=1.0955 acc=0.3915 | val_loss=1.0696 acc=0.4270 | time=18.9s\n",
            "Epoch 007 | train_loss=1.0820 acc=0.4183 | val_loss=1.0134 acc=0.5730 | time=18.3s\n",
            "Epoch 008 | train_loss=1.0317 acc=0.4905 | val_loss=0.9829 acc=0.6165 | time=18.6s\n",
            "Epoch 009 | train_loss=1.0027 acc=0.5433 | val_loss=1.0021 acc=0.6320 | time=18.5s\n",
            "Epoch 010 | train_loss=0.9560 acc=0.5736 | val_loss=0.9903 acc=0.6056 | time=18.3s\n",
            "Epoch 011 | train_loss=0.9299 acc=0.5759 | val_loss=0.8320 acc=0.6444 | time=18.8s\n",
            "Epoch 012 | train_loss=0.8908 acc=0.5981 | val_loss=0.8974 acc=0.6211 | time=18.3s\n",
            "Epoch 013 | train_loss=0.8667 acc=0.6144 | val_loss=0.7930 acc=0.6661 | time=18.9s\n",
            "Epoch 014 | train_loss=0.8291 acc=0.6237 | val_loss=0.7877 acc=0.6553 | time=18.5s\n",
            "Epoch 015 | train_loss=0.8162 acc=0.6233 | val_loss=0.7612 acc=0.7050 | time=18.7s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-ddad44ead300>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-ddad44ead300>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Change the block = 1 -> 2, head = 3 -> 2\n",
        "\n",
        "- After changing the segment = 3, it turns out that the model shows too high validation loss and accuracy is stuck around 66%\n",
        "- Block = 2, Head = 2\n",
        "- Remaining the dropout rate and layer num as before\n",
        "- Turn back to segment = 5"
      ],
      "metadata": {
        "id": "z2kNvIgMQJYp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_4 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 5e-4\n",
        "NUM_FILTERS = 120\n",
        "NUM_BLOCKS = 2\n",
        "NUM_HEADS = 2\n",
        "NUM_SEGMENTS = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler parameters ─────────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA = 0.7\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        # wandb run 시작\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-5\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 / 옵티마이저 / 손실함수\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=FIXED_GAMMA,\n",
        "            patience=FIXED_STEP_SIZE,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            # — 터미널 출력 —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # — wandb 로깅 —\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # best 갱신\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_val_acc    = val_acc\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "\n",
        "        # Fold 종료 시 summary 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        # cleanup\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 로깅 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    # 별도 W&B run 으로 평균 지표 기록\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-5\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # ─── 5-Fold CV Summary 터미널 출력 ────────────────────────────\n",
        "    print(\"\\n===== 5-Fold CV Summary =====\")\n",
        "    for i, res in enumerate(fold_results, 1):\n",
        "        print(\n",
        "            f\" Fold {i:2d}: \"\n",
        "            f\"train_loss = {res['train_loss']:.4f}, \"\n",
        "            f\"train_acc = {res['train_acc']:.4f}, \"\n",
        "            f\"val_loss = {res['val_loss']:.4f}, \"\n",
        "            f\"val_acc = {res['val_acc']:.4f}\"\n",
        "        )\n",
        "    print(\n",
        "        f\"\\n Average: \"\n",
        "        f\"train_loss = {avg['train_loss']:.4f}, \"\n",
        "        f\"train_acc = {avg['train_acc']:.4f}, \"\n",
        "        f\"val_loss = {avg['val_loss']:.4f}, \"\n",
        "        f\"val_acc = {avg['val_acc']:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_Sr2YiuyPox-",
        "outputId": "9c23019a-421c-4251-d88f-3714cfbb4227"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_223029-2uwa2v3j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5/runs/2uwa2v3j' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5/runs/2uwa2v3j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5/runs/2uwa2v3j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1641 acc=0.3674 | val_loss=1.0901 acc=0.4006 | time=25.0s\n",
            "Epoch 002 | train_loss=1.1204 acc=0.3895 | val_loss=1.0927 acc=0.4006 | time=24.2s\n",
            "Epoch 003 | train_loss=1.1167 acc=0.3876 | val_loss=1.1304 acc=0.4006 | time=24.3s\n",
            "Epoch 004 | train_loss=1.0982 acc=0.3988 | val_loss=1.0918 acc=0.4006 | time=24.2s\n",
            "Epoch 005 | train_loss=1.0962 acc=0.3973 | val_loss=1.0883 acc=0.4006 | time=24.1s\n",
            "Epoch 006 | train_loss=1.0847 acc=0.4268 | val_loss=1.0964 acc=0.4006 | time=24.0s\n",
            "Epoch 007 | train_loss=1.0878 acc=0.4113 | val_loss=1.0909 acc=0.4006 | time=24.1s\n",
            "Epoch 008 | train_loss=1.0616 acc=0.4505 | val_loss=1.0474 acc=0.5311 | time=24.2s\n",
            "Epoch 009 | train_loss=1.0163 acc=0.5130 | val_loss=1.0542 acc=0.5590 | time=24.3s\n",
            "Epoch 010 | train_loss=0.9332 acc=0.5825 | val_loss=0.8620 acc=0.6304 | time=24.1s\n",
            "Epoch 011 | train_loss=0.8714 acc=0.5996 | val_loss=0.8327 acc=0.6522 | time=24.3s\n",
            "Epoch 012 | train_loss=0.8411 acc=0.6202 | val_loss=0.8908 acc=0.6351 | time=24.1s\n",
            "Epoch 013 | train_loss=0.8134 acc=0.6318 | val_loss=0.7423 acc=0.7065 | time=24.2s\n",
            "Epoch 014 | train_loss=0.7724 acc=0.6369 | val_loss=0.7303 acc=0.7019 | time=24.2s\n",
            "Epoch 015 | train_loss=0.7327 acc=0.6590 | val_loss=0.7144 acc=0.7220 | time=24.1s\n",
            "Epoch 016 | train_loss=0.6982 acc=0.6816 | val_loss=0.7148 acc=0.6972 | time=23.9s\n",
            "Epoch 017 | train_loss=0.6967 acc=0.6796 | val_loss=0.8308 acc=0.6708 | time=24.2s\n",
            "Epoch 018 | train_loss=0.6747 acc=0.6983 | val_loss=0.6939 acc=0.7127 | time=24.2s\n",
            "Epoch 019 | train_loss=0.6456 acc=0.7138 | val_loss=0.7292 acc=0.7205 | time=24.3s\n",
            "Epoch 020 | train_loss=0.6309 acc=0.7204 | val_loss=0.9693 acc=0.6755 | time=24.3s\n",
            "Epoch 021 | train_loss=0.6379 acc=0.7223 | val_loss=0.7524 acc=0.7422 | time=24.2s\n",
            "Epoch 022 | train_loss=0.6260 acc=0.7235 | val_loss=0.7249 acc=0.7220 | time=24.3s\n",
            "Epoch 023 | train_loss=0.6058 acc=0.7301 | val_loss=0.7286 acc=0.6786 | time=24.1s\n",
            "Epoch 024 | train_loss=0.5633 acc=0.7530 | val_loss=0.7185 acc=0.7065 | time=24.1s\n",
            "Epoch 025 | train_loss=0.5399 acc=0.7674 | val_loss=0.8043 acc=0.6413 | time=24.0s\n",
            "Epoch 026 | train_loss=0.5280 acc=0.7821 | val_loss=0.6679 acc=0.6925 | time=24.1s\n",
            "Epoch 027 | train_loss=0.5168 acc=0.7872 | val_loss=0.7720 acc=0.6848 | time=24.3s\n",
            "Epoch 028 | train_loss=0.5170 acc=0.7709 | val_loss=0.7633 acc=0.6335 | time=24.3s\n",
            "Epoch 029 | train_loss=0.5037 acc=0.7767 | val_loss=0.8678 acc=0.6817 | time=24.1s\n",
            "Epoch 030 | train_loss=0.4476 acc=0.8175 | val_loss=0.8985 acc=0.6739 | time=24.1s\n",
            "Epoch 031 | train_loss=0.4477 acc=0.8047 | val_loss=0.8732 acc=0.6817 | time=24.2s\n",
            "Epoch 032 | train_loss=0.4536 acc=0.8124 | val_loss=1.0025 acc=0.6724 | time=24.2s\n",
            "Epoch 033 | train_loss=0.4118 acc=0.8221 | val_loss=1.0004 acc=0.6739 | time=24.1s\n",
            "Epoch 034 | train_loss=0.4084 acc=0.8221 | val_loss=0.9111 acc=0.6661 | time=24.0s\n",
            "Epoch 035 | train_loss=0.4001 acc=0.8280 | val_loss=1.0731 acc=0.6817 | time=24.0s\n",
            "Epoch 036 | train_loss=0.4064 acc=0.8229 | val_loss=1.0475 acc=0.6568 | time=24.2s\n",
            "Epoch 037 | train_loss=0.3751 acc=0.8295 | val_loss=0.9754 acc=0.6894 | time=24.2s\n",
            "Epoch 038 | train_loss=0.3968 acc=0.8214 | val_loss=1.1537 acc=0.6553 | time=24.3s\n",
            "Epoch 039 | train_loss=0.3483 acc=0.8427 | val_loss=1.0868 acc=0.6444 | time=24.1s\n",
            "Epoch 040 | train_loss=0.3425 acc=0.8482 | val_loss=1.0111 acc=0.6832 | time=24.1s\n",
            "Epoch 041 | train_loss=0.3673 acc=0.8497 | val_loss=1.0838 acc=0.6553 | time=24.2s\n",
            "Epoch 042 | train_loss=0.3630 acc=0.8443 | val_loss=1.1092 acc=0.6584 | time=24.1s\n",
            "Epoch 043 | train_loss=0.3166 acc=0.8649 | val_loss=1.0696 acc=0.6584 | time=24.2s\n",
            "Epoch 044 | train_loss=0.3184 acc=0.8524 | val_loss=1.1523 acc=0.6879 | time=24.3s\n",
            "Epoch 045 | train_loss=0.3207 acc=0.8575 | val_loss=1.0638 acc=0.6615 | time=24.2s\n",
            "Epoch 046 | train_loss=0.3242 acc=0.8617 | val_loss=1.3124 acc=0.6429 | time=24.2s\n",
            "Epoch 047 | train_loss=0.3267 acc=0.8602 | val_loss=1.3208 acc=0.6475 | time=24.4s\n",
            "Epoch 048 | train_loss=0.3208 acc=0.8555 | val_loss=1.2756 acc=0.6553 | time=24.2s\n",
            "Epoch 049 | train_loss=0.3206 acc=0.8559 | val_loss=1.1358 acc=0.6832 | time=24.1s\n",
            "Epoch 050 | train_loss=0.2976 acc=0.8687 | val_loss=1.3147 acc=0.6646 | time=24.1s\n",
            "Epoch 051 | train_loss=0.3064 acc=0.8652 | val_loss=1.2729 acc=0.6630 | time=24.0s\n",
            "Epoch 052 | train_loss=0.3147 acc=0.8629 | val_loss=1.3917 acc=0.6646 | time=24.2s\n",
            "Epoch 053 | train_loss=0.3080 acc=0.8699 | val_loss=1.1889 acc=0.6677 | time=23.9s\n",
            "Epoch 054 | train_loss=0.2981 acc=0.8656 | val_loss=1.3063 acc=0.6460 | time=24.0s\n",
            "Epoch 055 | train_loss=0.2781 acc=0.8773 | val_loss=1.2404 acc=0.6786 | time=23.9s\n",
            "Epoch 056 | train_loss=0.3076 acc=0.8583 | val_loss=1.3531 acc=0.6630 | time=23.8s\n",
            "Epoch 057 | train_loss=0.2987 acc=0.8672 | val_loss=1.2006 acc=0.6708 | time=23.9s\n",
            "Epoch 058 | train_loss=0.3048 acc=0.8571 | val_loss=1.3074 acc=0.6599 | time=23.8s\n",
            "Epoch 059 | train_loss=0.3014 acc=0.8567 | val_loss=1.2446 acc=0.6677 | time=23.8s\n",
            "Epoch 060 | train_loss=0.2947 acc=0.8645 | val_loss=1.1638 acc=0.6661 | time=23.9s\n",
            "Epoch 061 | train_loss=0.2801 acc=0.8707 | val_loss=1.2108 acc=0.6599 | time=23.7s\n",
            "Epoch 062 | train_loss=0.3005 acc=0.8598 | val_loss=1.3043 acc=0.6522 | time=23.9s\n",
            "Epoch 063 | train_loss=0.2761 acc=0.8738 | val_loss=1.2789 acc=0.6739 | time=24.0s\n",
            "Epoch 064 | train_loss=0.3090 acc=0.8598 | val_loss=1.3199 acc=0.6739 | time=23.8s\n",
            "Epoch 065 | train_loss=0.2858 acc=0.8641 | val_loss=1.3088 acc=0.6537 | time=23.8s\n",
            "Epoch 066 | train_loss=0.2882 acc=0.8672 | val_loss=1.3146 acc=0.6537 | time=23.8s\n",
            "Epoch 067 | train_loss=0.2827 acc=0.8680 | val_loss=1.3040 acc=0.6615 | time=23.7s\n",
            "Epoch 068 | train_loss=0.2959 acc=0.8660 | val_loss=1.2541 acc=0.6786 | time=23.8s\n",
            "Epoch 069 | train_loss=0.2988 acc=0.8649 | val_loss=1.3204 acc=0.6693 | time=23.9s\n",
            "Epoch 070 | train_loss=0.2763 acc=0.8777 | val_loss=1.3353 acc=0.6724 | time=23.9s\n",
            "Epoch 071 | train_loss=0.2675 acc=0.8738 | val_loss=1.2802 acc=0.6677 | time=23.9s\n",
            "Epoch 072 | train_loss=0.2684 acc=0.8796 | val_loss=1.2816 acc=0.6630 | time=23.9s\n",
            "Epoch 073 | train_loss=0.2870 acc=0.8641 | val_loss=1.3281 acc=0.6677 | time=23.8s\n",
            "Epoch 074 | train_loss=0.2691 acc=0.8839 | val_loss=1.3371 acc=0.6615 | time=23.9s\n",
            "Epoch 075 | train_loss=0.2801 acc=0.8645 | val_loss=1.2553 acc=0.6724 | time=23.9s\n",
            "Epoch 076 | train_loss=0.2772 acc=0.8718 | val_loss=1.3169 acc=0.6755 | time=23.8s\n",
            "Epoch 077 | train_loss=0.2693 acc=0.8773 | val_loss=1.3426 acc=0.6677 | time=23.8s\n",
            "Epoch 078 | train_loss=0.2665 acc=0.8753 | val_loss=1.3310 acc=0.6630 | time=24.0s\n",
            "Epoch 079 | train_loss=0.2881 acc=0.8637 | val_loss=1.2889 acc=0.6646 | time=24.0s\n",
            "Epoch 080 | train_loss=0.2721 acc=0.8765 | val_loss=1.3703 acc=0.6599 | time=24.4s\n",
            "Epoch 081 | train_loss=0.2668 acc=0.8742 | val_loss=1.3883 acc=0.6615 | time=24.6s\n",
            "Epoch 082 | train_loss=0.2867 acc=0.8711 | val_loss=1.3875 acc=0.6630 | time=24.4s\n",
            "Epoch 083 | train_loss=0.2684 acc=0.8715 | val_loss=1.3463 acc=0.6661 | time=24.6s\n",
            "Epoch 084 | train_loss=0.2512 acc=0.8831 | val_loss=1.3993 acc=0.6522 | time=24.5s\n",
            "Epoch 085 | train_loss=0.2625 acc=0.8769 | val_loss=1.4222 acc=0.6599 | time=24.2s\n",
            "Epoch 086 | train_loss=0.2869 acc=0.8656 | val_loss=1.4806 acc=0.6661 | time=24.6s\n",
            "Epoch 087 | train_loss=0.2675 acc=0.8753 | val_loss=1.3692 acc=0.6755 | time=24.5s\n",
            "Epoch 088 | train_loss=0.2590 acc=0.8858 | val_loss=1.4010 acc=0.6693 | time=24.7s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-60d9863268f3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-60d9863268f3>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_4 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 5e-2\n",
        "NUM_FILTERS = 120\n",
        "NUM_BLOCKS = 2\n",
        "NUM_HEADS = 2\n",
        "NUM_SEGMENTS = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler parameters ─────────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA = 0.7\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        # wandb run 시작\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-6\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 / 옵티마이저 / 손실함수\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=LR,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=FIXED_GAMMA,\n",
        "            patience=FIXED_STEP_SIZE,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            # — 터미널 출력 —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # — wandb 로깅 —\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # best 갱신\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_val_acc    = val_acc\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "\n",
        "        # Fold 종료 시 summary 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        # cleanup\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 로깅 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    # 별도 W&B run 으로 평균 지표 기록\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-6\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # ─── 5-Fold CV Summary 터미널 출력 ────────────────────────────\n",
        "    print(\"\\n===== 5-Fold CV Summary =====\")\n",
        "    for i, res in enumerate(fold_results, 1):\n",
        "        print(\n",
        "            f\" Fold {i:2d}: \"\n",
        "            f\"train_loss = {res['train_loss']:.4f}, \"\n",
        "            f\"train_acc = {res['train_acc']:.4f}, \"\n",
        "            f\"val_loss = {res['val_loss']:.4f}, \"\n",
        "            f\"val_acc = {res['val_acc']:.4f}\"\n",
        "        )\n",
        "    print(\n",
        "        f\"\\n Average: \"\n",
        "        f\"train_loss = {avg['train_loss']:.4f}, \"\n",
        "        f\"train_acc = {avg['train_acc']:.4f}, \"\n",
        "        f\"val_loss = {avg['val_loss']:.4f}, \"\n",
        "        f\"val_acc = {avg['val_acc']:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WZwD3vpURSf8",
        "outputId": "aa4e88cd-4c67-484c-d127-82bac17ceaa8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▃▄▄▅▅▅▆▆▆▆▇▆▆▇▇▇▇▇▇████████████████████</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▅▅▅▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▄▆▆██▇▇█▇█▇▇▇▇▇▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>validation_loss</td><td>▅▅▂▃▁▂▁▁▃▁▂▂▃▃▄▃▄▄▄▅▄▆▆▇▆▆▅▆▆▇▆▇▆▆▆▇▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>88</td></tr><tr><td>train_accuracy</td><td>0.88583</td></tr><tr><td>train_loss</td><td>0.25898</td></tr><tr><td>validation_accuracy</td><td>0.66925</td></tr><tr><td>validation_loss</td><td>1.40101</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5/runs/2uwa2v3j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5/runs/2uwa2v3j</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-5</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_223029-2uwa2v3j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_230721-vkgomcmz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/vkgomcmz' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/vkgomcmz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/vkgomcmz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1807 acc=0.3557 | val_loss=1.1243 acc=0.2391 | time=23.8s\n",
            "Epoch 002 | train_loss=1.1356 acc=0.3755 | val_loss=1.0979 acc=0.4006 | time=23.8s\n",
            "Epoch 003 | train_loss=1.1200 acc=0.4027 | val_loss=1.0907 acc=0.4006 | time=24.1s\n",
            "Epoch 004 | train_loss=1.1065 acc=0.3969 | val_loss=1.0937 acc=0.4006 | time=24.0s\n",
            "Epoch 005 | train_loss=1.1034 acc=0.3981 | val_loss=1.0901 acc=0.4006 | time=23.8s\n",
            "Epoch 006 | train_loss=1.0948 acc=0.3918 | val_loss=1.0897 acc=0.4006 | time=23.8s\n",
            "Epoch 007 | train_loss=1.0820 acc=0.4210 | val_loss=1.0957 acc=0.4006 | time=23.8s\n",
            "Epoch 008 | train_loss=1.0715 acc=0.4190 | val_loss=1.0869 acc=0.4006 | time=23.8s\n",
            "Epoch 009 | train_loss=1.0759 acc=0.4295 | val_loss=1.0733 acc=0.4006 | time=23.9s\n",
            "Epoch 010 | train_loss=1.0440 acc=0.4664 | val_loss=1.0426 acc=0.5559 | time=23.8s\n",
            "Epoch 011 | train_loss=0.9854 acc=0.5472 | val_loss=0.9198 acc=0.6258 | time=24.1s\n",
            "Epoch 012 | train_loss=0.9262 acc=0.5841 | val_loss=0.8927 acc=0.6180 | time=23.9s\n",
            "Epoch 013 | train_loss=0.8976 acc=0.6004 | val_loss=0.8210 acc=0.6366 | time=23.9s\n",
            "Epoch 014 | train_loss=0.8709 acc=0.6109 | val_loss=0.9969 acc=0.3758 | time=23.8s\n",
            "Epoch 015 | train_loss=0.8386 acc=0.6124 | val_loss=0.8798 acc=0.6118 | time=23.9s\n",
            "Epoch 016 | train_loss=0.7827 acc=0.6303 | val_loss=1.1173 acc=0.5202 | time=23.9s\n",
            "Epoch 017 | train_loss=0.7739 acc=0.6373 | val_loss=0.8171 acc=0.6708 | time=24.0s\n",
            "Epoch 018 | train_loss=0.7453 acc=0.6699 | val_loss=0.9325 acc=0.4845 | time=23.9s\n",
            "Epoch 019 | train_loss=0.7348 acc=0.6769 | val_loss=0.7115 acc=0.7034 | time=24.2s\n",
            "Epoch 020 | train_loss=0.6921 acc=0.6804 | val_loss=0.7892 acc=0.6661 | time=24.1s\n",
            "Epoch 021 | train_loss=0.6708 acc=0.7002 | val_loss=0.6871 acc=0.7422 | time=24.0s\n",
            "Epoch 022 | train_loss=0.6814 acc=0.6897 | val_loss=0.8317 acc=0.6165 | time=23.9s\n",
            "Epoch 023 | train_loss=0.6525 acc=0.7076 | val_loss=0.7164 acc=0.7283 | time=23.8s\n",
            "Epoch 024 | train_loss=0.6407 acc=0.7278 | val_loss=0.7222 acc=0.7112 | time=23.8s\n",
            "Epoch 025 | train_loss=0.6407 acc=0.7184 | val_loss=0.7894 acc=0.6941 | time=23.8s\n",
            "Epoch 026 | train_loss=0.6259 acc=0.7262 | val_loss=0.7835 acc=0.6708 | time=23.7s\n",
            "Epoch 027 | train_loss=0.5899 acc=0.7437 | val_loss=0.7630 acc=0.6801 | time=24.0s\n",
            "Epoch 028 | train_loss=0.5983 acc=0.7417 | val_loss=0.7119 acc=0.6786 | time=23.9s\n",
            "Epoch 029 | train_loss=0.5612 acc=0.7534 | val_loss=1.0029 acc=0.6553 | time=23.9s\n",
            "Epoch 030 | train_loss=0.5446 acc=0.7674 | val_loss=0.7652 acc=0.7112 | time=23.9s\n",
            "Epoch 031 | train_loss=0.5293 acc=0.7697 | val_loss=0.8864 acc=0.6630 | time=23.9s\n",
            "Epoch 032 | train_loss=0.5236 acc=0.7751 | val_loss=0.8203 acc=0.6848 | time=23.7s\n",
            "Epoch 033 | train_loss=0.5085 acc=0.7794 | val_loss=0.9318 acc=0.6863 | time=23.8s\n",
            "Epoch 034 | train_loss=0.4794 acc=0.7907 | val_loss=0.8826 acc=0.6537 | time=23.9s\n",
            "Epoch 035 | train_loss=0.4724 acc=0.8027 | val_loss=0.9009 acc=0.6599 | time=24.1s\n",
            "Epoch 036 | train_loss=0.4789 acc=0.7883 | val_loss=0.8064 acc=0.7065 | time=24.1s\n",
            "Epoch 037 | train_loss=0.4478 acc=0.8151 | val_loss=0.7214 acc=0.7205 | time=23.7s\n",
            "Epoch 038 | train_loss=0.4535 acc=0.8035 | val_loss=0.9097 acc=0.6770 | time=23.7s\n",
            "Epoch 039 | train_loss=0.4449 acc=0.8132 | val_loss=1.3435 acc=0.6040 | time=23.9s\n",
            "Epoch 040 | train_loss=0.4395 acc=0.8128 | val_loss=0.7721 acc=0.7252 | time=23.9s\n",
            "Epoch 041 | train_loss=0.4219 acc=0.8229 | val_loss=0.9089 acc=0.6972 | time=23.9s\n",
            "Epoch 042 | train_loss=0.4153 acc=0.8283 | val_loss=0.8314 acc=0.6988 | time=23.8s\n",
            "Epoch 043 | train_loss=0.4327 acc=0.8109 | val_loss=0.8995 acc=0.7065 | time=24.1s\n",
            "Epoch 044 | train_loss=0.4124 acc=0.8186 | val_loss=1.1699 acc=0.6630 | time=24.1s\n",
            "Epoch 045 | train_loss=0.3998 acc=0.8233 | val_loss=1.2152 acc=0.6382 | time=23.8s\n",
            "Epoch 046 | train_loss=0.3797 acc=0.8318 | val_loss=1.3370 acc=0.6211 | time=23.9s\n",
            "Epoch 047 | train_loss=0.3996 acc=0.8287 | val_loss=1.1689 acc=0.6693 | time=23.8s\n",
            "Epoch 048 | train_loss=0.3906 acc=0.8338 | val_loss=1.5633 acc=0.6258 | time=23.8s\n",
            "Epoch 049 | train_loss=0.3826 acc=0.8353 | val_loss=1.3508 acc=0.6196 | time=23.8s\n",
            "Epoch 050 | train_loss=0.3567 acc=0.8450 | val_loss=1.2327 acc=0.6413 | time=23.8s\n",
            "Epoch 051 | train_loss=0.3454 acc=0.8482 | val_loss=1.3054 acc=0.6165 | time=23.9s\n",
            "Epoch 052 | train_loss=0.3564 acc=0.8450 | val_loss=1.1818 acc=0.6584 | time=24.0s\n",
            "Epoch 053 | train_loss=0.3495 acc=0.8443 | val_loss=1.3272 acc=0.6258 | time=23.8s\n",
            "Epoch 054 | train_loss=0.3470 acc=0.8544 | val_loss=1.4493 acc=0.6149 | time=23.8s\n",
            "Epoch 055 | train_loss=0.3664 acc=0.8419 | val_loss=1.2853 acc=0.6491 | time=23.8s\n",
            "Epoch 056 | train_loss=0.3475 acc=0.8361 | val_loss=1.4336 acc=0.6429 | time=24.0s\n",
            "Epoch 057 | train_loss=0.3302 acc=0.8571 | val_loss=1.4301 acc=0.6211 | time=23.8s\n",
            "Epoch 058 | train_loss=0.3495 acc=0.8520 | val_loss=1.3674 acc=0.6537 | time=23.9s\n",
            "Epoch 059 | train_loss=0.3383 acc=0.8454 | val_loss=1.3329 acc=0.6568 | time=23.9s\n",
            "Epoch 060 | train_loss=0.3062 acc=0.8680 | val_loss=1.4792 acc=0.6475 | time=23.9s\n",
            "Epoch 061 | train_loss=0.3367 acc=0.8548 | val_loss=1.4960 acc=0.6460 | time=23.9s\n",
            "Epoch 062 | train_loss=0.3318 acc=0.8563 | val_loss=1.3922 acc=0.6289 | time=23.8s\n",
            "Epoch 063 | train_loss=0.3274 acc=0.8563 | val_loss=1.3770 acc=0.6398 | time=23.9s\n",
            "Epoch 064 | train_loss=0.3157 acc=0.8641 | val_loss=1.4326 acc=0.6382 | time=23.7s\n",
            "Epoch 065 | train_loss=0.3301 acc=0.8555 | val_loss=1.3469 acc=0.6537 | time=23.8s\n",
            "Epoch 066 | train_loss=0.3311 acc=0.8485 | val_loss=1.3120 acc=0.6444 | time=23.9s\n",
            "Epoch 067 | train_loss=0.3372 acc=0.8443 | val_loss=1.3712 acc=0.6382 | time=24.1s\n",
            "Epoch 068 | train_loss=0.3364 acc=0.8454 | val_loss=1.4110 acc=0.6382 | time=24.0s\n",
            "Epoch 069 | train_loss=0.3274 acc=0.8586 | val_loss=1.4162 acc=0.6522 | time=23.7s\n",
            "Epoch 070 | train_loss=0.3117 acc=0.8590 | val_loss=1.4538 acc=0.6398 | time=23.7s\n",
            "Epoch 071 | train_loss=0.3345 acc=0.8493 | val_loss=1.4177 acc=0.6444 | time=23.9s\n",
            "Epoch 072 | train_loss=0.3254 acc=0.8474 | val_loss=1.4800 acc=0.6460 | time=23.9s\n",
            "Epoch 073 | train_loss=0.3138 acc=0.8594 | val_loss=1.4252 acc=0.6444 | time=23.8s\n",
            "Epoch 074 | train_loss=0.3124 acc=0.8590 | val_loss=1.4675 acc=0.6444 | time=23.8s\n",
            "Epoch 075 | train_loss=0.2975 acc=0.8699 | val_loss=1.5161 acc=0.6429 | time=24.0s\n",
            "Epoch 076 | train_loss=0.3035 acc=0.8695 | val_loss=1.4865 acc=0.6382 | time=23.9s\n",
            "Epoch 077 | train_loss=0.2944 acc=0.8788 | val_loss=1.5238 acc=0.6444 | time=23.9s\n",
            "Epoch 078 | train_loss=0.3053 acc=0.8563 | val_loss=1.4945 acc=0.6460 | time=23.8s\n",
            "Epoch 079 | train_loss=0.3252 acc=0.8520 | val_loss=1.4565 acc=0.6522 | time=23.7s\n",
            "Epoch 080 | train_loss=0.3195 acc=0.8478 | val_loss=1.4099 acc=0.6615 | time=23.9s\n",
            "Epoch 081 | train_loss=0.3080 acc=0.8575 | val_loss=1.4057 acc=0.6677 | time=23.8s\n",
            "Epoch 082 | train_loss=0.3143 acc=0.8610 | val_loss=1.4209 acc=0.6522 | time=24.0s\n",
            "Epoch 083 | train_loss=0.3182 acc=0.8559 | val_loss=1.4051 acc=0.6537 | time=24.0s\n",
            "Epoch 084 | train_loss=0.3054 acc=0.8672 | val_loss=1.4270 acc=0.6568 | time=24.0s\n",
            "Epoch 085 | train_loss=0.3125 acc=0.8583 | val_loss=1.4581 acc=0.6537 | time=23.9s\n",
            "Epoch 086 | train_loss=0.3178 acc=0.8575 | val_loss=1.4381 acc=0.6584 | time=23.9s\n",
            "Epoch 087 | train_loss=0.2966 acc=0.8656 | val_loss=1.5285 acc=0.6304 | time=23.8s\n",
            "Epoch 088 | train_loss=0.3016 acc=0.8567 | val_loss=1.4520 acc=0.6522 | time=23.7s\n",
            "Epoch 089 | train_loss=0.3225 acc=0.8509 | val_loss=1.5077 acc=0.6460 | time=23.8s\n",
            "Epoch 090 | train_loss=0.3048 acc=0.8652 | val_loss=1.4179 acc=0.6522 | time=23.8s\n",
            "Epoch 091 | train_loss=0.2919 acc=0.8726 | val_loss=1.4723 acc=0.6429 | time=24.0s\n",
            "Epoch 092 | train_loss=0.2869 acc=0.8703 | val_loss=1.4274 acc=0.6553 | time=23.9s\n",
            "Epoch 093 | train_loss=0.3119 acc=0.8555 | val_loss=1.4729 acc=0.6553 | time=23.9s\n",
            "Epoch 094 | train_loss=0.3178 acc=0.8602 | val_loss=1.4159 acc=0.6568 | time=23.9s\n",
            "Epoch 095 | train_loss=0.3112 acc=0.8606 | val_loss=1.4685 acc=0.6506 | time=23.8s\n",
            "Epoch 096 | train_loss=0.2979 acc=0.8672 | val_loss=1.4380 acc=0.6537 | time=23.9s\n",
            "Epoch 097 | train_loss=0.3241 acc=0.8501 | val_loss=1.4783 acc=0.6366 | time=23.8s\n",
            "Epoch 098 | train_loss=0.3043 acc=0.8664 | val_loss=1.4526 acc=0.6491 | time=23.9s\n",
            "Epoch 099 | train_loss=0.3063 acc=0.8617 | val_loss=1.4431 acc=0.6444 | time=24.0s\n",
            "Epoch 100 | train_loss=0.3119 acc=0.8606 | val_loss=1.4391 acc=0.6615 | time=23.9s\n",
            "Fold 1 best_train_loss=0.6708, best_train_acc=0.7002, best_val_loss=0.6871, best_val_acc=0.7422\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇████</td></tr><tr><td>train_accuracy</td><td>▁▁▂▂▄▄▅▅▅▆▆▆▇▇▇▇████████████████████████</td></tr><tr><td>train_loss</td><td>█████▆▅▄▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▄▆▆▇▇▇▆██▇▆▇▆▇█▅▆▇▆▆▇▆▆▆▆▆▆▆▆▆▇▆▆▆▆▆</td></tr><tr><td>validation_loss</td><td>▄▄▄▄▄▃▃▁▂▃▃▂▁▃▆▃▇▆▇▇▆▇▇▆▇▇▇▇████▇▇▇▇▇█▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.70019</td></tr><tr><td>best_train_loss</td><td>0.67081</td></tr><tr><td>best_val_accuracy</td><td>0.74224</td></tr><tr><td>best_val_loss</td><td>0.68714</td></tr><tr><td>epoch</td><td>100</td></tr><tr><td>train_accuracy</td><td>0.86058</td></tr><tr><td>train_loss</td><td>0.31192</td></tr><tr><td>validation_accuracy</td><td>0.66149</td></tr><tr><td>validation_loss</td><td>1.4391</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/vkgomcmz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/vkgomcmz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250503_230721-vkgomcmz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250503_234712-17himijp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/17himijp' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/17himijp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-6/runs/17himijp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1596 acc=0.3817 | val_loss=1.0851 acc=0.4255 | time=24.0s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-36e963ae7326>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-10-36e963ae7326>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             \u001b[0mtloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtcorrect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mttotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_shutdown\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m             \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1459\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tasks_outstanding\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1419\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m                 \u001b[0msuccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_get_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1421\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0msuccess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1249\u001b[0m         \u001b[0;31m#   (bool: whether successfully get data, any: data if successful else None)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1250\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1251\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data_queue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1252\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1253\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/queues.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_rlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0;31m# unserialize the data after having released the lock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_ForkingPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mqsize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/multiprocessing/reductions.py\u001b[0m in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    539\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrebuild_storage_fd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m     \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m         \u001b[0mstorage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage_from_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mdetach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0;34m'''Get the fd.  This should only be called once.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m             \u001b[0;32mwith\u001b[0m \u001b[0m_resource_sharer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_handle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/resource_sharer.py\u001b[0m in \u001b[0;36mget_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mident\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maddress\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauthkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/multiprocessing/process.py\u001b[0m in \u001b[0;36mauthkey\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'daemon'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdaemonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mauthkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_config\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'authkey'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Num_Filters = 120 -> 60\n",
        "#### N1=16 ->8, N2=32->16\n",
        "- Due to Overfitting\n",
        "- Decrease the N1 = 16 -> 8, N2 = 32 -> 16"
      ],
      "metadata": {
        "id": "tyb-j5zgk_Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 5e-4\n",
        "WEIGHT_DECAY = 5e-2\n",
        "NUM_FILTERS = 60\n",
        "NUM_BLOCKS = 2\n",
        "NUM_HEADS = 2\n",
        "NUM_SEGMENTS = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler parameters ─────────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA = 0.7\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        # wandb run 시작\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-7\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            }\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 / 옵티마이저 / 손실함수\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        base_lr = LR\n",
        "        base_wd = WEIGHT_DECAY\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=base_lr,\n",
        "            weight_decay=base_wd\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "            optimizer,\n",
        "            mode='min',\n",
        "            factor=FIXED_GAMMA,\n",
        "            patience=FIXED_STEP_SIZE,\n",
        "            min_lr=1e-6\n",
        "        )\n",
        "\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            # — 터미널 출력 —\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # — wandb 로깅 —\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "            }, step=epoch)\n",
        "\n",
        "            # Scheduler\n",
        "            scheduler.step(val_loss)\n",
        "\n",
        "            # Weight Decay Scheduling\n",
        "            current_lr = optimizer.param_groups[0]['lr']\n",
        "            new_wd = base_wd * (current_lr / LR)\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = new_wd\n",
        "\n",
        "            # best 갱신\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_val_acc    = val_acc\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "\n",
        "        # Fold 종료 시 summary 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        # cleanup\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 로깅 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    # 별도 W&B run 으로 평균 지표 기록\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-7\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        }\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "    # ─── 5-Fold CV Summary 터미널 출력 ────────────────────────────\n",
        "    print(\"\\n===== 5-Fold CV Summary =====\")\n",
        "    for i, res in enumerate(fold_results, 1):\n",
        "        print(\n",
        "            f\" Fold {i:2d}: \"\n",
        "            f\"train_loss = {res['train_loss']:.4f}, \"\n",
        "            f\"train_acc = {res['train_acc']:.4f}, \"\n",
        "            f\"val_loss = {res['val_loss']:.4f}, \"\n",
        "            f\"val_acc = {res['val_acc']:.4f}\"\n",
        "        )\n",
        "    print(\n",
        "        f\"\\n Average: \"\n",
        "        f\"train_loss = {avg['train_loss']:.4f}, \"\n",
        "        f\"train_acc = {avg['train_acc']:.4f}, \"\n",
        "        f\"val_loss = {avg['val_loss']:.4f}, \"\n",
        "        f\"val_acc = {avg['val_acc']:.4f}\"\n",
        "    )\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i94Tyvztk_p7",
        "outputId": "4fff2e64-a516-4fe8-c8c9-ca12f4433976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_004529-u5ubth4c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-7/runs/u5ubth4c' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-7' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-7/runs/u5ubth4c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-7/runs/u5ubth4c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1397 acc=0.3767 | val_loss=1.0947 acc=0.4006 | time=16.5s\n",
            "Epoch 002 | train_loss=1.1166 acc=0.3817 | val_loss=1.0917 acc=0.3587 | time=16.4s\n",
            "Epoch 003 | train_loss=1.1068 acc=0.3969 | val_loss=1.0911 acc=0.4006 | time=16.8s\n",
            "Epoch 004 | train_loss=1.0894 acc=0.4054 | val_loss=1.0915 acc=0.4006 | time=16.5s\n",
            "Epoch 005 | train_loss=1.0901 acc=0.4050 | val_loss=1.0934 acc=0.4006 | time=16.4s\n",
            "Epoch 006 | train_loss=1.0871 acc=0.4198 | val_loss=1.0925 acc=0.4006 | time=16.7s\n",
            "Epoch 007 | train_loss=1.0848 acc=0.4132 | val_loss=1.0887 acc=0.4006 | time=16.6s\n",
            "Epoch 008 | train_loss=1.0714 acc=0.4353 | val_loss=1.0761 acc=0.4006 | time=16.4s\n",
            "Epoch 009 | train_loss=1.0620 acc=0.4781 | val_loss=1.0360 acc=0.5854 | time=16.5s\n",
            "Epoch 010 | train_loss=1.0379 acc=0.5002 | val_loss=0.9679 acc=0.6149 | time=16.9s\n",
            "Epoch 011 | train_loss=0.9817 acc=0.5569 | val_loss=0.9548 acc=0.6444 | time=16.3s\n",
            "Epoch 012 | train_loss=0.9680 acc=0.5709 | val_loss=0.9775 acc=0.6413 | time=16.5s\n",
            "Epoch 013 | train_loss=0.9306 acc=0.5841 | val_loss=0.9104 acc=0.6460 | time=16.7s\n",
            "Epoch 014 | train_loss=0.9211 acc=0.5915 | val_loss=0.8508 acc=0.6770 | time=16.5s\n",
            "Epoch 015 | train_loss=0.8999 acc=0.6136 | val_loss=0.8621 acc=0.6615 | time=16.7s\n",
            "Epoch 016 | train_loss=0.8615 acc=0.6388 | val_loss=0.8827 acc=0.5870 | time=16.6s\n",
            "Epoch 017 | train_loss=0.8681 acc=0.6485 | val_loss=0.8629 acc=0.6801 | time=16.7s\n",
            "Epoch 018 | train_loss=0.8556 acc=0.6602 | val_loss=0.8171 acc=0.7081 | time=16.6s\n",
            "Epoch 019 | train_loss=0.8387 acc=0.6695 | val_loss=0.8440 acc=0.6863 | time=16.5s\n",
            "Epoch 020 | train_loss=0.8366 acc=0.6649 | val_loss=0.8451 acc=0.7003 | time=16.6s\n",
            "Epoch 021 | train_loss=0.8353 acc=0.6742 | val_loss=0.8883 acc=0.6661 | time=16.5s\n",
            "Epoch 022 | train_loss=0.8085 acc=0.6963 | val_loss=0.9062 acc=0.6506 | time=16.6s\n",
            "Epoch 023 | train_loss=0.8107 acc=0.6750 | val_loss=0.7953 acc=0.7050 | time=16.7s\n",
            "Epoch 024 | train_loss=0.8085 acc=0.6862 | val_loss=0.8285 acc=0.7205 | time=16.7s\n",
            "Epoch 025 | train_loss=0.7913 acc=0.6920 | val_loss=0.8526 acc=0.7019 | time=17.0s\n",
            "Epoch 026 | train_loss=0.7790 acc=0.6963 | val_loss=0.8520 acc=0.6972 | time=17.0s\n",
            "Epoch 027 | train_loss=0.7754 acc=0.7130 | val_loss=0.8291 acc=0.6739 | time=16.7s\n",
            "Epoch 028 | train_loss=0.7675 acc=0.7184 | val_loss=0.8474 acc=0.6661 | time=16.6s\n",
            "Epoch 029 | train_loss=0.7516 acc=0.7250 | val_loss=0.8359 acc=0.6724 | time=16.5s\n",
            "Epoch 030 | train_loss=0.7376 acc=0.7460 | val_loss=0.8686 acc=0.6925 | time=16.7s\n",
            "Epoch 031 | train_loss=0.7402 acc=0.7351 | val_loss=0.8911 acc=0.6879 | time=16.7s\n",
            "Epoch 032 | train_loss=0.7123 acc=0.7483 | val_loss=1.0164 acc=0.6460 | time=16.6s\n",
            "Epoch 033 | train_loss=0.7101 acc=0.7584 | val_loss=0.8820 acc=0.6817 | time=16.7s\n",
            "Epoch 034 | train_loss=0.7160 acc=0.7425 | val_loss=0.8874 acc=0.6646 | time=16.5s\n",
            "Epoch 035 | train_loss=0.6992 acc=0.7616 | val_loss=0.9606 acc=0.6413 | time=16.6s\n",
            "Epoch 036 | train_loss=0.6834 acc=0.7705 | val_loss=0.9175 acc=0.6615 | time=16.6s\n",
            "Epoch 037 | train_loss=0.6877 acc=0.7720 | val_loss=0.9698 acc=0.6615 | time=16.8s\n",
            "Epoch 038 | train_loss=0.6648 acc=0.7817 | val_loss=0.9105 acc=0.6739 | time=16.4s\n",
            "Epoch 039 | train_loss=0.6677 acc=0.7872 | val_loss=1.0265 acc=0.5916 | time=16.6s\n",
            "Epoch 040 | train_loss=0.6669 acc=0.7899 | val_loss=0.9583 acc=0.6491 | time=16.7s\n",
            "Epoch 041 | train_loss=0.6546 acc=0.7950 | val_loss=0.9883 acc=0.6615 | time=16.5s\n",
            "Epoch 042 | train_loss=0.6328 acc=0.7981 | val_loss=0.9955 acc=0.6615 | time=16.4s\n",
            "Epoch 043 | train_loss=0.6231 acc=0.8124 | val_loss=0.9898 acc=0.6584 | time=16.7s\n",
            "Epoch 044 | train_loss=0.6190 acc=0.8249 | val_loss=0.9669 acc=0.6848 | time=16.6s\n",
            "Epoch 045 | train_loss=0.6404 acc=0.7988 | val_loss=1.0778 acc=0.6211 | time=16.6s\n",
            "Epoch 046 | train_loss=0.6296 acc=0.7992 | val_loss=0.9814 acc=0.6801 | time=16.4s\n",
            "Epoch 047 | train_loss=0.6247 acc=0.8124 | val_loss=0.9963 acc=0.6677 | time=16.8s\n",
            "Epoch 048 | train_loss=0.6248 acc=0.8256 | val_loss=1.0319 acc=0.6630 | time=16.5s\n",
            "Epoch 049 | train_loss=0.6078 acc=0.8190 | val_loss=1.0339 acc=0.6568 | time=16.2s\n",
            "Epoch 050 | train_loss=0.6249 acc=0.8124 | val_loss=1.0247 acc=0.6553 | time=16.7s\n",
            "Epoch 051 | train_loss=0.6199 acc=0.8093 | val_loss=1.0142 acc=0.6739 | time=16.5s\n",
            "Epoch 052 | train_loss=0.6048 acc=0.8353 | val_loss=1.1009 acc=0.6227 | time=16.3s\n",
            "Epoch 053 | train_loss=0.6100 acc=0.8233 | val_loss=1.0673 acc=0.6506 | time=16.3s\n",
            "Epoch 054 | train_loss=0.6137 acc=0.8245 | val_loss=1.0731 acc=0.6398 | time=16.6s\n",
            "Epoch 055 | train_loss=0.6079 acc=0.8210 | val_loss=1.1018 acc=0.6335 | time=16.5s\n",
            "Epoch 056 | train_loss=0.5989 acc=0.8307 | val_loss=1.0569 acc=0.6506 | time=16.7s\n",
            "Epoch 057 | train_loss=0.5960 acc=0.8287 | val_loss=1.0930 acc=0.6351 | time=16.5s\n",
            "Epoch 058 | train_loss=0.6211 acc=0.8078 | val_loss=1.1281 acc=0.6165 | time=16.6s\n",
            "Epoch 059 | train_loss=0.5983 acc=0.8264 | val_loss=1.0718 acc=0.6429 | time=16.4s\n",
            "Epoch 060 | train_loss=0.6041 acc=0.8338 | val_loss=1.1056 acc=0.6196 | time=16.5s\n",
            "Epoch 061 | train_loss=0.5888 acc=0.8326 | val_loss=1.0777 acc=0.6491 | time=16.7s\n",
            "Epoch 062 | train_loss=0.6051 acc=0.8217 | val_loss=1.0466 acc=0.6615 | time=16.5s\n",
            "Epoch 063 | train_loss=0.6050 acc=0.8167 | val_loss=1.0806 acc=0.6460 | time=16.8s\n",
            "Epoch 064 | train_loss=0.5940 acc=0.8291 | val_loss=1.1020 acc=0.6304 | time=16.7s\n",
            "Epoch 065 | train_loss=0.5956 acc=0.8326 | val_loss=1.0856 acc=0.6460 | time=16.4s\n",
            "Epoch 066 | train_loss=0.5866 acc=0.8272 | val_loss=1.0668 acc=0.6413 | time=16.5s\n",
            "Epoch 067 | train_loss=0.6008 acc=0.8322 | val_loss=1.1335 acc=0.6289 | time=16.4s\n",
            "Epoch 068 | train_loss=0.5855 acc=0.8283 | val_loss=1.1118 acc=0.6335 | time=16.6s\n",
            "Epoch 069 | train_loss=0.5853 acc=0.8373 | val_loss=1.0962 acc=0.6429 | time=16.8s\n",
            "Epoch 070 | train_loss=0.5929 acc=0.8311 | val_loss=1.1080 acc=0.6258 | time=16.7s\n",
            "Epoch 071 | train_loss=0.5783 acc=0.8388 | val_loss=1.1365 acc=0.6196 | time=16.5s\n",
            "Epoch 072 | train_loss=0.5853 acc=0.8365 | val_loss=1.1269 acc=0.6258 | time=16.8s\n",
            "Epoch 073 | train_loss=0.5897 acc=0.8287 | val_loss=1.1289 acc=0.6180 | time=16.5s\n",
            "Epoch 074 | train_loss=0.5961 acc=0.8245 | val_loss=1.1413 acc=0.6180 | time=16.6s\n",
            "Epoch 075 | train_loss=0.5762 acc=0.8454 | val_loss=1.1153 acc=0.6304 | time=16.7s\n",
            "Epoch 076 | train_loss=0.5823 acc=0.8377 | val_loss=1.1129 acc=0.6320 | time=16.7s\n",
            "Epoch 077 | train_loss=0.5800 acc=0.8408 | val_loss=1.1392 acc=0.6242 | time=16.8s\n",
            "Epoch 078 | train_loss=0.5851 acc=0.8384 | val_loss=1.1067 acc=0.6366 | time=16.9s\n",
            "Epoch 079 | train_loss=0.5712 acc=0.8416 | val_loss=1.1273 acc=0.6351 | time=16.9s\n",
            "Epoch 080 | train_loss=0.5901 acc=0.8334 | val_loss=1.1698 acc=0.6196 | time=16.6s\n",
            "Epoch 081 | train_loss=0.5823 acc=0.8365 | val_loss=1.1404 acc=0.6289 | time=16.7s\n",
            "Epoch 082 | train_loss=0.5844 acc=0.8462 | val_loss=1.1578 acc=0.6071 | time=16.7s\n",
            "Epoch 083 | train_loss=0.5833 acc=0.8373 | val_loss=1.1043 acc=0.6413 | time=16.6s\n",
            "Epoch 084 | train_loss=0.5862 acc=0.8311 | val_loss=1.1258 acc=0.6351 | time=16.6s\n",
            "Epoch 085 | train_loss=0.5791 acc=0.8419 | val_loss=1.0923 acc=0.6444 | time=16.7s\n",
            "Epoch 086 | train_loss=0.5670 acc=0.8497 | val_loss=1.1027 acc=0.6382 | time=16.4s\n",
            "Epoch 087 | train_loss=0.5885 acc=0.8291 | val_loss=1.1868 acc=0.6056 | time=16.4s\n",
            "Epoch 088 | train_loss=0.5960 acc=0.8272 | val_loss=1.1657 acc=0.6165 | time=16.6s\n",
            "Epoch 089 | train_loss=0.5741 acc=0.8439 | val_loss=1.1436 acc=0.6227 | time=16.7s\n",
            "Epoch 090 | train_loss=0.5715 acc=0.8482 | val_loss=1.1334 acc=0.6335 | time=16.8s\n",
            "Epoch 091 | train_loss=0.5852 acc=0.8388 | val_loss=1.1277 acc=0.6304 | time=16.5s\n",
            "Epoch 092 | train_loss=0.5882 acc=0.8256 | val_loss=1.1309 acc=0.6304 | time=16.7s\n",
            "Epoch 093 | train_loss=0.5740 acc=0.8458 | val_loss=1.1339 acc=0.6258 | time=16.4s\n",
            "Epoch 094 | train_loss=0.5705 acc=0.8505 | val_loss=1.1199 acc=0.6398 | time=16.6s\n",
            "Epoch 095 | train_loss=0.5780 acc=0.8381 | val_loss=1.0978 acc=0.6413 | time=16.7s\n",
            "Epoch 096 | train_loss=0.5817 acc=0.8423 | val_loss=1.1121 acc=0.6320 | time=16.6s\n",
            "Epoch 097 | train_loss=0.5760 acc=0.8423 | val_loss=1.0895 acc=0.6475 | time=16.5s\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-3f3112c3a7bb>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m     \u001b[0mtrain_and_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-8-3f3112c3a7bb>\u001b[0m in \u001b[0;36mtrain_and_evaluate\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m                 \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m                 \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m                 \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Block = 2, Head = 2\n",
        "- Early Stopping\n",
        "- OneCycleLR\n",
        "- Prevent overfitting: 5e-2 -> 1e-1\n"
      ],
      "metadata": {
        "id": "rOLx44Se3nBG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from wandb import Settings\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 1e-1\n",
        "NUM_FILTERS = 60\n",
        "NUM_BLOCKS = 2\n",
        "NUM_HEADS = 2\n",
        "NUM_SEGMENTS = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "PATIENCE = 10                   # ← MOD: EarlyStopping patience\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-12\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS\n",
        "            },\n",
        "            settings=Settings(init_timeout=120)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 생성\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # ← MOD: base_lr/base_wd 저장\n",
        "        base_lr = LR\n",
        "        base_wd = WEIGHT_DECAY\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=base_lr,\n",
        "            weight_decay=base_wd\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        total_steps = MAX_EPOCHS * len(train_loader)\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=base_lr,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=0.2,\n",
        "            anneal_strategy='cos',\n",
        "            div_factor=10,\n",
        "            final_div_factor=100\n",
        "        )\n",
        "\n",
        "        # ← MOD: EarlyStopping 변수 초기화\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "        no_improve = 0\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # ← MOD: OneCycleLR 배치 단위 step\n",
        "                scheduler.step()\n",
        "\n",
        "                # ← MOD: lr 변화에 맞춰 weight_decay 조정\n",
        "                cur_lr = optimizer.param_groups[0]['lr']\n",
        "                new_wd = base_wd * (cur_lr / base_lr)\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['weight_decay'] = new_wd\n",
        "\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "                \"lr\":                  optimizer.param_groups[0]['lr']\n",
        "            }, step=epoch)\n",
        "\n",
        "            # ← MOD: EarlyStopping 로직\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "                best_val_acc    = val_acc\n",
        "                no_improve      = 0\n",
        "            else:\n",
        "                no_improve += 1\n",
        "                if no_improve >= PATIENCE:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "        # Fold 결과 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 기록 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-12\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        },\n",
        "        settings=Settings(init_timeout=120)\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b3Zb0MYF0r69",
        "outputId": "6fb0d1fd-22e9-4a00-f077-c67b570f6ff0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_015120-idnu0s8p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/idnu0s8p' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/idnu0s8p' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/idnu0s8p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1429 acc=0.3817 | val_loss=1.0908 acc=0.4006 | time=16.5s\n",
            "Epoch 002 | train_loss=1.1409 acc=0.3635 | val_loss=1.0923 acc=0.4006 | time=16.5s\n",
            "Epoch 003 | train_loss=1.1410 acc=0.3631 | val_loss=1.0905 acc=0.4006 | time=16.5s\n",
            "Epoch 004 | train_loss=1.1272 acc=0.3650 | val_loss=1.0909 acc=0.4006 | time=16.7s\n",
            "Epoch 005 | train_loss=1.1080 acc=0.3988 | val_loss=1.0937 acc=0.4006 | time=16.5s\n",
            "Epoch 006 | train_loss=1.1047 acc=0.3926 | val_loss=1.0899 acc=0.4006 | time=16.4s\n",
            "Epoch 007 | train_loss=1.0957 acc=0.3969 | val_loss=1.0729 acc=0.4488 | time=16.5s\n",
            "Epoch 008 | train_loss=1.0476 acc=0.4885 | val_loss=0.9947 acc=0.5947 | time=16.5s\n",
            "Epoch 009 | train_loss=1.0207 acc=0.5293 | val_loss=0.9561 acc=0.6366 | time=16.2s\n",
            "Epoch 010 | train_loss=0.9783 acc=0.5798 | val_loss=0.9371 acc=0.6102 | time=16.4s\n",
            "Epoch 011 | train_loss=0.9537 acc=0.6008 | val_loss=0.8983 acc=0.6460 | time=16.5s\n",
            "Epoch 012 | train_loss=0.9410 acc=0.5872 | val_loss=0.9097 acc=0.6149 | time=16.7s\n",
            "Epoch 013 | train_loss=0.9257 acc=0.6082 | val_loss=0.8817 acc=0.6475 | time=16.6s\n",
            "Epoch 014 | train_loss=0.9121 acc=0.6058 | val_loss=0.8713 acc=0.6351 | time=16.6s\n",
            "Epoch 015 | train_loss=0.8756 acc=0.6229 | val_loss=0.8510 acc=0.6506 | time=16.7s\n",
            "Epoch 016 | train_loss=0.9072 acc=0.6148 | val_loss=0.9581 acc=0.5885 | time=16.3s\n",
            "Epoch 017 | train_loss=0.8894 acc=0.6214 | val_loss=0.8541 acc=0.6382 | time=16.5s\n",
            "Epoch 018 | train_loss=0.8755 acc=0.6291 | val_loss=0.8393 acc=0.7019 | time=16.6s\n",
            "Epoch 019 | train_loss=0.8428 acc=0.6454 | val_loss=0.8550 acc=0.6599 | time=16.6s\n",
            "Epoch 020 | train_loss=0.8457 acc=0.6594 | val_loss=0.8677 acc=0.6568 | time=16.5s\n",
            "Epoch 021 | train_loss=0.8258 acc=0.6703 | val_loss=0.8431 acc=0.6770 | time=16.6s\n",
            "Epoch 022 | train_loss=0.8234 acc=0.6738 | val_loss=0.8235 acc=0.6848 | time=16.6s\n",
            "Epoch 023 | train_loss=0.8061 acc=0.6823 | val_loss=0.8651 acc=0.6755 | time=16.5s\n",
            "Epoch 024 | train_loss=0.8023 acc=0.6909 | val_loss=1.0174 acc=0.6025 | time=16.3s\n",
            "Epoch 025 | train_loss=0.7941 acc=0.6936 | val_loss=1.0153 acc=0.5652 | time=16.7s\n",
            "Epoch 026 | train_loss=0.7833 acc=0.7010 | val_loss=0.7880 acc=0.7158 | time=16.3s\n",
            "Epoch 027 | train_loss=0.7719 acc=0.7091 | val_loss=0.8249 acc=0.6739 | time=16.5s\n",
            "Epoch 028 | train_loss=0.7505 acc=0.7188 | val_loss=0.8437 acc=0.6848 | time=16.6s\n",
            "Epoch 029 | train_loss=0.7491 acc=0.7184 | val_loss=0.8437 acc=0.6786 | time=16.6s\n",
            "Epoch 030 | train_loss=0.7405 acc=0.7258 | val_loss=0.8441 acc=0.6661 | time=16.5s\n",
            "Epoch 031 | train_loss=0.7422 acc=0.7367 | val_loss=0.8939 acc=0.6817 | time=16.6s\n",
            "Epoch 032 | train_loss=0.7255 acc=0.7417 | val_loss=0.8595 acc=0.6708 | time=16.7s\n",
            "Epoch 033 | train_loss=0.7303 acc=0.7414 | val_loss=0.9606 acc=0.6599 | time=16.3s\n",
            "Epoch 034 | train_loss=0.7129 acc=0.7542 | val_loss=0.9846 acc=0.5714 | time=16.4s\n",
            "Epoch 035 | train_loss=0.7017 acc=0.7577 | val_loss=0.8830 acc=0.6646 | time=16.4s\n",
            "Epoch 036 | train_loss=0.6988 acc=0.7701 | val_loss=0.9251 acc=0.6491 | time=16.6s\n",
            "Early stopping at epoch 36\n",
            "Fold 1 best_train_loss=0.7833, best_train_acc=0.7010, best_val_loss=0.7880, best_val_acc=0.7158\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▂▂▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>████▇▇▇▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▂▅▆▆▆▆▆▆▇▅▆█▇▇▇▇▇▅▅█▇▇▇▇▇▇▇▅▇▇</td></tr><tr><td>validation_loss</td><td>███████▆▅▄▄▄▃▃▂▅▃▂▃▃▂▂▃▆▆▁▂▂▂▂▃▃▅▆▃▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.70097</td></tr><tr><td>best_train_loss</td><td>0.78334</td></tr><tr><td>best_val_accuracy</td><td>0.71584</td></tr><tr><td>best_val_loss</td><td>0.78803</td></tr><tr><td>epoch</td><td>36</td></tr><tr><td>lr</td><td>0.0009</td></tr><tr><td>train_accuracy</td><td>0.7701</td></tr><tr><td>train_loss</td><td>0.69876</td></tr><tr><td>validation_accuracy</td><td>0.64907</td></tr><tr><td>validation_loss</td><td>0.92513</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/idnu0s8p' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/idnu0s8p</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_015120-idnu0s8p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (8.5s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_020244-42rktiju</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/42rktiju' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/42rktiju' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/42rktiju</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1798 acc=0.3841 | val_loss=1.0839 acc=0.4255 | time=16.4s\n",
            "Epoch 002 | train_loss=1.1702 acc=0.3709 | val_loss=1.0856 acc=0.4255 | time=16.5s\n",
            "Epoch 003 | train_loss=1.1351 acc=0.3829 | val_loss=1.0856 acc=0.4255 | time=16.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: 500 encountered ({\"error\":\"context deadline exceeded\"}), retrying request\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 500 encountered ({\"error\":\"context deadline exceeded\"}), retrying request\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 004 | train_loss=1.1425 acc=0.3596 | val_loss=1.0885 acc=0.4255 | time=16.7s\n",
            "Epoch 005 | train_loss=1.1245 acc=0.3794 | val_loss=1.0833 acc=0.4255 | time=16.6s\n",
            "Epoch 006 | train_loss=1.1170 acc=0.3837 | val_loss=1.0816 acc=0.4255 | time=16.4s\n",
            "Epoch 007 | train_loss=1.1127 acc=0.3841 | val_loss=1.0812 acc=0.4255 | time=16.7s\n",
            "Epoch 008 | train_loss=1.0991 acc=0.3907 | val_loss=1.0837 acc=0.4255 | time=16.6s\n",
            "Epoch 009 | train_loss=1.0931 acc=0.4085 | val_loss=1.0789 acc=0.4255 | time=16.4s\n",
            "Epoch 010 | train_loss=1.0941 acc=0.4058 | val_loss=1.0746 acc=0.4255 | time=16.6s\n",
            "Epoch 011 | train_loss=1.0832 acc=0.4179 | val_loss=1.0619 acc=0.4845 | time=16.6s\n",
            "Epoch 012 | train_loss=1.0546 acc=0.4854 | val_loss=0.9922 acc=0.5947 | time=16.4s\n",
            "Epoch 013 | train_loss=1.0125 acc=0.5383 | val_loss=0.9416 acc=0.6506 | time=16.3s\n",
            "Epoch 014 | train_loss=0.9695 acc=0.5849 | val_loss=0.8911 acc=0.6661 | time=16.6s\n",
            "Epoch 015 | train_loss=0.9534 acc=0.5849 | val_loss=0.9271 acc=0.6366 | time=16.5s\n",
            "Epoch 016 | train_loss=0.9412 acc=0.5938 | val_loss=0.9384 acc=0.6615 | time=16.5s\n",
            "Epoch 017 | train_loss=0.9256 acc=0.5938 | val_loss=0.8795 acc=0.6661 | time=16.5s\n",
            "Epoch 018 | train_loss=0.9006 acc=0.6136 | val_loss=0.8444 acc=0.6910 | time=16.9s\n",
            "Epoch 019 | train_loss=0.9039 acc=0.6124 | val_loss=0.8295 acc=0.6599 | time=16.5s\n",
            "Epoch 020 | train_loss=0.8872 acc=0.6105 | val_loss=0.9344 acc=0.5807 | time=16.6s\n",
            "Epoch 021 | train_loss=0.8636 acc=0.6318 | val_loss=0.8865 acc=0.6708 | time=16.5s\n",
            "Epoch 022 | train_loss=0.8630 acc=0.6400 | val_loss=0.8415 acc=0.6941 | time=16.4s\n",
            "Epoch 023 | train_loss=0.8469 acc=0.6419 | val_loss=0.8614 acc=0.6630 | time=16.6s\n",
            "Epoch 024 | train_loss=0.8360 acc=0.6633 | val_loss=0.8011 acc=0.7065 | time=16.4s\n",
            "Epoch 025 | train_loss=0.8348 acc=0.6602 | val_loss=0.9442 acc=0.5947 | time=16.6s\n",
            "Epoch 026 | train_loss=0.8362 acc=0.6567 | val_loss=0.7997 acc=0.7081 | time=16.6s\n",
            "Epoch 027 | train_loss=0.8228 acc=0.6722 | val_loss=0.8728 acc=0.6739 | time=16.8s\n",
            "Epoch 028 | train_loss=0.8040 acc=0.6827 | val_loss=0.8021 acc=0.7127 | time=16.7s\n",
            "Epoch 029 | train_loss=0.8129 acc=0.6893 | val_loss=0.8379 acc=0.6848 | time=16.4s\n",
            "Epoch 030 | train_loss=0.7922 acc=0.6951 | val_loss=0.9243 acc=0.6258 | time=16.4s\n",
            "Epoch 031 | train_loss=0.7897 acc=0.6959 | val_loss=0.8967 acc=0.6429 | time=16.4s\n",
            "Epoch 032 | train_loss=0.7768 acc=0.7064 | val_loss=0.8000 acc=0.7096 | time=16.8s\n",
            "Epoch 033 | train_loss=0.7558 acc=0.7212 | val_loss=0.8274 acc=0.7220 | time=16.5s\n",
            "Epoch 034 | train_loss=0.7733 acc=0.7080 | val_loss=0.8811 acc=0.6180 | time=16.5s\n",
            "Epoch 035 | train_loss=0.7618 acc=0.7165 | val_loss=0.8496 acc=0.6848 | time=16.6s\n",
            "Epoch 036 | train_loss=0.7400 acc=0.7379 | val_loss=0.8091 acc=0.7345 | time=16.5s\n",
            "Early stopping at epoch 36\n",
            "Fold 2 best_train_loss=0.8362, best_train_acc=0.6567, best_val_loss=0.7997, best_val_acc=0.7081\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▂▂▂▃▄▅▅▅▅▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇█▇██</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▇▇▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▁▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▂▅▆▆▆▆▆▇▆▅▇▇▆▇▅▇▇█▇▆▆▇█▅▇█</td></tr><tr><td>validation_loss</td><td>██████████▇▆▄▃▄▄▃▂▂▄▃▂▂▁▅▁▃▁▂▄▃▁▂▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.6567</td></tr><tr><td>best_train_loss</td><td>0.83622</td></tr><tr><td>best_val_accuracy</td><td>0.70807</td></tr><tr><td>best_val_loss</td><td>0.79969</td></tr><tr><td>epoch</td><td>36</td></tr><tr><td>lr</td><td>0.0009</td></tr><tr><td>train_accuracy</td><td>0.73786</td></tr><tr><td>train_loss</td><td>0.73996</td></tr><tr><td>validation_accuracy</td><td>0.73447</td></tr><tr><td>validation_loss</td><td>0.80906</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/42rktiju' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/42rktiju</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_020244-42rktiju/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 3 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_021251-fjwxfdhl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/fjwxfdhl' target=\"_blank\">fold_3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/fjwxfdhl' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/fjwxfdhl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1651 acc=0.3511 | val_loss=1.0836 acc=0.4410 | time=16.5s\n",
            "Epoch 002 | train_loss=1.1422 acc=0.3654 | val_loss=1.0819 acc=0.4410 | time=16.4s\n",
            "Epoch 003 | train_loss=1.1404 acc=0.3596 | val_loss=1.0818 acc=0.4410 | time=16.4s\n",
            "Epoch 004 | train_loss=1.1220 acc=0.3833 | val_loss=1.0823 acc=0.4410 | time=16.3s\n",
            "Epoch 005 | train_loss=1.1190 acc=0.3724 | val_loss=1.0826 acc=0.4410 | time=16.5s\n",
            "Epoch 006 | train_loss=1.1043 acc=0.4101 | val_loss=1.0820 acc=0.4410 | time=16.5s\n",
            "Epoch 007 | train_loss=1.1056 acc=0.3957 | val_loss=1.0793 acc=0.4472 | time=16.6s\n",
            "Epoch 008 | train_loss=1.0922 acc=0.3988 | val_loss=1.0585 acc=0.5140 | time=16.3s\n",
            "Epoch 009 | train_loss=1.0558 acc=0.4676 | val_loss=1.0001 acc=0.6180 | time=16.8s\n",
            "Epoch 010 | train_loss=1.0307 acc=0.5336 | val_loss=0.9371 acc=0.6382 | time=17.4s\n",
            "Epoch 011 | train_loss=0.9688 acc=0.5751 | val_loss=0.9394 acc=0.6242 | time=17.0s\n",
            "Epoch 012 | train_loss=0.9511 acc=0.5946 | val_loss=0.9211 acc=0.6475 | time=16.8s\n",
            "Epoch 013 | train_loss=0.9282 acc=0.5992 | val_loss=0.9876 acc=0.5575 | time=16.5s\n",
            "Epoch 014 | train_loss=0.9138 acc=0.6105 | val_loss=0.9047 acc=0.6599 | time=16.6s\n",
            "Epoch 015 | train_loss=0.9057 acc=0.6221 | val_loss=0.8659 acc=0.7034 | time=16.5s\n",
            "Epoch 016 | train_loss=0.8781 acc=0.6350 | val_loss=0.9120 acc=0.6786 | time=16.7s\n",
            "Epoch 017 | train_loss=0.8677 acc=0.6493 | val_loss=0.8710 acc=0.6957 | time=16.6s\n",
            "Epoch 018 | train_loss=0.8573 acc=0.6586 | val_loss=0.8570 acc=0.6879 | time=16.7s\n",
            "Epoch 019 | train_loss=0.8575 acc=0.6470 | val_loss=0.8294 acc=0.6894 | time=16.4s\n",
            "Epoch 020 | train_loss=0.8392 acc=0.6610 | val_loss=0.8588 acc=0.7205 | time=16.6s\n",
            "Epoch 021 | train_loss=0.8434 acc=0.6633 | val_loss=0.8130 acc=0.7096 | time=16.9s\n",
            "Epoch 022 | train_loss=0.8284 acc=0.6769 | val_loss=0.8970 acc=0.6429 | time=17.1s\n",
            "Epoch 023 | train_loss=0.8279 acc=0.6796 | val_loss=0.8646 acc=0.7174 | time=16.7s\n",
            "Epoch 024 | train_loss=0.8121 acc=0.6866 | val_loss=0.9059 acc=0.6180 | time=16.4s\n",
            "Epoch 025 | train_loss=0.7898 acc=0.7037 | val_loss=0.8658 acc=0.6848 | time=16.5s\n",
            "Epoch 026 | train_loss=0.7832 acc=0.7076 | val_loss=0.8203 acc=0.7127 | time=16.5s\n",
            "Epoch 027 | train_loss=0.7897 acc=0.6963 | val_loss=0.9190 acc=0.5994 | time=16.4s\n",
            "Epoch 028 | train_loss=0.7650 acc=0.7052 | val_loss=0.7738 acc=0.7220 | time=16.5s\n",
            "Epoch 029 | train_loss=0.7567 acc=0.7169 | val_loss=0.9247 acc=0.5528 | time=16.6s\n",
            "Epoch 030 | train_loss=0.7424 acc=0.7348 | val_loss=0.8678 acc=0.7003 | time=16.6s\n",
            "Epoch 031 | train_loss=0.7511 acc=0.7258 | val_loss=0.8028 acc=0.7283 | time=16.3s\n",
            "Epoch 032 | train_loss=0.7118 acc=0.7588 | val_loss=0.9128 acc=0.6522 | time=16.3s\n",
            "Epoch 033 | train_loss=0.7299 acc=0.7367 | val_loss=0.8936 acc=0.6755 | time=16.8s\n",
            "Epoch 034 | train_loss=0.7253 acc=0.7445 | val_loss=0.8123 acc=0.6848 | time=16.6s\n",
            "Epoch 035 | train_loss=0.7054 acc=0.7553 | val_loss=0.8690 acc=0.6708 | time=16.4s\n",
            "Epoch 036 | train_loss=0.7006 acc=0.7639 | val_loss=0.9556 acc=0.6460 | time=16.5s\n",
            "Epoch 037 | train_loss=0.6992 acc=0.7658 | val_loss=0.8871 acc=0.6444 | time=16.7s\n",
            "Epoch 038 | train_loss=0.7026 acc=0.7643 | val_loss=0.8970 acc=0.6568 | time=16.6s\n",
            "Early stopping at epoch 38\n",
            "Fold 3 best_train_loss=0.7650, best_train_acc=0.7052, best_val_loss=0.7738, best_val_acc=0.7220\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▁▂▂▂▃▄▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▃▅▆▅▆▄▆▇▇▇▇▇██▆█▅▇█▅█▄▇█▆▇▇▇▆▆▆</td></tr><tr><td>validation_loss</td><td>███████▇▆▅▅▄▆▄▃▄▃▃▂▃▂▄▃▄▃▂▄▁▄▃▂▄▄▂▃▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.70524</td></tr><tr><td>best_train_loss</td><td>0.76497</td></tr><tr><td>best_val_accuracy</td><td>0.72205</td></tr><tr><td>best_val_loss</td><td>0.77376</td></tr><tr><td>epoch</td><td>38</td></tr><tr><td>lr</td><td>0.00088</td></tr><tr><td>train_accuracy</td><td>0.76427</td></tr><tr><td>train_loss</td><td>0.70263</td></tr><tr><td>validation_accuracy</td><td>0.65683</td></tr><tr><td>validation_loss</td><td>0.89702</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/fjwxfdhl' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/fjwxfdhl</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_021251-fjwxfdhl/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 4 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_022324-x0v07jal</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/x0v07jal' target=\"_blank\">fold_4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/x0v07jal' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/x0v07jal</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1785 acc=0.3647 | val_loss=1.0804 acc=0.4534 | time=16.6s\n",
            "Epoch 002 | train_loss=1.1375 acc=0.3814 | val_loss=1.0838 acc=0.4534 | time=16.5s\n",
            "Epoch 003 | train_loss=1.1326 acc=0.3860 | val_loss=1.0833 acc=0.4534 | time=16.6s\n",
            "Epoch 004 | train_loss=1.1189 acc=0.3926 | val_loss=1.0808 acc=0.4534 | time=16.5s\n",
            "Epoch 005 | train_loss=1.1223 acc=0.3790 | val_loss=1.0792 acc=0.4534 | time=16.5s\n",
            "Epoch 006 | train_loss=1.1168 acc=0.3825 | val_loss=1.0799 acc=0.4534 | time=16.7s\n",
            "Epoch 007 | train_loss=1.1067 acc=0.3899 | val_loss=1.0800 acc=0.4534 | time=16.5s\n",
            "Epoch 008 | train_loss=1.0985 acc=0.4043 | val_loss=1.0756 acc=0.4565 | time=16.5s\n",
            "Epoch 009 | train_loss=1.0846 acc=0.4202 | val_loss=1.0526 acc=0.5109 | time=16.9s\n",
            "Epoch 010 | train_loss=1.0485 acc=0.4792 | val_loss=0.9925 acc=0.6320 | time=16.6s\n",
            "Epoch 011 | train_loss=1.0024 acc=0.5480 | val_loss=0.9689 acc=0.5854 | time=16.2s\n",
            "Epoch 012 | train_loss=0.9686 acc=0.5763 | val_loss=0.9151 acc=0.6522 | time=16.3s\n",
            "Epoch 013 | train_loss=0.9614 acc=0.5841 | val_loss=0.8946 acc=0.6537 | time=16.6s\n",
            "Epoch 014 | train_loss=0.9403 acc=0.5860 | val_loss=0.8871 acc=0.6646 | time=16.5s\n",
            "Epoch 015 | train_loss=0.9207 acc=0.6074 | val_loss=0.8761 acc=0.6677 | time=16.5s\n",
            "Epoch 016 | train_loss=0.9049 acc=0.6109 | val_loss=0.8750 acc=0.6708 | time=16.7s\n",
            "Epoch 017 | train_loss=0.8848 acc=0.6159 | val_loss=0.8272 acc=0.6925 | time=16.6s\n",
            "Epoch 018 | train_loss=0.8739 acc=0.6322 | val_loss=0.8989 acc=0.6584 | time=16.5s\n",
            "Epoch 019 | train_loss=0.8849 acc=0.6217 | val_loss=0.8228 acc=0.6894 | time=16.3s\n",
            "Epoch 020 | train_loss=0.8483 acc=0.6447 | val_loss=0.8024 acc=0.6941 | time=16.5s\n",
            "Epoch 021 | train_loss=0.8543 acc=0.6497 | val_loss=0.9384 acc=0.6009 | time=16.6s\n",
            "Epoch 022 | train_loss=0.8532 acc=0.6482 | val_loss=0.8402 acc=0.7019 | time=16.4s\n",
            "Epoch 023 | train_loss=0.8299 acc=0.6575 | val_loss=0.8501 acc=0.6661 | time=16.7s\n",
            "Epoch 024 | train_loss=0.8161 acc=0.6850 | val_loss=0.7682 acc=0.7329 | time=16.6s\n",
            "Epoch 025 | train_loss=0.8197 acc=0.6625 | val_loss=0.8092 acc=0.6957 | time=16.6s\n",
            "Epoch 026 | train_loss=0.8157 acc=0.6641 | val_loss=0.7491 acc=0.7298 | time=16.5s\n",
            "Epoch 027 | train_loss=0.8116 acc=0.6827 | val_loss=0.8518 acc=0.6894 | time=16.7s\n",
            "Epoch 028 | train_loss=0.7951 acc=0.6854 | val_loss=0.7703 acc=0.7205 | time=16.7s\n",
            "Epoch 029 | train_loss=0.8042 acc=0.6827 | val_loss=0.7589 acc=0.7329 | time=16.4s\n",
            "Epoch 030 | train_loss=0.7825 acc=0.6878 | val_loss=0.8662 acc=0.6460 | time=16.6s\n",
            "Epoch 031 | train_loss=0.7742 acc=0.7037 | val_loss=0.7471 acc=0.7500 | time=16.7s\n",
            "Epoch 032 | train_loss=0.7602 acc=0.7134 | val_loss=0.7760 acc=0.7283 | time=16.6s\n",
            "Epoch 033 | train_loss=0.7607 acc=0.7153 | val_loss=0.8386 acc=0.6941 | time=16.5s\n",
            "Epoch 034 | train_loss=0.7296 acc=0.7429 | val_loss=0.8028 acc=0.7065 | time=16.6s\n",
            "Epoch 035 | train_loss=0.7340 acc=0.7188 | val_loss=0.8109 acc=0.6817 | time=16.5s\n",
            "Epoch 036 | train_loss=0.7335 acc=0.7344 | val_loss=0.8255 acc=0.7345 | time=16.6s\n",
            "Epoch 037 | train_loss=0.7083 acc=0.7464 | val_loss=0.8725 acc=0.7081 | time=16.5s\n",
            "Epoch 038 | train_loss=0.7121 acc=0.7406 | val_loss=0.8950 acc=0.6134 | time=16.6s\n",
            "Epoch 039 | train_loss=0.7156 acc=0.7600 | val_loss=0.8085 acc=0.7019 | time=16.5s\n",
            "Epoch 040 | train_loss=0.6828 acc=0.7748 | val_loss=0.8863 acc=0.6848 | time=16.6s\n",
            "Epoch 041 | train_loss=0.6918 acc=0.7748 | val_loss=0.8552 acc=0.7112 | time=16.7s\n",
            "Early stopping at epoch 41\n",
            "Fold 4 best_train_loss=0.7742, best_train_acc=0.7037, best_val_loss=0.7471, best_val_acc=0.7500\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▂▃▄▅▅▅▅▅▅▆▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇██</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▂▅▄▆▆▆▆▆▇▆▇▇▄▇▆█▇█▇▇█▆█▇▇▇▆█▇▅▇▇</td></tr><tr><td>validation_loss</td><td>████████▇▆▆▄▄▄▄▄▃▄▃▂▅▃▃▁▂▁▃▁▁▃▁▂▃▂▂▃▄▄▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.70369</td></tr><tr><td>best_train_loss</td><td>0.77415</td></tr><tr><td>best_val_accuracy</td><td>0.75</td></tr><tr><td>best_val_loss</td><td>0.74715</td></tr><tr><td>epoch</td><td>41</td></tr><tr><td>lr</td><td>0.00084</td></tr><tr><td>train_accuracy</td><td>0.77476</td></tr><tr><td>train_loss</td><td>0.69183</td></tr><tr><td>validation_accuracy</td><td>0.71118</td></tr><tr><td>validation_loss</td><td>0.85523</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/x0v07jal' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/x0v07jal</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_022324-x0v07jal/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 5 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (2.0s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_023518-s07x6rsl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/s07x6rsl' target=\"_blank\">fold_5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/s07x6rsl' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/s07x6rsl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.2039 acc=0.3571 | val_loss=1.1145 acc=0.3359 | time=16.8s\n",
            "Epoch 002 | train_loss=1.1825 acc=0.3641 | val_loss=1.1155 acc=0.3359 | time=16.5s\n",
            "Epoch 003 | train_loss=1.1675 acc=0.3769 | val_loss=1.1068 acc=0.3359 | time=17.1s\n",
            "Epoch 004 | train_loss=1.1506 acc=0.3676 | val_loss=1.0913 acc=0.4355 | time=16.6s\n",
            "Epoch 005 | train_loss=1.1326 acc=0.3824 | val_loss=1.0854 acc=0.4355 | time=16.7s\n",
            "Epoch 006 | train_loss=1.1221 acc=0.3750 | val_loss=1.0863 acc=0.4355 | time=16.7s\n",
            "Epoch 007 | train_loss=1.1107 acc=0.3859 | val_loss=1.0859 acc=0.4635 | time=16.5s\n",
            "Epoch 008 | train_loss=1.0883 acc=0.4173 | val_loss=1.0353 acc=0.5739 | time=16.5s\n",
            "Epoch 009 | train_loss=1.0361 acc=0.4973 | val_loss=0.9642 acc=0.6112 | time=16.5s\n",
            "Epoch 010 | train_loss=1.0040 acc=0.5454 | val_loss=0.9724 acc=0.5910 | time=16.7s\n",
            "Epoch 011 | train_loss=0.9771 acc=0.5675 | val_loss=0.9276 acc=0.6423 | time=16.5s\n",
            "Epoch 012 | train_loss=0.9606 acc=0.5776 | val_loss=0.9373 acc=0.6065 | time=16.3s\n",
            "Epoch 013 | train_loss=0.9427 acc=0.5885 | val_loss=0.8995 acc=0.6579 | time=16.4s\n",
            "Epoch 014 | train_loss=0.9144 acc=0.6141 | val_loss=0.9318 acc=0.5988 | time=16.6s\n",
            "Epoch 015 | train_loss=0.9219 acc=0.6126 | val_loss=1.0281 acc=0.5194 | time=16.4s\n",
            "Epoch 016 | train_loss=0.8965 acc=0.6219 | val_loss=0.8587 acc=0.6765 | time=16.7s\n",
            "Epoch 017 | train_loss=0.8900 acc=0.6211 | val_loss=0.8622 acc=0.7030 | time=16.8s\n",
            "Epoch 018 | train_loss=0.8798 acc=0.6343 | val_loss=0.8621 acc=0.6765 | time=16.5s\n",
            "Epoch 019 | train_loss=0.8654 acc=0.6440 | val_loss=0.8124 acc=0.6905 | time=16.3s\n",
            "Epoch 020 | train_loss=0.8554 acc=0.6452 | val_loss=0.8781 acc=0.6314 | time=16.4s\n",
            "Epoch 021 | train_loss=0.8292 acc=0.6658 | val_loss=0.8152 acc=0.6921 | time=16.8s\n",
            "Epoch 022 | train_loss=0.8200 acc=0.6720 | val_loss=0.8525 acc=0.6579 | time=16.6s\n",
            "Epoch 023 | train_loss=0.8102 acc=0.6712 | val_loss=0.8302 acc=0.6905 | time=16.3s\n",
            "Epoch 024 | train_loss=0.8197 acc=0.6778 | val_loss=0.8049 acc=0.7309 | time=16.8s\n",
            "Epoch 025 | train_loss=0.8026 acc=0.6766 | val_loss=0.8144 acc=0.7201 | time=16.6s\n",
            "Epoch 026 | train_loss=0.7980 acc=0.6786 | val_loss=0.9427 acc=0.5863 | time=16.4s\n",
            "Epoch 027 | train_loss=0.7791 acc=0.7003 | val_loss=0.8361 acc=0.6625 | time=16.6s\n",
            "Epoch 028 | train_loss=0.7918 acc=0.6953 | val_loss=0.8238 acc=0.6843 | time=16.7s\n",
            "Epoch 029 | train_loss=0.7728 acc=0.7127 | val_loss=0.8170 acc=0.6874 | time=16.3s\n",
            "Epoch 030 | train_loss=0.7595 acc=0.7135 | val_loss=1.1511 acc=0.4448 | time=16.6s\n",
            "Epoch 031 | train_loss=0.7679 acc=0.6960 | val_loss=0.8335 acc=0.7030 | time=16.6s\n",
            "Epoch 032 | train_loss=0.7347 acc=0.7352 | val_loss=0.8805 acc=0.6719 | time=16.3s\n",
            "Epoch 033 | train_loss=0.7445 acc=0.7341 | val_loss=0.8708 acc=0.6579 | time=16.3s\n",
            "Epoch 034 | train_loss=0.7350 acc=0.7372 | val_loss=0.8522 acc=0.6734 | time=16.5s\n",
            "Early stopping at epoch 34\n",
            "Fold 5 best_train_loss=0.8197, best_train_acc=0.6778, best_val_loss=0.8049, best_val_acc=0.7309\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▂▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██▇███</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▆▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▃▃▃▃▅▆▆▆▆▇▆▄▇█▇▇▆▇▇▇██▅▇▇▇▃█▇▇▇</td></tr><tr><td>validation_loss</td><td>▇▇▇▇▇▇▇▆▄▄▃▄▃▄▆▂▂▂▁▂▁▂▂▁▁▄▂▁▁█▂▃▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.6778</td></tr><tr><td>best_train_loss</td><td>0.81969</td></tr><tr><td>best_val_accuracy</td><td>0.73095</td></tr><tr><td>best_val_loss</td><td>0.80494</td></tr><tr><td>epoch</td><td>34</td></tr><tr><td>lr</td><td>0.00093</td></tr><tr><td>train_accuracy</td><td>0.73719</td></tr><tr><td>train_loss</td><td>0.73504</td></tr><tr><td>validation_accuracy</td><td>0.67341</td></tr><tr><td>validation_loss</td><td>0.85218</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/s07x6rsl' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/s07x6rsl</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_023518-s07x6rsl/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_024455-ttzieao1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/ttzieao1' target=\"_blank\">fold_average</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/ttzieao1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/ttzieao1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_accuracy</td><td>▁</td></tr><tr><td>avg_train_loss</td><td>▁</td></tr><tr><td>avg_val_accuracy</td><td>▁</td></tr><tr><td>avg_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_accuracy</td><td>0.68888</td></tr><tr><td>avg_train_loss</td><td>0.79568</td></tr><tr><td>avg_val_accuracy</td><td>0.72538</td></tr><tr><td>avg_val_loss</td><td>0.78271</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_average</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/ttzieao1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12/runs/ttzieao1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_024455-ttzieao1/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Block = 1, Head = 3\n",
        "- Regularization to be strong\n",
        "- Plateu detect ->  Weight Decay 1.5 times strong"
      ],
      "metadata": {
        "id": "ijGIO6V8E7mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "from wandb import Settings\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_5 import EEGformer\n",
        "\n",
        "# ─── Fixed Hyperparameters ─────────────────────────────────────\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY = 5e-2\n",
        "NUM_FILTERS   = 60\n",
        "NUM_BLOCKS    = 1\n",
        "NUM_HEADS     = 3\n",
        "NUM_SEGMENTS  = 5\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS   = 100\n",
        "BATCH_SIZE   = 32\n",
        "ES_PATIENCE  = 15        # ← EarlyStopping 전용 patience\n",
        "WD_PATIENCE  = 5         # ← WD 강화 전용 patience\n",
        "MAX_WD       = 0.5       # ← WD 상한값\n",
        "NUM_WORKERS  = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def train_and_evaluate():\n",
        "    # 1) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels      = [d[\"label\"] for d in train_meta]\n",
        "    input_length= full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 2) 5-Fold split\n",
        "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(full_ds, labels), 1):\n",
        "        print(f\"\\n===== Fold {fold} =====\")\n",
        "        wandb.init(\n",
        "            project=\"eeg-5fold-cv-14\",\n",
        "            name=f\"fold_{fold}\",\n",
        "            config={\n",
        "                \"lr\": LR,\n",
        "                \"weight_decay\": WEIGHT_DECAY,\n",
        "                \"num_blocks\": NUM_BLOCKS,\n",
        "                \"num_heads\": NUM_HEADS,\n",
        "                \"num_segments\": NUM_SEGMENTS,\n",
        "                \"batch_size\": BATCH_SIZE,\n",
        "                \"max_epochs\": MAX_EPOCHS,\n",
        "                \"es_patience\": ES_PATIENCE,\n",
        "                \"wd_patience\": WD_PATIENCE,\n",
        "                \"max_wd\": MAX_WD\n",
        "            },\n",
        "            settings=Settings(init_timeout=120)\n",
        "        )\n",
        "\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 3) 모델 생성\n",
        "        model = EEGformer(\n",
        "            in_channels=19,\n",
        "            input_length=input_length,\n",
        "            kernel_size=10,\n",
        "            num_filters=NUM_FILTERS,\n",
        "            num_heads=NUM_HEADS,\n",
        "            num_blocks=NUM_BLOCKS,\n",
        "            num_segments=NUM_SEGMENTS,\n",
        "            num_classes=3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        # ← MOD: base_lr/base_wd 저장 (base_wd를 plateau 때 갱신)\n",
        "        base_lr = LR\n",
        "        base_wd = WEIGHT_DECAY\n",
        "\n",
        "        optimizer = torch.optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=base_lr,\n",
        "            weight_decay=base_wd\n",
        "        )\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
        "\n",
        "        total_steps = MAX_EPOCHS * len(train_loader)\n",
        "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "            optimizer,\n",
        "            max_lr=base_lr,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=0.2,\n",
        "            anneal_strategy='cos',\n",
        "            div_factor=10,\n",
        "            final_div_factor=100\n",
        "        )\n",
        "\n",
        "        # ← MOD: EarlyStopping & Plateau counters\n",
        "        best_train_loss = best_train_acc = None\n",
        "        best_val_loss = float(\"inf\")\n",
        "        best_val_acc = 0\n",
        "        es_count = 0       # EarlyStopping counter\n",
        "        wd_count = 0       # WD-plateau counter\n",
        "\n",
        "        # 4) Epoch 루프\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "\n",
        "            # — train —\n",
        "            model.train()\n",
        "            tloss = tcorrect = ttotal = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                # ← MOD: OneCycleLR 배치 단위 step\n",
        "                scheduler.step()\n",
        "\n",
        "                # ← MOD: lr 변화에 맞춰 weight_decay 재계산\n",
        "                cur_lr = optimizer.param_groups[0]['lr']\n",
        "                new_wd = base_wd * (cur_lr / base_lr)\n",
        "                for g in optimizer.param_groups:\n",
        "                    g['weight_decay'] = new_wd\n",
        "\n",
        "                tloss    += loss.item()\n",
        "                tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                ttotal   += y.size(0)\n",
        "\n",
        "            train_loss = tloss / len(train_loader)\n",
        "            train_acc  = tcorrect / ttotal\n",
        "\n",
        "            # — validate —\n",
        "            model.eval()\n",
        "            vloss = vcorrect = vtotal = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    vloss  += loss.item()\n",
        "                    vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                    vtotal   += y.size(0)\n",
        "            val_loss = vloss / len(val_loader)\n",
        "            val_acc  = vcorrect / vtotal\n",
        "            elapsed  = time.time() - t0\n",
        "\n",
        "            print(\n",
        "                f\"Epoch {epoch:03d} | \"\n",
        "                f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "                f\"time={elapsed:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # ← MOD: weight_decay & lr도 함께 로깅\n",
        "            wandb.log({\n",
        "                \"epoch\":               epoch,\n",
        "                \"train_loss\":          train_loss,\n",
        "                \"train_accuracy\":      train_acc,\n",
        "                \"validation_loss\":     val_loss,\n",
        "                \"validation_accuracy\": val_acc,\n",
        "                \"lr\":                  cur_lr,\n",
        "                \"weight_decay\":        new_wd\n",
        "            }, step=epoch)\n",
        "\n",
        "            # ← MOD: EarlyStopping & WD 스케줄링 분리\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss   = val_loss\n",
        "                best_train_loss = train_loss\n",
        "                best_train_acc  = train_acc\n",
        "                best_val_acc    = val_acc\n",
        "                es_count = wd_count = 0\n",
        "            else:\n",
        "                es_count += 1\n",
        "                wd_count += 1\n",
        "\n",
        "                # WD_PATIENCE마다 base_wd 강화 + 즉시 로그\n",
        "                if wd_count >= WD_PATIENCE:\n",
        "                    base_wd = min(base_wd * 1.5, MAX_WD)\n",
        "                    for g in optimizer.param_groups:\n",
        "                        # 재계산 후 적용\n",
        "                        g['weight_decay'] = base_wd * (cur_lr / base_lr)\n",
        "                    # ← MOD: 강화된 값 즉시 로깅\n",
        "                    wandb.log({\n",
        "                        \"epoch\":          epoch,\n",
        "                        \"weight_decay\":   optimizer.param_groups[0]['weight_decay'],\n",
        "                        \"note\":           \"WD increased\"\n",
        "                    }, step=epoch)\n",
        "                    print(f\"  → base_wd increased to {base_wd:.5f} at epoch {epoch}\")\n",
        "                    wd_count = 0\n",
        "\n",
        "                # ES_PATIENCE마다 학습 중단\n",
        "                if es_count >= ES_PATIENCE:\n",
        "                    print(f\"Early stopping at epoch {epoch}\")\n",
        "                    break\n",
        "\n",
        "        # Fold 결과 기록\n",
        "        print(\n",
        "            f\"Fold {fold} best_train_loss={best_train_loss:.4f}, \"\n",
        "            f\"best_train_acc={best_train_acc:.4f}, \"\n",
        "            f\"best_val_loss={best_val_loss:.4f}, \"\n",
        "            f\"best_val_acc={best_val_acc:.4f}\"\n",
        "        )\n",
        "        wandb.summary[\"best_train_loss\"]     = best_train_loss\n",
        "        wandb.summary[\"best_train_accuracy\"] = best_train_acc\n",
        "        wandb.summary[\"best_val_loss\"]       = best_val_loss\n",
        "        wandb.summary[\"best_val_accuracy\"]   = best_val_acc\n",
        "\n",
        "        fold_results.append({\n",
        "            \"train_loss\": best_train_loss,\n",
        "            \"train_acc\":  best_train_acc,\n",
        "            \"val_loss\":   best_val_loss,\n",
        "            \"val_acc\":    best_val_acc\n",
        "        })\n",
        "\n",
        "        wandb.finish()\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # ─── 5-Fold Average Metrics 기록 ─────────────────────────────\n",
        "    avg = {k: sum(res[k] for res in fold_results) / len(fold_results)\n",
        "           for k in fold_results[0]}\n",
        "\n",
        "    wandb.init(\n",
        "        project=\"eeg-5fold-cv-14\",\n",
        "        name=\"fold_average\",\n",
        "        reinit=True,\n",
        "        config={\n",
        "            \"lr\": LR,\n",
        "            \"weight_decay\": WEIGHT_DECAY,\n",
        "            \"num_blocks\": NUM_BLOCKS,\n",
        "            \"num_heads\": NUM_HEADS,\n",
        "            \"num_segments\": NUM_SEGMENTS,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"max_epochs\": MAX_EPOCHS\n",
        "        },\n",
        "        settings=Settings(init_timeout=120)\n",
        "    )\n",
        "    wandb.log({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.summary.update({\n",
        "        \"avg_train_loss\":      avg[\"train_loss\"],\n",
        "        \"avg_train_accuracy\":  avg[\"train_acc\"],\n",
        "        \"avg_val_loss\":        avg[\"val_loss\"],\n",
        "        \"avg_val_accuracy\":    avg[\"val_acc\"],\n",
        "    })\n",
        "    wandb.finish()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_evaluate()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z1COWRKeE746",
        "outputId": "9a6e2194-9781-439b-b9d3-e4f764b85fcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            "===== Fold 1 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_042358-lef8g97s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/lef8g97s' target=\"_blank\">fold_1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/lef8g97s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/lef8g97s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1593 acc=0.3417 | val_loss=1.0958 acc=0.3587 | time=16.5s\n",
            "Epoch 002 | train_loss=1.1366 acc=0.3522 | val_loss=1.0967 acc=0.3587 | time=16.7s\n",
            "Epoch 003 | train_loss=1.1226 acc=0.3666 | val_loss=1.0929 acc=0.3587 | time=16.5s\n",
            "Epoch 004 | train_loss=1.1099 acc=0.3798 | val_loss=1.0907 acc=0.3618 | time=16.7s\n",
            "Epoch 005 | train_loss=1.1122 acc=0.3926 | val_loss=1.0865 acc=0.4379 | time=16.6s\n",
            "Epoch 006 | train_loss=1.0964 acc=0.3984 | val_loss=1.0780 acc=0.5404 | time=16.6s\n",
            "Epoch 007 | train_loss=1.0801 acc=0.4404 | val_loss=1.0419 acc=0.5683 | time=16.5s\n",
            "Epoch 008 | train_loss=1.0426 acc=0.4936 | val_loss=0.9907 acc=0.6056 | time=16.5s\n",
            "Epoch 009 | train_loss=1.0044 acc=0.5235 | val_loss=0.9840 acc=0.5792 | time=16.4s\n",
            "Epoch 010 | train_loss=0.9868 acc=0.5495 | val_loss=0.9637 acc=0.6351 | time=16.5s\n",
            "Epoch 011 | train_loss=0.9637 acc=0.5790 | val_loss=0.9285 acc=0.6382 | time=16.2s\n",
            "Epoch 012 | train_loss=0.9385 acc=0.5903 | val_loss=0.9332 acc=0.6180 | time=16.7s\n",
            "Epoch 013 | train_loss=0.9292 acc=0.5957 | val_loss=0.8663 acc=0.6320 | time=16.4s\n",
            "Epoch 014 | train_loss=0.9158 acc=0.5953 | val_loss=0.8930 acc=0.6304 | time=16.5s\n",
            "Epoch 015 | train_loss=0.8998 acc=0.6004 | val_loss=0.8890 acc=0.6087 | time=16.3s\n",
            "Epoch 016 | train_loss=0.8784 acc=0.6217 | val_loss=0.8958 acc=0.6149 | time=16.4s\n",
            "Epoch 017 | train_loss=0.8853 acc=0.6047 | val_loss=0.8444 acc=0.6398 | time=16.3s\n",
            "Epoch 018 | train_loss=0.8791 acc=0.6334 | val_loss=0.8774 acc=0.6444 | time=16.6s\n",
            "Epoch 019 | train_loss=0.8597 acc=0.6260 | val_loss=1.0426 acc=0.5450 | time=16.4s\n",
            "Epoch 020 | train_loss=0.8547 acc=0.6361 | val_loss=0.8847 acc=0.6460 | time=16.2s\n",
            "Epoch 021 | train_loss=0.8487 acc=0.6470 | val_loss=0.8113 acc=0.6708 | time=16.4s\n",
            "Epoch 022 | train_loss=0.8352 acc=0.6594 | val_loss=0.8373 acc=0.6599 | time=16.3s\n",
            "Epoch 023 | train_loss=0.8111 acc=0.6804 | val_loss=0.8797 acc=0.6258 | time=16.3s\n",
            "Epoch 024 | train_loss=0.7966 acc=0.6850 | val_loss=0.8011 acc=0.7283 | time=16.2s\n",
            "Epoch 025 | train_loss=0.8044 acc=0.6823 | val_loss=0.8617 acc=0.6460 | time=16.5s\n",
            "Epoch 026 | train_loss=0.8132 acc=0.6680 | val_loss=0.8126 acc=0.7112 | time=16.2s\n",
            "Epoch 027 | train_loss=0.7946 acc=0.6967 | val_loss=0.9317 acc=0.6025 | time=16.5s\n",
            "Epoch 028 | train_loss=0.7825 acc=0.7006 | val_loss=0.8291 acc=0.6972 | time=16.3s\n",
            "Epoch 029 | train_loss=0.7753 acc=0.7091 | val_loss=0.7825 acc=0.7127 | time=16.5s\n",
            "Epoch 030 | train_loss=0.7676 acc=0.7014 | val_loss=0.7683 acc=0.7252 | time=16.3s\n",
            "Epoch 031 | train_loss=0.7761 acc=0.7068 | val_loss=0.7870 acc=0.6894 | time=16.4s\n",
            "Epoch 032 | train_loss=0.7600 acc=0.7208 | val_loss=0.8452 acc=0.6087 | time=16.5s\n",
            "Epoch 033 | train_loss=0.7797 acc=0.7037 | val_loss=0.8289 acc=0.6925 | time=16.3s\n",
            "Epoch 034 | train_loss=0.7431 acc=0.7293 | val_loss=0.8313 acc=0.6724 | time=16.2s\n",
            "Epoch 035 | train_loss=0.7538 acc=0.7134 | val_loss=0.8542 acc=0.6677 | time=16.3s\n",
            "  → base_wd increased to 0.07500 at epoch 35\n",
            "Epoch 036 | train_loss=0.7399 acc=0.7184 | val_loss=0.8312 acc=0.7127 | time=16.4s\n",
            "Epoch 037 | train_loss=0.7256 acc=0.7437 | val_loss=0.7966 acc=0.6957 | time=16.2s\n",
            "Epoch 038 | train_loss=0.7200 acc=0.7414 | val_loss=0.8155 acc=0.6988 | time=16.3s\n",
            "Epoch 039 | train_loss=0.7263 acc=0.7379 | val_loss=0.8232 acc=0.6832 | time=16.3s\n",
            "Epoch 040 | train_loss=0.7047 acc=0.7604 | val_loss=0.8610 acc=0.6972 | time=16.6s\n",
            "  → base_wd increased to 0.11250 at epoch 40\n",
            "Epoch 041 | train_loss=0.6894 acc=0.7697 | val_loss=0.8396 acc=0.7143 | time=16.3s\n",
            "Epoch 042 | train_loss=0.6878 acc=0.7658 | val_loss=0.7903 acc=0.6910 | time=16.4s\n",
            "Epoch 043 | train_loss=0.6850 acc=0.7779 | val_loss=0.8216 acc=0.7189 | time=16.3s\n",
            "Epoch 044 | train_loss=0.7009 acc=0.7522 | val_loss=0.8495 acc=0.7019 | time=16.6s\n",
            "Epoch 045 | train_loss=0.6928 acc=0.7627 | val_loss=0.8657 acc=0.6677 | time=16.3s\n",
            "  → base_wd increased to 0.16875 at epoch 45\n",
            "Early stopping at epoch 45\n",
            "Fold 1 best_train_loss=0.7676, best_train_acc=0.7014, best_val_loss=0.7683, best_val_acc=0.7252\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▅▆▆▇▇▇███████████████▇▇▇▇▇▇▇▇▇▆</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▂▂▃▃▄▅▅▅▅▅▅▅▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▇▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▃▄▅▆▆▆▆▆▆▆▆▆▅▆▇▇▆█▆█▇██▇▆▇▇▇▇▇▇▇█▇█▇</td></tr><tr><td>validation_loss</td><td>██████▇▆▅▄▅▃▄▄▄▃▇▃▂▂▃▂▃▂▂▁▁▁▃▂▂▃▂▂▂▃▃▁▂▃</td></tr><tr><td>weight_decay</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▅▄▄▄▆▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.70136</td></tr><tr><td>best_train_loss</td><td>0.7676</td></tr><tr><td>best_val_accuracy</td><td>0.72516</td></tr><tr><td>best_val_loss</td><td>0.7683</td></tr><tr><td>epoch</td><td>45</td></tr><tr><td>lr</td><td>0.00078</td></tr><tr><td>note</td><td>WD increased</td></tr><tr><td>train_accuracy</td><td>0.76272</td></tr><tr><td>train_loss</td><td>0.69276</td></tr><tr><td>validation_accuracy</td><td>0.6677</td></tr><tr><td>validation_loss</td><td>0.86566</td></tr><tr><td>weight_decay</td><td>0.13125</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/lef8g97s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/lef8g97s</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_042358-lef8g97s/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 2 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_043619-aiddxihe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/aiddxihe' target=\"_blank\">fold_2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/aiddxihe' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/aiddxihe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1613 acc=0.3670 | val_loss=1.0812 acc=0.4255 | time=16.3s\n",
            "Epoch 002 | train_loss=1.1437 acc=0.3786 | val_loss=1.0812 acc=0.4255 | time=16.3s\n",
            "Epoch 003 | train_loss=1.1447 acc=0.3701 | val_loss=1.0827 acc=0.4255 | time=16.1s\n",
            "Epoch 004 | train_loss=1.1316 acc=0.3643 | val_loss=1.0820 acc=0.4255 | time=16.3s\n",
            "Epoch 005 | train_loss=1.1254 acc=0.3810 | val_loss=1.0827 acc=0.4488 | time=16.3s\n",
            "Epoch 006 | train_loss=1.1112 acc=0.3895 | val_loss=1.0891 acc=0.3665 | time=16.4s\n",
            "Epoch 007 | train_loss=1.1041 acc=0.3872 | val_loss=1.0811 acc=0.4255 | time=16.3s\n",
            "Epoch 008 | train_loss=1.0987 acc=0.3950 | val_loss=1.0791 acc=0.4255 | time=16.4s\n",
            "Epoch 009 | train_loss=1.0872 acc=0.4058 | val_loss=1.0325 acc=0.5839 | time=16.2s\n",
            "Epoch 010 | train_loss=1.0393 acc=0.5091 | val_loss=0.9785 acc=0.6351 | time=16.3s\n",
            "Epoch 011 | train_loss=0.9943 acc=0.5398 | val_loss=0.9624 acc=0.6398 | time=16.2s\n",
            "Epoch 012 | train_loss=0.9700 acc=0.5588 | val_loss=0.8991 acc=0.6677 | time=16.1s\n",
            "Epoch 013 | train_loss=0.9362 acc=0.5930 | val_loss=0.8756 acc=0.6770 | time=16.4s\n",
            "Epoch 014 | train_loss=0.9180 acc=0.5977 | val_loss=0.9358 acc=0.5901 | time=16.3s\n",
            "Epoch 015 | train_loss=0.8894 acc=0.6233 | val_loss=0.9317 acc=0.6475 | time=16.2s\n",
            "Epoch 016 | train_loss=0.8888 acc=0.6113 | val_loss=0.8538 acc=0.6988 | time=16.0s\n",
            "Epoch 017 | train_loss=0.8764 acc=0.6249 | val_loss=0.9170 acc=0.5761 | time=16.4s\n",
            "Epoch 018 | train_loss=0.8526 acc=0.6435 | val_loss=0.8782 acc=0.5916 | time=16.2s\n",
            "Epoch 019 | train_loss=0.8662 acc=0.6408 | val_loss=0.8157 acc=0.7127 | time=16.1s\n",
            "Epoch 020 | train_loss=0.8561 acc=0.6493 | val_loss=0.9153 acc=0.5947 | time=16.1s\n",
            "Epoch 021 | train_loss=0.8357 acc=0.6590 | val_loss=0.8517 acc=0.6863 | time=16.5s\n",
            "Epoch 022 | train_loss=0.8268 acc=0.6672 | val_loss=1.0963 acc=0.5668 | time=16.0s\n",
            "Epoch 023 | train_loss=0.8413 acc=0.6567 | val_loss=0.8585 acc=0.6879 | time=16.1s\n",
            "Epoch 024 | train_loss=0.8245 acc=0.6711 | val_loss=0.8620 acc=0.6755 | time=16.0s\n",
            "  → base_wd increased to 0.07500 at epoch 24\n",
            "Epoch 025 | train_loss=0.8006 acc=0.6870 | val_loss=0.8552 acc=0.7050 | time=16.4s\n",
            "Epoch 026 | train_loss=0.8009 acc=0.6854 | val_loss=0.9138 acc=0.6630 | time=16.2s\n",
            "Epoch 027 | train_loss=0.7871 acc=0.6979 | val_loss=0.8776 acc=0.6988 | time=16.2s\n",
            "Epoch 028 | train_loss=0.8002 acc=0.6920 | val_loss=0.8444 acc=0.7096 | time=16.3s\n",
            "Epoch 029 | train_loss=0.7801 acc=0.7033 | val_loss=0.8226 acc=0.6863 | time=16.3s\n",
            "  → base_wd increased to 0.11250 at epoch 29\n",
            "Epoch 030 | train_loss=0.7663 acc=0.7080 | val_loss=0.8226 acc=0.6894 | time=16.3s\n",
            "Epoch 031 | train_loss=0.7617 acc=0.7169 | val_loss=0.8786 acc=0.6289 | time=16.0s\n",
            "Epoch 032 | train_loss=0.7534 acc=0.7293 | val_loss=0.8370 acc=0.7127 | time=16.1s\n",
            "Epoch 033 | train_loss=0.7600 acc=0.7157 | val_loss=0.8577 acc=0.6537 | time=16.3s\n",
            "Epoch 034 | train_loss=0.7770 acc=0.7161 | val_loss=0.8549 acc=0.6925 | time=16.2s\n",
            "  → base_wd increased to 0.16875 at epoch 34\n",
            "Early stopping at epoch 34\n",
            "Fold 2 best_train_loss=0.8662, best_train_acc=0.6408, best_val_loss=0.8157, best_val_acc=0.7127\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▂▄▄▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>███▇▇▇▇▇▇▆▅▅▄▄▃▃▃▃▃▃▂▂▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▂▂▂▂▃▁▂▂▅▆▇▇▇▆▇█▅▆█▆▇▅▇▇█▇██▇█▆█▇█</td></tr><tr><td>validation_loss</td><td>████████▆▅▅▃▂▄▄▂▄▃▁▃▂█▂▂▂▃▃▂▁▁▃▂▂▂</td></tr><tr><td>weight_decay</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▆▆▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.64078</td></tr><tr><td>best_train_loss</td><td>0.86618</td></tr><tr><td>best_val_accuracy</td><td>0.71273</td></tr><tr><td>best_val_loss</td><td>0.81566</td></tr><tr><td>epoch</td><td>34</td></tr><tr><td>lr</td><td>0.00093</td></tr><tr><td>note</td><td>WD increased</td></tr><tr><td>train_accuracy</td><td>0.71612</td></tr><tr><td>train_loss</td><td>0.77703</td></tr><tr><td>validation_accuracy</td><td>0.69255</td></tr><tr><td>validation_loss</td><td>0.8549</td></tr><tr><td>weight_decay</td><td>0.15631</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/aiddxihe' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/aiddxihe</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_043619-aiddxihe/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 3 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_044533-q8ptn23h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/q8ptn23h' target=\"_blank\">fold_3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/q8ptn23h' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/q8ptn23h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.2098 acc=0.3515 | val_loss=1.0817 acc=0.4410 | time=16.6s\n",
            "Epoch 002 | train_loss=1.1582 acc=0.3759 | val_loss=1.0848 acc=0.4410 | time=16.5s\n",
            "Epoch 003 | train_loss=1.1458 acc=0.3790 | val_loss=1.0814 acc=0.4410 | time=16.2s\n",
            "Epoch 004 | train_loss=1.1402 acc=0.3783 | val_loss=1.0817 acc=0.4410 | time=16.1s\n",
            "Epoch 005 | train_loss=1.1278 acc=0.3810 | val_loss=1.0843 acc=0.4410 | time=16.1s\n",
            "Epoch 006 | train_loss=1.1295 acc=0.3926 | val_loss=1.0832 acc=0.4410 | time=16.3s\n",
            "Epoch 007 | train_loss=1.1273 acc=0.3674 | val_loss=1.0816 acc=0.4410 | time=16.2s\n",
            "Epoch 008 | train_loss=1.1111 acc=0.3891 | val_loss=1.0862 acc=0.4410 | time=16.1s\n",
            "  → base_wd increased to 0.07500 at epoch 8\n",
            "Epoch 009 | train_loss=1.1056 acc=0.3953 | val_loss=1.0808 acc=0.4410 | time=16.3s\n",
            "Epoch 010 | train_loss=1.0979 acc=0.3876 | val_loss=1.0820 acc=0.4410 | time=16.3s\n",
            "Epoch 011 | train_loss=1.0900 acc=0.3992 | val_loss=1.0801 acc=0.4410 | time=16.3s\n",
            "Epoch 012 | train_loss=1.0820 acc=0.4272 | val_loss=1.0574 acc=0.4876 | time=16.1s\n",
            "Epoch 013 | train_loss=1.0273 acc=0.5204 | val_loss=0.9764 acc=0.6258 | time=16.3s\n",
            "Epoch 014 | train_loss=0.9863 acc=0.5600 | val_loss=0.9805 acc=0.5792 | time=16.2s\n",
            "Epoch 015 | train_loss=0.9700 acc=0.5794 | val_loss=0.9499 acc=0.6553 | time=16.0s\n",
            "Epoch 016 | train_loss=0.9471 acc=0.5825 | val_loss=0.9715 acc=0.5761 | time=16.1s\n",
            "Epoch 017 | train_loss=0.9468 acc=0.5899 | val_loss=0.8908 acc=0.6568 | time=16.4s\n",
            "Epoch 018 | train_loss=0.9228 acc=0.5973 | val_loss=0.9495 acc=0.5512 | time=16.3s\n",
            "Epoch 019 | train_loss=0.9174 acc=0.5918 | val_loss=0.9292 acc=0.6149 | time=16.1s\n",
            "Epoch 020 | train_loss=0.8899 acc=0.6214 | val_loss=0.9339 acc=0.6134 | time=16.2s\n",
            "Epoch 021 | train_loss=0.9041 acc=0.6167 | val_loss=0.8935 acc=0.6646 | time=16.5s\n",
            "Epoch 022 | train_loss=0.8604 acc=0.6505 | val_loss=0.8759 acc=0.6646 | time=16.0s\n",
            "Epoch 023 | train_loss=0.8591 acc=0.6311 | val_loss=0.8946 acc=0.6227 | time=16.0s\n",
            "Epoch 024 | train_loss=0.8318 acc=0.6742 | val_loss=0.8505 acc=0.6786 | time=16.1s\n",
            "Epoch 025 | train_loss=0.8146 acc=0.6656 | val_loss=0.9344 acc=0.5901 | time=16.6s\n",
            "Epoch 026 | train_loss=0.8213 acc=0.6765 | val_loss=0.8586 acc=0.6817 | time=16.3s\n",
            "Epoch 027 | train_loss=0.8199 acc=0.6695 | val_loss=0.8802 acc=0.6817 | time=16.1s\n",
            "Epoch 028 | train_loss=0.8103 acc=0.6656 | val_loss=0.8392 acc=0.6522 | time=16.5s\n",
            "Epoch 029 | train_loss=0.8006 acc=0.6917 | val_loss=0.8590 acc=0.6009 | time=16.7s\n",
            "Epoch 030 | train_loss=0.7993 acc=0.6850 | val_loss=0.8733 acc=0.6429 | time=16.4s\n",
            "Epoch 031 | train_loss=0.7998 acc=0.6893 | val_loss=0.8538 acc=0.6599 | time=16.1s\n",
            "Epoch 032 | train_loss=0.7730 acc=0.7010 | val_loss=0.8248 acc=0.6599 | time=16.1s\n",
            "Epoch 033 | train_loss=0.7658 acc=0.7169 | val_loss=0.8407 acc=0.6102 | time=16.2s\n",
            "Epoch 034 | train_loss=0.7605 acc=0.7157 | val_loss=0.9055 acc=0.5559 | time=16.3s\n",
            "Epoch 035 | train_loss=0.7479 acc=0.7301 | val_loss=0.8707 acc=0.6429 | time=16.2s\n",
            "Epoch 036 | train_loss=0.7425 acc=0.7262 | val_loss=0.8119 acc=0.6770 | time=16.2s\n",
            "Epoch 037 | train_loss=0.7395 acc=0.7348 | val_loss=0.8659 acc=0.6304 | time=16.1s\n",
            "Epoch 038 | train_loss=0.7089 acc=0.7499 | val_loss=0.8795 acc=0.6382 | time=16.2s\n",
            "Epoch 039 | train_loss=0.7117 acc=0.7468 | val_loss=0.8557 acc=0.6755 | time=16.4s\n",
            "Epoch 040 | train_loss=0.7129 acc=0.7487 | val_loss=0.8759 acc=0.6786 | time=16.2s\n",
            "Epoch 041 | train_loss=0.7080 acc=0.7647 | val_loss=0.8871 acc=0.6258 | time=16.4s\n",
            "  → base_wd increased to 0.11250 at epoch 41\n",
            "Epoch 042 | train_loss=0.7059 acc=0.7604 | val_loss=0.8998 acc=0.6568 | time=16.3s\n",
            "Epoch 043 | train_loss=0.6870 acc=0.7717 | val_loss=0.9087 acc=0.6506 | time=16.5s\n",
            "Epoch 044 | train_loss=0.6826 acc=0.7779 | val_loss=0.8828 acc=0.6941 | time=16.3s\n",
            "Epoch 045 | train_loss=0.6772 acc=0.7689 | val_loss=0.8446 acc=0.6863 | time=16.5s\n",
            "Epoch 046 | train_loss=0.6715 acc=0.7837 | val_loss=0.9420 acc=0.6009 | time=16.4s\n",
            "  → base_wd increased to 0.16875 at epoch 46\n",
            "Epoch 047 | train_loss=0.6628 acc=0.7868 | val_loss=0.8913 acc=0.6537 | time=16.7s\n",
            "Epoch 048 | train_loss=0.6579 acc=0.7872 | val_loss=1.0229 acc=0.6165 | time=16.3s\n",
            "Epoch 049 | train_loss=0.6581 acc=0.7930 | val_loss=0.8781 acc=0.6786 | time=16.4s\n",
            "Epoch 050 | train_loss=0.6618 acc=0.7825 | val_loss=0.9591 acc=0.6196 | time=16.3s\n",
            "Epoch 051 | train_loss=0.6466 acc=0.7973 | val_loss=1.0024 acc=0.6289 | time=16.8s\n",
            "  → base_wd increased to 0.25313 at epoch 51\n",
            "Early stopping at epoch 51\n",
            "Fold 3 best_train_loss=0.7425, best_train_acc=0.7262, best_val_loss=0.8119, best_val_acc=0.6770\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▂▂▄▄▅▆▆▇██████████████▇▇▇▇▇▇▇▇▇▆▆▆▆▆▅</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▂▂▂▄▄▅▅▅▅▅▅▆▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█▇▇▇▇▇▇▇▇▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▆▅▇▅▇▆▆▇▇▆██▇▅▇▄▇█▆▆█▆▇▇█▅▇▆█▆</td></tr><tr><td>validation_loss</td><td>█████████▇▅▅▅▅▄▃▃▂▄▂▂▂▃▂▁▃▃▁▂▃▃▃▃▃▃▄▃▆▃▆</td></tr><tr><td>weight_decay</td><td>▁▁▁▁▁▂▂▃▃▃▃▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▄▅▅▅▄▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.72621</td></tr><tr><td>best_train_loss</td><td>0.74249</td></tr><tr><td>best_val_accuracy</td><td>0.67702</td></tr><tr><td>best_val_loss</td><td>0.81188</td></tr><tr><td>epoch</td><td>51</td></tr><tr><td>lr</td><td>0.00067</td></tr><tr><td>note</td><td>WD increased</td></tr><tr><td>train_accuracy</td><td>0.79728</td></tr><tr><td>train_loss</td><td>0.64661</td></tr><tr><td>validation_accuracy</td><td>0.62888</td></tr><tr><td>validation_loss</td><td>1.00238</td></tr><tr><td>weight_decay</td><td>0.17039</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/q8ptn23h' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/q8ptn23h</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_044533-q8ptn23h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 4 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_045925-s4np8lkv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/s4np8lkv' target=\"_blank\">fold_4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/s4np8lkv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/s4np8lkv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.2026 acc=0.3452 | val_loss=1.1071 acc=0.3168 | time=16.3s\n",
            "Epoch 002 | train_loss=1.1695 acc=0.3654 | val_loss=1.1015 acc=0.3168 | time=16.3s\n",
            "Epoch 003 | train_loss=1.1595 acc=0.3810 | val_loss=1.1021 acc=0.3168 | time=16.6s\n",
            "Epoch 004 | train_loss=1.1538 acc=0.3748 | val_loss=1.0993 acc=0.3168 | time=16.3s\n",
            "Epoch 005 | train_loss=1.1316 acc=0.3891 | val_loss=1.1045 acc=0.3168 | time=16.4s\n",
            "Epoch 006 | train_loss=1.1226 acc=0.3849 | val_loss=1.0859 acc=0.4099 | time=16.6s\n",
            "Epoch 007 | train_loss=1.0872 acc=0.4280 | val_loss=1.0110 acc=0.5994 | time=16.5s\n",
            "Epoch 008 | train_loss=1.0375 acc=0.5002 | val_loss=1.0113 acc=0.5450 | time=16.5s\n",
            "Epoch 009 | train_loss=0.9960 acc=0.5468 | val_loss=0.9305 acc=0.6351 | time=16.5s\n",
            "Epoch 010 | train_loss=0.9726 acc=0.5786 | val_loss=0.9294 acc=0.6351 | time=16.5s\n",
            "Epoch 011 | train_loss=0.9562 acc=0.5833 | val_loss=0.9197 acc=0.6351 | time=16.3s\n",
            "Epoch 012 | train_loss=0.9348 acc=0.5965 | val_loss=0.9058 acc=0.6506 | time=16.5s\n",
            "Epoch 013 | train_loss=0.9330 acc=0.5907 | val_loss=0.8940 acc=0.6522 | time=16.4s\n",
            "Epoch 014 | train_loss=0.9272 acc=0.6023 | val_loss=0.9161 acc=0.6382 | time=16.4s\n",
            "Epoch 015 | train_loss=0.9026 acc=0.6109 | val_loss=0.9353 acc=0.5916 | time=16.5s\n",
            "Epoch 016 | train_loss=0.8863 acc=0.6120 | val_loss=0.9246 acc=0.5932 | time=16.4s\n",
            "Epoch 017 | train_loss=0.8916 acc=0.6256 | val_loss=0.8931 acc=0.6646 | time=16.4s\n",
            "Epoch 018 | train_loss=0.8739 acc=0.6315 | val_loss=0.8377 acc=0.6817 | time=16.2s\n",
            "Epoch 019 | train_loss=0.8634 acc=0.6493 | val_loss=0.9174 acc=0.5947 | time=16.5s\n",
            "Epoch 020 | train_loss=0.8601 acc=0.6513 | val_loss=0.8448 acc=0.6755 | time=16.1s\n",
            "Epoch 021 | train_loss=0.8546 acc=0.6431 | val_loss=0.8918 acc=0.6102 | time=16.5s\n",
            "Epoch 022 | train_loss=0.8312 acc=0.6649 | val_loss=0.8714 acc=0.6537 | time=16.3s\n",
            "Epoch 023 | train_loss=0.8333 acc=0.6715 | val_loss=0.8388 acc=0.6708 | time=16.2s\n",
            "  → base_wd increased to 0.07500 at epoch 23\n",
            "Epoch 024 | train_loss=0.8298 acc=0.6781 | val_loss=0.8845 acc=0.6522 | time=16.2s\n",
            "Epoch 025 | train_loss=0.7977 acc=0.6990 | val_loss=0.8477 acc=0.6724 | time=16.9s\n",
            "Epoch 026 | train_loss=0.8176 acc=0.6816 | val_loss=0.8628 acc=0.6646 | time=16.3s\n",
            "Epoch 027 | train_loss=0.8077 acc=0.6917 | val_loss=0.8755 acc=0.6677 | time=16.6s\n",
            "Epoch 028 | train_loss=0.7919 acc=0.6967 | val_loss=0.8013 acc=0.6972 | time=16.5s\n",
            "Epoch 029 | train_loss=0.7855 acc=0.6983 | val_loss=0.8371 acc=0.6910 | time=16.4s\n",
            "Epoch 030 | train_loss=0.7779 acc=0.7118 | val_loss=0.8500 acc=0.6693 | time=16.4s\n",
            "Epoch 031 | train_loss=0.7767 acc=0.7064 | val_loss=0.8187 acc=0.6879 | time=16.2s\n",
            "Epoch 032 | train_loss=0.7601 acc=0.7107 | val_loss=0.7854 acc=0.7205 | time=16.5s\n",
            "Epoch 033 | train_loss=0.7483 acc=0.7219 | val_loss=0.8156 acc=0.6941 | time=16.6s\n",
            "Epoch 034 | train_loss=0.7753 acc=0.7045 | val_loss=0.8339 acc=0.7003 | time=16.7s\n",
            "Epoch 035 | train_loss=0.7319 acc=0.7449 | val_loss=0.8510 acc=0.6304 | time=16.5s\n",
            "Epoch 036 | train_loss=0.7452 acc=0.7309 | val_loss=0.8512 acc=0.6584 | time=16.8s\n",
            "Epoch 037 | train_loss=0.7173 acc=0.7390 | val_loss=0.8323 acc=0.7081 | time=16.3s\n",
            "  → base_wd increased to 0.11250 at epoch 37\n",
            "Epoch 038 | train_loss=0.7157 acc=0.7518 | val_loss=0.8432 acc=0.6661 | time=16.6s\n",
            "Epoch 039 | train_loss=0.7158 acc=0.7596 | val_loss=0.8205 acc=0.6863 | time=16.7s\n",
            "Epoch 040 | train_loss=0.7177 acc=0.7542 | val_loss=0.9341 acc=0.6056 | time=16.4s\n",
            "Epoch 041 | train_loss=0.7006 acc=0.7588 | val_loss=0.9887 acc=0.5776 | time=16.3s\n",
            "Epoch 042 | train_loss=0.7001 acc=0.7588 | val_loss=0.9207 acc=0.6444 | time=16.8s\n",
            "  → base_wd increased to 0.16875 at epoch 42\n",
            "Epoch 043 | train_loss=0.6778 acc=0.7794 | val_loss=0.9720 acc=0.6273 | time=16.5s\n",
            "Epoch 044 | train_loss=0.6841 acc=0.7755 | val_loss=0.8602 acc=0.6925 | time=16.5s\n",
            "Epoch 045 | train_loss=0.6525 acc=0.7946 | val_loss=0.9838 acc=0.6429 | time=16.4s\n",
            "Epoch 046 | train_loss=0.6804 acc=0.7779 | val_loss=0.9343 acc=0.6382 | time=16.7s\n",
            "Epoch 047 | train_loss=0.6553 acc=0.7981 | val_loss=0.9644 acc=0.6491 | time=16.2s\n",
            "  → base_wd increased to 0.25313 at epoch 47\n",
            "Early stopping at epoch 47\n",
            "Fold 4 best_train_loss=0.7601, best_train_acc=0.7107, best_val_loss=0.7854, best_val_acc=0.7205\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▄▄▅▆▆▇▇██████████████▇▇▇▇▇▇▇▇▇▆▆▆</td></tr><tr><td>train_accuracy</td><td>▁▁▂▁▂▂▃▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇████</td></tr><tr><td>train_loss</td><td>██▇▇▇▇▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▃▂▃▂▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▃▅▇▇▇▇▇▆▆▇▇▆▆▇▇▇▇▇█▇▇▇██▆▇█▇▇▆▇▆█▇▇</td></tr><tr><td>validation_loss</td><td>██████▆▄▄▄▄▃▄▄▃▂▄▃▃▂▃▂▃▁▂▂▂▁▂▂▂▂▂▂▅▄▅▃▅▅</td></tr><tr><td>weight_decay</td><td>▁▁▁▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▄▄▄▃▃▃▅▅▅▄▆▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.71068</td></tr><tr><td>best_train_loss</td><td>0.76009</td></tr><tr><td>best_val_accuracy</td><td>0.7205</td></tr><tr><td>best_val_loss</td><td>0.78544</td></tr><tr><td>epoch</td><td>47</td></tr><tr><td>lr</td><td>0.00074</td></tr><tr><td>note</td><td>WD increased</td></tr><tr><td>train_accuracy</td><td>0.79806</td></tr><tr><td>train_loss</td><td>0.65527</td></tr><tr><td>validation_accuracy</td><td>0.64907</td></tr><tr><td>validation_loss</td><td>0.96445</td></tr><tr><td>weight_decay</td><td>0.18841</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/s4np8lkv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/s4np8lkv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_045925-s4np8lkv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Fold 5 =====\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_051220-g05g3m24</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/g05g3m24' target=\"_blank\">fold_5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/g05g3m24' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/g05g3m24</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1645 acc=0.3381 | val_loss=1.0874 acc=0.4355 | time=16.1s\n",
            "Epoch 002 | train_loss=1.1551 acc=0.3606 | val_loss=1.0913 acc=0.4355 | time=16.4s\n",
            "Epoch 003 | train_loss=1.1567 acc=0.3424 | val_loss=1.1020 acc=0.4246 | time=16.4s\n",
            "Epoch 004 | train_loss=1.1367 acc=0.3637 | val_loss=1.0933 acc=0.4355 | time=16.4s\n",
            "Epoch 005 | train_loss=1.1163 acc=0.3715 | val_loss=1.0977 acc=0.4355 | time=16.4s\n",
            "Epoch 006 | train_loss=1.1208 acc=0.3800 | val_loss=1.0879 acc=0.4355 | time=16.5s\n",
            "  → base_wd increased to 0.07500 at epoch 6\n",
            "Epoch 007 | train_loss=1.1093 acc=0.3855 | val_loss=1.0913 acc=0.4355 | time=16.3s\n",
            "Epoch 008 | train_loss=1.1028 acc=0.3839 | val_loss=1.0902 acc=0.4355 | time=16.4s\n",
            "Epoch 009 | train_loss=1.1008 acc=0.3781 | val_loss=1.0854 acc=0.4355 | time=16.5s\n",
            "Epoch 010 | train_loss=1.0947 acc=0.3956 | val_loss=1.0874 acc=0.4355 | time=16.4s\n",
            "Epoch 011 | train_loss=1.0775 acc=0.4410 | val_loss=1.0764 acc=0.5008 | time=16.1s\n",
            "Epoch 012 | train_loss=1.0591 acc=0.4647 | val_loss=1.0954 acc=0.3904 | time=16.2s\n",
            "Epoch 013 | train_loss=1.0256 acc=0.5128 | val_loss=0.9944 acc=0.5925 | time=16.4s\n",
            "Epoch 014 | train_loss=0.9853 acc=0.5512 | val_loss=0.9427 acc=0.6330 | time=16.2s\n",
            "Epoch 015 | train_loss=0.9635 acc=0.5839 | val_loss=0.9482 acc=0.6299 | time=16.4s\n",
            "Epoch 016 | train_loss=0.9286 acc=0.5916 | val_loss=0.9484 acc=0.5925 | time=16.6s\n",
            "Epoch 017 | train_loss=0.9244 acc=0.6044 | val_loss=0.8873 acc=0.6299 | time=16.6s\n",
            "Epoch 018 | train_loss=0.9185 acc=0.6110 | val_loss=0.9103 acc=0.6081 | time=16.2s\n",
            "Epoch 019 | train_loss=0.8997 acc=0.6242 | val_loss=0.8729 acc=0.6672 | time=16.3s\n",
            "Epoch 020 | train_loss=0.8953 acc=0.6332 | val_loss=0.8394 acc=0.6983 | time=16.5s\n",
            "Epoch 021 | train_loss=0.8629 acc=0.6561 | val_loss=0.9519 acc=0.5754 | time=16.5s\n",
            "Epoch 022 | train_loss=0.8572 acc=0.6448 | val_loss=0.8598 acc=0.6781 | time=16.5s\n",
            "Epoch 023 | train_loss=0.8474 acc=0.6607 | val_loss=0.8435 acc=0.6983 | time=16.3s\n",
            "Epoch 024 | train_loss=0.8510 acc=0.6471 | val_loss=0.8534 acc=0.6796 | time=16.6s\n",
            "Epoch 025 | train_loss=0.8413 acc=0.6681 | val_loss=0.8301 acc=0.6998 | time=16.3s\n",
            "Epoch 026 | train_loss=0.8288 acc=0.6735 | val_loss=0.7687 acc=0.7263 | time=16.3s\n",
            "Epoch 027 | train_loss=0.8238 acc=0.6751 | val_loss=0.8261 acc=0.7045 | time=16.2s\n",
            "Epoch 028 | train_loss=0.8205 acc=0.6759 | val_loss=0.8373 acc=0.6827 | time=16.3s\n",
            "Epoch 029 | train_loss=0.7930 acc=0.6957 | val_loss=0.8271 acc=0.7201 | time=16.4s\n",
            "Epoch 030 | train_loss=0.8103 acc=0.6856 | val_loss=0.8432 acc=0.6765 | time=16.2s\n",
            "Epoch 031 | train_loss=0.7996 acc=0.6871 | val_loss=0.9464 acc=0.6594 | time=16.4s\n",
            "  → base_wd increased to 0.11250 at epoch 31\n",
            "Epoch 032 | train_loss=0.7860 acc=0.7011 | val_loss=0.7717 acc=0.7387 | time=16.3s\n",
            "Epoch 033 | train_loss=0.7702 acc=0.7158 | val_loss=0.8352 acc=0.6967 | time=16.5s\n",
            "Epoch 034 | train_loss=0.7728 acc=0.7166 | val_loss=0.8980 acc=0.6128 | time=16.2s\n",
            "Epoch 035 | train_loss=0.7853 acc=0.7085 | val_loss=0.8357 acc=0.6890 | time=16.9s\n",
            "Epoch 036 | train_loss=0.7631 acc=0.7069 | val_loss=0.8306 acc=0.6936 | time=16.7s\n",
            "  → base_wd increased to 0.16875 at epoch 36\n",
            "Epoch 037 | train_loss=0.7592 acc=0.7143 | val_loss=0.8762 acc=0.6781 | time=16.2s\n",
            "Epoch 038 | train_loss=0.7587 acc=0.7170 | val_loss=0.7767 acc=0.6967 | time=16.2s\n",
            "Epoch 039 | train_loss=0.7571 acc=0.7147 | val_loss=0.8087 acc=0.7247 | time=16.6s\n",
            "Epoch 040 | train_loss=0.7430 acc=0.7321 | val_loss=0.8205 acc=0.6812 | time=16.5s\n",
            "Epoch 041 | train_loss=0.7333 acc=0.7352 | val_loss=0.8025 acc=0.6827 | time=16.1s\n",
            "  → base_wd increased to 0.25313 at epoch 41\n",
            "Early stopping at epoch 41\n",
            "Fold 5 best_train_loss=0.8288, best_train_acc=0.6735, best_val_loss=0.7687, best_val_acc=0.7263\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>lr</td><td>▁▁▁▂▂▂▃▃▄▄▅▆▆▇▇▇█████████████████▇▇▇▇▇▇▇</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▄▅▅▅▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>████▇▇▇▇▇▇▇▆▆▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▂▂▂▂▂▂▂▂▂▂▃▁▅▆▆▅▆▅▇▇▅▇▇▇▇█▇▇█▇▆█▇▅▇▇▇▇█▇</td></tr><tr><td>validation_loss</td><td>██████████▇█▆▅▅▅▃▄▃▂▅▃▃▃▂▁▂▂▂▃▅▁▂▄▂▂▃▁▂▂</td></tr><tr><td>weight_decay</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▃▄▄▄▄▄▆▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_train_accuracy</td><td>0.67352</td></tr><tr><td>best_train_loss</td><td>0.82884</td></tr><tr><td>best_val_accuracy</td><td>0.72628</td></tr><tr><td>best_val_loss</td><td>0.76866</td></tr><tr><td>epoch</td><td>41</td></tr><tr><td>lr</td><td>0.00084</td></tr><tr><td>note</td><td>WD increased</td></tr><tr><td>train_accuracy</td><td>0.73525</td></tr><tr><td>train_loss</td><td>0.73335</td></tr><tr><td>validation_accuracy</td><td>0.68274</td></tr><tr><td>validation_loss</td><td>0.8025</td></tr><tr><td>weight_decay</td><td>0.21247</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/g05g3m24' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/g05g3m24</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_051220-g05g3m24/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250504_052333-jvosp26x</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/jvosp26x' target=\"_blank\">fold_average</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/jvosp26x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/jvosp26x</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_train_accuracy</td><td>▁</td></tr><tr><td>avg_train_loss</td><td>▁</td></tr><tr><td>avg_val_accuracy</td><td>▁</td></tr><tr><td>avg_val_loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_train_accuracy</td><td>0.69051</td></tr><tr><td>avg_train_loss</td><td>0.79304</td></tr><tr><td>avg_val_accuracy</td><td>0.71234</td></tr><tr><td>avg_val_loss</td><td>0.78999</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fold_average</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/jvosp26x' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14/runs/jvosp26x</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-5fold-cv-14</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250504_052333-jvosp26x/logs</code>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}