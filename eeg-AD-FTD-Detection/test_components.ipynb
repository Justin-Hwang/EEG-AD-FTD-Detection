{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1DCNN Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ODCM(nn.Module):\n",
    "    def __init__(self, input_channels = 19, kernel_size = 10, dtype=torch.float32):\n",
    "        super(ODCM, self).__init__()\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size  # 1 X 10\n",
    "        self.ncf = 120  # The number of the depth-wise convolutional filter used in the three layers is set to 120\n",
    "        self.dtype = dtype\n",
    "        self.cvf1 = nn.Conv1d(in_channels=self.input_channels, out_channels=self.input_channels, kernel_size=self.kernel_size, padding='valid', stride=1, groups=self.input_channels, dtype=self.dtype)\n",
    "        self.cvf2 = nn.Conv1d(in_channels=self.cvf1.out_channels, out_channels=self.cvf1.out_channels, kernel_size=self.kernel_size, padding='valid', stride=1, groups=self.cvf1.out_channels, dtype=self.dtype)\n",
    "        # For each channel (19 channels), has 120 independent features due to the depthwise convolution in third layer\n",
    "        # 채널 한 개당 120개의 filter 를 할당해서 feature 를 뽑아낸다\n",
    "        self.cvf3 = nn.Conv1d(in_channels=self.cvf2.out_channels, out_channels=self.ncf * self.cvf2.out_channels, kernel_size=self.kernel_size, padding='valid', stride=1, groups=self.cvf2.out_channels, dtype=self.dtype)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cvf1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.cvf2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.cvf3(x)\n",
    "        x = self.relu(x)\n",
    "        # x = torch.reshape(x, ((int)(x.shape[0] / self.ncf), self.ncf, (int)(x.shape[1])))\n",
    "        # Dimension: (Batch, 2280, L3) → (Batch, 120, 19, L3) → (Batch, 19, 120, L3)\n",
    "        # x = x.view(x.shape[0], self.ncf, self.input_channels, -1)  # (Batch, 120, 19, L3)\n",
    "        # x = x.permute(0, 2, 1, 3)  # (Batch, 19, 120, L3)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "import torch\n",
    "\n",
    "file_path = '/Users/hwangjeongho/Desktop/EEG Transformer/model-data/train/sub-001_eeg_chunk_1.set'\n",
    "\n",
    "raw = mne.io.read_raw_eeglab(file_path, preload=True)\n",
    "data = raw.get_data()  # shape: (19, 1425)\n",
    "\n",
    "eeg_tensor = torch.tensor(data, dtype=torch.float32).unsqueeze(0)  # shape: (1, 19, 1425)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 19, 1425])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eeg_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([1, 19, 120, 1398])\n"
     ]
    }
   ],
   "source": [
    "model = ODCM(input_channels=19, kernel_size=10)\n",
    "output = model(eeg_tensor)\n",
    "\n",
    "print(\"Output shape:\", output.shape)  # Expected: (1, 19, 120, Le)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Truncated Normal Distribution Function\n",
    "- For Positional Embedding (Initialization)\n",
    "- To initialize the Weight or Embedding Vector\n",
    "- To avoid too big weights or too small weights\n",
    "- To avoid exploding gradient / vanishing gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 안의 weight나 embedding vector를 초기화하는 함수\n",
    "- 훈련 시작 전에 좋은 초기값을 주기 위해 사용함.\n",
    "- 평균이 0이고 표준편차가 1인 정규분포에서 특정 범위 (-2, +2) Sigma 사이에서만 값이 나오게 하고 싶다\n",
    "- PyTorch 에서는 Truncated Normal 이 없기 때문에 돌아서 작업해야한다.\n",
    "- 정규분포를 직접 뽑기 어려우니, 먼저 Uniform 분포 (0~1)에서 뽑은 다음, 그걸 정규분포의 형태로 바꿔서 쓰자\n",
    "\n",
    "\n",
    "Step 1] 원하는 정규분포 범위 [a, b] → 확률 구간 [l, u]\n",
    "\n",
    "Step 2] [l, u] → [-1, 1] 매핑 → uniform 값 생성\n",
    "\n",
    "Step 3] uniform → 정규분포로 변환 (inverse CDF = erfinv)\n",
    "\n",
    "Step 4] 평균/표준편차 맞춤 → 정규분포 값 완성\n",
    "\n",
    "Step 5] 너무 큰 값은 clamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def trunc_normal(tensor, mean=0., std=1., a=-2., b=2.):  # for positional embedding - borrowed from Meta\n",
    "    def norm_cdf(x):  # Computes standard normal cumulative distribution function\n",
    "        return (1. + math.erf(x / math.sqrt(2.))) / 2.\n",
    "\n",
    "    if (mean < a - 2 * std) or (mean > b + 2 * std):\n",
    "        print(\"mean is more than 2 std from [a, b] in nn.init.trunc_normal\\nThe distribution of values may be incorrect.\")\n",
    "\n",
    "    with torch.no_grad():  # Values are generated by using a truncated uniform distribution and then using the inverse CDF for the normal distribution.\n",
    "        # Get upper and lower cdf values\n",
    "        l = norm_cdf((a - mean) / std)\n",
    "        u = norm_cdf((b - mean) / std)\n",
    "\n",
    "        # Uniformly fill tensor with values from [l, u], then translate to [2l-1, 2u-1].\n",
    "        tensor.uniform_(2 * l - 1, 2 * u - 1)\n",
    "\n",
    "        # Use inverse cdf transform for normal distribution to get truncated standard normal\n",
    "        tensor.erfinv_()\n",
    "\n",
    "        # Transform to proper mean, std\n",
    "        tensor.mul_(std * math.sqrt(2.))\n",
    "        tensor.add_(mean)\n",
    "\n",
    "        # Clamp to ensure it's in the proper range\n",
    "        tensor.clamp_(min=a, max=b)\n",
    "        return tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Mlp(nn.Module): # Multilayer perceptron\n",
    "    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0., dtype=torch.float32):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.fc1 = nn.Linear(in_features, hidden_features, dtype=dtype)\n",
    "        self.act = act_layer()\n",
    "        self.fc2 = nn.Linear(hidden_features, out_features, dtype=dtype)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.act(x)\n",
    "        x = self.drop(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.drop(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention + FeedForward Network (= Generic Transformer Block)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericTFB(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, dtype):\n",
    "        super(GenericTFB, self).__init__()\n",
    "\n",
    "        self.M_size1 = emb_size  # -> D (Dimension of Feature Vector)\n",
    "        self.dtype = dtype\n",
    "        self.hA = num_heads  # number of multi-head self-attention units (A is the number of units in a block)\n",
    "        self.Dh = int(self.M_size1 / self.hA)  # Dh is the quotient computed by D/A and denotes the dimension number of three vectors.\n",
    "        self.Wqkv = nn.Parameter(torch.randn((3, self.hA, self.Dh, self.M_size1), dtype=self.dtype)) # 3 (=Q,K,V), # of Head, Dimension of each Head, Total embedding dimension  \n",
    "        self.Wo = nn.Parameter(torch.randn(self.M_size1, self.M_size1, dtype=self.dtype))\n",
    "\n",
    "        self.lnorm = nn.LayerNorm(self.M_size1, dtype=self.dtype)  # LayerNorm operation for dimension D\n",
    "        self.lnormz = nn.LayerNorm(self.M_size1, dtype=self.dtype)  # LayerNorm operation for z\n",
    "        self.mlp = Mlp(in_features=self.M_size1, hidden_features=int(self.M_size1 * 4), act_layer=nn.GELU, dtype=self.dtype)  # mlp_ratio=4\n",
    "\n",
    "    def forward(self, x, savespace):\n",
    "        # x.shape[2] = S \n",
    "        # x.shape[0] + 1 = C + Classification Token (=1)\n",
    "        qkvspace = torch.zeros(3, x.shape[2], x.shape[0] + 1, self.hA, self.Dh, dtype=self.dtype).to(device)  # To store Q, K, V space\n",
    "        atspace = torch.zeros(x.shape[2], self.hA, x.shape[0] + 1, x.shape[0] + 1, dtype=self.dtype).to(device) # To store Attention Score Matrix\n",
    "        imv = torch.zeros(x.shape[2], x.shape[0] + 1, self.hA, self.Dh, dtype=self.dtype).to(device) # To store Attention Output Vector\n",
    "\n",
    "        # Calculation; shape =  (3, S, C+1, hA, Dh)\n",
    "        qkvspace = torch.einsum('xhdm,ijm -> xijhd', self.Wqkv, self.lnorm(savespace))\n",
    "\n",
    "        # - Attention score\n",
    "        # - Query: (S, C+1, hA, Dh) -> (S, hA, C+1, Dh)\n",
    "        # - Key: (S, C+1, hA, Dh) -> (S, hA, Dh, C+1)\n",
    "        # - QK^T: (S, hA, C+1, C+1)\n",
    "        atspace = (qkvspace[0].clone().transpose(1, 2) / math.sqrt(self.Dh)) @ qkvspace[1].clone().transpose(1,2).transpose(-2, -1)\n",
    "\n",
    "        # - Intermediate vectors\n",
    "        # - Value: (S, C+1, hA, Dh) -> (S, hA, C+1, Dh)\n",
    "        # - (QK^T)V: (S, hA, C+1, Dh)\n",
    "        # - After Transpose: (S, C+1, hA, Dh)\n",
    "        imv = (atspace.clone() @ qkvspace[2].clone().transpose(1, 2)).transpose(1, 2)\n",
    "\n",
    "        # - NOW SAY HELLO TO NEW Z!\n",
    "        # - imv reshape: (S, C+1, hA, Dh) -> (S, C+1, D)\n",
    "        # - W0 shape: (D, D)\n",
    "        # - Linear Projection to W0 to organize each information from heads\n",
    "        # - Result: (S, C+1, D)\n",
    "        # - Add Residual Connection\n",
    "        savespace = torch.einsum('nm,ijm -> ijn', self.Wo, imv.clone().reshape(x.shape[2], x.shape[0] + 1, self.M_size1)) + savespace  # z'\n",
    "\n",
    "        # - normalized by LN() and passed through a multilayer perceptron (MLP)\n",
    "        savespace = self.mlp(self.lnormz(savespace)) + savespace  # new z\n",
    "\n",
    "        return savespace\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal Transformer Block (Multi-Head Self-Attention + Feed Forward Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalTFB(nn.Module):\n",
    "    def __init__(self, emb_size, num_heads, avgf, dtype):\n",
    "        super(TemporalTFB, self).__init__()\n",
    "\n",
    "        self.avgf = avgf  # average factor (M)\n",
    "        self.M_size1 = emb_size  # Feature Vector -> D\n",
    "        self.dtype = dtype\n",
    "        self.hA = num_heads  # number of multi-head self-attention units (A is the number of units in a block)\n",
    "        self.Dh = int(self.M_size1 / self.hA)  # Dh is the quotient computed by D/A and denotes the dimension number of three vectors.\n",
    "        self.Wqkv = nn.Parameter(torch.randn((3, self.hA, self.Dh, self.M_size1), dtype=self.dtype)) # (3, hA, Dh, D)\n",
    "        self.Wo = nn.Parameter(torch.randn(self.M_size1, self.M_size1, dtype=self.dtype)) # (D, D)\n",
    "\n",
    "        self.lnorm = nn.LayerNorm(self.M_size1, dtype=self.dtype)  # LayerNorm operation for dimension D\n",
    "        self.lnormz = nn.LayerNorm(self.M_size1, dtype=self.dtype)  # LayerNorm operation for z\n",
    "        self.mlp = Mlp(in_features=self.M_size1, hidden_features=int(self.M_size1 * 4), act_layer=nn.GELU, dtype=self.dtype)  # mlp_ratio=4\n",
    "\n",
    "    def forward(self, x, savespace):\n",
    "        # Q, K, V = (3, M+1, hA, Dh)\n",
    "        qkvspace = torch.zeros(3, self.avgf + 1, self.hA, self.Dh, dtype=self.dtype).to(device)  # Q, K, V\n",
    "        # attention space = (hA, M+1, M+1)\n",
    "        atspace = torch.zeros(self.hA, self.avgf + 1, self.avgf + 1, dtype=self.dtype).to(device)\n",
    "        # intermediate vector space = (M+1, hA, Dh)\n",
    "        imv = torch.zeros(self.avgf + 1, self.hA, self.Dh, dtype=self.dtype).to(device)\n",
    "\n",
    "        # (3, hA, Dh, D) x (M+1, D) -> (3, M+1 , hA, Dh) \n",
    "        qkvspace = torch.einsum('xhdm,im -> xihd', self.Wqkv, self.lnorm(savespace))  # Q, K, V\n",
    "\n",
    "        # - Attention score\n",
    "        # - Query: (hA, M+1, Dh)\n",
    "        # - Key: (hA, Dh, M+1)\n",
    "        # - QK^T: (hA, M+1, M+1)\n",
    "        atspace = (qkvspace[0].clone().transpose(0, 1) / math.sqrt(self.Dh)) @ qkvspace[1].clone().transpose(0, 1).transpose(-2, -1)\n",
    "\n",
    "        # - Intermediate vectors\n",
    "        # - Value: (M+1 , hA, Dh)\n",
    "        # - (QK^T)V: (hA, M+1, M+1) x (hA, M+1, Dh) = (hA, M+1, Dh)\n",
    "        # - Result: (M+1, hA, Dh)\n",
    "        imv = (atspace.clone() @ qkvspace[2].clone().transpose(0, 1)).transpose(0, 1)\n",
    "\n",
    "        # - NOW SAY HELLO TO NEW Z!\n",
    "        # - (D, D) x (M+1, D) = (M+1, D)\n",
    "        savespace = torch.einsum('nm,im -> in', self.Wo, imv.clone().reshape(self.avgf + 1, self.M_size1)) + savespace  # z'\n",
    "\n",
    "        # - normalized by LN() and passed through a multilayer perceptron (MLP)\n",
    "        savespace = self.mlp(self.lnormz(savespace)) + savespace  # new z\n",
    "\n",
    "        return savespace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regional Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RTM(nn.Module):  # Regional transformer module\n",
    "    def __init__(self, input, num_blocks, num_heads, dtype):  # input -> S x C x D\n",
    "        super(RTM, self).__init__()\n",
    "        self.inputshape = input.transpose(0, 1).transpose(1, 2).shape  # C x D x S\n",
    "        self.M_size1 = self.inputshape[1]  # -> D\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.tK = num_blocks  # number of transformer blocks - K in the paper\n",
    "        self.hA = num_heads  # number of multi-head self-attention units (A is the number of units in a block)\n",
    "        self.Dh = int(self.M_size1 / self.hA)  # Dh is the quotient computed by D/A and denotes the dimension number of three vectors.\n",
    "\n",
    "        # Each head should have the same dimension of feature matrix\n",
    "        if self.M_size1 % self.hA != 0 or int(self.M_size1 / self.hA) == 0:\n",
    "            print(f\"ERROR 1 - RTM : self.Dh = {int(self.M_size1 / self.hA)} != {self.M_size1}/{self.hA} \\nTry with different num_heads\")\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(self.M_size1, self.inputshape[1], dtype=self.dtype)) # (D, D)\n",
    "        self.bias = nn.Parameter(torch.zeros(self.inputshape[2], self.inputshape[0] + 1, self.M_size1, dtype=self.dtype))  # S x C x D\n",
    "        self.cls = nn.Parameter(torch.zeros(self.inputshape[2], 1, self.M_size1, dtype=self.dtype)) # (S, 1, D)\n",
    "        trunc_normal(self.bias, std=.02)\n",
    "        trunc_normal(self.cls, std=.02)\n",
    "        self.tfb = nn.ModuleList([GenericTFB(self.M_size1, self.hA, self.dtype) for _ in range(self.tK)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.transpose(0, 1).transpose(1, 2)  # C x D x S\n",
    "\n",
    "        savespace = torch.zeros(x.shape[2], x.shape[0], self.M_size1, dtype=self.dtype).to(device)  # S x C x D\n",
    "        savespace = torch.einsum('lm,jmi -> ijl', self.weight, x) # (D, D) x (C, D, S) = (S, C, D)\n",
    "        savespace = torch.cat((self.cls, savespace), dim=1)  # ! -> S x (C+1) x D\n",
    "        savespace = torch.add(savespace, self.bias)  # z -> S x C x D\n",
    "\n",
    "        for tfb in self.tfb:\n",
    "            savespace = tfb(x, savespace)\n",
    "\n",
    "        return savespace  # S x C x D - z4 in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class STM(nn.Module):  # Synchronous transformer module\n",
    "    def __init__(self, input, num_blocks, num_heads, dtype):  # input -> # S x C x D\n",
    "        super(STM, self).__init__()\n",
    "        self.inputshape = input.transpose(1, 2).shape  # S x D x C (S x Le x C in the paper)\n",
    "        self.M_size1 = self.inputshape[1]  # -> D\n",
    "        self.dtype = dtype\n",
    "\n",
    "        self.tK = num_blocks  # number of transformer blocks - K in the paper\n",
    "        self.hA = num_heads  # number of multi-head self-attention units (A is the number of units in a block)\n",
    "        self.Dh = int(self.M_size1 / self.hA)  # Dh is the quotient computed by D/A and denotes the dimension number of three vectors.\n",
    "\n",
    "        if self.M_size1 % self.hA != 0 or int(self.M_size1 / self.hA) == 0:\n",
    "            print(f\"ERROR 2 - STM : self.Dh = {int(self.M_size1 / self.hA)} != {self.M_size1}/{self.hA} \\nTry with different num_heads\")\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(self.M_size1, self.inputshape[1], dtype=self.dtype)) # (D x D)\n",
    "        self.bias = nn.Parameter(torch.zeros(self.inputshape[2], self.inputshape[0] + 1, self.M_size1, dtype=self.dtype)) # (C, S+1, D)\n",
    "        self.cls = nn.Parameter(torch.zeros(self.inputshape[2], 1, self.M_size1, dtype=self.dtype)) # (C, 1, D)\n",
    "        trunc_normal(self.bias, std=.02)\n",
    "        trunc_normal(self.cls, std=.02)\n",
    "        self.tfb = nn.ModuleList([GenericTFB(self.M_size1, self.hA, self.dtype) for _ in range(self.tK)])\n",
    "\n",
    "    def forward(self, x):  # S x C x D -> x\n",
    "        x = x.transpose(1, 2)  # S x D x C\n",
    "\n",
    "        savespace = torch.zeros(x.shape[2], x.shape[0] + 1, self.M_size1, dtype=self.dtype).to(device)  # (C x S+1 x D)\n",
    "        savespace = torch.einsum('lm,jmi -> ijl', self.weight, x) # (D x D) x (S x D x C) = (C x S x D)\n",
    "        savespace = torch.cat((self.cls, savespace), dim=1)  # (C x S+1 x D)\n",
    "        savespace = torch.add(savespace, self.bias)  # z -> (C x S+1 x D)\n",
    "\n",
    "        for tfb in self.tfb:\n",
    "            savespace = tfb(x, savespace)\n",
    "\n",
    "        return savespace  # C x S x D - z5 in the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Temporal Transformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTM(nn.Module):  # Temporal transformer module\n",
    "    def __init__(self, input, num_submatrices, num_blocks, num_heads, dtype):  # input -> # C x S x D\n",
    "        super(TTM, self).__init__()\n",
    "        self.dtype = dtype\n",
    "        self.avgf = num_submatrices  # average factor (M)\n",
    "        self.input = input.transpose(0, 2)  # D x S x C\n",
    "        self.seg = self.input.shape[0] / self.avgf\n",
    "\n",
    "        if self.input.shape[0] % self.avgf != 0 or int(self.input.shape[0] / self.avgf) == 0:\n",
    "            print(f\"ERROR 3 - TTM : self.seg = {self.seg} != {self.input.shape[0]}/{self.avgf}\")\n",
    "\n",
    "        self.M_size1 = self.input.shape[1] * self.input.shape[2] # Submatrix is flattened to a 1D vector (= L1) = S x C = D\n",
    "        self.tK = num_blocks  # number of transformer blocks - K in the paper\n",
    "        self.hA = num_heads  # number of multi-head self-attention units (A is the number of units in a block)\n",
    "        self.Dh = int(self.M_size1 / self.hA)\n",
    "\n",
    "        if self.M_size1 % self.hA != 0 or int(self.M_size1 / self.hA) == 0:  # - Dh = 121*(S+1) / num_heads\n",
    "            print(f\"ERROR 4 - TTM : self.Dh = {int(self.M_size1 / self.hA)} != {self.M_size1}/{self.hA} \\nTry with different num_heads\")\n",
    "\n",
    "        self.weight = nn.Parameter(torch.randn(self.M_size1, self.input.shape[1] * self.input.shape[2], dtype=self.dtype)) # (D, D)\n",
    "        self.bias = nn.Parameter(torch.zeros(self.avgf + 1, self.M_size1, dtype=self.dtype)) # (M+1, D)\n",
    "        self.cls = nn.Parameter(torch.zeros(1, self.M_size1, dtype=self.dtype)) # (1, D)\n",
    "        trunc_normal(self.bias, std=.02)\n",
    "        trunc_normal(self.cls, std=.02)\n",
    "        self.tfb = nn.ModuleList([TemporalTFB(self.M_size1, self.hA, self.avgf, self.dtype) for _ in range(self.tK)])\n",
    "\n",
    "        self.lnorm_extra = nn.LayerNorm(self.M_size1, dtype=self.dtype)  # EXPERIMENTAL\n",
    "\n",
    "    def forward(self, x):\n",
    "        input = x.transpose(0, 2)  # D x S x C\n",
    "        inputc = torch.zeros(self.avgf, input.shape[1], input.shape[2], dtype=self.dtype).to(device)  # M x S x C\n",
    "        for i in range(0, self.avgf):  # each i consists self.input.shape[0]/avgf\n",
    "            for j in range(int(i * self.seg), int((i + 1) * self.seg)):  # int(i*self.seg), int((i+1)*self.seg)\n",
    "                inputc[i, :, :] = inputc[i, :, :] + input[j, :, :]\n",
    "            inputc[i, :, :] = inputc[i, :, :] / self.seg\n",
    "\n",
    "        altx = inputc.reshape(self.avgf, input.shape[1] * input.shape[2]).to(device)  # M x L -> M x (S*C)\n",
    "\n",
    "        savespace = torch.zeros(self.avgf, self.M_size1, dtype=self.dtype).to(device)  # M x D\n",
    "        savespace = torch.einsum('lm,im -> il', self.weight, altx.clone()) # (D, D) x (M, D) = (M, D)\n",
    "        savespace = torch.cat((self.cls, savespace), dim=0) # (M+1, D)\n",
    "        savespace = torch.add(savespace, self.bias)  # z -> (M+1 x D)\n",
    "\n",
    "        for tfb in self.tfb:\n",
    "            savespace = tfb(x, savespace)\n",
    "\n",
    "        savespace = self.lnorm_extra(savespace)  # EXPERIMENTAL\n",
    "        return savespace.reshape(self.avgf + 1, input.shape[1], input.shape[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNdecoder(nn.Module):  # EEGformer decoder\n",
    "    def __init__(self, input, num_cls, CF_second, dtype):  # input -> # M x S x C\n",
    "        super(CNNdecoder, self).__init__()\n",
    "        self.input = input.transpose(0, 1).transpose(1, 2)  # S x C x M\n",
    "        self.s = self.input.shape[0]  # S\n",
    "        self.c = self.input.shape[1]  # C\n",
    "        self.m = self.input.shape[2]  # M\n",
    "        self.n = CF_second\n",
    "        self.dtype = dtype\n",
    "        self.cvd1 = nn.Conv1d(in_channels=self.c, out_channels=1, kernel_size=1, dtype=self.dtype)  # S x M\n",
    "        self.cvd2 = nn.Conv1d(in_channels=self.s, out_channels=self.n, kernel_size=1, dtype=self.dtype)\n",
    "        self.cvd3 = nn.Conv1d(in_channels=self.m, out_channels=int(self.m / 2), kernel_size=1, dtype=self.dtype)\n",
    "        self.fc = nn.Linear(int(self.m / 2) * self.n, num_cls, dtype=self.dtype)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):  # x -> M x S x C\n",
    "        x = x.transpose(0, 1).transpose(1, 2)  # S x C x M\n",
    "        x = self.cvd1(x)  # S x M\n",
    "        x = self.relu(x)\n",
    "        x = x[:, 0, :] # can be replaced with x.squeeze(x,1) in torch 2.0 or higher\n",
    "        x = self.cvd2(x).transpose(0, 1)  # N x M transposed to M x N\n",
    "        x = self.relu(x)\n",
    "        x = self.cvd3(x)  # M/2 x N\n",
    "        x = self.relu(x)\n",
    "        x = self.fc(x.reshape(1, x.shape[0] * x.shape[1]))\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EEGformer Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGformer(nn.Module):\n",
    "    def __init__(self, input, num_cls, input_channels, kernel_size, num_blocks, num_heads_RTM, num_heads_STM, num_heads_TTM, num_submatrices, CF_second, dtype=torch.float32):\n",
    "        super(EEGformer, self).__init__()\n",
    "        self.dtype = dtype\n",
    "        self.ncf = 120\n",
    "        self.num_cls = num_cls\n",
    "        self.input_channels = input_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.tK = num_blocks\n",
    "        self.hA_rtm = num_heads_RTM\n",
    "        self.hA_stm = num_heads_STM\n",
    "        self.hA_ttm = num_heads_TTM\n",
    "        self.avgf = num_submatrices\n",
    "        self.cfs = CF_second\n",
    "\n",
    "        self.outshape1 = torch.zeros(self.input_channels, self.ncf, input.shape[0] - 3 * (self.kernel_size - 1)).to(device) # (S, C, Le) | 3 layer depthwise convolution\n",
    "        self.outshape2 = torch.zeros(self.outshape1.shape[0], self.outshape1.shape[1] + 1, self.outshape1.shape[2]).to(device) # (S, C+1, Le)\n",
    "        self.outshape3 = torch.zeros(self.outshape2.shape[1], self.outshape2.shape[0] + 1, self.outshape2.shape[2]).to(device) # (C+1, S+1, Le)\n",
    "        self.outshape4 = torch.zeros(self.avgf + 1, self.outshape3.shape[1], self.outshape3.shape[0]).to(device) # (M+1, S+1, C+1)\n",
    "\n",
    "        self.odcm = ODCM(input_channels, self.kernel_size, self.dtype) # (C, T) -> (S x C x Le)\n",
    "        self.rtm = RTM(self.outshape1, self.tK, self.hA_rtm, self.dtype) # (C x Le x S) -> (S x C x D)\n",
    "        self.stm = STM(self.outshape2, self.tK, self.hA_stm, self.dtype) # (S x C x D) -> (S x Le x C) -> (C x S x D)\n",
    "        self.ttm = TTM(self.outshape3, self.avgf, self.tK, self.hA_ttm, self.dtype) # (C x S x D) -> (M x S x C) -> (M x L1)\n",
    "        self.cnndecoder = CNNdecoder(self.outshape4, self.num_cls, self.cfs, self.dtype) # (M x L1) -> (M/2 x N)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.odcm(x.transpose(0, 1))\n",
    "        x = self.rtm(x)\n",
    "        x = self.stm(x)\n",
    "        x = self.ttm(x)\n",
    "        x = self.cnndecoder(x)\n",
    "\n",
    "        return torch.softmax(x, dim=1)\n",
    "\n",
    "    # Cross Entropy Loss (CE)\n",
    "    # CE - uses one hot encoded label or similar(such as multi class probability label)\n",
    "    def eegloss(self, xf, label, L1_reg_const):  # CE Loss with L1 regularization\n",
    "        wt = self.sa(self.cnndecoder.fc.weight) + self.sa(self.cnndecoder.cvd1.weight) + self.sa(self.cnndecoder.cvd2.weight) + self.sa(self.cnndecoder.cvd3.weight)\n",
    "        wt += self.sa(self.ttm.mlp.fc1.weight) + self.sa(self.ttm.mlp.fc2.weight) + self.sa(self.ttm.lnorm.weight) + self.sa(self.ttm.lnormz.weight) + self.sa(self.ttm.weight)\n",
    "        wt += self.sa(self.stm.mlp.fc1.weight) + self.sa(self.stm.mlp.fc2.weight) + self.sa(self.stm.lnorm.weight) + self.sa(self.stm.lnormz.weight) + self.sa(self.stm.weight)\n",
    "        wt += self.sa(self.rtm.mlp.fc1.weight) + self.sa(self.rtm.mlp.fc2.weight) + self.sa(self.rtm.lnorm.weight) + self.sa(self.rtm.lnormz.weight) + self.sa(self.rtm.weight)\n",
    "        wt += self.sa(self.odcm.cvf1.weight) + self.sa(self.odcm.cvf2.weight) + self.sa(self.odcm.cvf3.weight)\n",
    "\n",
    "        for tfb in self.rtm.tfb:\n",
    "            wt += self.sa(tfb.Wo) + self.sa(tfb.Wqkv)\n",
    "        for tfb in self.stm.tfb:\n",
    "            wt += self.sa(tfb.Wo) + self.sa(tfb.Wqkv)\n",
    "        for tfb in self.ttm.tfb:\n",
    "            wt += self.sa(tfb.Wo) + self.sa(tfb.Wqkv)\n",
    "\n",
    "        ls = -(label * torch.log(xf) + (1 - label) * torch.log(1 - xf)) # xf = softmax output\n",
    "        ls = torch.mean(ls) + L1_reg_const * wt\n",
    "\n",
    "        return ls\n",
    "\n",
    "    # Apply L1 Regularization to CNN Decoder only\n",
    "    def eegloss_light(self, xf, label, L1_reg_const):  # takes the weight sum of cnndecoder only\n",
    "        wt = self.sa(self.cnndecoder.fc.weight) + self.sa(self.cnndecoder.cvd1.weight) + self.sa(self.cnndecoder.cvd2.weight) + self.sa(self.cnndecoder.cvd3.weight)\n",
    "        ls = -(label * torch.log(xf) + (1 - label) * torch.log(1 - xf))\n",
    "        ls = torch.mean(ls) + L1_reg_const * wt\n",
    "        return ls\n",
    "\n",
    "    def eegloss_wol1(self, xf, label):  # without L1\n",
    "        ls = -(label * torch.log(xf) + (1 - label) * torch.log(1 - xf))\n",
    "        ls = torch.mean(ls)\n",
    "        return ls\n",
    "\n",
    "    # BCE - does not need one hot encoding\n",
    "    def bceloss(self, xf, label):  # BCE loss\n",
    "        ls = -(label * torch.log(xf[:, 1]) + (1 - label) * torch.log(xf[:, 0]))\n",
    "        ls = torch.mean(ls)\n",
    "        return ls\n",
    "\n",
    "    def bceloss_w(self, xf, label, numpos, numtot):  # Weighted BCE loss\n",
    "        w0 = numtot / (2 * (numtot - numpos))\n",
    "        w1 = numtot / (2 * numpos)\n",
    "        ls = -(w1 * label * torch.log(xf[:, 1]) + w0 * (1 - label) * torch.log(xf[:, 0]))\n",
    "        ls = torch.mean(ls)\n",
    "        return ls\n",
    "\n",
    "    # Sum of absolute values = L1 Norm\n",
    "    def sa(self, t):\n",
    "        return torch.sum(torch.abs(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "내 컴퓨터에서 사용할 수 있는 worker 수: 8\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"내 컴퓨터에서 사용할 수 있는 worker 수:\", os.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
