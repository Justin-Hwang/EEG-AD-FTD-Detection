{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyP9c3LL/HOPY6AaISMv8aU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Hwang/EEG-AD-FTD-Detection/blob/main/Initial_Model_Training_Sanity_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_K0PsifvBxzc",
        "outputId": "13eeb4dc-ae51-4d77-e25c-769b49cc4ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MyDrive 루트에 어떤 폴더들이 있는지 확인\n",
        "!ls /content/drive/MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdKSaPDxC924",
        "outputId": "80a81363-c93c-4449-ae65-98dab5083f8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'0405 Research report.gdoc'\n",
            "'2022 한화 글로벌 인턴십 최종 발표 자료.pptx'\n",
            "'2023 Spring Semester'\n",
            "'2024 Fall Lab Research'\n",
            "'2024 Fall Semester'\n",
            "'2024 Hanwha Global Internship'\n",
            "'2024 Resume'\n",
            "'2024 Spring Semester'\n",
            "'2024 생각정리.gdoc'\n",
            "'2024 신년 계획.gsheet'\n",
            " 2025_Lab_Research\n",
            "'2025 신년 계획.gsheet'\n",
            "'Act Utilitarianism.gdoc'\n",
            " A_LAB3_wireless_communication.gdoc\n",
            "'A_Lab7&8_Carrer_Selep_Hwang.gdoc'\n",
            "'Anheuser Busch final round interview.gdoc'\n",
            "'Anheuser-Busch ML Engineer Qualification.gdoc'\n",
            " Assignment2.zip\n",
            "'Assignment 4.gdoc'\n",
            "'Assignment 7 [Output Screenshots].gdoc'\n",
            " Assignments\n",
            "'Assignments 2.gdoc'\n",
            "'Berkeley Personal History.gdoc'\n",
            "'Big Data Final Project'\n",
            "'Big Data.zip'\n",
            "'B_Lab 2_Justin Hwang .gdoc'\n",
            "'Bondit Internship'\n",
            "'Bonus Point.gdoc'\n",
            "'Career development'\n",
            "'Coding Test'\n",
            "'Colab Notebooks'\n",
            "'Columbia University Office Hour.gdoc'\n",
            "'Columbia University Video Interview.gdoc'\n",
            "'Columbia Univ Interview Preparation Answer sheet.gdoc'\n",
            "'Copy of cover-letter-guide-and-samples.gdoc'\n",
            "'Copy of NYU Presentation (Bold) Official Template (1).gslides'\n",
            "'Copy of NYU Presentation (Bold) Official Template (2).gslides'\n",
            "'Copy of NYU Presentation (Bold) Official Template.gslides'\n",
            "'Cover Letter.gdoc'\n",
            "'# CPPCMake README.gdoc'\n",
            "'CS 2204_ Digital Logic - Lab 5.pdf'\n",
            "'CS-GY 6083 HW4.gdoc'\n",
            " CS-GY-6083-HW6.gdoc\n",
            "'Customer Interview.gform'\n",
            "'Customer Interview (Responses).gsheet'\n",
            "'CV_Justin (Jung Ho)  Hwang.pdf'\n",
            "'Data Analysis (1).zip'\n",
            "'Data Analysis.gdoc'\n",
            "'Data Analysis.zip'\n",
            "'DB Mini Project Option 3.gdoc'\n",
            "'DB Mini Project Presentation'\n",
            " DB_Project1.drawio\n",
            "'DB Script.gdoc'\n",
            "'Decompilation Research.gdoc'\n",
            "'DeepLearning HW1.ipynb'\n",
            "'Delieverable 1.gdoc'\n",
            "'Deliverable 1 (Justin Hwang).gdoc'\n",
            "'Deliverable 1 (아이디어).gdoc'\n",
            "'DET 시험 요령.gdoc'\n",
            " dfg.gdoc\n",
            "'Digital Logic Homework 6.gdoc'\n",
            "'Digital Logic 공부할 것.gdoc'\n",
            " dlf23_hw1_prob5.ipynb\n",
            "'Duo Lingo.gdoc'\n",
            "'ECE 4001 Assignment .gdoc'\n",
            "'ECE_4001 Homework 6 .gdoc'\n",
            "'ECE 4001: Homework 8, Justin Hwang.gdoc'\n",
            "'ECE-UG Research reports'\n",
            "'ECE-UY 4144 Lab3 Report.gdoc'\n",
            "'EE4144 Quiz 1a.pdf'\n",
            "'Electronics 1 class material (공부자료)'\n",
            "'Electronics1 Lab'\n",
            "'Embedded System'\n",
            "'Embedded System Homework 1.gdoc'\n",
            "'Embedded System Lecture.gdoc'\n",
            "'Energy Conversion HW_6_Q4 & Q5 - Bar chart 1.gsheet'\n",
            "'Energy Conversion HW_6_Q4 & Q5.gdoc'\n",
            "'Essay 1 practice.gdoc'\n",
            "'Essay 1 한국어 버젼.gdoc'\n",
            "'Ethics report.gdoc'\n",
            "'Feedback 반영할 것들.gdoc'\n",
            "'Final Draft.pdf'\n",
            "'Final presentation script for Cyber Security Internship.gdoc'\n",
            "'Final Project - Business Plan - Line chart 1.gsheet'\n",
            "'Final version.gdoc'\n",
            "'Financial Appeal.gdoc'\n",
            "'finding aid.gdoc'\n",
            "'Google Application'\n",
            "'Graduate Applications'\n",
            "'Gross Sales.gsheet'\n",
            "'Hanwha Remote Project'\n",
            "'Have to remeber for the Verilog Design.gdoc'\n",
            "'Homework 2_Hands on_Justin Hwang.gdoc'\n",
            " HW0\n",
            " hw0.ipynb\n",
            " HW5.Q5.gdoc\n",
            " image.png\n",
            " IMG_4636.mov\n",
            "'Internship apply.gdoc'\n",
            "'Internship final presentation.gdoc'\n",
            "'Internship Period Details.gdoc'\n",
            "'Interview Preparation'\n",
            " Introduction.gdoc\n",
            "'JUNG HO HWANG_CV.pdf'\n",
            " Jungho_Hwang_internship_agreement_form.pdf\n",
            "'Jung Ho Hwang_Offer Letter for CPT.pdf'\n",
            "'Jungho_Hwang_Resume (1).pdf'\n",
            "'Jung Ho Hwang Resume.docx'\n",
            " JungHoHwang_Resume__.docx\n",
            "'JungHoHwang_Resume_Final Version.gdoc'\n",
            " JungHoHwang_Resume_.gdoc\n",
            " Jungho_Hwang_Resume.pdf\n",
            "'JungHo Hwang_Resume.pdf'\n",
            " JungHoHwang_Resume_.pdf\n",
            " JungHoHwang_Resume__.pdf\n",
            "'Jung Ho Hwang Resume_pdf.pdf'\n",
            "'JungHoHwang_Resume_ softwareengineering.docx'\n",
            "'Jung Ho Hwang Resume - TA (1).docx'\n",
            "'Jung Ho Hwang Resume - TA (2).docx'\n",
            "'Jung Ho Hwang Resume - TA (3).docx'\n",
            "'Jung Ho Hwang Resume - TA.docx'\n",
            " Jungho_Hwang_unofficial_transcript.pdf\n",
            "'Justin Hwang'\n",
            "'Justin Hwang HW5 submission.MOV'\n",
            "'Justin Hwang_Weekly Report (05 24).gdoc'\n",
            "\"Justin's Photo.jpeg\"\n",
            "'Korea tour recommendation.gdoc'\n",
            "'Lab 11.gdoc'\n",
            "'LAB 11.gdoc'\n",
            "'LAB 1 report.gdoc'\n",
            "'Lab #2 ECE-UY 4144 .gdoc'\n",
            "'Lab 2 Energy Conversion.gdoc'\n",
            "'Lab 2 Energy Conversion - Line chart 1.gsheet'\n",
            "'Lab 2 Energy Conversion - Line chart 2.gsheet'\n",
            "'Lab 2 Energy Conversion - Line chart 3.gsheet'\n",
            "'LAB 2 : MATLAB.gdoc'\n",
            "'Lab 2_myself.gdoc'\n",
            "'Lab 2 Wireless Communication justin hwang.docx'\n",
            "'Lab 3_Energy Conversion.gdoc'\n",
            "'Lab 3_Energy Conversion - Line chart 1.gsheet'\n",
            "'Lab 3_Energy Conversion - Line chart 3.gsheet'\n",
            "'Lab 3_Energy Conversion - Line chart 5.gsheet'\n",
            "'Lab 3_Energy Conversion - Line chart 6.gsheet'\n",
            "'Lab 3_Energy Conversion - Line chart 7 (1).gsheet'\n",
            "'Lab 3_Energy Conversion - Line chart 7.gsheet'\n",
            "'Lab 3 wireless.gdoc'\n",
            "'Lab_4 Energy Conversion.gdoc'\n",
            "'Lab_4 Energy Conversion - Line chart 1.gsheet'\n",
            "'lab6 digital logic.gdoc'\n",
            "'Lab_6 Submission'\n",
            " Lab_6.zip\n",
            "'Lab Assignment 1.gdoc'\n",
            "'Lab Assignment 4_Group D4_Video Submission'\n",
            "'Lab contact script.gdoc'\n",
            "'Lab Report 1_Energy Conversion_Justin Hwang.gdoc'\n",
            "'Lab Report 1_Energy Conversion_Justin Hwang - Line chart 1.gsheet'\n",
            "'Lab Report 1_Energy Conversion_Justin Hwang - Line chart 2.gsheet'\n",
            "'Lab Report 1_Energy Conversion_Justin Hwang - Line chart 3.gsheet'\n",
            "'Lab Report 1_Energy Conversion_Justin Hwang - Line chart 4.gsheet'\n",
            "'Lab Report Energy Conversion_Justin Hwang.gdoc'\n",
            "'Lab report share file.gdoc'\n",
            "'LG AI Research'\n",
            " LG_AI_Research_2.ipynb\n",
            " LG_AI_Research_CodingTest.ipynb\n",
            "'LG AI Research Summer Internship'\n",
            "'LG AI Research 개인 서류 다운받았던 것들'\n",
            "'LG CNS 면접 준비.gdoc'\n",
            "'Machine Learning Engineer Role Summary.gdoc'\n",
            "'Machine Learning Final Project'\n",
            "'Machine Learning Project.gslides'\n",
            "'Machine Learning 공부'\n",
            "'MATLAB ECE Undergraduate Research Report.gdoc'\n",
            "'MATLAB 정리.gdoc'\n",
            "'Mini Assignment 4.gdoc'\n",
            "'Mini Assignment 5.gdoc'\n",
            " MINI_PROJECT_FALL24\n",
            " model-data.zip\n",
            "'Morgan Stanley Interview Prep.gdoc'\n",
            "'My SQL 공부'\n",
            "'Network Protocol 정리.gdoc'\n",
            "'Neural Network.xlsx'\n",
            "'NYU 2022 Fall semester'\n",
            "'NYU Fall 2023'\n",
            " NYUID_Backside.jpeg\n",
            " NYUID_Frontside.jpeg\n",
            "'NYU SOP Final Version.gdoc'\n",
            "'NYU Statement of Purpose ph.D application.gdoc'\n",
            "'NYU Undergraduate Research'\n",
            "'NYU Unoffical transcript.pdf'\n",
            "'NYU Unofficial Transcript (1).pdf'\n",
            "'NYU Unofficial Transcript_2023 (1).pdf'\n",
            "'NYU Unofficial Transcript_2023 (2).pdf'\n",
            "'NYU Unofficial Transcript_2023 (3).pdf'\n",
            "'NYU Unofficial Transcript_2023 (4).pdf'\n",
            "'NYU Unofficial Transcript_2023 (5).pdf'\n",
            "'NYU Unofficial Transcript_2023.pdf'\n",
            "'NYU Unofficial Transcript.pdf'\n",
            "'OPT or CPT.gdoc'\n",
            "'OPT Visa.gdoc'\n",
            " Paperpile\n",
            "'PDR work (Justin Hwang).gdoc'\n",
            "'Personal Statement.gdoc'\n",
            "'PH-UY 2121 Lab2.gdoc'\n",
            "'PHY-2121 Lab Report'\n",
            " Project_Part2_A.drawio\n",
            " Project_Part2_A.drawio.pdf\n",
            "'Project Report.gdoc'\n",
            "'Purpose and Prototype of our product.gdoc'\n",
            "'Question List.gdoc'\n",
            " Resume_Justin_Hwang.pdf\n",
            " Resume.pdf\n",
            "'Resume 직무별'\n",
            "'Resume 참고용'\n",
            "'Revision that I did.gdoc'\n",
            "'rogue aerospace reciept.png'\n",
            "'Rough Draft.gdoc'\n",
            " Script.gdoc\n",
            "'Self Introduction.gdoc'\n",
            "'share folder.gdoc'\n",
            "'Sig Video Slides - Line chart 1 (1).gsheet'\n",
            "'Sig Video Slides - Line chart 1.gsheet'\n",
            "'Silencing the past: power and the production of history.gdoc'\n",
            "'Statement of Purpose: Justin Hwang.gdoc'\n",
            " students_skills.csv\n",
            "'SUBLEASE AGREEMENT.gdoc'\n",
            "'Syllabus [2016-1st Semester] [T01107905] Introduction to Computer & Lab.gdoc'\n",
            "'Syllabus [2016-1st Semester] [T01107905] Introduction to Computer & Lab.pdf'\n",
            "'Syllabus [2016-1st Semester] [T01215301] Engineering Design Principles.pdf'\n",
            "'Syllabus [2016-1st Semester] [T04101601] Physics & Laboratory 1 (1).pdf'\n",
            "'Syllabus [2016-1st Semester] [T04101601] Physics & Laboratory 1.pdf'\n",
            "'Syllabus [2016-1st Semester] [U72149201] PC Hardware.pdf'\n",
            "'Syllabus [2016-1st Semester] [Y11114C01] Seminar for Freshman.pdf'\n",
            "'Syllabus [2016-1st Semester] [Y12101101] Minerva Liberal Arts Lecture 1_ Human Being and Civilization.pdf'\n",
            "'Syllabus [2016-1st Semester] [Y13101106] Communicative English 1.pdf'\n",
            "'Syllabus [2016-1st Semester] [Y55103101] Elementary Calculus 1.pdf'\n",
            "'Syllabus [2016-2nd Semester] [F05108607] Computer Programming & Lab.pdf'\n",
            "'Syllabus [2016-2nd Semester] [T04106701] Physics & Laboratory 2.gdoc'\n",
            "'Syllabus [2016-2nd Semester] [T04106701] Physics & Laboratory 2.pdf'\n",
            "'Syllabus [2016-2nd Semester] [T04107201] Digital Engineering.pdf'\n",
            "'Syllabus [2016-2nd Semester] [U51512202] Physical Education (Outdoor Activity and Leadership).pdf'\n",
            "'Syllabus [2016-2nd Semester] [U76240201] Language and Cognition.pdf'\n",
            "'Syllabus [2016-2nd Semester] [U76267201] Understanding of History of Korea.pdf'\n",
            "'Syllabus [2016-2nd Semester] [Y12102101] Minerva Liberal Arts Lecture 2_ Global Communication and Care.pdf'\n",
            "'Syllabus [2016-2nd Semester] [Y13102105] Communicative English 2.pdf'\n",
            "'Syllabus [2019-1st Semester][C02205501] Understanding Korean Government.pdf'\n",
            "'Syllabus [2019-1st Semester] [F02314201] Electromagnetics.pdf'\n",
            "'Syllabus [2019-1st Semester] [T01201502] Engineering Mathematics 1.pdf'\n",
            "'Syllabus [2019-1st Semester] [T03113102] Electrical Circuit & Lab.pdf'\n",
            "'Syllabus [2019-1st Semester] [T03202101] Data Structure & Algorithm.pdf'\n",
            "'Syllabus [2019-1st Semester] [T0321710] Basic Digital Design & Lab.pdf'\n",
            "'Syllabus [2019-1st Semester] [Y12103201] HUFS Career Vision Mentoring (Defining Career Path and Development).pdf'\n",
            "'Syllabus [2019-2nd Semester] [T01301301] Signals & Systems.pdf'\n",
            "'Syllabus [2019-2nd Semester] [T03373101] Physics Electronics.pdf'\n",
            "'Syllabus [2019-2nd Semester] [T04107401] Digital Engineering(Retake).pdf'\n",
            "'Syllabus [2019-2nd Semester] [U37119301] Introduction to East Asian History.pdf'\n",
            "'Syllabus [2019-2nd Semester] [U72149201] PC Hardware (Retake).pdf'\n",
            "'Syllabus [2019-2nd Semester] [Y55104101] Elementary Calculus 2.pdf'\n",
            "'Syllabus [2019-2nd Semester] [Y91106201] Circuit Theory.pdf'\n",
            "'Syllabus for credit evaluation'\n",
            "'TCS-UY 4504 Assignment 1.gdoc'\n",
            "'TCS-UY 4504 Assignment 2.gdoc'\n",
            "'Teensy 3.2 and recent version.gdoc'\n",
            "'UC Berkeley SOP.gdoc'\n",
            "'UMN_Math2374 - Syllabus.pdf'\n",
            "'Undergraduate Research (Justin Hwang).gdoc'\n",
            " Untitled0.ipynb\n",
            " Untitled1.ipynb\n",
            "'Untitled Diagram (1).drawio'\n",
            "'Untitled Diagram.drawio'\n",
            "'Untitled document (10).gdoc'\n",
            "'Untitled document (11).gdoc'\n",
            "'Untitled document (12).gdoc'\n",
            "'Untitled document (13).gdoc'\n",
            "'Untitled document (14).gdoc'\n",
            "'Untitled document (15).gdoc'\n",
            "'Untitled document (16).gdoc'\n",
            "'Untitled document (17).gdoc'\n",
            "'Untitled document (18).gdoc'\n",
            "'Untitled document (19).gdoc'\n",
            "'Untitled document (1).gdoc'\n",
            "'Untitled document (20).gdoc'\n",
            "'Untitled document (21).gdoc'\n",
            "'Untitled document (22).gdoc'\n",
            "'Untitled document (23).gdoc'\n",
            "'Untitled document (24).gdoc'\n",
            "'Untitled document (25).gdoc'\n",
            "'Untitled document (26).gdoc'\n",
            "'Untitled document (27).gdoc'\n",
            "'Untitled document (28).gdoc'\n",
            "'Untitled document (29).gdoc'\n",
            "'Untitled document (2).gdoc'\n",
            "'Untitled document (30).gdoc'\n",
            "'Untitled document (31).gdoc'\n",
            "'Untitled document (32).gdoc'\n",
            "'Untitled document (33).gdoc'\n",
            "'Untitled document (34).gdoc'\n",
            "'Untitled document (35).gdoc'\n",
            "'Untitled document (36).gdoc'\n",
            "'Untitled document (37).gdoc'\n",
            "'Untitled document (38).gdoc'\n",
            "'Untitled document (39).gdoc'\n",
            "'Untitled document (3).gdoc'\n",
            "'Untitled document (40).gdoc'\n",
            "'Untitled document (41).gdoc'\n",
            "'Untitled document (42).gdoc'\n",
            "'Untitled document (43).gdoc'\n",
            "'Untitled document (44).gdoc'\n",
            "'Untitled document (45).gdoc'\n",
            "'Untitled document (46).gdoc'\n",
            "'Untitled document (47).gdoc'\n",
            "'Untitled document (48).gdoc'\n",
            "'Untitled document (49).gdoc'\n",
            "'Untitled document (4).gdoc'\n",
            "'Untitled document (50).gdoc'\n",
            "'Untitled document (51).gdoc'\n",
            "'Untitled document (52).gdoc'\n",
            "'Untitled document (53).gdoc'\n",
            "'Untitled document (54).gdoc'\n",
            "'Untitled document (55).gdoc'\n",
            "'Untitled document (56).gdoc'\n",
            "'Untitled document (5).gdoc'\n",
            "'Untitled document (6).gdoc'\n",
            "'Untitled document (7).gdoc'\n",
            "'Untitled document (8).gdoc'\n",
            "'Untitled document (9).gdoc'\n",
            "'Untitled document - Column chart 1.gsheet'\n",
            "'Untitled document.gdoc'\n",
            "'Untitled document - Line chart 2 (1).gsheet'\n",
            "'Untitled document - Line chart 2.gsheet'\n",
            "'Untitled presentation.gslides'\n",
            "'Verilog Files'\n",
            "'VISA Questions: 여권 분실 및 Course Rergistration.gdoc'\n",
            "'Web Application (웹개발 정리)'\n",
            "'Week 1 VIP summary.gdoc'\n",
            "'Weekly Report 05 31.gdoc'\n",
            "'Weekly Report 06 07.gdoc'\n",
            "'Wireless Communication Lab'\n",
            "'Wireless Communication Project File'\n",
            "'Wireless Communication Project.gdoc'\n",
            "'Wireless Communication 자료'\n",
            "'Wireless Project'\n",
            "'Wireless Project MVP.gslides'\n",
            "'Wireless 개념정리'\n",
            "'과외 자료'\n",
            "'김종하 과외 시간.gsheet'\n",
            "'나의 첫 코랩.ipynb'\n",
            "'대학원 faculty 조사.gdoc'\n",
            "'독서 모음집'\n",
            "'동영상 12-19-24 오후 10.50.mov'\n",
            "'만나 DAO 발표 script.gdoc'\n",
            "'면접자한테 물어볼 질문.gdoc'\n",
            "'발표 스크립트.gdoc'\n",
            "'사본: SUBLEASE AGREEMENT.docx'\n",
            "'사본: SUBLEASE AGREEMENT.gdoc'\n",
            "'삼성바이오로직스 인턴 지원.gdoc'\n",
            "'삼성 자소서.gdoc'\n",
            "'삼성 자소서 참고.gdoc'\n",
            "'삼성전자 컨퍼런스.gdoc'\n",
            "'상속 행정처리'\n",
            "'생각날 때 끄적이는 글'\n",
            "'서블렛 짐.gdoc'\n",
            "'수상 내역.gdoc'\n",
            "'시장조사 - Line chart 1.gsheet'\n",
            "'우리은행 면접준비.gdoc'\n",
            "'제목 없는 다이어그램.drawio'\n",
            "'제주도 일정.gdoc'\n",
            "'종하 lab report - Line chart 1.gsheet'\n",
            "'줄리아 자소서.gdoc'\n",
            "'콜롬비아 인터뷰 준비.gdoc'\n",
            " 한국투자증권\n",
            "'한국투자증권 면접 준비.gdoc'\n",
            "'한화 금융 인턴 기록 일지.gdoc'\n",
            "'한화금융 인턴십 폴더'\n",
            "'한화 리모트 프로젝트'\n",
            "'한화 리모트 프로젝트 예선.pdf'\n",
            "'한화 리모트 프로젝트 예선 (수환 edit) - Line chart 1.gsheet'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그중에 2025 Lab Research 폴더 안을 확인\n",
        "!ls \"/content/drive/MyDrive/2025_Lab_Research\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_6j0Bv_DdxY",
        "outputId": "1427927c-9425-4e3f-dedd-ef08b68d33b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'Colab Files'\t\t\t  eeg_holdout.db\n",
            "'Data Preparation.gdoc'\t\t  eeg_holdout_fixed_1.db\n",
            " eeg_dataset.py\t\t\t  eeg_optuna_trial_1.db\n",
            " EEGformer_model_training.ipynb   eeg_optuna_trial_2.db\n",
            " eegformer_optuna_cv_3.db\t  eeg_optuna_trial_3.db\n",
            " eegformer_optuna_cv_4.db\t 'EEG Transformer Architecture.gdoc'\n",
            " eegformer_optuna_cv_5.db\t 'Lab Info'\n",
            " eeg_holdout-1.db\t\t 'Lab Research Paper Review'\n",
            " eeg_holdout-2.db\t\t 'Meeting Note.gdoc'\n",
            " eeg_holdout-3.db\t\t  model-data\n",
            " eeg_holdout-4.db\t\t  model-data.zip\n",
            " eeg_holdout-5.db\t\t  models_depracated.py\n",
            " eeg_holdout-6.db\t\t  models.py\n",
            " eeg_holdout-7.db\t\t  Practice_Note0.ipynb\n",
            " eeg_holdout-8.db\t\t  __pycache__\n",
            " eeg_holdout-9.db\t\t  Untitled\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/2025_Lab_Research')"
      ],
      "metadata": {
        "id": "ECSY18dbD-Io"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on\", DEVICE)  # → “cuda” 가 뜨면 GPU 정상"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oq0APRftFMPp",
        "outputId": "9e9da209-62ab-4236-a15c-2c859f0f9cb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()  # 첫 실행 시 API 키 입력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQzVSpioFTYV",
        "outputId": "39c32cdc-b4de-4555-ba47-6b937f418b83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjh8032\u001b[0m (\u001b[33mjh8032-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install wandb\n",
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgnbvhSD4KIR",
        "outputId": "1d9a4936-2e26-4e24-fb2e-2409b8cc16aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: mne in /usr/local/lib/python3.11/dist-packages (1.9.0)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Model Overfit Test"
      ],
      "metadata": {
        "id": "Tsl_NaKwYp78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "def main():\n",
        "    # ─── 설정 ──────────────────────────────────────────\n",
        "    DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "    LABEL_FILE  = \"labels.json\"\n",
        "    BATCH_SIZE  = 32\n",
        "    LR          = 1e-3\n",
        "    NUM_ITERS   = 100\n",
        "    KERNEL_SIZE = 10      # ODCM Kernel Size\n",
        "    NUM_FILTERS = 120     # ODCM Filter (C)\n",
        "    NUM_HEADS   = 4       # Transformer Heads\n",
        "    NUM_BLOCKS  = 2       # Transformer Blocks\n",
        "    NUM_SEGMENTS= 15      # TTM time segments (M)\n",
        "    NUM_CLASSES = 3       # Class\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "    # ─── Meta Data Load─────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "\n",
        "    # ─── Dataset & DataLoader ───────────────────────────\n",
        "    dataset    = EEGDataset(DATA_DIR, train_meta)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        num_workers=4,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # ─── Extract One Batch ─────────────────────────────────\n",
        "    X_small, y_small = next(iter(dataloader))\n",
        "    X_small, y_small = X_small.to(device), y_small.to(device)\n",
        "    B, S, L = X_small.shape  # Batch, Channels, Time-length\n",
        "\n",
        "    print(f\"Overfit Test Batch Shape: X_small={X_small.shape}, y_small={y_small.shape} on {device}\")\n",
        "\n",
        "    # ─── Model, Loss, Optimizer ────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = S,\n",
        "        input_length = L,\n",
        "        kernel_size  = KERNEL_SIZE,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = NUM_HEADS,\n",
        "        num_blocks   = NUM_BLOCKS,\n",
        "        num_segments = NUM_SEGMENTS,\n",
        "        num_classes  = NUM_CLASSES,\n",
        "    ).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "\n",
        "    # ─── Overfit Training Loop ─────────────────────────────────\n",
        "    model.train()\n",
        "    for i in range(1, NUM_ITERS+1):\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X_small)            # [BATCH_SIZE, NUM_CLASSES]\n",
        "        loss   = criterion(logits, y_small)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i == 1 or i % 10 == 0:\n",
        "            # 배치 정확도 계산\n",
        "            with torch.no_grad():\n",
        "                preds = logits.argmax(dim=1)\n",
        "                acc   = (preds == y_small).float().mean().item() * 100\n",
        "            print(f\"Iter {i:03d} | loss = {loss.item():.6f} | acc = {acc:5.2f}%\")\n",
        "\n",
        "    print(\"Finished overfit test.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0p9NHP5JYvbV",
        "outputId": "5533c7a6-d468-4a38-eec3-5b336cab376b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "Overfit Test Batch Shape: X_small=torch.Size([32, 19, 1425]), y_small=torch.Size([32]) on cuda\n",
            "Iter 001 | loss = 1.102822 | acc = 28.12%\n",
            "Iter 010 | loss = 1.007585 | acc = 53.12%\n",
            "Iter 020 | loss = 0.973565 | acc = 53.12%\n",
            "Iter 030 | loss = 0.077861 | acc = 100.00%\n",
            "Iter 040 | loss = 0.000408 | acc = 100.00%\n",
            "Iter 050 | loss = 0.000021 | acc = 100.00%\n",
            "Iter 060 | loss = 0.000009 | acc = 100.00%\n",
            "Iter 070 | loss = 0.000005 | acc = 100.00%\n",
            "Iter 080 | loss = 0.000003 | acc = 100.00%\n",
            "Iter 090 | loss = 0.000002 | acc = 100.00%\n",
            "Iter 100 | loss = 0.000002 | acc = 100.00%\n",
            "Finished overfit test.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search the best Hyperparameter using Hold-out set"
      ],
      "metadata": {
        "id": "uu7_E2GNCG-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with dropout rate = 0.1\n",
        "- Transformer Model: Revised the model to add a dropout parameter in self-attention head and transformer block\n",
        "- Change the Learning Rate Scheduler from LRstep to ReduceLROnPlateau for improving the validation loss decrease\n",
        "- Step Size = 5, gamma = 0.5 -> If validation loss does not decrease in 5 epochs, the LR will be decreased by half"
      ],
      "metadata": {
        "id": "LaWT2sWyo5VA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Implement Grid Search to find the search space"
      ],
      "metadata": {
        "id": "xWUyPCW2VgyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [3e-4, 4e-4, 5e-4]\n",
        "WD_CHOICES          = [5e-5, 5e-4, 1e-3]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [2]\n",
        "NUM_HEAD_CHOICES    = [3, 4]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler Hyperparameter ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "gJjg7ZDpSqbF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0d5143af-edec-4336-96d5-07c13658924c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 00:26:22,491] A new study created in RDB with name: eeg_holdout_grid_search\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_002622-047iissi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/047iissi' target=\"_blank\">woven-snow-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/047iissi' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/047iissi</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=3.00e-04, wd=5.00e-04, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0704 acc=0.4237 | val_loss=1.0744 acc=0.4317 | time=30.6s\n",
            "Epoch 002 | train_loss=1.0677 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=29.9s\n",
            "Epoch 003 | train_loss=1.0672 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=29.9s\n",
            "Epoch 004 | train_loss=1.0685 acc=0.4276 | val_loss=1.0817 acc=0.4317 | time=30.0s\n",
            "Epoch 005 | train_loss=1.0665 acc=0.4260 | val_loss=1.0804 acc=0.4317 | time=29.9s\n",
            "Epoch 006 | train_loss=1.0696 acc=0.4276 | val_loss=1.0754 acc=0.4317 | time=29.8s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=30.0s\n",
            "Epoch 008 | train_loss=1.0669 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=29.9s\n",
            "Epoch 009 | train_loss=1.0666 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.0s\n",
            "Epoch 010 | train_loss=1.0665 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.0s\n",
            "Epoch 011 | train_loss=1.0671 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.0s\n",
            "Epoch 012 | train_loss=1.0666 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 013 | train_loss=1.0669 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=29.9s\n",
            "Epoch 014 | train_loss=1.0661 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=29.9s\n",
            "Epoch 015 | train_loss=1.0655 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.2s\n",
            "Epoch 016 | train_loss=1.0664 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=29.8s\n",
            "Epoch 017 | train_loss=1.0665 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=29.9s\n",
            "Epoch 018 | train_loss=1.0660 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.0s\n",
            "Epoch 019 | train_loss=1.0661 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=29.7s\n",
            "Epoch 020 | train_loss=1.0658 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.0s\n",
            "Epoch 021 | train_loss=1.0662 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=29.9s\n",
            "Epoch 022 | train_loss=1.0669 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=29.8s\n",
            "Epoch 023 | train_loss=1.0654 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.0s\n",
            "Epoch 024 | train_loss=1.0671 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=29.9s\n",
            "Epoch 025 | train_loss=1.0660 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=29.9s\n",
            "Epoch 026 | train_loss=1.0666 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.1s\n",
            "Epoch 027 | train_loss=1.0658 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=29.9s\n",
            "Epoch 028 | train_loss=1.0660 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=29.9s\n",
            "Epoch 029 | train_loss=1.0665 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.0s\n",
            "Epoch 030 | train_loss=1.0664 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=29.8s\n",
            "Epoch 031 | train_loss=1.0660 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.0s\n",
            "Epoch 032 | train_loss=1.0660 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.0s\n",
            "Epoch 033 | train_loss=1.0658 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=29.8s\n",
            "Epoch 034 | train_loss=1.0660 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=29.9s\n",
            "Epoch 035 | train_loss=1.0657 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.0s\n",
            "Epoch 036 | train_loss=1.0664 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=29.8s\n",
            "Epoch 037 | train_loss=1.0658 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.0s\n",
            "★ Early stopping at epoch 37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁██▅▃▅███████████████████████████████</td></tr><tr><td>train_loss</td><td>█▄▃▅▃▇▄▃▃▃▃▃▃▂▁▂▃▂▂▂▂▃▁▃▂▃▂▂▃▂▂▂▂▂▁▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▂▃█▇▂▃▁▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▂▂▁▂▂▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06584</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07483</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">woven-snow-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/047iissi' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/047iissi</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_002622-047iissi/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 00:44:55,381] Trial 0 finished with value: 1.0743041322344826 and parameters: {'lr': 0.0003, 'weight_decay': 0.0005, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5}. Best is trial 0 with value: 1.0743041322344826.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_004455-2w4wuv9d</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2w4wuv9d' target=\"_blank\">trim-monkey-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2w4wuv9d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2w4wuv9d</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=4.00e-04, wd=5.00e-05, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0702 acc=0.4217 | val_loss=1.0841 acc=0.4317 | time=29.9s\n",
            "Epoch 002 | train_loss=1.0691 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=29.9s\n",
            "Epoch 003 | train_loss=1.0697 acc=0.4283 | val_loss=1.0795 acc=0.4317 | time=30.1s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4280 | val_loss=1.0785 acc=0.4317 | time=29.8s\n",
            "Epoch 005 | train_loss=1.0686 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=29.8s\n",
            "Epoch 006 | train_loss=1.0686 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=29.9s\n",
            "Epoch 007 | train_loss=1.0678 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=29.8s\n",
            "Epoch 008 | train_loss=1.0672 acc=0.4311 | val_loss=1.0767 acc=0.4317 | time=30.1s\n",
            "Epoch 009 | train_loss=1.0677 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.0s\n",
            "Epoch 010 | train_loss=1.0662 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=29.9s\n",
            "Epoch 011 | train_loss=1.0663 acc=0.4311 | val_loss=1.0721 acc=0.4317 | time=30.1s\n",
            "Epoch 012 | train_loss=1.0467 acc=0.4792 | val_loss=1.0064 acc=0.5466 | time=29.9s\n",
            "Epoch 013 | train_loss=0.9750 acc=0.5685 | val_loss=0.9840 acc=0.5668 | time=29.9s\n",
            "Epoch 014 | train_loss=0.9446 acc=0.5810 | val_loss=0.9516 acc=0.5776 | time=30.1s\n",
            "Epoch 015 | train_loss=0.9096 acc=0.6035 | val_loss=0.9644 acc=0.5761 | time=30.0s\n",
            "Epoch 016 | train_loss=0.9001 acc=0.6066 | val_loss=0.9468 acc=0.5823 | time=30.0s\n",
            "Epoch 017 | train_loss=0.8672 acc=0.6206 | val_loss=0.9405 acc=0.5730 | time=30.0s\n",
            "Epoch 018 | train_loss=0.8395 acc=0.6377 | val_loss=0.9585 acc=0.5699 | time=29.8s\n",
            "Epoch 019 | train_loss=0.8204 acc=0.6443 | val_loss=0.9266 acc=0.5978 | time=29.9s\n",
            "Epoch 020 | train_loss=0.7726 acc=0.6668 | val_loss=0.9374 acc=0.5947 | time=30.1s\n",
            "Epoch 021 | train_loss=0.7275 acc=0.6878 | val_loss=0.9421 acc=0.5901 | time=29.9s\n",
            "Epoch 022 | train_loss=0.7402 acc=0.6769 | val_loss=0.9357 acc=0.5932 | time=30.0s\n",
            "Epoch 023 | train_loss=0.6597 acc=0.7150 | val_loss=0.9722 acc=0.5761 | time=29.9s\n",
            "Epoch 024 | train_loss=0.6042 acc=0.7483 | val_loss=1.1282 acc=0.5978 | time=29.9s\n",
            "Epoch 025 | train_loss=0.5622 acc=0.7631 | val_loss=1.0988 acc=0.6180 | time=30.0s\n",
            "Epoch 026 | train_loss=0.4836 acc=0.8062 | val_loss=1.0899 acc=0.6118 | time=29.8s\n",
            "Epoch 027 | train_loss=0.4220 acc=0.8311 | val_loss=1.1776 acc=0.6087 | time=29.8s\n",
            "Epoch 028 | train_loss=0.3626 acc=0.8660 | val_loss=1.2138 acc=0.6304 | time=30.0s\n",
            "Epoch 029 | train_loss=0.3235 acc=0.8796 | val_loss=1.2688 acc=0.6087 | time=29.9s\n",
            "Epoch 030 | train_loss=0.2963 acc=0.8924 | val_loss=1.2402 acc=0.6165 | time=29.8s\n",
            "Epoch 031 | train_loss=0.2987 acc=0.8932 | val_loss=1.3308 acc=0.6102 | time=30.0s\n",
            "Epoch 032 | train_loss=0.2345 acc=0.9192 | val_loss=1.3684 acc=0.6289 | time=29.8s\n",
            "Epoch 033 | train_loss=0.2060 acc=0.9289 | val_loss=1.4423 acc=0.6273 | time=29.9s\n",
            "Epoch 034 | train_loss=0.1876 acc=0.9421 | val_loss=1.4969 acc=0.5947 | time=29.8s\n",
            "Epoch 035 | train_loss=0.1757 acc=0.9480 | val_loss=1.4824 acc=0.6242 | time=29.9s\n",
            "Epoch 036 | train_loss=0.1684 acc=0.9495 | val_loss=1.6216 acc=0.6242 | time=30.0s\n",
            "Epoch 037 | train_loss=0.1579 acc=0.9507 | val_loss=1.6185 acc=0.6180 | time=29.9s\n",
            "Epoch 038 | train_loss=0.1319 acc=0.9650 | val_loss=1.6125 acc=0.6413 | time=29.8s\n",
            "Epoch 039 | train_loss=0.1233 acc=0.9662 | val_loss=1.5999 acc=0.6366 | time=30.1s\n",
            "★ Early stopping at epoch 39\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>████████████▇▇▇▇▆▆▆▆▅▆▅▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▆▆▆▆▆▆▇▆▆▆▆▇▇▇▇█▇▇▇██▆▇▇▇██</td></tr><tr><td>validation_loss</td><td>▃▂▃▃▂▃▃▃▂▃▂▂▂▁▁▁▁▁▁▁▁▁▁▃▃▃▄▄▄▄▅▅▆▇▇████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_accuracy</td><td>0.96621</td></tr><tr><td>train_loss</td><td>0.12327</td></tr><tr><td>validation_accuracy</td><td>0.63665</td></tr><tr><td>validation_loss</td><td>1.5999</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trim-monkey-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2w4wuv9d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2w4wuv9d</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_004455-2w4wuv9d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 01:04:25,881] Trial 1 finished with value: 0.9266147471609569 and parameters: {'lr': 0.0004, 'weight_decay': 5e-05, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5}. Best is trial 1 with value: 0.9266147471609569.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_010426-kzdokg3p</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/kzdokg3p' target=\"_blank\">happy-sun-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/kzdokg3p' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/kzdokg3p</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=4.00e-04, wd=1.00e-03, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0724 acc=0.4140 | val_loss=1.0747 acc=0.4317 | time=29.9s\n",
            "Epoch 002 | train_loss=1.0685 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.2s\n",
            "Epoch 003 | train_loss=1.0678 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=29.7s\n",
            "Epoch 004 | train_loss=1.0686 acc=0.4198 | val_loss=1.0745 acc=0.4317 | time=29.8s\n",
            "Epoch 005 | train_loss=1.0677 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.1s\n",
            "Epoch 006 | train_loss=1.0686 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=29.8s\n",
            "Epoch 007 | train_loss=1.0688 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=29.8s\n",
            "Epoch 008 | train_loss=1.0671 acc=0.4311 | val_loss=1.0782 acc=0.4317 | time=30.1s\n",
            "Epoch 009 | train_loss=1.0679 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=29.8s\n",
            "Epoch 010 | train_loss=1.0670 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.0s\n",
            "Epoch 011 | train_loss=1.0670 acc=0.4311 | val_loss=1.0741 acc=0.4317 | time=30.0s\n",
            "Epoch 012 | train_loss=1.0664 acc=0.4311 | val_loss=1.0739 acc=0.4317 | time=29.9s\n",
            "Epoch 013 | train_loss=1.0681 acc=0.4202 | val_loss=1.0743 acc=0.4317 | time=30.1s\n",
            "Epoch 014 | train_loss=1.0679 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=29.9s\n",
            "Epoch 015 | train_loss=1.0691 acc=0.4183 | val_loss=1.0810 acc=0.4317 | time=29.9s\n",
            "Epoch 016 | train_loss=1.0661 acc=0.4311 | val_loss=1.0705 acc=0.4317 | time=30.1s\n",
            "Epoch 017 | train_loss=1.0227 acc=0.5064 | val_loss=0.9853 acc=0.5342 | time=29.8s\n",
            "Epoch 018 | train_loss=0.9534 acc=0.5810 | val_loss=0.9712 acc=0.5466 | time=29.9s\n",
            "Epoch 019 | train_loss=0.9444 acc=0.5817 | val_loss=0.9711 acc=0.5652 | time=30.2s\n",
            "Epoch 020 | train_loss=0.9281 acc=0.5891 | val_loss=0.9536 acc=0.5528 | time=30.0s\n",
            "Epoch 021 | train_loss=0.9150 acc=0.5996 | val_loss=0.9593 acc=0.5528 | time=30.0s\n",
            "Epoch 022 | train_loss=0.8966 acc=0.6008 | val_loss=0.9744 acc=0.5792 | time=29.9s\n",
            "Epoch 023 | train_loss=0.8775 acc=0.6132 | val_loss=0.9580 acc=0.5590 | time=29.9s\n",
            "Epoch 024 | train_loss=0.8533 acc=0.6210 | val_loss=0.9228 acc=0.5885 | time=30.1s\n",
            "Epoch 025 | train_loss=0.8287 acc=0.6334 | val_loss=0.9882 acc=0.5683 | time=30.0s\n",
            "Epoch 026 | train_loss=0.8047 acc=0.6353 | val_loss=0.9024 acc=0.5745 | time=29.9s\n",
            "Epoch 027 | train_loss=0.7665 acc=0.6614 | val_loss=0.8625 acc=0.6335 | time=30.1s\n",
            "Epoch 028 | train_loss=0.7077 acc=0.7010 | val_loss=0.8392 acc=0.6227 | time=29.8s\n",
            "Epoch 029 | train_loss=0.6266 acc=0.7344 | val_loss=0.9231 acc=0.6351 | time=29.9s\n",
            "Epoch 030 | train_loss=0.5748 acc=0.7658 | val_loss=0.8621 acc=0.6677 | time=30.1s\n",
            "Epoch 031 | train_loss=0.5579 acc=0.7751 | val_loss=0.7996 acc=0.6801 | time=29.8s\n",
            "Epoch 032 | train_loss=0.4587 acc=0.8186 | val_loss=0.8767 acc=0.6646 | time=29.9s\n",
            "Epoch 033 | train_loss=0.4067 acc=0.8435 | val_loss=0.8572 acc=0.6661 | time=30.1s\n",
            "Epoch 034 | train_loss=0.3603 acc=0.8668 | val_loss=0.8648 acc=0.6693 | time=29.9s\n",
            "Epoch 035 | train_loss=0.3411 acc=0.8683 | val_loss=1.0492 acc=0.6599 | time=30.1s\n",
            "Epoch 036 | train_loss=0.3067 acc=0.8854 | val_loss=1.1625 acc=0.6568 | time=30.0s\n",
            "Epoch 037 | train_loss=0.2890 acc=0.8878 | val_loss=0.9819 acc=0.6894 | time=29.8s\n",
            "Epoch 038 | train_loss=0.2158 acc=0.9247 | val_loss=1.1444 acc=0.6568 | time=30.1s\n",
            "Epoch 039 | train_loss=0.1697 acc=0.9445 | val_loss=1.2624 acc=0.6568 | time=29.8s\n",
            "Epoch 040 | train_loss=0.1493 acc=0.9542 | val_loss=1.3666 acc=0.6506 | time=29.8s\n",
            "Epoch 041 | train_loss=0.1318 acc=0.9534 | val_loss=1.3839 acc=0.6677 | time=30.0s\n",
            "Epoch 042 | train_loss=0.1237 acc=0.9612 | val_loss=1.4855 acc=0.6646 | time=29.7s\n",
            "Epoch 043 | train_loss=0.1083 acc=0.9635 | val_loss=1.5418 acc=0.6786 | time=29.8s\n",
            "Epoch 044 | train_loss=0.0833 acc=0.9724 | val_loss=1.5420 acc=0.6615 | time=30.0s\n",
            "Epoch 045 | train_loss=0.0610 acc=0.9845 | val_loss=1.6122 acc=0.6615 | time=29.9s\n",
            "Epoch 046 | train_loss=0.0540 acc=0.9864 | val_loss=1.6256 acc=0.6724 | time=29.9s\n",
            "Epoch 047 | train_loss=0.0525 acc=0.9872 | val_loss=1.6616 acc=0.6724 | time=30.0s\n",
            "Epoch 048 | train_loss=0.0506 acc=0.9860 | val_loss=1.6820 acc=0.6739 | time=29.9s\n",
            "Epoch 049 | train_loss=0.0457 acc=0.9903 | val_loss=1.7785 acc=0.6630 | time=30.0s\n",
            "Epoch 050 | train_loss=0.0377 acc=0.9907 | val_loss=1.7837 acc=0.6677 | time=29.9s\n",
            "Epoch 051 | train_loss=0.0346 acc=0.9918 | val_loss=1.8040 acc=0.6801 | time=29.9s\n",
            "★ Early stopping at epoch 51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▃▄▄▄▄▅▅▆▆▆▇██████████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▇▇▇▆▆▆▅▅▅▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▄▄▅▅▆▆▇▇█▇▇▇█▇▇▇▇█▇███▇█</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▁▁▂▁▁▁▁▃▄▂▄▅▅▆▆▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>51</td></tr><tr><td>train_accuracy</td><td>0.99184</td></tr><tr><td>train_loss</td><td>0.03457</td></tr><tr><td>validation_accuracy</td><td>0.68012</td></tr><tr><td>validation_loss</td><td>1.80403</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">happy-sun-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/kzdokg3p' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/kzdokg3p</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_010426-kzdokg3p/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 01:29:56,139] Trial 2 finished with value: 0.7996216373784202 and parameters: {'lr': 0.0004, 'weight_decay': 0.001, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5}. Best is trial 2 with value: 0.7996216373784202.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_012956-qwfx8qqu</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/qwfx8qqu' target=\"_blank\">fearless-sun-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/qwfx8qqu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/qwfx8qqu</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=4.00e-04, wd=5.00e-04, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0716 acc=0.4202 | val_loss=1.0754 acc=0.4317 | time=36.8s\n",
            "Epoch 002 | train_loss=1.0671 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.0s\n",
            "Epoch 003 | train_loss=1.0690 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=36.0s\n",
            "Epoch 004 | train_loss=1.0683 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.0s\n",
            "Epoch 005 | train_loss=1.0684 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=36.1s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=36.3s\n",
            "Epoch 007 | train_loss=1.0675 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=36.1s\n",
            "Epoch 008 | train_loss=1.0655 acc=0.4311 | val_loss=1.0681 acc=0.4317 | time=36.0s\n",
            "Epoch 009 | train_loss=1.0513 acc=0.4738 | val_loss=1.0173 acc=0.5575 | time=36.2s\n",
            "Epoch 010 | train_loss=0.9770 acc=0.5650 | val_loss=0.9725 acc=0.5606 | time=36.1s\n",
            "Epoch 011 | train_loss=0.9497 acc=0.5821 | val_loss=0.9709 acc=0.5730 | time=36.1s\n",
            "Epoch 012 | train_loss=0.9107 acc=0.5965 | val_loss=0.9802 acc=0.5295 | time=36.2s\n",
            "Epoch 013 | train_loss=0.8911 acc=0.6074 | val_loss=0.9729 acc=0.5823 | time=36.3s\n",
            "Epoch 014 | train_loss=0.8696 acc=0.6241 | val_loss=0.9302 acc=0.5776 | time=36.2s\n",
            "Epoch 015 | train_loss=0.8513 acc=0.6373 | val_loss=0.9824 acc=0.5730 | time=36.1s\n",
            "Epoch 016 | train_loss=0.8249 acc=0.6369 | val_loss=0.9160 acc=0.6009 | time=36.1s\n",
            "Epoch 017 | train_loss=0.7874 acc=0.6602 | val_loss=0.9049 acc=0.5885 | time=36.1s\n",
            "Epoch 018 | train_loss=0.7609 acc=0.6750 | val_loss=0.9158 acc=0.6118 | time=36.0s\n",
            "Epoch 019 | train_loss=0.7578 acc=0.6680 | val_loss=0.9140 acc=0.6056 | time=36.1s\n",
            "Epoch 020 | train_loss=0.6821 acc=0.7033 | val_loss=0.9051 acc=0.6429 | time=36.3s\n",
            "Epoch 021 | train_loss=0.6386 acc=0.7181 | val_loss=0.9026 acc=0.6196 | time=36.2s\n",
            "Epoch 022 | train_loss=0.5684 acc=0.7507 | val_loss=0.9895 acc=0.6102 | time=36.2s\n",
            "Epoch 023 | train_loss=0.5386 acc=0.7666 | val_loss=0.9750 acc=0.6040 | time=36.2s\n",
            "Epoch 024 | train_loss=0.4550 acc=0.8066 | val_loss=1.1275 acc=0.6351 | time=36.2s\n",
            "Epoch 025 | train_loss=0.4046 acc=0.8462 | val_loss=1.1239 acc=0.6071 | time=36.0s\n",
            "Epoch 026 | train_loss=0.3698 acc=0.8505 | val_loss=0.9785 acc=0.6444 | time=36.0s\n",
            "Epoch 027 | train_loss=0.3466 acc=0.8614 | val_loss=1.2010 acc=0.6273 | time=36.2s\n",
            "Epoch 028 | train_loss=0.2230 acc=0.9212 | val_loss=1.4003 acc=0.6398 | time=36.2s\n",
            "Epoch 029 | train_loss=0.1806 acc=0.9363 | val_loss=1.5581 acc=0.6273 | time=36.2s\n",
            "Epoch 030 | train_loss=0.1526 acc=0.9507 | val_loss=1.5553 acc=0.6335 | time=36.0s\n",
            "Epoch 031 | train_loss=0.1421 acc=0.9495 | val_loss=1.7002 acc=0.6460 | time=36.0s\n",
            "Epoch 032 | train_loss=0.1150 acc=0.9643 | val_loss=2.0228 acc=0.5978 | time=36.1s\n",
            "Epoch 033 | train_loss=0.1259 acc=0.9550 | val_loss=1.8492 acc=0.6149 | time=36.2s\n",
            "Epoch 034 | train_loss=0.0741 acc=0.9810 | val_loss=2.0057 acc=0.6289 | time=36.2s\n",
            "Epoch 035 | train_loss=0.0480 acc=0.9883 | val_loss=2.2505 acc=0.6165 | time=36.3s\n",
            "Epoch 036 | train_loss=0.0469 acc=0.9880 | val_loss=2.0686 acc=0.6351 | time=36.1s\n",
            "Epoch 037 | train_loss=0.0351 acc=0.9926 | val_loss=2.2338 acc=0.6320 | time=36.2s\n",
            "Epoch 038 | train_loss=0.0445 acc=0.9880 | val_loss=2.1888 acc=0.6211 | time=36.2s\n",
            "Epoch 039 | train_loss=0.0360 acc=0.9907 | val_loss=2.4701 acc=0.6149 | time=36.1s\n",
            "Epoch 040 | train_loss=0.0299 acc=0.9922 | val_loss=2.3377 acc=0.6180 | time=36.0s\n",
            "Epoch 041 | train_loss=0.0201 acc=0.9961 | val_loss=2.4482 acc=0.6273 | time=36.2s\n",
            "★ Early stopping at epoch 41\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇█▇███████</td></tr><tr><td>train_loss</td><td>█████████▇▇▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▅▅▆▄▆▆▆▇▆▇▇█▇▇▇█▇█▇█▇██▆▇▇▇██▇▇▇</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▂▃▄▄▅▆▅▆▇▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_accuracy</td><td>0.99612</td></tr><tr><td>train_loss</td><td>0.02005</td></tr><tr><td>validation_accuracy</td><td>0.62733</td></tr><tr><td>validation_loss</td><td>2.44821</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fearless-sun-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/qwfx8qqu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/qwfx8qqu</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_012956-qwfx8qqu/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 01:54:41,284] Trial 3 finished with value: 0.9026221718106952 and parameters: {'lr': 0.0004, 'weight_decay': 0.0005, 'num_blocks': 2, 'num_heads': 4, 'num_segments': 5}. Best is trial 2 with value: 0.7996216373784202.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_015441-8ovbu8zs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/8ovbu8zs' target=\"_blank\">devout-wave-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/8ovbu8zs' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/8ovbu8zs</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=5.00e-04, wd=1.00e-03, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0722 acc=0.4229 | val_loss=1.0775 acc=0.4317 | time=29.9s\n",
            "Epoch 002 | train_loss=1.0677 acc=0.4315 | val_loss=1.0770 acc=0.4317 | time=30.1s\n",
            "Epoch 003 | train_loss=1.0693 acc=0.4202 | val_loss=1.0753 acc=0.4317 | time=29.9s\n",
            "Epoch 004 | train_loss=1.0677 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=30.0s\n",
            "Epoch 005 | train_loss=1.0665 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.4s\n",
            "Epoch 006 | train_loss=1.0654 acc=0.4311 | val_loss=1.0494 acc=0.5559 | time=29.9s\n",
            "Epoch 007 | train_loss=0.9910 acc=0.5398 | val_loss=1.0136 acc=0.5109 | time=30.2s\n",
            "Epoch 008 | train_loss=0.9423 acc=0.5825 | val_loss=0.9692 acc=0.5854 | time=30.0s\n",
            "Epoch 009 | train_loss=0.9173 acc=0.6004 | val_loss=0.9431 acc=0.5854 | time=30.0s\n",
            "Epoch 010 | train_loss=0.8819 acc=0.6167 | val_loss=1.0466 acc=0.5668 | time=30.0s\n",
            "Epoch 011 | train_loss=0.8719 acc=0.6245 | val_loss=0.9881 acc=0.5932 | time=30.0s\n",
            "Epoch 012 | train_loss=0.8411 acc=0.6357 | val_loss=1.0971 acc=0.5093 | time=29.9s\n",
            "Epoch 013 | train_loss=0.8265 acc=0.6408 | val_loss=0.9239 acc=0.6040 | time=30.2s\n",
            "Epoch 014 | train_loss=0.7671 acc=0.6629 | val_loss=0.9151 acc=0.6071 | time=30.0s\n",
            "Epoch 015 | train_loss=0.7480 acc=0.6715 | val_loss=0.9149 acc=0.5932 | time=29.9s\n",
            "Epoch 016 | train_loss=0.6999 acc=0.6901 | val_loss=0.9429 acc=0.6118 | time=30.2s\n",
            "Epoch 017 | train_loss=0.6943 acc=0.7014 | val_loss=0.8971 acc=0.6040 | time=30.0s\n",
            "Epoch 018 | train_loss=0.6081 acc=0.7379 | val_loss=0.8873 acc=0.5699 | time=30.0s\n",
            "Epoch 019 | train_loss=0.5836 acc=0.7581 | val_loss=0.9032 acc=0.6134 | time=30.1s\n",
            "Epoch 020 | train_loss=0.5311 acc=0.7783 | val_loss=1.2225 acc=0.6304 | time=30.0s\n",
            "Epoch 021 | train_loss=0.5014 acc=0.7915 | val_loss=0.9013 acc=0.6382 | time=30.2s\n",
            "Epoch 022 | train_loss=0.4362 acc=0.8245 | val_loss=1.0205 acc=0.6040 | time=30.2s\n",
            "Epoch 023 | train_loss=0.4124 acc=0.8326 | val_loss=0.9729 acc=0.6429 | time=29.9s\n",
            "Epoch 024 | train_loss=0.3815 acc=0.8509 | val_loss=0.9267 acc=0.6366 | time=30.2s\n",
            "Epoch 025 | train_loss=0.2798 acc=0.8870 | val_loss=1.1443 acc=0.6568 | time=30.1s\n",
            "Epoch 026 | train_loss=0.2415 acc=0.9103 | val_loss=1.2502 acc=0.6413 | time=30.0s\n",
            "Epoch 027 | train_loss=0.2139 acc=0.9208 | val_loss=1.3243 acc=0.6491 | time=30.2s\n",
            "Epoch 028 | train_loss=0.1760 acc=0.9324 | val_loss=1.3370 acc=0.6304 | time=30.1s\n",
            "Epoch 029 | train_loss=0.1752 acc=0.9390 | val_loss=1.4384 acc=0.6460 | time=30.1s\n",
            "Epoch 030 | train_loss=0.1510 acc=0.9445 | val_loss=1.4556 acc=0.6708 | time=30.1s\n",
            "Epoch 031 | train_loss=0.0984 acc=0.9717 | val_loss=1.6266 acc=0.6677 | time=30.2s\n",
            "Epoch 032 | train_loss=0.0781 acc=0.9763 | val_loss=1.7215 acc=0.6661 | time=30.4s\n",
            "Epoch 033 | train_loss=0.0772 acc=0.9751 | val_loss=1.9000 acc=0.6537 | time=30.3s\n",
            "Epoch 034 | train_loss=0.0619 acc=0.9810 | val_loss=1.8576 acc=0.6599 | time=30.4s\n",
            "Epoch 035 | train_loss=0.0432 acc=0.9868 | val_loss=1.9999 acc=0.6817 | time=30.3s\n",
            "Epoch 036 | train_loss=0.0560 acc=0.9814 | val_loss=1.9632 acc=0.6677 | time=30.3s\n",
            "Epoch 037 | train_loss=0.0388 acc=0.9895 | val_loss=2.0192 acc=0.6646 | time=30.5s\n",
            "Epoch 038 | train_loss=0.0300 acc=0.9938 | val_loss=2.1168 acc=0.6693 | time=30.3s\n",
            "★ Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▃▃▃▄▄▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>██████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▄▃▅▅▅▆▃▆▆▆▆▆▅▆▇▇▆▇▇▇▇▇▇▇███▇▇████</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▂▁▁▂▂▂▁▁▁▁▁▁▁▃▁▂▁▁▂▃▃▄▄▄▅▆▇▇▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_accuracy</td><td>0.99379</td></tr><tr><td>train_loss</td><td>0.02996</td></tr><tr><td>validation_accuracy</td><td>0.66925</td></tr><tr><td>validation_loss</td><td>2.11681</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">devout-wave-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/8ovbu8zs' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/8ovbu8zs</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_015441-8ovbu8zs/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:13:48,905] Trial 4 finished with value: 0.8873245375497001 and parameters: {'lr': 0.0005, 'weight_decay': 0.001, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5}. Best is trial 2 with value: 0.7996216373784202.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_021349-1j0dl5d7</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/1j0dl5d7' target=\"_blank\">restful-elevator-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/1j0dl5d7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/1j0dl5d7</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=3.00e-04, wd=5.00e-04, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0696 acc=0.4225 | val_loss=1.0820 acc=0.4317 | time=36.6s\n",
            "Epoch 002 | train_loss=1.0675 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.3s\n",
            "Epoch 003 | train_loss=1.0674 acc=0.4311 | val_loss=1.0766 acc=0.4317 | time=36.6s\n",
            "Epoch 004 | train_loss=1.0678 acc=0.4311 | val_loss=1.0741 acc=0.4317 | time=36.7s\n",
            "Epoch 005 | train_loss=1.0662 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=36.4s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=36.5s\n",
            "Epoch 007 | train_loss=1.0170 acc=0.5181 | val_loss=0.9864 acc=0.5683 | time=36.5s\n",
            "Epoch 008 | train_loss=0.9484 acc=0.5794 | val_loss=0.9693 acc=0.5714 | time=36.3s\n",
            "Epoch 009 | train_loss=0.9235 acc=0.5996 | val_loss=0.9490 acc=0.5745 | time=36.3s\n",
            "Epoch 010 | train_loss=0.9049 acc=0.6097 | val_loss=0.9441 acc=0.5497 | time=36.5s\n",
            "Epoch 011 | train_loss=0.8867 acc=0.6148 | val_loss=0.9505 acc=0.5963 | time=36.6s\n",
            "Epoch 012 | train_loss=0.8565 acc=0.6342 | val_loss=0.9302 acc=0.6040 | time=36.6s\n",
            "Epoch 013 | train_loss=0.8256 acc=0.6505 | val_loss=0.9410 acc=0.5854 | time=36.7s\n",
            "Epoch 014 | train_loss=0.8086 acc=0.6505 | val_loss=0.9430 acc=0.6009 | time=36.4s\n",
            "Epoch 015 | train_loss=0.7901 acc=0.6559 | val_loss=0.9205 acc=0.5823 | time=36.4s\n",
            "Epoch 016 | train_loss=0.7751 acc=0.6645 | val_loss=0.9505 acc=0.5776 | time=36.4s\n",
            "Epoch 017 | train_loss=0.7492 acc=0.6750 | val_loss=0.9406 acc=0.5839 | time=36.4s\n",
            "Epoch 018 | train_loss=0.7179 acc=0.6963 | val_loss=1.0245 acc=0.5839 | time=36.5s\n",
            "Epoch 019 | train_loss=0.6798 acc=0.7153 | val_loss=0.9546 acc=0.5823 | time=36.6s\n",
            "Epoch 020 | train_loss=0.6398 acc=0.7336 | val_loss=1.0184 acc=0.5839 | time=36.4s\n",
            "Epoch 021 | train_loss=0.6197 acc=0.7406 | val_loss=1.0497 acc=0.5963 | time=36.3s\n",
            "Epoch 022 | train_loss=0.5532 acc=0.7775 | val_loss=1.0198 acc=0.5885 | time=36.4s\n",
            "Epoch 023 | train_loss=0.5349 acc=0.7821 | val_loss=1.1187 acc=0.5839 | time=36.5s\n",
            "Epoch 024 | train_loss=0.5074 acc=0.7973 | val_loss=1.1057 acc=0.5668 | time=36.4s\n",
            "Epoch 025 | train_loss=0.4830 acc=0.7996 | val_loss=1.0848 acc=0.5978 | time=36.7s\n",
            "Epoch 026 | train_loss=0.4598 acc=0.8132 | val_loss=1.2285 acc=0.5823 | time=36.6s\n",
            "Epoch 027 | train_loss=0.4328 acc=0.8249 | val_loss=1.3764 acc=0.5978 | time=36.3s\n",
            "Epoch 028 | train_loss=0.3907 acc=0.8474 | val_loss=1.4108 acc=0.5854 | time=36.5s\n",
            "Epoch 029 | train_loss=0.3636 acc=0.8676 | val_loss=1.3693 acc=0.5978 | time=36.4s\n",
            "Epoch 030 | train_loss=0.3483 acc=0.8629 | val_loss=1.4821 acc=0.5978 | time=36.4s\n",
            "Epoch 031 | train_loss=0.3453 acc=0.8649 | val_loss=1.5372 acc=0.5978 | time=36.5s\n",
            "Epoch 032 | train_loss=0.3300 acc=0.8784 | val_loss=1.5759 acc=0.5854 | time=36.4s\n",
            "Epoch 033 | train_loss=0.3132 acc=0.8839 | val_loss=1.5978 acc=0.5714 | time=36.6s\n",
            "Epoch 034 | train_loss=0.2921 acc=0.8944 | val_loss=1.7388 acc=0.5947 | time=36.6s\n",
            "Epoch 035 | train_loss=0.2748 acc=0.9006 | val_loss=1.7120 acc=0.5839 | time=36.5s\n",
            "★ Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇█▇▇████</td></tr><tr><td>train_loss</td><td>███████▇▇▇▆▆▆▆▆▅▅▅▅▄▄▃▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▇▇▇▆██▇█▇▇▇▇▇▇█▇▇▆█▇█▇███▇▇█▇</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▂▂▃▃▂▄▅▅▅▆▆▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_accuracy</td><td>0.90058</td></tr><tr><td>train_loss</td><td>0.27484</td></tr><tr><td>validation_accuracy</td><td>0.58385</td></tr><tr><td>validation_loss</td><td>1.71205</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">restful-elevator-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/1j0dl5d7' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/1j0dl5d7</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_021349-1j0dl5d7/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:35:08,760] Trial 5 finished with value: 0.9205371141433716 and parameters: {'lr': 0.0003, 'weight_decay': 0.0005, 'num_blocks': 2, 'num_heads': 4, 'num_segments': 5}. Best is trial 2 with value: 0.7996216373784202.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_023508-4qd4i1jw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/4qd4i1jw' target=\"_blank\">fanciful-wildflower-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/4qd4i1jw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/4qd4i1jw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 6 =====\n",
            " lr=3.00e-04, wd=1.00e-03, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0712 acc=0.4128 | val_loss=1.0749 acc=0.4317 | time=30.2s\n",
            "Epoch 002 | train_loss=1.0695 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.3s\n",
            "Epoch 003 | train_loss=1.0688 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=30.3s\n",
            "Epoch 004 | train_loss=1.0677 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.2s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=30.4s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=30.3s\n",
            "Epoch 007 | train_loss=1.0674 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁██████</td></tr><tr><td>train_loss</td><td>█▅▄▂▄▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▃▁█▁▅▇▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06738</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07445</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fanciful-wildflower-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/4qd4i1jw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/4qd4i1jw</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_023508-4qd4i1jw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:39:13,111] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 6 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_023913-sxvxywib</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/sxvxywib' target=\"_blank\">stellar-moon-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/sxvxywib' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/sxvxywib</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 7 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0752 acc=0.4171 | val_loss=1.0795 acc=0.4317 | time=36.7s\n",
            "Epoch 002 | train_loss=1.0691 acc=0.4311 | val_loss=1.0824 acc=0.4317 | time=36.6s\n",
            "Epoch 003 | train_loss=1.0690 acc=0.4311 | val_loss=1.0770 acc=0.4317 | time=36.4s\n",
            "Epoch 004 | train_loss=1.0668 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.4s\n",
            "Epoch 005 | train_loss=1.0676 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.6s\n",
            "Epoch 006 | train_loss=1.0675 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>train_accuracy</td><td>▁█████</td></tr><tr><td>train_loss</td><td>█▃▃▁▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▅█▃▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06751</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0747</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-moon-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/sxvxywib' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/sxvxywib</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_023913-sxvxywib/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:43:30,529] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 7 pruned at epoch 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_024330-eirg9a0s</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/eirg9a0s' target=\"_blank\">misunderstood-star-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/eirg9a0s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/eirg9a0s</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 8 =====\n",
            " lr=5.00e-04, wd=1.00e-03, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0718 acc=0.4179 | val_loss=1.0790 acc=0.4317 | time=36.3s\n",
            "Epoch 002 | train_loss=1.0678 acc=0.4311 | val_loss=1.0780 acc=0.4317 | time=36.6s\n",
            "Epoch 003 | train_loss=1.0694 acc=0.4311 | val_loss=1.0781 acc=0.4317 | time=36.7s\n",
            "Epoch 004 | train_loss=1.0675 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.6s\n",
            "Epoch 005 | train_loss=1.0671 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.5s\n",
            "Epoch 006 | train_loss=1.0676 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=36.5s\n",
            "Epoch 007 | train_loss=1.0662 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=36.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁██████</td></tr><tr><td>train_loss</td><td>█▃▅▃▂▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▇▇▁▁▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06618</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07492</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">misunderstood-star-9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/eirg9a0s' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/eirg9a0s</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_024330-eirg9a0s/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:48:24,622] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 8 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "creating run (2.5s)<br>  <strong style=\"color:red\">ERROR</strong> retrying HTTP 503 Service Unavailable"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_024824-o6p2d0oh</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/o6p2d0oh' target=\"_blank\">eager-violet-10</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/o6p2d0oh' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/o6p2d0oh</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 9 =====\n",
            " lr=4.00e-04, wd=5.00e-05, blocks=2, heads=4, segs=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: 503 encountered (\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <html><head>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\">\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <title>503 Server Error</title>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: </head>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <body text=#000000 bgcolor=#ffffff>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <h1>Error: Server Error</h1>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <h2>The service you requested is not available at this time.<p>Service error -27.</h2>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: <h2></h2>\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: </body></html>), retrying request\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0693 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=36.3s\n",
            "Epoch 002 | train_loss=1.0679 acc=0.4311 | val_loss=1.0822 acc=0.4317 | time=36.2s\n",
            "Epoch 003 | train_loss=1.0711 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.4s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=36.6s\n",
            "Epoch 005 | train_loss=1.0675 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=36.6s\n",
            "Epoch 006 | train_loss=1.0678 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.3s\n",
            "Epoch 007 | train_loss=1.0672 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=36.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>▅▂█▃▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂█▁▁▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06717</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07449</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-violet-10</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/o6p2d0oh' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/o6p2d0oh</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_024824-o6p2d0oh/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:53:20,047] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 9 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_025320-flqnfmt4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/flqnfmt4' target=\"_blank\">dashing-night-11</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/flqnfmt4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/flqnfmt4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 10 =====\n",
            " lr=4.00e-04, wd=5.00e-04, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0735 acc=0.4194 | val_loss=1.0757 acc=0.4317 | time=30.4s\n",
            "Epoch 002 | train_loss=1.0677 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.2s\n",
            "Epoch 003 | train_loss=1.0689 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=30.3s\n",
            "Epoch 004 | train_loss=1.0685 acc=0.4311 | val_loss=1.0826 acc=0.4317 | time=30.3s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.3s\n",
            "Epoch 006 | train_loss=1.0725 acc=0.4109 | val_loss=1.0796 acc=0.4317 | time=30.4s\n",
            "Epoch 007 | train_loss=1.0679 acc=0.4311 | val_loss=1.0773 acc=0.4317 | time=30.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▄████▁█</td></tr><tr><td>train_loss</td><td>█▁▂▂▂▇▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▁▂█▁▅▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06794</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07732</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dashing-night-11</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/flqnfmt4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/flqnfmt4</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_025320-flqnfmt4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:57:24,256] Trial 10 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 10 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_025724-xnu6gj4j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/xnu6gj4j' target=\"_blank\">helpful-snow-12</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/xnu6gj4j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/xnu6gj4j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 11 =====\n",
            " lr=5.00e-04, wd=5.00e-04, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0699 acc=0.4186 | val_loss=1.0754 acc=0.4317 | time=30.3s\n",
            "Epoch 002 | train_loss=1.0688 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=30.1s\n",
            "Epoch 003 | train_loss=1.0677 acc=0.4311 | val_loss=1.0769 acc=0.4317 | time=30.5s\n",
            "Epoch 004 | train_loss=1.0689 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=30.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁███</td></tr><tr><td>train_loss</td><td>█▅▁▅</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▃█▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06888</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07629</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">helpful-snow-12</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/xnu6gj4j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/xnu6gj4j</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_025724-xnu6gj4j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 02:59:58,069] Trial 11 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 11 pruned at epoch 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_025958-2rxmt2hr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2rxmt2hr' target=\"_blank\">vivid-galaxy-13</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2rxmt2hr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2rxmt2hr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 12 =====\n",
            " lr=4.00e-04, wd=1.00e-03, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0708 acc=0.4311 | val_loss=1.0768 acc=0.4317 | time=36.1s\n",
            "Epoch 002 | train_loss=1.0689 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.4s\n",
            "Epoch 003 | train_loss=1.0685 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.3s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=36.5s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4311 | val_loss=1.0791 acc=0.4317 | time=36.5s\n",
            "Epoch 006 | train_loss=1.0678 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=36.5s\n",
            "Epoch 007 | train_loss=1.0667 acc=0.4311 | val_loss=1.0776 acc=0.4317 | time=36.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▄▄▅▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▅▁▁▂█▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06665</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07759</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vivid-galaxy-13</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2rxmt2hr' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/2rxmt2hr</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_025958-2rxmt2hr/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:04:51,071] Trial 12 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 12 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_030451-nkteq369</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/nkteq369' target=\"_blank\">prime-fog-14</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/nkteq369' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/nkteq369</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 13 =====\n",
            " lr=3.00e-04, wd=5.00e-05, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0712 acc=0.4245 | val_loss=1.0749 acc=0.4317 | time=36.5s\n",
            "Epoch 002 | train_loss=1.0698 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=36.5s\n",
            "Epoch 003 | train_loss=1.0676 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=36.3s\n",
            "Epoch 004 | train_loss=1.0681 acc=0.4311 | val_loss=1.0800 acc=0.4317 | time=36.4s\n",
            "Epoch 005 | train_loss=1.0666 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.2s\n",
            "Epoch 006 | train_loss=1.0677 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=36.5s\n",
            "Epoch 007 | train_loss=1.0661 acc=0.4311 | val_loss=1.0770 acc=0.4317 | time=36.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁██████</td></tr><tr><td>train_loss</td><td>█▆▃▄▂▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▂▃█▁▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06615</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07702</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">prime-fog-14</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/nkteq369' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/nkteq369</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_030451-nkteq369/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:09:44,770] Trial 13 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 13 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_030944-e1yyglx6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/e1yyglx6' target=\"_blank\">fallen-microwave-15</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/e1yyglx6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/e1yyglx6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 14 =====\n",
            " lr=3.00e-04, wd=5.00e-05, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0696 acc=0.4241 | val_loss=1.0744 acc=0.4317 | time=30.3s\n",
            "Epoch 002 | train_loss=1.0703 acc=0.4229 | val_loss=1.0748 acc=0.4317 | time=30.1s\n",
            "Epoch 003 | train_loss=1.0695 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.6s\n",
            "Epoch 004 | train_loss=1.0667 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.2s\n",
            "Epoch 005 | train_loss=1.0673 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=30.0s\n",
            "Epoch 006 | train_loss=1.0670 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.4s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=30.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▂▁█████</td></tr><tr><td>train_loss</td><td>▇█▆▁▂▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▄▁▅█▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06731</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07425</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fallen-microwave-15</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/e1yyglx6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/e1yyglx6</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_030944-e1yyglx6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:13:48,887] Trial 14 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 14 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_031349-bypj5uvj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/bypj5uvj' target=\"_blank\">dashing-water-16</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/bypj5uvj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/bypj5uvj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 15 =====\n",
            " lr=3.00e-04, wd=1.00e-03, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0707 acc=0.4311 | val_loss=1.0788 acc=0.4317 | time=36.5s\n",
            "Epoch 002 | train_loss=1.0691 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=36.4s\n",
            "Epoch 003 | train_loss=1.0689 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=36.3s\n",
            "Epoch 004 | train_loss=1.0679 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=36.4s\n",
            "Epoch 005 | train_loss=1.0677 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.3s\n",
            "Epoch 006 | train_loss=1.0659 acc=0.4311 | val_loss=1.0780 acc=0.4317 | time=36.2s\n",
            "Epoch 007 | train_loss=1.0667 acc=0.4311 | val_loss=1.0769 acc=0.4317 | time=36.6s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▄▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▄▁▄▁▇▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06671</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07692</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dashing-water-16</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/bypj5uvj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/bypj5uvj</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_031349-bypj5uvj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:18:42,869] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 15 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_031843-t9e1aa0b</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/t9e1aa0b' target=\"_blank\">polished-cherry-17</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/t9e1aa0b' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/t9e1aa0b</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 16 =====\n",
            " lr=5.00e-04, wd=5.00e-05, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0706 acc=0.4280 | val_loss=1.0789 acc=0.4317 | time=36.6s\n",
            "Epoch 002 | train_loss=1.0695 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=36.6s\n",
            "Epoch 003 | train_loss=1.0683 acc=0.4311 | val_loss=1.0771 acc=0.4317 | time=36.5s\n",
            "Epoch 004 | train_loss=1.0680 acc=0.4311 | val_loss=1.0781 acc=0.4317 | time=36.1s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.4s\n",
            "Epoch 006 | train_loss=1.0672 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.2s\n",
            "Epoch 007 | train_loss=1.0682 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=36.5s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁██████</td></tr><tr><td>train_loss</td><td>█▆▃▃▄▁▃</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▅▇▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06821</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07442</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-cherry-17</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/t9e1aa0b' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/t9e1aa0b</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_031843-t9e1aa0b/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:23:36,811] Trial 16 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 16 pruned at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250502_032336-zpgul009</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/zpgul009' target=\"_blank\">astral-lion-18</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/zpgul009' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/zpgul009</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 17 =====\n",
            " lr=5.00e-04, wd=5.00e-05, blocks=2, heads=3, segs=5\n",
            "Epoch 001 | train_loss=1.0704 acc=0.4311 | val_loss=1.0816 acc=0.4317 | time=30.2s\n",
            "Epoch 002 | train_loss=1.0702 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.4s\n",
            "Epoch 003 | train_loss=1.0670 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.1s\n",
            "Epoch 004 | train_loss=1.0681 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.1s\n",
            "Epoch 005 | train_loss=1.0677 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=30.1s\n",
            "Epoch 006 | train_loss=1.0678 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=30.2s\n",
            "Epoch 007 | train_loss=1.0667 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▇▁▃▃▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▁▁▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06674</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07487</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">astral-lion-18</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/zpgul009' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search/runs/zpgul009</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-grid-search</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250502_032336-zpgul009/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-02 03:27:41,000] Trial 17 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 17 pruned at epoch 8\n",
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss       = 0.799622\n",
            "best_train_loss     = 0.557883\n",
            "best_train_accuracy = 0.7751\n",
            "best_val_accuracy   = 0.6801\n",
            "best params:\n",
            "  lr: 0.0004\n",
            "  weight_decay: 0.001\n",
            "  num_blocks: 2\n",
            "  num_heads: 3\n",
            "  num_segments: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "===== Best Trial =====\n",
        "best_val_loss       = 0.799622\n",
        "best_train_loss     = 0.557883\n",
        "best_train_accuracy = 0.7751\n",
        "best_val_accuracy   = 0.6801\n",
        "best params:\n",
        "  lr: 0.0004\n",
        "  weight_decay: 0.001\n",
        "  num_blocks: 2\n",
        "  num_heads: 3\n",
        "  num_segments: 5"
      ],
      "metadata": {
        "id": "rCar8E2wtWqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reduce the Model depth and width\n",
        "- Decrease the model complexity due to overfitting\n",
        "- Decrease the model blocks from 2 to 1\n",
        "- Test it with different heads = [1,2,3]"
      ],
      "metadata": {
        "id": "SuEWd7ALtYIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Grid Search 후보값 ────────────────────────────────────────\n",
        "LR_CHOICES          = [3e-4, 4e-4, 5e-4]\n",
        "WD_CHOICES          = [5e-5, 5e-4, 1e-3]\n",
        "NUM_FILTERS         = 120\n",
        "NUM_BLOCK_CHOICES   = [1]\n",
        "NUM_HEAD_CHOICES    = [2, 3, 4]\n",
        "SEGMENT_CHOICES     = [5]\n",
        "\n",
        "# ─── Training configuration ───────────────────────────────────\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ──────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── Scheduler 하이퍼파라미터 ─────────────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─── 1) Grid용 sampling ───────────────────────────────────\n",
        "    lr           = trial.suggest_categorical(\"lr\", LR_CHOICES)\n",
        "    weight_decay = trial.suggest_categorical(\"weight_decay\", WD_CHOICES)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─── 2) 데이터 로드 ────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─── 3) Hold-out split ─────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─── 4) W&B init ───────────────────────────────────────────\n",
        "    wandb.init(\n",
        "        project=\"eeg-holdout-grid-search\",\n",
        "        config={\n",
        "            \"lr\": lr,\n",
        "            \"weight_decay\": weight_decay,\n",
        "            \"num_blocks\": num_blocks,\n",
        "            \"num_heads\": num_heads,\n",
        "            \"num_segments\": num_segments\n",
        "        }\n",
        "    )\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\"\n",
        "    )\n",
        "\n",
        "    # ─── 5) Model / optimizer / loss ──────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── 6) Scheduler ──────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─── 7) Training loop w/ Early Stopping & Pruning ─────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # ─── GridSampler용 파라미터 그리드 ─────────────────────────\n",
        "    param_grid = {\n",
        "        \"lr\":            LR_CHOICES,\n",
        "        \"weight_decay\":  WD_CHOICES,\n",
        "        \"num_blocks\":    NUM_BLOCK_CHOICES,\n",
        "        \"num_heads\":     NUM_HEAD_CHOICES,\n",
        "        \"num_segments\":  SEGMENT_CHOICES,\n",
        "    }\n",
        "\n",
        "    sampler = optuna.samplers.GridSampler(param_grid)\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=sampler,\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_grid_search\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_grid_search.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout)  # grid 크기만큼 자동 실행\n",
        "\n",
        "    # ─── 결과 출력 ─────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "TDWzmETptWsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RQzNowCwtW0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8zrAFOHUSqdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-4, 1e-2\n",
        "WD_MIN, WD_MAX     = 1e-6, 1e-3\n",
        "\n",
        "NUM_FILTERS        = 120\n",
        "NUM_BLOCK_CHOICES  = [2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 4]\n",
        "SEGMENT_CHOICES    = [5]\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "MAX_EPOCHS  = 100   # 100 epochs 고정\n",
        "PATIENCE    = 20    # Early stopping patience\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# ─── 고정 스케줄러 하이퍼파라미터 ─────────────────────────\n",
        "FIXED_STEP_SIZE = 5\n",
        "FIXED_GAMMA     = 0.5\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─ 1) Sample hyperparameters ────────────────────────────────\n",
        "    lr           = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # ─ 2) Load data ────────────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─ 3) Hold-out split ──────────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─ 4) W&B init ────────────────────────────────────────────────\n",
        "    wandb.init(project=\"eeg-holdout-tuning-9\", config=trial.params)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}\")\n",
        "\n",
        "    # ─ 5) Model / optimizer / loss ───────────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay  # L2\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─ 6) Scheduler ───────────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer,\n",
        "        mode='min',\n",
        "        factor=FIXED_GAMMA,\n",
        "        patience=FIXED_STEP_SIZE,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # ─ 7) Training loop w/ Early Stopping & Pruning ──────────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y    = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits  = model(X)\n",
        "                loss    = criterion(logits, y)\n",
        "                vloss   += loss.item()\n",
        "                vcorrect+= (logits.argmax(1) == y).sum().item()\n",
        "                vtotal  += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s | \"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_trial-9\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_holdout-9.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout, n_trials=20)\n",
        "\n",
        "    # ─── 결과 출력 ────────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "HhjV5A82o4Sq",
        "outputId": "5f9e9dde-4b93-47f3-b1bf-8af38b8cf120"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 22:22:27,459] Using an existing study with name 'eeg_holdout_trial-9' instead of creating a new one.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_222228-cl91lc4d</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/cl91lc4d' target=\"_blank\">avid-serenity-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/cl91lc4d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/cl91lc4d</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=2.35e-04, wd=7.48e-05, blocks=2, heads=4, segs=5\n",
            "Epoch 001 | train_loss=1.0738 acc=0.4214 | val_loss=1.0746 acc=0.4317 | time=36.7s | \n",
            "Epoch 002 | train_loss=1.0684 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=36.0s | \n",
            "Epoch 003 | train_loss=1.0684 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=36.3s | \n",
            "Epoch 004 | train_loss=1.0671 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=36.3s | \n",
            "Epoch 005 | train_loss=1.0675 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=36.1s | \n",
            "Epoch 006 | train_loss=1.0661 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.1s | \n",
            "Epoch 007 | train_loss=1.0669 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=36.0s | \n",
            "Epoch 008 | train_loss=1.0677 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=36.1s | \n",
            "Epoch 009 | train_loss=1.0667 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=36.2s | \n",
            "Epoch 010 | train_loss=1.0671 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=36.3s | \n",
            "Epoch 011 | train_loss=1.0677 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=36.3s | \n",
            "Epoch 012 | train_loss=1.0674 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=36.3s | \n",
            "Epoch 013 | train_loss=1.0662 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=36.3s | \n",
            "Epoch 014 | train_loss=1.0667 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.3s | \n",
            "Epoch 015 | train_loss=1.0671 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=36.4s | \n",
            "Epoch 016 | train_loss=1.0666 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=36.4s | \n",
            "Epoch 017 | train_loss=1.0669 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=36.3s | \n",
            "Epoch 018 | train_loss=1.0658 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=36.4s | \n",
            "Epoch 019 | train_loss=1.0665 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=36.3s | \n",
            "Epoch 020 | train_loss=1.0656 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.0s | \n",
            "Epoch 021 | train_loss=1.0661 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=36.2s | \n",
            "Epoch 022 | train_loss=1.0670 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.2s | \n",
            "Epoch 023 | train_loss=1.0663 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=36.4s | \n",
            "Epoch 024 | train_loss=1.0666 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=36.2s | \n",
            "Epoch 025 | train_loss=1.0657 acc=0.4311 | val_loss=1.0738 acc=0.4317 | time=36.3s | \n",
            "Epoch 026 | train_loss=1.0663 acc=0.4311 | val_loss=1.0722 acc=0.4317 | time=36.1s | \n",
            "Epoch 027 | train_loss=1.0516 acc=0.4466 | val_loss=1.0364 acc=0.4317 | time=36.3s | \n",
            "Epoch 028 | train_loss=1.0108 acc=0.5433 | val_loss=1.0168 acc=0.5575 | time=36.0s | \n",
            "Epoch 029 | train_loss=0.9803 acc=0.5802 | val_loss=0.9972 acc=0.5590 | time=36.3s | \n",
            "Epoch 030 | train_loss=0.9621 acc=0.5833 | val_loss=0.9941 acc=0.5373 | time=36.4s | \n",
            "Epoch 031 | train_loss=0.9513 acc=0.5868 | val_loss=0.9764 acc=0.5606 | time=36.2s | \n",
            "Epoch 032 | train_loss=0.9358 acc=0.5895 | val_loss=0.9681 acc=0.5761 | time=36.2s | \n",
            "Epoch 033 | train_loss=0.9244 acc=0.5984 | val_loss=0.9594 acc=0.5683 | time=36.1s | \n",
            "Epoch 034 | train_loss=0.9177 acc=0.5992 | val_loss=0.9501 acc=0.5854 | time=36.1s | \n",
            "Epoch 035 | train_loss=0.8918 acc=0.6132 | val_loss=0.9525 acc=0.5745 | time=36.1s | \n",
            "Epoch 036 | train_loss=0.8749 acc=0.6186 | val_loss=0.9705 acc=0.5807 | time=36.2s | \n",
            "Epoch 037 | train_loss=0.8619 acc=0.6237 | val_loss=0.9409 acc=0.5823 | time=36.3s | \n",
            "Epoch 038 | train_loss=0.8438 acc=0.6350 | val_loss=0.9210 acc=0.5854 | time=36.3s | \n",
            "Epoch 039 | train_loss=0.8414 acc=0.6330 | val_loss=0.9521 acc=0.6009 | time=36.1s | \n",
            "Epoch 040 | train_loss=0.8296 acc=0.6373 | val_loss=0.9154 acc=0.6025 | time=36.0s | \n",
            "Epoch 041 | train_loss=0.8151 acc=0.6381 | val_loss=0.9453 acc=0.6009 | time=36.0s | \n",
            "Epoch 042 | train_loss=0.8039 acc=0.6454 | val_loss=0.9052 acc=0.6056 | time=36.3s | \n",
            "Epoch 043 | train_loss=0.8187 acc=0.6454 | val_loss=0.9185 acc=0.5823 | time=36.2s | \n",
            "Epoch 044 | train_loss=0.8105 acc=0.6404 | val_loss=0.9082 acc=0.6009 | time=36.1s | \n",
            "Epoch 045 | train_loss=0.7927 acc=0.6528 | val_loss=0.9161 acc=0.5978 | time=36.0s | \n",
            "Epoch 046 | train_loss=0.7737 acc=0.6505 | val_loss=0.9128 acc=0.6056 | time=36.0s | \n",
            "Epoch 047 | train_loss=0.7777 acc=0.6540 | val_loss=0.9182 acc=0.6071 | time=36.0s | \n",
            "Epoch 048 | train_loss=0.7631 acc=0.6649 | val_loss=0.9005 acc=0.5916 | time=36.0s | \n",
            "Epoch 049 | train_loss=0.7569 acc=0.6602 | val_loss=0.8970 acc=0.5870 | time=36.0s | \n",
            "Epoch 050 | train_loss=0.7582 acc=0.6695 | val_loss=0.9076 acc=0.6134 | time=36.1s | \n",
            "Epoch 051 | train_loss=0.7635 acc=0.6602 | val_loss=0.9258 acc=0.6087 | time=36.2s | \n",
            "Epoch 052 | train_loss=0.7301 acc=0.6761 | val_loss=0.9103 acc=0.5901 | time=36.1s | \n",
            "Epoch 053 | train_loss=0.7164 acc=0.6823 | val_loss=0.9047 acc=0.5947 | time=36.1s | \n",
            "Epoch 054 | train_loss=0.7143 acc=0.6788 | val_loss=0.9109 acc=0.6025 | time=36.2s | \n",
            "Epoch 055 | train_loss=0.7035 acc=0.6917 | val_loss=0.8956 acc=0.6289 | time=36.1s | \n",
            "Epoch 056 | train_loss=0.6901 acc=0.7017 | val_loss=0.9735 acc=0.5978 | time=36.1s | \n",
            "Epoch 057 | train_loss=0.6936 acc=0.7041 | val_loss=0.8705 acc=0.6444 | time=36.1s | \n",
            "Epoch 058 | train_loss=0.6647 acc=0.7103 | val_loss=0.9098 acc=0.6475 | time=36.1s | \n",
            "Epoch 059 | train_loss=0.6605 acc=0.7150 | val_loss=0.8999 acc=0.6413 | time=36.2s | \n",
            "Epoch 060 | train_loss=0.6505 acc=0.7297 | val_loss=0.9656 acc=0.6258 | time=36.1s | \n",
            "Epoch 061 | train_loss=0.6495 acc=0.7309 | val_loss=0.9174 acc=0.6491 | time=36.2s | \n",
            "Epoch 062 | train_loss=0.6273 acc=0.7383 | val_loss=0.8992 acc=0.6180 | time=36.0s | \n",
            "Epoch 063 | train_loss=0.6165 acc=0.7456 | val_loss=0.8882 acc=0.6460 | time=36.0s | \n",
            "Epoch 064 | train_loss=0.6015 acc=0.7569 | val_loss=0.9145 acc=0.6506 | time=36.1s | \n",
            "Epoch 065 | train_loss=0.5842 acc=0.7654 | val_loss=0.9077 acc=0.6475 | time=36.0s | \n",
            "Epoch 066 | train_loss=0.5730 acc=0.7709 | val_loss=0.9280 acc=0.6568 | time=36.0s | \n",
            "Epoch 067 | train_loss=0.5723 acc=0.7763 | val_loss=0.9185 acc=0.6553 | time=36.1s | \n",
            "Epoch 068 | train_loss=0.5673 acc=0.7748 | val_loss=0.9300 acc=0.6491 | time=36.3s | \n",
            "Epoch 069 | train_loss=0.5535 acc=0.7763 | val_loss=0.9186 acc=0.6304 | time=36.2s | \n",
            "Epoch 070 | train_loss=0.5387 acc=0.7802 | val_loss=0.9395 acc=0.6475 | time=36.2s | \n",
            "Epoch 071 | train_loss=0.5341 acc=0.7903 | val_loss=0.9403 acc=0.6398 | time=36.2s | \n",
            "Epoch 072 | train_loss=0.5323 acc=0.7895 | val_loss=0.9639 acc=0.6475 | time=36.1s | \n",
            "Epoch 073 | train_loss=0.5271 acc=0.7934 | val_loss=0.9518 acc=0.6553 | time=36.0s | \n",
            "Epoch 074 | train_loss=0.5276 acc=0.7899 | val_loss=0.9333 acc=0.6429 | time=36.0s | \n",
            "Epoch 075 | train_loss=0.5249 acc=0.7946 | val_loss=0.9279 acc=0.6584 | time=35.9s | \n",
            "Epoch 076 | train_loss=0.5156 acc=0.8016 | val_loss=0.9298 acc=0.6599 | time=36.1s | \n",
            "Epoch 077 | train_loss=0.5193 acc=0.7965 | val_loss=0.9353 acc=0.6584 | time=36.1s | \n",
            "★ Early stopping at epoch 77\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇███████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▆▆▆▅▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▆▆▆▆▆▆▆▆▆▇▆█████████▇█</td></tr><tr><td>validation_loss</td><td>██████████████▆▅▄▄▃▃▂▂▃▂▂▂▂▁▂▂▄▁▂▁▁▂▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>77</td></tr><tr><td>train_accuracy</td><td>0.7965</td></tr><tr><td>train_loss</td><td>0.51931</td></tr><tr><td>validation_accuracy</td><td>0.65839</td></tr><tr><td>validation_loss</td><td>0.93526</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">avid-serenity-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/cl91lc4d' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/cl91lc4d</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_222228-cl91lc4d/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 23:08:57,626] Trial 1 finished with value: 0.8704692948432196 and parameters: {'lr': 0.00023450279490485625, 'weight_decay': 7.480682204115078e-05, 'num_blocks': 2, 'num_heads': 4, 'num_segments': 5}. Best is trial 1 with value: 0.8704692948432196.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_230857-jm7x1yaw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/jm7x1yaw' target=\"_blank\">electric-elevator-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/jm7x1yaw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/jm7x1yaw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=6.33e-04, wd=6.11e-06, blocks=2, heads=2, segs=5\n",
            "Epoch 001 | train_loss=1.0701 acc=0.4291 | val_loss=1.0755 acc=0.4317 | time=24.5s | \n",
            "Epoch 002 | train_loss=1.0675 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=24.5s | \n",
            "Epoch 003 | train_loss=1.0682 acc=0.4311 | val_loss=1.0789 acc=0.4317 | time=24.4s | \n",
            "Epoch 004 | train_loss=1.0670 acc=0.4311 | val_loss=1.0736 acc=0.4317 | time=24.4s | \n",
            "Epoch 005 | train_loss=1.0680 acc=0.4171 | val_loss=1.0712 acc=0.4317 | time=24.4s | \n",
            "Epoch 006 | train_loss=1.0287 acc=0.4854 | val_loss=1.0088 acc=0.5652 | time=24.5s | \n",
            "Epoch 007 | train_loss=0.9581 acc=0.5821 | val_loss=0.9513 acc=0.5776 | time=24.5s | \n",
            "Epoch 008 | train_loss=0.9300 acc=0.5868 | val_loss=1.0152 acc=0.5730 | time=24.6s | \n",
            "Epoch 009 | train_loss=0.8953 acc=0.6062 | val_loss=0.9252 acc=0.5963 | time=24.5s | \n",
            "Epoch 010 | train_loss=0.8730 acc=0.6151 | val_loss=0.9047 acc=0.6025 | time=24.6s | \n",
            "Epoch 011 | train_loss=0.8523 acc=0.6299 | val_loss=0.9220 acc=0.5807 | time=24.5s | \n",
            "Epoch 012 | train_loss=0.8307 acc=0.6400 | val_loss=0.9153 acc=0.5916 | time=24.5s | \n",
            "Epoch 013 | train_loss=0.8096 acc=0.6548 | val_loss=0.9975 acc=0.5916 | time=24.6s | \n",
            "Epoch 014 | train_loss=0.8072 acc=0.6548 | val_loss=0.9460 acc=0.5792 | time=24.6s | \n",
            "Epoch 015 | train_loss=0.7670 acc=0.6641 | val_loss=0.9521 acc=0.5776 | time=24.7s | \n",
            "Epoch 016 | train_loss=0.7530 acc=0.6711 | val_loss=0.9601 acc=0.5683 | time=24.7s | \n",
            "Epoch 017 | train_loss=0.7001 acc=0.6979 | val_loss=0.9703 acc=0.5807 | time=24.6s | \n",
            "Epoch 018 | train_loss=0.6626 acc=0.7134 | val_loss=1.0157 acc=0.5466 | time=24.8s | \n",
            "Epoch 019 | train_loss=0.6526 acc=0.7250 | val_loss=0.9992 acc=0.5807 | time=24.7s | \n",
            "Epoch 020 | train_loss=0.6222 acc=0.7301 | val_loss=1.0325 acc=0.5978 | time=24.7s | \n",
            "Epoch 021 | train_loss=0.5965 acc=0.7394 | val_loss=1.0185 acc=0.5947 | time=24.5s | \n",
            "Epoch 022 | train_loss=0.5760 acc=0.7697 | val_loss=0.9362 acc=0.6040 | time=24.7s | \n",
            "Epoch 023 | train_loss=0.5174 acc=0.7942 | val_loss=1.1092 acc=0.6009 | time=24.5s | \n",
            "Epoch 024 | train_loss=0.4906 acc=0.8047 | val_loss=1.0509 acc=0.6071 | time=24.6s | \n",
            "Epoch 025 | train_loss=0.4675 acc=0.8171 | val_loss=1.0893 acc=0.6134 | time=24.4s | \n",
            "Epoch 026 | train_loss=0.4420 acc=0.8287 | val_loss=1.1572 acc=0.6149 | time=24.6s | \n",
            "Epoch 027 | train_loss=0.4376 acc=0.8311 | val_loss=1.1652 acc=0.5963 | time=24.5s | \n",
            "Epoch 028 | train_loss=0.4023 acc=0.8493 | val_loss=1.1689 acc=0.6102 | time=24.6s | \n",
            "Epoch 029 | train_loss=0.3674 acc=0.8652 | val_loss=1.2273 acc=0.6102 | time=24.5s | \n",
            "Epoch 030 | train_loss=0.3547 acc=0.8695 | val_loss=1.3524 acc=0.6149 | time=24.4s | \n",
            "★ Early stopping at epoch 30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▂▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██████▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▆▇▆▇█▇▇▇▇▇▆▇▅▇▇▇█▇███▇███</td></tr><tr><td>validation_loss</td><td>▄▄▄▄▄▃▂▃▁▁▁▁▂▂▂▂▂▃▂▃▃▁▄▃▄▅▅▅▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>30</td></tr><tr><td>train_accuracy</td><td>0.86951</td></tr><tr><td>train_loss</td><td>0.35468</td></tr><tr><td>validation_accuracy</td><td>0.61491</td></tr><tr><td>validation_loss</td><td>1.35241</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">electric-elevator-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/jm7x1yaw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/jm7x1yaw</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_230857-jm7x1yaw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 23:21:21,543] Trial 2 finished with value: 0.9047478267124721 and parameters: {'lr': 0.0006332497165117276, 'weight_decay': 6.1060665269356535e-06, 'num_blocks': 2, 'num_heads': 2, 'num_segments': 5}. Best is trial 1 with value: 0.8704692948432196.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_232121-hri7etrk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/hri7etrk' target=\"_blank\">sage-waterfall-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/hri7etrk' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-9/runs/hri7etrk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.07e-03, wd=1.51e-05, blocks=3, heads=4, segs=5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-05-01 23:21:28,840] Trial 3 failed with parameters: {'lr': 0.005073462485729068, 'weight_decay': 1.5078657828589909e-05, 'num_blocks': 3, 'num_heads': 4, 'num_segments': 5} because of the following error: OutOfMemoryError('CUDA out of memory. Tried to allocate 2.48 GiB. GPU 0 has a total capacity of 39.56 GiB of which 2.06 GiB is free. Process 734759 has 37.49 GiB memory in use. Of the allocated memory 35.18 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)').\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-173f8f6107e4>\", line 126, in objective_holdout\n",
            "    logits = model(X)\n",
            "             ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/2025_Lab_Research/models.py\", line 367, in forward\n",
            "    z5 = self.stm(z4)\n",
            "         ^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/2025_Lab_Research/models.py\", line 168, in forward\n",
            "    seq = self.blocks(seq) # [B, 1+S*C, D]\n",
            "          ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\", line 250, in forward\n",
            "    input = module(input)\n",
            "            ^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/2025_Lab_Research/models.py\", line 47, in forward\n",
            "    y, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\", line 1373, in forward\n",
            "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
            "                                       ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 6374, in multi_head_attention_forward\n",
            "    attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\", line 2140, in softmax\n",
            "    ret = input.softmax(dim)\n",
            "          ^^^^^^^^^^^^^^^^^^\n",
            "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.48 GiB. GPU 0 has a total capacity of 39.56 GiB of which 2.06 GiB is free. Process 734759 has 37.49 GiB memory in use. Of the allocated memory 35.18 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "[W 2025-05-01 23:21:28,861] Trial 3 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.48 GiB. GPU 0 has a total capacity of 39.56 GiB of which 2.06 GiB is free. Process 734759 has 37.49 GiB memory in use. Of the allocated memory 35.18 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-173f8f6107e4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     )\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;31m# ─── 결과 출력 ────────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-173f8f6107e4>\u001b[0m in \u001b[0;36mobjective_holdout\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/2025_Lab_Research/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0mz4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrtm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;31m# 3) Synchronous transformer → [B, S, C, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m         \u001b[0mz5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m         \u001b[0;31m# 4) Temporal transformer → [B, M, S*C]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0mz6\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mttm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/2025_Lab_Research/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0;31m# 6) TransformerBlock × K\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [B, 1+S*C, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0;31m# 7) Remove Classification token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/drive/MyDrive/2025_Lab_Research/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;31m# 1) Self-Attention: Q=K=V=LN(x), attn_output: [B, Seq, D]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;31m# 2) Residual connection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1371\u001b[0m             )\n\u001b[1;32m   1372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1373\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1374\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1375\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6372\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6373\u001b[0m             \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6374\u001b[0;31m         \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6375\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdropout_p\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6376\u001b[0m             \u001b[0mattn_output_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_output_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdropout_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   2138\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2139\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2140\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2142\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.48 GiB. GPU 0 has a total capacity of 39.56 GiB of which 2.06 GiB is free. Process 734759 has 37.49 GiB memory in use. Of the allocated memory 35.18 GiB is allocated by PyTorch, and 1.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k2WYuscEo4U-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NiH4wDFdo4X1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UH8MgjLZo4c8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with dropout = 0.3\n",
        "- Transformer Model: Revised the model to add a dropout parameter in self-attention head and transformer block\n",
        "- Change the Learning Rate Scheduler from LRstep to ReduceLROnPlateau for improving the validation loss decrease"
      ],
      "metadata": {
        "id": "rRd0yAXdJU8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-5, 5e-3\n",
        "WD_MIN, WD_MAX     = 1e-6, 1e-3\n",
        "L1_MIN, L1_MAX     = 1e-7, 1e-3\n",
        "\n",
        "NUM_FILTERS        = 120\n",
        "NUM_BLOCK_CHOICES  = [1, 2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 3, 4]\n",
        "SEGMENT_CHOICES    = [5, 10, 15]\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "MAX_EPOCHS  = 100   # 100 epochs 고정\n",
        "PATIENCE    = 20    # Early stopping patience\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─ 1) Sample hyperparameters ────────────────────────────────\n",
        "    lr           = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    use_l1       = trial.suggest_categorical(\"use_l1\", [False, True])\n",
        "    l1_lambda    = trial.suggest_float(\"l1_lambda\", L1_MIN, L1_MAX, log=True) if use_l1 else 0.0\n",
        "\n",
        "    step_size    = trial.suggest_int(\"step_size\", 10, 20, step=5)\n",
        "    gamma        = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"L1={'on' if use_l1 else 'off'}{f'({l1_lambda:.2e})' if use_l1 else ''}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}, \"\n",
        "        f\"step_size={step_size}, gamma={gamma:.2f}\"\n",
        "    )\n",
        "\n",
        "    # ─ 2) Load data ────────────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─ 3) Hold-out split ──────────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─ 4) W&B init ────────────────────────────────────────────────\n",
        "    wandb.init(project=\"eeg-holdout-tuning-4\", config=trial.params)\n",
        "\n",
        "    # ─ 5) Model / optimizer / loss ───────────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay  # L2\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─ 6) Scheduler ───────────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min',\n",
        "                                                           factor=gamma, patience=step_size,\n",
        "                                                           min_lr=1e-6)\n",
        "\n",
        "    # ─ 7) Training loop w/ Early Stopping & Pruning ──────────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # placeholders for best-epoch metrics\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            if l1_lambda > 0:\n",
        "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "                loss    = loss + l1_lambda * l1_norm\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss    += loss.item()\n",
        "                vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step(val_loss)\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_trial-4\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_holdout-4.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout, n_trials=20)\n",
        "\n",
        "    # ─── 결과 출력 ────────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "ZCAhgKr0ATHl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7bef5041-76bb-45b8-ce95-ed26fb78192c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 10:20:21,793] A new study created in RDB with name: eeg_holdout_trial-4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=6.47e-05, wd=3.44e-05, L1=on(3.29e-05), blocks=1, heads=3, segs=5, step_size=15, gamma=0.72\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_102022-2qg6pbs0</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/2qg6pbs0' target=\"_blank\">happy-waterfall-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/2qg6pbs0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/2qg6pbs0</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=17.0697 acc=0.4206 | val_loss=1.0745 acc=0.4317 | time=223.0s\n",
            "Epoch 002 | train_loss=16.8072 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.7s\n",
            "Epoch 003 | train_loss=16.5682 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.7s\n",
            "Epoch 004 | train_loss=16.3461 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.7s\n",
            "Epoch 005 | train_loss=16.1411 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.6s\n",
            "Epoch 006 | train_loss=15.9511 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.8s\n",
            "Epoch 007 | train_loss=15.7722 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.6s\n",
            "Epoch 008 | train_loss=15.6012 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=18.8s\n",
            "Epoch 009 | train_loss=15.4385 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.5s\n",
            "Epoch 010 | train_loss=15.2845 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.6s\n",
            "Epoch 011 | train_loss=15.1332 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.5s\n",
            "Epoch 012 | train_loss=14.9882 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=18.6s\n",
            "Epoch 013 | train_loss=14.8495 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=18.8s\n",
            "Epoch 014 | train_loss=14.7141 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.5s\n",
            "Epoch 015 | train_loss=14.5851 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=18.7s\n",
            "Epoch 016 | train_loss=14.4588 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.7s\n",
            "Epoch 017 | train_loss=14.3388 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.8s\n",
            "Epoch 018 | train_loss=14.2397 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.6s\n",
            "Epoch 019 | train_loss=14.1591 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.6s\n",
            "Epoch 020 | train_loss=14.0810 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.6s\n",
            "Epoch 021 | train_loss=14.0038 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.8s\n",
            "Epoch 022 | train_loss=13.9269 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.7s\n",
            "Epoch 023 | train_loss=13.8523 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=18.6s\n",
            "★ Early stopping at epoch 23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁██████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▆▅▅▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▃▁▂▇▁▂▆▇▃▃▅▅▇▆▇▂▄▅▄▄▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>23</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>13.85227</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07512</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">happy-waterfall-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/2qg6pbs0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/2qg6pbs0</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_102022-2qg6pbs0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 10:30:58,770] Trial 0 finished with value: 1.074448063260033 and parameters: {'lr': 6.469169596116346e-05, 'weight_decay': 3.4401973392369906e-05, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 5, 'use_l1': True, 'l1_lambda': 3.2893090509651604e-05, 'step_size': 15, 'gamma': 0.7165871970832125}. Best is trial 0 with value: 1.074448063260033.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=1.04e-03, wd=4.53e-05, L1=on(5.52e-07), blocks=2, heads=3, segs=5, step_size=10, gamma=0.54\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_103058-z9fyt7jv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/z9fyt7jv' target=\"_blank\">summer-lion-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/z9fyt7jv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/z9fyt7jv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.3416 acc=0.4217 | val_loss=1.0852 acc=0.4317 | time=30.2s\n",
            "Epoch 002 | train_loss=1.3065 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=30.2s\n",
            "Epoch 003 | train_loss=1.2809 acc=0.4311 | val_loss=1.0791 acc=0.4317 | time=30.2s\n",
            "Epoch 004 | train_loss=1.2550 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.1s\n",
            "Epoch 005 | train_loss=1.2329 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.0s\n",
            "Epoch 006 | train_loss=1.2133 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.3s\n",
            "Epoch 007 | train_loss=1.1943 acc=0.4311 | val_loss=1.0780 acc=0.4317 | time=30.1s\n",
            "Epoch 008 | train_loss=1.1818 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=30.0s\n",
            "Epoch 009 | train_loss=1.1666 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=30.3s\n",
            "Epoch 010 | train_loss=1.1535 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=30.1s\n",
            "Epoch 011 | train_loss=1.1423 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.1s\n",
            "Epoch 012 | train_loss=1.1314 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=30.3s\n",
            "Epoch 013 | train_loss=1.1228 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 014 | train_loss=1.1150 acc=0.4311 | val_loss=1.0765 acc=0.4317 | time=30.2s\n",
            "Epoch 015 | train_loss=1.1080 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.2s\n",
            "Epoch 016 | train_loss=1.1029 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.2s\n",
            "Epoch 017 | train_loss=1.0970 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.4s\n",
            "Epoch 018 | train_loss=1.0926 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.1s\n",
            "Epoch 019 | train_loss=1.0896 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.1s\n",
            "Epoch 020 | train_loss=1.0857 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=30.3s\n",
            "Epoch 021 | train_loss=1.0815 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.1s\n",
            "Epoch 022 | train_loss=1.0797 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.2s\n",
            "Epoch 023 | train_loss=1.0788 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=30.4s\n",
            "Epoch 024 | train_loss=1.0758 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=30.0s\n",
            "Epoch 025 | train_loss=1.0744 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.2s\n",
            "Epoch 026 | train_loss=1.0731 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 027 | train_loss=1.0719 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.0s\n",
            "Epoch 028 | train_loss=1.0720 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.3s\n",
            "Epoch 029 | train_loss=1.0715 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=30.2s\n",
            "Epoch 030 | train_loss=1.0710 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.0s\n",
            "Epoch 031 | train_loss=1.0703 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.2s\n",
            "Epoch 032 | train_loss=1.0698 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.0s\n",
            "Epoch 033 | train_loss=1.0697 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.2s\n",
            "Epoch 034 | train_loss=1.0700 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.2s\n",
            "Epoch 035 | train_loss=1.0695 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.1s\n",
            "★ Early stopping at epoch 35\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁██████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▆▆▅▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▄▂▂▁▃▁▂▂▁▂▁▂▁▁▁▁▁▂▁▁▂▂▁▁▁▁▂▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06946</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07497</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">summer-lion-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/z9fyt7jv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/z9fyt7jv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_103058-z9fyt7jv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 10:48:37,548] Trial 1 finished with value: 1.0742701008206321 and parameters: {'lr': 0.0010362332645279116, 'weight_decay': 4.5293156757788826e-05, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5, 'use_l1': True, 'l1_lambda': 5.515543108510157e-07, 'step_size': 10, 'gamma': 0.5430885137659128}. Best is trial 1 with value: 1.0742701008206321.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=2.66e-04, wd=1.38e-06, L1=on(9.28e-06), blocks=1, heads=2, segs=5, step_size=20, gamma=0.33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_104837-tjlw52h3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/tjlw52h3' target=\"_blank\">revived-lion-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/tjlw52h3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/tjlw52h3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=5.4962 acc=0.4283 | val_loss=1.0746 acc=0.4317 | time=17.3s\n",
            "Epoch 002 | train_loss=5.2780 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=16.9s\n",
            "Epoch 003 | train_loss=5.0948 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=16.9s\n",
            "Epoch 004 | train_loss=4.9363 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.3s\n",
            "Epoch 005 | train_loss=4.7950 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=17.0s\n",
            "Epoch 006 | train_loss=4.6692 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=16.7s\n",
            "Epoch 007 | train_loss=4.5498 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.0s\n",
            "Epoch 008 | train_loss=4.4367 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.6s\n",
            "Epoch 009 | train_loss=4.3264 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.7s\n",
            "Epoch 010 | train_loss=4.2226 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=16.9s\n",
            "Epoch 011 | train_loss=4.1222 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=16.9s\n",
            "Epoch 012 | train_loss=4.0256 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=16.9s\n",
            "Epoch 013 | train_loss=3.9319 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.6s\n",
            "Epoch 014 | train_loss=3.8422 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=16.9s\n",
            "Epoch 015 | train_loss=3.7566 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.1s\n",
            "Epoch 016 | train_loss=3.6731 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=16.8s\n",
            "Epoch 017 | train_loss=3.5917 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.0s\n",
            "Epoch 018 | train_loss=3.5112 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=16.9s\n",
            "Epoch 019 | train_loss=3.4340 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=16.8s\n",
            "Epoch 020 | train_loss=3.3575 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=17.0s\n",
            "Epoch 021 | train_loss=3.2829 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.7s\n",
            "Epoch 022 | train_loss=3.2101 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.0s\n",
            "Epoch 023 | train_loss=3.1399 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=16.6s\n",
            "Epoch 024 | train_loss=3.0713 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.8s\n",
            "Epoch 025 | train_loss=3.0038 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=16.8s\n",
            "Epoch 026 | train_loss=2.9592 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=16.8s\n",
            "Epoch 027 | train_loss=2.9378 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.0s\n",
            "Epoch 028 | train_loss=2.9169 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.9s\n",
            "Epoch 029 | train_loss=2.8956 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.0s\n",
            "Epoch 030 | train_loss=2.8758 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.1s\n",
            "Epoch 031 | train_loss=2.8543 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.9s\n",
            "Epoch 032 | train_loss=2.8342 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.8s\n",
            "Epoch 033 | train_loss=2.8135 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.8s\n",
            "Epoch 034 | train_loss=2.7934 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=16.8s\n",
            "Epoch 035 | train_loss=2.7738 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=16.6s\n",
            "Epoch 036 | train_loss=2.7528 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=16.9s\n",
            "Epoch 037 | train_loss=2.7336 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=17.0s\n",
            "Epoch 038 | train_loss=2.7150 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=16.6s\n",
            "★ Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁█████████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▃▂▆▁▆▅▂▂▂▂▇▄▄▃▁▇▄▁▇▆▄▃▂▄█▃▄▄▃▄▄▄▄▃▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>2.71504</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07492</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">revived-lion-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/tjlw52h3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/tjlw52h3</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_104837-tjlw52h3/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 10:59:22,037] Trial 2 finished with value: 1.0742079076312838 and parameters: {'lr': 0.0002662198643972252, 'weight_decay': 1.375317596527714e-06, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5, 'use_l1': True, 'l1_lambda': 9.280814582588644e-06, 'step_size': 20, 'gamma': 0.3311066554634084}. Best is trial 2 with value: 1.0742079076312838.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=1.45e-03, wd=3.51e-04, L1=on(2.15e-04), blocks=2, heads=3, segs=5, step_size=10, gamma=0.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_105922-ovmxckvx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/ovmxckvx' target=\"_blank\">gallant-energy-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/ovmxckvx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/ovmxckvx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=94.0929 acc=0.4237 | val_loss=1.0755 acc=0.4317 | time=30.2s\n",
            "Epoch 002 | train_loss=76.8012 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.1s\n",
            "Epoch 003 | train_loss=64.9220 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.0s\n",
            "Epoch 004 | train_loss=54.7576 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=30.3s\n",
            "Epoch 005 | train_loss=45.8319 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.2s\n",
            "Epoch 006 | train_loss=38.0489 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.1s\n",
            "Epoch 007 | train_loss=31.3293 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=30.2s\n",
            "Epoch 008 | train_loss=25.5848 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.1s\n",
            "Epoch 009 | train_loss=20.7351 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=30.1s\n",
            "Epoch 010 | train_loss=16.7056 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.2s\n",
            "Epoch 011 | train_loss=13.3832 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=30.2s\n",
            "Epoch 012 | train_loss=10.6744 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=30.2s\n",
            "Epoch 013 | train_loss=8.4858 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 014 | train_loss=6.7391 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.1s\n",
            "Epoch 015 | train_loss=5.3612 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.3s\n",
            "Epoch 016 | train_loss=4.2892 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.2s\n",
            "Epoch 017 | train_loss=3.4642 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=30.0s\n",
            "Epoch 018 | train_loss=2.8380 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.2s\n",
            "Epoch 019 | train_loss=2.3628 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 020 | train_loss=2.0132 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=30.2s\n",
            "Epoch 021 | train_loss=1.7582 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=30.3s\n",
            "Epoch 022 | train_loss=1.5732 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=30.0s\n",
            "Epoch 023 | train_loss=1.4196 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.2s\n",
            "Epoch 024 | train_loss=1.3955 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 025 | train_loss=1.3816 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 026 | train_loss=1.3676 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.3s\n",
            "Epoch 027 | train_loss=1.3545 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.0s\n",
            "Epoch 028 | train_loss=1.3420 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.2s\n",
            "Epoch 029 | train_loss=1.3300 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.2s\n",
            "Epoch 030 | train_loss=1.3184 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 031 | train_loss=1.3076 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.3s\n",
            "★ Early stopping at epoch 31\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁██████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▄▄▃▃▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▅▂▂▅▂▂▆▂▅▁▁█▂▃▄▁▇▃▂▆▂▂▃▃▂▃▂▃▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>31</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.30755</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0747</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gallant-energy-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/ovmxckvx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/ovmxckvx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_105922-ovmxckvx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 11:15:00,197] Trial 3 finished with value: 1.0742230727559043 and parameters: {'lr': 0.0014549118742377173, 'weight_decay': 0.0003509188413769776, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5, 'use_l1': True, 'l1_lambda': 0.0002150115435194463, 'step_size': 10, 'gamma': 0.11897525372056572}. Best is trial 2 with value: 1.0742079076312838.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=4.22e-03, wd=1.19e-04, L1=off, blocks=2, heads=3, segs=10, step_size=20, gamma=0.27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_111500-0jqsjsla</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/0jqsjsla' target=\"_blank\">smart-durian-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/0jqsjsla' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/0jqsjsla</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0722 acc=0.4206 | val_loss=1.0742 acc=0.4317 | time=30.1s\n",
            "Epoch 002 | train_loss=1.0689 acc=0.4311 | val_loss=1.0837 acc=0.4317 | time=30.1s\n",
            "Epoch 003 | train_loss=1.0686 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.2s\n",
            "Epoch 004 | train_loss=1.0686 acc=0.4280 | val_loss=1.0819 acc=0.4317 | time=30.2s\n",
            "Epoch 005 | train_loss=1.0694 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.3s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.1s\n",
            "Epoch 007 | train_loss=1.0697 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.1s\n",
            "Epoch 008 | train_loss=1.0669 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=30.3s\n",
            "Epoch 009 | train_loss=1.0653 acc=0.4311 | val_loss=1.0800 acc=0.4317 | time=30.2s\n",
            "Epoch 010 | train_loss=1.0682 acc=0.4311 | val_loss=1.0765 acc=0.4317 | time=30.3s\n",
            "Epoch 011 | train_loss=1.0680 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.4s\n",
            "Epoch 012 | train_loss=1.0668 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=30.2s\n",
            "Epoch 013 | train_loss=1.0677 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=30.4s\n",
            "Epoch 014 | train_loss=1.0673 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=30.2s\n",
            "Epoch 015 | train_loss=1.0681 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=30.1s\n",
            "Epoch 016 | train_loss=1.0663 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.6s\n",
            "Epoch 017 | train_loss=1.0671 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.2s\n",
            "Epoch 018 | train_loss=1.0669 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.4s\n",
            "Epoch 019 | train_loss=1.0664 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=30.3s\n",
            "Epoch 020 | train_loss=1.0663 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=30.2s\n",
            "Epoch 021 | train_loss=1.0669 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=30.5s\n",
            "Epoch 022 | train_loss=1.0668 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=30.2s\n",
            "Epoch 023 | train_loss=1.0661 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=30.3s\n",
            "Epoch 024 | train_loss=1.0667 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.4s\n",
            "Epoch 025 | train_loss=1.0663 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.4s\n",
            "Epoch 026 | train_loss=1.0661 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.4s\n",
            "Epoch 027 | train_loss=1.0657 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.3s\n",
            "Epoch 028 | train_loss=1.0659 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=30.5s\n",
            "Epoch 029 | train_loss=1.0661 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=30.6s\n",
            "Epoch 030 | train_loss=1.0667 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.4s\n",
            "Epoch 031 | train_loss=1.0672 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=30.5s\n",
            "Epoch 032 | train_loss=1.0660 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.5s\n",
            "Epoch 033 | train_loss=1.0667 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=30.5s\n",
            "Epoch 034 | train_loss=1.0658 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.7s\n",
            "Epoch 035 | train_loss=1.0670 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=30.5s\n",
            "Epoch 036 | train_loss=1.0653 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.3s\n",
            "Epoch 037 | train_loss=1.0664 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=30.7s\n",
            "Epoch 038 | train_loss=1.0658 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=30.4s\n",
            "Epoch 039 | train_loss=1.0662 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=30.7s\n",
            "★ Early stopping at epoch 39\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁██▆███████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▅▄▄▅▃▅▃▁▄▄▂▃▃▄▂▃▃▂▂▃▂▂▂▂▂▁▂▂▂▃▂▂▂▃▁▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁█▁▇▂▁▁▁▅▃▁▂▂▂▁▂▁▁▁▃▂▂▂▁▁▂▁▂▂▂▂▁▂▁▁▁▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06625</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07492</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">smart-durian-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/0jqsjsla' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/0jqsjsla</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_111500-0jqsjsla/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 11:34:46,786] Trial 4 finished with value: 1.0742088499523343 and parameters: {'lr': 0.00421821567844694, 'weight_decay': 0.00011946286871298436, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 10, 'use_l1': False, 'step_size': 20, 'gamma': 0.26986870444488426}. Best is trial 2 with value: 1.0742079076312838.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=1.41e-05, wd=4.77e-06, L1=on(2.59e-05), blocks=2, heads=4, segs=10, step_size=20, gamma=0.67\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_113446-q79e4mmo</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/q79e4mmo' target=\"_blank\">visionary-wave-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/q79e4mmo' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-4/runs/q79e4mmo</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=14.3605 acc=0.2975 | val_loss=1.0913 acc=0.4317 | time=36.8s\n",
            "Epoch 002 | train_loss=14.2887 acc=0.4311 | val_loss=1.0802 acc=0.4317 | time=36.7s\n",
            "Epoch 003 | train_loss=14.2266 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=37.0s\n",
            "Epoch 004 | train_loss=14.1707 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=36.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-05-01 11:37:44,572] Trial 5 failed with parameters: {'lr': 1.4148595050869106e-05, 'weight_decay': 4.7749477404437315e-06, 'num_blocks': 2, 'num_heads': 4, 'num_segments': 10, 'use_l1': True, 'l1_lambda': 2.5920124189920035e-05, 'step_size': 20, 'gamma': 0.6651222578443667} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-9-e3aa22c865ab>\", line 136, in objective_holdout\n",
            "    tloss    += loss.item()\n",
            "                ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-05-01 11:37:44,574] Trial 5 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e3aa22c865ab>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     )\n\u001b[0;32m--> 214\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;31m# ─── 결과 출력 ────────────────────────────────────────────────\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-e3aa22c865ab>\u001b[0m in \u001b[0;36mobjective_holdout\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    134\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0mtloss\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m             \u001b[0mtcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m             \u001b[0mttotal\u001b[0m   \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jRhtrfLMJTC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1MVlcT1zJTLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test with dropout = 0.2 Model\n",
        "- Cause Model Overfitting\n",
        "- Validation Loss fastly decreases and increases after that\n",
        "- Validation Accuracy increases, but the Validation Loss does not improve after some amount of epochs\n",
        "- This is due to the overfitting and over confidence that model predicts the correct label as answer but cannot sure about it's answer"
      ],
      "metadata": {
        "id": "C60JEqs7JXhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-5, 5e-3\n",
        "WD_MIN, WD_MAX     = 1e-6, 1e-3\n",
        "L1_MIN, L1_MAX     = 1e-7, 1e-3\n",
        "\n",
        "NUM_FILTERS        = 120\n",
        "NUM_BLOCK_CHOICES  = [1, 2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 3, 4]\n",
        "SEGMENT_CHOICES    = [5, 10, 15]\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "MAX_EPOCHS  = 100   # 100 epochs 고정\n",
        "PATIENCE    = 20    # Early stopping patience\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # ─ 1) Sample hyperparameters ────────────────────────────────\n",
        "    lr           = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    use_l1       = trial.suggest_categorical(\"use_l1\", [False, True])\n",
        "    l1_lambda    = trial.suggest_float(\"l1_lambda\", L1_MIN, L1_MAX, log=True) if use_l1 else 0.0\n",
        "\n",
        "    step_size    = trial.suggest_int(\"step_size\", 10, 30, step=10)\n",
        "    gamma        = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, \"\n",
        "        f\"L1={'on' if use_l1 else 'off'}{f'({l1_lambda:.2e})' if use_l1 else ''}, \"\n",
        "        f\"blocks={num_blocks}, heads={num_heads}, segs={num_segments}, \"\n",
        "        f\"step_size={step_size}, gamma={gamma:.2f}\"\n",
        "    )\n",
        "\n",
        "    # ─ 2) Load data ────────────────────────────────────────────────\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # ─ 3) Hold-out split ──────────────────────────────────────────\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # ─ 4) W&B init ────────────────────────────────────────────────\n",
        "    wandb.init(project=\"eeg-holdout-tuning-3\", config=trial.params)\n",
        "\n",
        "    # ─ 5) Model / optimizer / loss ───────────────────────────────\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay  # L2\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─ 6) Scheduler ───────────────────────────────────────────────\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=step_size,\n",
        "        gamma=gamma\n",
        "    )\n",
        "\n",
        "    # ─ 7) Training loop w/ Early Stopping & Pruning ──────────────\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # placeholders for best-epoch metrics\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            if l1_lambda > 0:\n",
        "                l1_norm = sum(p.abs().sum() for p in model.parameters())\n",
        "                loss    = loss + l1_lambda * l1_norm\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss    += loss.item()\n",
        "                vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # — report & pruning check —\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            wandb.finish()\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        # — print & log —\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "        wandb.log({\n",
        "            \"epoch\":               epoch,\n",
        "            \"train_loss\":          train_loss,\n",
        "            \"train_accuracy\":      train_acc,\n",
        "            \"validation_loss\":     val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        # — early stopping logic & save best metrics —\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_trial-3\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_holdout-3.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout, n_trials=30)\n",
        "\n",
        "    # ─── 결과 출력 ────────────────────────────────────────────────\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss       = {best.value:.6f}\")\n",
        "    print(f\"best_train_loss     = {best.user_attrs['best_train_loss']:.6f}\")\n",
        "    print(f\"best_train_accuracy = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(f\"best_val_accuracy   = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "gZyVBUKOQeMn",
        "outputId": "a71dabbd-9397-4b30-df60-90caac3cb4f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 05:26:38,791] A new study created in RDB with name: eeg_holdout_trial-3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=3.77e-03, wd=2.27e-06, L1=off, blocks=3, heads=3, segs=15, step_size=20, gamma=0.39\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>██▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▃▂▄▂▁▃▁▂▂▁▁▁▂▂▁▂▁▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>50</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>3.89059</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0748</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fine-sunset-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-2/runs/5bczqbqn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-2/runs/5bczqbqn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_043203-5bczqbqn/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_052638-nwsfl4au</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/nwsfl4au' target=\"_blank\">sunny-deluge-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/nwsfl4au' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/nwsfl4au</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0715 acc=0.4291 | val_loss=1.0786 acc=0.4317 | time=38.8s\n",
            "Epoch 002 | train_loss=1.0694 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=38.6s\n",
            "Epoch 003 | train_loss=1.0699 acc=0.4260 | val_loss=1.0767 acc=0.4317 | time=38.7s\n",
            "Epoch 004 | train_loss=1.0687 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=38.5s\n",
            "Epoch 005 | train_loss=1.0678 acc=0.4221 | val_loss=1.0767 acc=0.4317 | time=38.7s\n",
            "Epoch 006 | train_loss=1.0675 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=38.6s\n",
            "Epoch 007 | train_loss=1.0685 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=38.7s\n",
            "Epoch 008 | train_loss=1.0674 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=38.7s\n",
            "Epoch 009 | train_loss=1.0675 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=38.8s\n",
            "Epoch 010 | train_loss=1.0671 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=38.8s\n",
            "Epoch 011 | train_loss=1.0670 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=38.8s\n",
            "Epoch 012 | train_loss=1.0665 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=38.7s\n",
            "Epoch 013 | train_loss=1.0665 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=38.8s\n",
            "Epoch 014 | train_loss=1.0673 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=38.6s\n",
            "Epoch 015 | train_loss=1.0668 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=38.5s\n",
            "Epoch 016 | train_loss=1.0666 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=38.7s\n",
            "Epoch 017 | train_loss=1.0679 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=38.6s\n",
            "Epoch 018 | train_loss=1.0684 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=38.6s\n",
            "Epoch 019 | train_loss=1.0667 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=38.6s\n",
            "Epoch 020 | train_loss=1.0675 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=38.9s\n",
            "Epoch 021 | train_loss=1.0661 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=38.8s\n",
            "Epoch 022 | train_loss=1.0660 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=38.8s\n",
            "Epoch 023 | train_loss=1.0671 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=38.6s\n",
            "Epoch 024 | train_loss=1.0660 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=38.7s\n",
            "Epoch 025 | train_loss=1.0657 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=38.5s\n",
            "Epoch 026 | train_loss=1.0667 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=38.6s\n",
            "Epoch 027 | train_loss=1.0666 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=38.6s\n",
            "Epoch 028 | train_loss=1.0657 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=38.6s\n",
            "Epoch 029 | train_loss=1.0661 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=38.8s\n",
            "Epoch 030 | train_loss=1.0662 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=38.8s\n",
            "Epoch 031 | train_loss=1.0665 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=38.8s\n",
            "Epoch 032 | train_loss=1.0658 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=38.8s\n",
            "Epoch 033 | train_loss=1.0662 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=38.8s\n",
            "Epoch 034 | train_loss=1.0656 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=38.7s\n",
            "★ Early stopping at epoch 34\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▆█▄█▁█████████████████████████████</td></tr><tr><td>train_loss</td><td>█▆▆▅▄▃▄▃▃▃▃▂▂▃▂▂▄▄▂▃▂▂▃▁▁▂▂▁▂▂▂▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▅▁▅▅▃▂▂▁▁▂▃▁▂▁▂▂▂▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06561</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0747</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sunny-deluge-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/nwsfl4au' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/nwsfl4au</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_052638-nwsfl4au/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 05:48:37,850] Trial 0 finished with value: 1.0742357401620775 and parameters: {'lr': 0.003765783429817261, 'weight_decay': 2.2676625492339326e-06, 'num_blocks': 3, 'num_heads': 3, 'num_segments': 15, 'use_l1': False, 'step_size': 20, 'gamma': 0.391554470062229}. Best is trial 0 with value: 1.0742357401620775.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=1.64e-03, wd=1.76e-05, L1=off, blocks=1, heads=2, segs=5, step_size=30, gamma=0.29\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_054837-ys63kv5f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/ys63kv5f' target=\"_blank\">copper-yogurt-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/ys63kv5f' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/ys63kv5f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0704 acc=0.4264 | val_loss=1.0764 acc=0.4317 | time=17.5s\n",
            "Epoch 002 | train_loss=1.0709 acc=0.4237 | val_loss=1.0755 acc=0.4317 | time=17.7s\n",
            "Epoch 003 | train_loss=1.0689 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=17.4s\n",
            "Epoch 004 | train_loss=1.0687 acc=0.4311 | val_loss=1.0766 acc=0.5497 | time=17.7s\n",
            "Epoch 005 | train_loss=1.0689 acc=0.4237 | val_loss=1.0878 acc=0.4317 | time=17.5s\n",
            "Epoch 006 | train_loss=1.0701 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=17.6s\n",
            "Epoch 007 | train_loss=1.0669 acc=0.4311 | val_loss=1.0740 acc=0.4317 | time=17.8s\n",
            "Epoch 008 | train_loss=1.0350 acc=0.4761 | val_loss=0.9835 acc=0.5668 | time=17.3s\n",
            "Epoch 009 | train_loss=0.9477 acc=0.5852 | val_loss=0.9568 acc=0.5435 | time=17.4s\n",
            "Epoch 010 | train_loss=0.9278 acc=0.5899 | val_loss=0.9399 acc=0.5637 | time=17.4s\n",
            "Epoch 011 | train_loss=0.8934 acc=0.6058 | val_loss=0.9082 acc=0.5590 | time=17.4s\n",
            "Epoch 012 | train_loss=0.8634 acc=0.6237 | val_loss=0.9219 acc=0.5823 | time=17.7s\n",
            "Epoch 013 | train_loss=0.8254 acc=0.6435 | val_loss=0.9576 acc=0.6227 | time=17.6s\n",
            "Epoch 014 | train_loss=0.7979 acc=0.6548 | val_loss=0.9165 acc=0.6366 | time=17.3s\n",
            "Epoch 015 | train_loss=0.7567 acc=0.6750 | val_loss=0.8679 acc=0.6444 | time=17.6s\n",
            "Epoch 016 | train_loss=0.7293 acc=0.6734 | val_loss=0.8308 acc=0.6382 | time=17.3s\n",
            "Epoch 017 | train_loss=0.6873 acc=0.7103 | val_loss=0.8092 acc=0.6398 | time=17.6s\n",
            "Epoch 018 | train_loss=0.6220 acc=0.7282 | val_loss=0.8295 acc=0.6429 | time=17.5s\n",
            "Epoch 019 | train_loss=0.5664 acc=0.7472 | val_loss=0.8053 acc=0.6661 | time=17.5s\n",
            "Epoch 020 | train_loss=0.5311 acc=0.7678 | val_loss=0.8430 acc=0.6335 | time=17.8s\n",
            "Epoch 021 | train_loss=0.5714 acc=0.7546 | val_loss=0.8029 acc=0.6770 | time=17.4s\n",
            "Epoch 022 | train_loss=0.4927 acc=0.7814 | val_loss=0.9707 acc=0.6630 | time=17.4s\n",
            "Epoch 023 | train_loss=0.4305 acc=0.8132 | val_loss=0.9705 acc=0.6910 | time=17.5s\n",
            "Epoch 024 | train_loss=0.3905 acc=0.8439 | val_loss=0.9625 acc=0.6630 | time=17.4s\n",
            "Epoch 025 | train_loss=0.3653 acc=0.8563 | val_loss=1.2743 acc=0.6491 | time=17.2s\n",
            "Epoch 026 | train_loss=0.3631 acc=0.8606 | val_loss=1.1648 acc=0.6599 | time=17.3s\n",
            "Epoch 027 | train_loss=0.3197 acc=0.8761 | val_loss=1.3001 acc=0.6351 | time=17.5s\n",
            "Epoch 028 | train_loss=0.2872 acc=0.8843 | val_loss=1.3284 acc=0.6708 | time=17.5s\n",
            "Epoch 029 | train_loss=0.2180 acc=0.9216 | val_loss=1.6784 acc=0.6553 | time=17.6s\n",
            "Epoch 030 | train_loss=0.1831 acc=0.9324 | val_loss=1.7803 acc=0.6522 | time=17.3s\n",
            "Epoch 031 | train_loss=0.0992 acc=0.9693 | val_loss=1.9102 acc=0.6661 | time=17.5s\n",
            "Epoch 032 | train_loss=0.0703 acc=0.9771 | val_loss=2.1234 acc=0.6693 | time=17.1s\n",
            "Epoch 033 | train_loss=0.0574 acc=0.9817 | val_loss=2.3842 acc=0.6584 | time=17.4s\n",
            "Epoch 034 | train_loss=0.0464 acc=0.9860 | val_loss=2.6706 acc=0.6599 | time=17.4s\n",
            "Epoch 035 | train_loss=0.0372 acc=0.9903 | val_loss=2.7793 acc=0.6630 | time=17.5s\n",
            "Epoch 036 | train_loss=0.0298 acc=0.9915 | val_loss=2.9492 acc=0.6522 | time=17.5s\n",
            "Epoch 037 | train_loss=0.0233 acc=0.9930 | val_loss=3.2666 acc=0.6553 | time=17.5s\n",
            "Epoch 038 | train_loss=0.0189 acc=0.9961 | val_loss=3.3744 acc=0.6599 | time=17.7s\n",
            "Epoch 039 | train_loss=0.0151 acc=0.9969 | val_loss=3.6688 acc=0.6599 | time=17.6s\n",
            "Epoch 040 | train_loss=0.0117 acc=0.9981 | val_loss=3.8972 acc=0.6615 | time=17.4s\n",
            "Epoch 041 | train_loss=0.0059 acc=0.9988 | val_loss=4.1621 acc=0.6599 | time=17.4s\n",
            "★ Early stopping at epoch 41\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>████████▇▇▇▇▆▆▆▆▅▅▅▄▅▄▄▄▃▃▃▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▄▁▁▁▅▄▅▄▅▆▇▇▇▇▇▇▆█▇█▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▂▂▂▃▃▃▄▄▅▅▅▆▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>train_accuracy</td><td>0.99883</td></tr><tr><td>train_loss</td><td>0.0059</td></tr><tr><td>validation_accuracy</td><td>0.65994</td></tr><tr><td>validation_loss</td><td>4.16215</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">copper-yogurt-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/ys63kv5f' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/ys63kv5f</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_054837-ys63kv5f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 06:00:37,715] Trial 1 finished with value: 0.8029196219784873 and parameters: {'lr': 0.0016414239569236648, 'weight_decay': 1.7560316464941144e-05, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5, 'use_l1': False, 'step_size': 30, 'gamma': 0.2883742978308369}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=4.43e-03, wd=2.92e-04, L1=off, blocks=1, heads=3, segs=15, step_size=30, gamma=0.58\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_060037-0dtg44o1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/0dtg44o1' target=\"_blank\">feasible-elevator-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/0dtg44o1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/0dtg44o1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0763 acc=0.4163 | val_loss=1.0788 acc=0.4317 | time=18.1s\n",
            "Epoch 002 | train_loss=1.0697 acc=0.4276 | val_loss=1.0811 acc=0.4317 | time=18.0s\n",
            "Epoch 003 | train_loss=1.0676 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.1s\n",
            "Epoch 004 | train_loss=1.0698 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=18.0s\n",
            "Epoch 005 | train_loss=1.0690 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.2s\n",
            "Epoch 006 | train_loss=1.0680 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.0s\n",
            "Epoch 007 | train_loss=1.0684 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.0s\n",
            "Epoch 008 | train_loss=1.0667 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=17.9s\n",
            "Epoch 009 | train_loss=1.0677 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=18.0s\n",
            "Epoch 010 | train_loss=1.0670 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=18.2s\n",
            "Epoch 011 | train_loss=1.0672 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=17.8s\n",
            "Epoch 012 | train_loss=1.0671 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.2s\n",
            "Epoch 013 | train_loss=1.0673 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=17.9s\n",
            "Epoch 014 | train_loss=1.0669 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=18.1s\n",
            "Epoch 015 | train_loss=1.0665 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=18.1s\n",
            "Epoch 016 | train_loss=1.0671 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.1s\n",
            "Epoch 017 | train_loss=1.0668 acc=0.4311 | val_loss=1.0788 acc=0.4317 | time=18.1s\n",
            "Epoch 018 | train_loss=1.0664 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=18.1s\n",
            "Epoch 019 | train_loss=1.0671 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.2s\n",
            "Epoch 020 | train_loss=1.0671 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.9s\n",
            "Epoch 021 | train_loss=1.0672 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.0s\n",
            "Epoch 022 | train_loss=1.0668 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.0s\n",
            "Epoch 023 | train_loss=1.0668 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.0s\n",
            "Epoch 024 | train_loss=1.0667 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=18.1s\n",
            "Epoch 025 | train_loss=1.0665 acc=0.4311 | val_loss=1.0767 acc=0.4317 | time=18.1s\n",
            "Epoch 026 | train_loss=1.0666 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=18.1s\n",
            "Epoch 027 | train_loss=1.0664 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.0s\n",
            "Epoch 028 | train_loss=1.0673 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.0s\n",
            "Epoch 029 | train_loss=1.0667 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.0s\n",
            "Epoch 030 | train_loss=1.0666 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=18.0s\n",
            "Epoch 031 | train_loss=1.0665 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.1s\n",
            "Epoch 032 | train_loss=1.0663 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.1s\n",
            "Epoch 033 | train_loss=1.0659 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.9s\n",
            "Epoch 034 | train_loss=1.0667 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=18.0s\n",
            "Epoch 035 | train_loss=1.0663 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.9s\n",
            "★ Early stopping at epoch 35\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▆█████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▄▂▄▃▂▃▂▂▂▂▂▂▂▁▂▂▁▂▂▂▂▂▂▁▁▁▂▂▁▁▁▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▆█▁▃▁▁▁▁▂▂▂▁▁▂▁▂▆▁▁▁▂▁▁▂▃▃▂▁▁▂▂▁▁▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>35</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06629</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07477</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">feasible-elevator-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/0dtg44o1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/0dtg44o1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_060037-0dtg44o1/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 06:11:11,814] Trial 2 finished with value: 1.074278161639259 and parameters: {'lr': 0.004432919051350885, 'weight_decay': 0.0002924091059973708, 'num_blocks': 1, 'num_heads': 3, 'num_segments': 15, 'use_l1': False, 'step_size': 30, 'gamma': 0.5790497300853228}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.27e-05, wd=2.07e-06, L1=off, blocks=2, heads=2, segs=15, step_size=30, gamma=0.52\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_061111-achwvrp3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/achwvrp3' target=\"_blank\">solar-dew-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/achwvrp3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/achwvrp3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0902 acc=0.3755 | val_loss=1.0777 acc=0.4317 | time=23.6s\n",
            "Epoch 002 | train_loss=1.0688 acc=0.4311 | val_loss=1.0808 acc=0.4317 | time=23.6s\n",
            "Epoch 003 | train_loss=1.0675 acc=0.4311 | val_loss=1.0771 acc=0.4317 | time=23.5s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4311 | val_loss=1.0773 acc=0.4317 | time=23.6s\n",
            "Epoch 005 | train_loss=1.0665 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=23.5s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=23.5s\n",
            "Epoch 007 | train_loss=1.0664 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=23.8s\n",
            "Epoch 008 | train_loss=1.0671 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=23.7s\n",
            "Epoch 009 | train_loss=1.0674 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=23.6s\n",
            "Epoch 010 | train_loss=1.0669 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=23.6s\n",
            "Epoch 011 | train_loss=1.0669 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=23.7s\n",
            "Epoch 012 | train_loss=1.0665 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=23.5s\n",
            "Epoch 013 | train_loss=1.0671 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=23.4s\n",
            "Epoch 014 | train_loss=1.0667 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=23.7s\n",
            "Epoch 015 | train_loss=1.0671 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=23.7s\n",
            "Epoch 016 | train_loss=1.0669 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=23.7s\n",
            "Epoch 017 | train_loss=1.0668 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=23.6s\n",
            "Epoch 018 | train_loss=1.0664 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=23.6s\n",
            "Epoch 019 | train_loss=1.0667 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=23.5s\n",
            "Epoch 020 | train_loss=1.0662 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=23.5s\n",
            "Epoch 021 | train_loss=1.0671 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=23.5s\n",
            "Epoch 022 | train_loss=1.0663 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=23.7s\n",
            "Epoch 023 | train_loss=1.0673 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=23.8s\n",
            "Epoch 024 | train_loss=1.0671 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=23.6s\n",
            "Epoch 025 | train_loss=1.0660 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=23.7s\n",
            "Epoch 026 | train_loss=1.0664 acc=0.4311 | val_loss=1.0740 acc=0.4317 | time=23.6s\n",
            "Epoch 027 | train_loss=1.0655 acc=0.4311 | val_loss=1.0733 acc=0.4317 | time=23.5s\n",
            "Epoch 028 | train_loss=1.0624 acc=0.4311 | val_loss=1.0635 acc=0.4317 | time=23.4s\n",
            "Epoch 029 | train_loss=1.0212 acc=0.5099 | val_loss=0.9826 acc=0.5559 | time=23.5s\n",
            "Epoch 030 | train_loss=0.9483 acc=0.5794 | val_loss=1.0158 acc=0.5435 | time=23.7s\n",
            "Epoch 031 | train_loss=0.9227 acc=0.5953 | val_loss=0.9639 acc=0.5559 | time=23.7s\n",
            "Epoch 032 | train_loss=0.8923 acc=0.6136 | val_loss=0.9576 acc=0.5792 | time=23.6s\n",
            "Epoch 033 | train_loss=0.8699 acc=0.6272 | val_loss=0.9244 acc=0.5901 | time=23.7s\n",
            "Epoch 034 | train_loss=0.8620 acc=0.6357 | val_loss=0.9307 acc=0.5978 | time=23.6s\n",
            "Epoch 035 | train_loss=0.8569 acc=0.6252 | val_loss=0.9438 acc=0.5870 | time=23.5s\n",
            "Epoch 036 | train_loss=0.8329 acc=0.6482 | val_loss=0.9058 acc=0.6134 | time=23.6s\n",
            "Epoch 037 | train_loss=0.8195 acc=0.6520 | val_loss=0.9127 acc=0.6134 | time=23.4s\n",
            "Epoch 038 | train_loss=0.8173 acc=0.6489 | val_loss=0.9020 acc=0.6040 | time=23.5s\n",
            "Epoch 039 | train_loss=0.8281 acc=0.6431 | val_loss=0.9172 acc=0.6040 | time=23.7s\n",
            "Epoch 040 | train_loss=0.8100 acc=0.6497 | val_loss=0.9129 acc=0.5994 | time=23.6s\n",
            "Epoch 041 | train_loss=0.8079 acc=0.6586 | val_loss=0.8952 acc=0.5963 | time=23.7s\n",
            "Epoch 042 | train_loss=0.7809 acc=0.6621 | val_loss=0.8939 acc=0.6087 | time=23.6s\n",
            "Epoch 043 | train_loss=0.7811 acc=0.6703 | val_loss=0.8850 acc=0.6071 | time=23.5s\n",
            "Epoch 044 | train_loss=0.7793 acc=0.6614 | val_loss=0.9116 acc=0.6071 | time=23.4s\n",
            "Epoch 045 | train_loss=0.7636 acc=0.6718 | val_loss=0.8699 acc=0.6087 | time=23.5s\n",
            "Epoch 046 | train_loss=0.7595 acc=0.6769 | val_loss=0.8743 acc=0.6071 | time=23.6s\n",
            "Epoch 047 | train_loss=0.7492 acc=0.6823 | val_loss=0.8855 acc=0.6149 | time=23.6s\n",
            "Epoch 048 | train_loss=0.7496 acc=0.6800 | val_loss=0.8919 acc=0.6273 | time=23.7s\n",
            "Epoch 049 | train_loss=0.7260 acc=0.6944 | val_loss=0.8783 acc=0.6227 | time=23.6s\n",
            "Epoch 050 | train_loss=0.7320 acc=0.6835 | val_loss=0.8782 acc=0.6211 | time=23.7s\n",
            "Epoch 051 | train_loss=0.7229 acc=0.6951 | val_loss=0.8622 acc=0.6320 | time=23.5s\n",
            "Epoch 052 | train_loss=0.7311 acc=0.6932 | val_loss=0.8801 acc=0.6320 | time=23.7s\n",
            "Epoch 053 | train_loss=0.6966 acc=0.7095 | val_loss=0.8731 acc=0.6320 | time=23.6s\n",
            "Epoch 054 | train_loss=0.6858 acc=0.7146 | val_loss=0.8970 acc=0.6196 | time=23.6s\n",
            "Epoch 055 | train_loss=0.6757 acc=0.7049 | val_loss=0.8749 acc=0.6273 | time=23.6s\n",
            "Epoch 056 | train_loss=0.6646 acc=0.7200 | val_loss=0.8695 acc=0.6320 | time=23.6s\n",
            "Epoch 057 | train_loss=0.6524 acc=0.7212 | val_loss=0.9429 acc=0.6320 | time=23.5s\n",
            "Epoch 058 | train_loss=0.6532 acc=0.7231 | val_loss=0.9154 acc=0.6335 | time=23.6s\n",
            "Epoch 059 | train_loss=0.6573 acc=0.7181 | val_loss=0.9055 acc=0.6258 | time=23.5s\n",
            "Epoch 060 | train_loss=0.6442 acc=0.7313 | val_loss=0.8971 acc=0.6196 | time=23.4s\n",
            "Epoch 061 | train_loss=0.6095 acc=0.7379 | val_loss=0.8955 acc=0.6382 | time=23.3s\n",
            "Epoch 062 | train_loss=0.5908 acc=0.7499 | val_loss=0.8721 acc=0.6491 | time=23.5s\n",
            "Epoch 063 | train_loss=0.5843 acc=0.7507 | val_loss=0.8855 acc=0.6398 | time=23.6s\n",
            "Epoch 064 | train_loss=0.5794 acc=0.7503 | val_loss=0.8909 acc=0.6335 | time=23.6s\n",
            "Epoch 065 | train_loss=0.5776 acc=0.7495 | val_loss=0.8976 acc=0.6413 | time=23.6s\n",
            "Epoch 066 | train_loss=0.5579 acc=0.7639 | val_loss=0.8860 acc=0.6491 | time=23.6s\n",
            "Epoch 067 | train_loss=0.5498 acc=0.7639 | val_loss=0.9087 acc=0.6444 | time=23.6s\n",
            "Epoch 068 | train_loss=0.5457 acc=0.7612 | val_loss=0.8623 acc=0.6506 | time=23.5s\n",
            "Epoch 069 | train_loss=0.5277 acc=0.7724 | val_loss=0.9022 acc=0.6444 | time=23.6s\n",
            "Epoch 070 | train_loss=0.5391 acc=0.7705 | val_loss=0.8942 acc=0.6506 | time=23.7s\n",
            "Epoch 071 | train_loss=0.5268 acc=0.7744 | val_loss=0.9059 acc=0.6506 | time=23.7s\n",
            "★ Early stopping at epoch 71\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>████████████████▇▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▂▃▂▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▅▅▅▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇██▇██</td></tr><tr><td>validation_loss</td><td>███████████████▄▃▂▂▃▂▂▃▁▁▂▂▁▂▁▁▁▄▃▂▂▂▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>71</td></tr><tr><td>train_accuracy</td><td>0.77437</td></tr><tr><td>train_loss</td><td>0.52679</td></tr><tr><td>validation_accuracy</td><td>0.65062</td></tr><tr><td>validation_loss</td><td>0.90594</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">solar-dew-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/achwvrp3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/achwvrp3</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_061111-achwvrp3/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 06:39:10,259] Trial 3 finished with value: 0.8621873628525507 and parameters: {'lr': 5.2710727169582986e-05, 'weight_decay': 2.0673073320955644e-06, 'num_blocks': 2, 'num_heads': 2, 'num_segments': 15, 'use_l1': False, 'step_size': 30, 'gamma': 0.5198228506140427}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=1.19e-04, wd=2.28e-06, L1=off, blocks=2, heads=3, segs=5, step_size=10, gamma=0.50\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_063910-6hd0ye05</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6hd0ye05' target=\"_blank\">chocolate-salad-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6hd0ye05' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6hd0ye05</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0758 acc=0.4132 | val_loss=1.0746 acc=0.4317 | time=27.9s\n",
            "Epoch 002 | train_loss=1.0662 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=27.9s\n",
            "Epoch 003 | train_loss=1.0669 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=28.0s\n",
            "Epoch 004 | train_loss=1.0659 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=28.1s\n",
            "Epoch 005 | train_loss=1.0668 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=27.9s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=27.9s\n",
            "Epoch 007 | train_loss=1.0669 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=27.9s\n",
            "Epoch 008 | train_loss=1.0679 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=28.2s\n",
            "Epoch 009 | train_loss=1.0675 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=27.8s\n",
            "Epoch 010 | train_loss=1.0674 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=27.8s\n",
            "Epoch 011 | train_loss=1.0669 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=27.9s\n",
            "Epoch 012 | train_loss=1.0660 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=28.0s\n",
            "Epoch 013 | train_loss=1.0670 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=28.1s\n",
            "Epoch 014 | train_loss=1.0669 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=27.9s\n",
            "Epoch 015 | train_loss=1.0664 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=28.0s\n",
            "Epoch 016 | train_loss=1.0661 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=27.8s\n",
            "Epoch 017 | train_loss=1.0665 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=28.1s\n",
            "Epoch 018 | train_loss=1.0661 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=27.9s\n",
            "Epoch 019 | train_loss=1.0663 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=27.8s\n",
            "Epoch 020 | train_loss=1.0656 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=27.7s\n",
            "Epoch 021 | train_loss=1.0662 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=28.1s\n",
            "Epoch 022 | train_loss=1.0658 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=27.8s\n",
            "Epoch 023 | train_loss=1.0667 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=27.9s\n",
            "Epoch 024 | train_loss=1.0656 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=27.9s\n",
            "Epoch 025 | train_loss=1.0663 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=27.9s\n",
            "Epoch 026 | train_loss=1.0661 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=28.1s\n",
            "Epoch 027 | train_loss=1.0665 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=27.9s\n",
            "Epoch 028 | train_loss=1.0669 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=27.8s\n",
            "★ Early stopping at epoch 28\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁███████████████████████████</td></tr><tr><td>train_loss</td><td>█▁▂▁▂▂▂▃▂▂▂▁▂▂▂▁▂▁▁▁▁▁▂▁▁▁▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▃▃█▂▁▇▁▅▂▃▂▃▂▂▃▄▁▆▆▃▃▃▃▄▂▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>28</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06686</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07468</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">chocolate-salad-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6hd0ye05' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6hd0ye05</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_063910-6hd0ye05/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 06:52:14,978] Trial 4 finished with value: 1.0743985800516038 and parameters: {'lr': 0.0001190383086698816, 'weight_decay': 2.2762893026346425e-06, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5, 'use_l1': False, 'step_size': 10, 'gamma': 0.5023523291402322}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=4.94e-03, wd=8.78e-05, L1=off, blocks=3, heads=2, segs=10, step_size=20, gamma=0.72\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_065215-eu3w6rlv</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/eu3w6rlv' target=\"_blank\">eager-aardvark-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/eu3w6rlv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/eu3w6rlv</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0747 acc=0.4171 | val_loss=1.0757 acc=0.4317 | time=31.5s\n",
            "Epoch 002 | train_loss=1.0735 acc=0.4311 | val_loss=1.0768 acc=0.4317 | time=31.5s\n",
            "Epoch 003 | train_loss=1.0687 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=31.3s\n",
            "Epoch 004 | train_loss=1.0692 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=31.5s\n",
            "Epoch 005 | train_loss=1.0676 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.4s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=31.3s\n",
            "Epoch 007 | train_loss=1.0678 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=31.4s\n",
            "Epoch 008 | train_loss=1.0667 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=31.4s\n",
            "Epoch 009 | train_loss=1.0671 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=31.5s\n",
            "Epoch 010 | train_loss=1.0680 acc=0.4311 | val_loss=1.0795 acc=0.4317 | time=31.5s\n",
            "Epoch 011 | train_loss=1.0667 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.5s\n",
            "Epoch 012 | train_loss=1.0659 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.5s\n",
            "Epoch 013 | train_loss=1.0665 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=31.3s\n",
            "Epoch 014 | train_loss=1.0667 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.5s\n",
            "Epoch 015 | train_loss=1.0669 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=31.3s\n",
            "Epoch 016 | train_loss=1.0672 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.5s\n",
            "Epoch 017 | train_loss=1.0667 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.4s\n",
            "Epoch 018 | train_loss=1.0667 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.3s\n",
            "Epoch 019 | train_loss=1.0665 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=31.4s\n",
            "Epoch 020 | train_loss=1.0665 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.5s\n",
            "Epoch 021 | train_loss=1.0665 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.4s\n",
            "Epoch 022 | train_loss=1.0664 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.5s\n",
            "Epoch 023 | train_loss=1.0659 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.2s\n",
            "Epoch 024 | train_loss=1.0660 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.6s\n",
            "Epoch 025 | train_loss=1.0660 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.4s\n",
            "Epoch 026 | train_loss=1.0671 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.5s\n",
            "Epoch 027 | train_loss=1.0668 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.6s\n",
            "Epoch 028 | train_loss=1.0667 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.8s\n",
            "Epoch 029 | train_loss=1.0659 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.4s\n",
            "Epoch 030 | train_loss=1.0667 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.6s\n",
            "Epoch 031 | train_loss=1.0663 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=31.5s\n",
            "Epoch 032 | train_loss=1.0670 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=31.5s\n",
            "Epoch 033 | train_loss=1.0661 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.2s\n",
            "Epoch 034 | train_loss=1.0666 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.6s\n",
            "★ Early stopping at epoch 34\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁█████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▃▄▂▂▃▂▂▃▂▁▂▂▂▂▂▂▂▁▁▁▁▁▁▂▂▂▁▂▁▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▃▄▃▃▁▃▄▃▃█▂▁▁▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06661</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07486</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-aardvark-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/eu3w6rlv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/eu3w6rlv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_065215-eu3w6rlv/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 07:10:07,066] Trial 5 finished with value: 1.0742452769052415 and parameters: {'lr': 0.004942789966732406, 'weight_decay': 8.779834965044777e-05, 'num_blocks': 3, 'num_heads': 2, 'num_segments': 10, 'use_l1': False, 'step_size': 20, 'gamma': 0.7236839001591417}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 6 =====\n",
            " lr=4.34e-04, wd=9.31e-05, L1=off, blocks=3, heads=4, segs=5, step_size=20, gamma=0.19\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_071007-je1gqt18</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/je1gqt18' target=\"_blank\">autumn-bush-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/je1gqt18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/je1gqt18</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0753 acc=0.4151 | val_loss=1.0768 acc=0.4317 | time=46.6s\n",
            "Epoch 002 | train_loss=1.0677 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=46.6s\n",
            "Epoch 003 | train_loss=1.0663 acc=0.4283 | val_loss=1.0762 acc=0.4317 | time=46.6s\n",
            "Epoch 004 | train_loss=1.0706 acc=0.4179 | val_loss=1.0748 acc=0.4317 | time=46.6s\n",
            "Epoch 005 | train_loss=1.0692 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.7s\n",
            "Epoch 006 | train_loss=1.0677 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=46.6s\n",
            "Epoch 007 | train_loss=1.0705 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=46.5s\n",
            "Epoch 008 | train_loss=1.0700 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=46.5s\n",
            "Epoch 009 | train_loss=1.0665 acc=0.4311 | val_loss=1.0809 acc=0.4317 | time=46.5s\n",
            "Epoch 010 | train_loss=1.0669 acc=0.4311 | val_loss=1.0809 acc=0.4317 | time=46.6s\n",
            "Epoch 011 | train_loss=1.0675 acc=0.4311 | val_loss=1.0730 acc=0.4317 | time=46.7s\n",
            "Epoch 012 | train_loss=1.0677 acc=0.4303 | val_loss=1.0747 acc=0.4317 | time=46.5s\n",
            "Epoch 013 | train_loss=1.0657 acc=0.4326 | val_loss=1.0522 acc=0.4969 | time=46.5s\n",
            "Epoch 014 | train_loss=1.0059 acc=0.5262 | val_loss=1.0221 acc=0.5031 | time=46.5s\n",
            "Epoch 015 | train_loss=0.9512 acc=0.5697 | val_loss=1.0299 acc=0.5528 | time=46.6s\n",
            "Epoch 016 | train_loss=0.9447 acc=0.5806 | val_loss=0.9497 acc=0.5559 | time=46.6s\n",
            "Epoch 017 | train_loss=0.9069 acc=0.6035 | val_loss=0.9341 acc=0.5807 | time=46.7s\n",
            "Epoch 018 | train_loss=0.8931 acc=0.6035 | val_loss=0.9175 acc=0.5761 | time=46.6s\n",
            "Epoch 019 | train_loss=0.8580 acc=0.6303 | val_loss=0.8952 acc=0.6009 | time=46.6s\n",
            "Epoch 020 | train_loss=0.8373 acc=0.6303 | val_loss=0.9810 acc=0.5870 | time=46.5s\n",
            "Epoch 021 | train_loss=0.8104 acc=0.6493 | val_loss=0.9084 acc=0.6134 | time=46.6s\n",
            "Epoch 022 | train_loss=0.7717 acc=0.6536 | val_loss=0.8846 acc=0.6165 | time=46.5s\n",
            "Epoch 023 | train_loss=0.7483 acc=0.6715 | val_loss=0.8912 acc=0.6165 | time=46.6s\n",
            "Epoch 024 | train_loss=0.7267 acc=0.6882 | val_loss=0.8917 acc=0.6242 | time=46.6s\n",
            "Epoch 025 | train_loss=0.7047 acc=0.6909 | val_loss=0.8977 acc=0.6289 | time=46.6s\n",
            "Epoch 026 | train_loss=0.6814 acc=0.7014 | val_loss=0.9285 acc=0.6289 | time=46.6s\n",
            "Epoch 027 | train_loss=0.6591 acc=0.7274 | val_loss=0.8929 acc=0.6335 | time=46.6s\n",
            "Epoch 028 | train_loss=0.6300 acc=0.7336 | val_loss=0.9356 acc=0.6289 | time=46.6s\n",
            "Epoch 029 | train_loss=0.6065 acc=0.7367 | val_loss=0.8906 acc=0.6444 | time=46.6s\n",
            "Epoch 030 | train_loss=0.5867 acc=0.7491 | val_loss=0.8883 acc=0.6537 | time=46.6s\n",
            "Epoch 031 | train_loss=0.5550 acc=0.7678 | val_loss=0.8402 acc=0.6304 | time=46.6s\n",
            "Epoch 032 | train_loss=0.5371 acc=0.7736 | val_loss=0.8850 acc=0.6568 | time=46.7s\n",
            "Epoch 033 | train_loss=0.5038 acc=0.7872 | val_loss=0.8582 acc=0.6599 | time=46.5s\n",
            "Epoch 034 | train_loss=0.4926 acc=0.7911 | val_loss=1.0110 acc=0.6599 | time=46.6s\n",
            "Epoch 035 | train_loss=0.4556 acc=0.8109 | val_loss=1.0138 acc=0.6646 | time=46.5s\n",
            "Epoch 036 | train_loss=0.4381 acc=0.8132 | val_loss=1.0065 acc=0.6708 | time=46.6s\n",
            "Epoch 037 | train_loss=0.4231 acc=0.8214 | val_loss=1.0247 acc=0.6677 | time=46.6s\n",
            "Epoch 038 | train_loss=0.4038 acc=0.8295 | val_loss=0.9486 acc=0.6801 | time=46.5s\n",
            "Epoch 039 | train_loss=0.3813 acc=0.8447 | val_loss=1.0193 acc=0.6646 | time=46.6s\n",
            "Epoch 040 | train_loss=0.3584 acc=0.8505 | val_loss=1.0797 acc=0.6630 | time=46.6s\n",
            "Epoch 041 | train_loss=0.3008 acc=0.8730 | val_loss=1.0485 acc=0.6739 | time=46.6s\n",
            "Epoch 042 | train_loss=0.2846 acc=0.8866 | val_loss=1.1269 acc=0.6661 | time=46.4s\n",
            "Epoch 043 | train_loss=0.2763 acc=0.8920 | val_loss=1.1183 acc=0.6786 | time=46.5s\n",
            "Epoch 044 | train_loss=0.2672 acc=0.8917 | val_loss=1.1500 acc=0.6801 | time=46.5s\n",
            "Epoch 045 | train_loss=0.2585 acc=0.8998 | val_loss=1.1978 acc=0.6739 | time=46.6s\n",
            "Epoch 046 | train_loss=0.2464 acc=0.8963 | val_loss=1.1782 acc=0.6817 | time=46.6s\n",
            "Epoch 047 | train_loss=0.2460 acc=0.9014 | val_loss=1.1755 acc=0.6770 | time=46.5s\n",
            "Epoch 048 | train_loss=0.2322 acc=0.9056 | val_loss=1.2127 acc=0.6957 | time=46.6s\n",
            "Epoch 049 | train_loss=0.2307 acc=0.9068 | val_loss=1.2144 acc=0.6879 | time=46.6s\n",
            "Epoch 050 | train_loss=0.2265 acc=0.9107 | val_loss=1.2027 acc=0.6770 | time=46.6s\n",
            "Epoch 051 | train_loss=0.2116 acc=0.9150 | val_loss=1.2868 acc=0.6801 | time=46.7s\n",
            "★ Early stopping at epoch 51\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>███████████▇▇▇▇▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▃▃▄▄▅▅▅▆▆▆▆▆▆▆▇▆▇▇▇▇▇█▇▇▇█████</td></tr><tr><td>validation_loss</td><td>▅▅▅▅▅▅▅▅▅▅▄▄▃▂▂▃▂▂▂▂▂▂▂▁▂▄▄▄▄▃▅▄▅▅▆▆▆▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>51</td></tr><tr><td>train_accuracy</td><td>0.91495</td></tr><tr><td>train_loss</td><td>0.21164</td></tr><tr><td>validation_accuracy</td><td>0.68012</td></tr><tr><td>validation_loss</td><td>1.28684</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">autumn-bush-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/je1gqt18' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/je1gqt18</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_071007-je1gqt18/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 07:49:46,021] Trial 6 finished with value: 0.8401936023008256 and parameters: {'lr': 0.0004336672000136861, 'weight_decay': 9.309453956390096e-05, 'num_blocks': 3, 'num_heads': 4, 'num_segments': 5, 'use_l1': False, 'step_size': 20, 'gamma': 0.19028122261932925}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 7 =====\n",
            " lr=5.62e-05, wd=4.70e-04, L1=on(1.20e-06), blocks=1, heads=2, segs=5, step_size=10, gamma=0.63\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_074946-6tnv8zjn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6tnv8zjn' target=\"_blank\">decent-breeze-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6tnv8zjn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6tnv8zjn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.6787 acc=0.3701 | val_loss=1.0764 acc=0.4317 | time=17.6s\n",
            "Epoch 002 | train_loss=1.6524 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.6s\n",
            "Epoch 003 | train_loss=1.6470 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=17.6s\n",
            "Epoch 004 | train_loss=1.6431 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=17.4s\n",
            "Epoch 005 | train_loss=1.6398 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.3s\n",
            "Epoch 006 | train_loss=1.6350 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.5s\n",
            "Epoch 007 | train_loss=1.6320 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=17.6s\n",
            "Epoch 008 | train_loss=1.6269 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.7s\n",
            "Epoch 009 | train_loss=1.6233 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=17.6s\n",
            "Epoch 010 | train_loss=1.6193 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=17.2s\n",
            "Epoch 011 | train_loss=1.6160 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.7s\n",
            "Epoch 012 | train_loss=1.6133 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=17.5s\n",
            "Epoch 013 | train_loss=1.6109 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=17.6s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_accuracy</td><td>▁████████████</td></tr><tr><td>train_loss</td><td>█▅▅▄▄▃▃▃▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▂▁▃▁▂▃▃▁▃▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.61093</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07528</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">decent-breeze-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6tnv8zjn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/6tnv8zjn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_074946-6tnv8zjn/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 07:53:53,935] Trial 7 pruned. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▸ Trial 7 pruned at epoch 14\n",
            "\n",
            "===== Trial 8 =====\n",
            " lr=1.14e-04, wd=1.26e-05, L1=on(3.97e-05), blocks=3, heads=3, segs=10, step_size=20, gamma=0.73\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_075354-twy7lejy</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/twy7lejy' target=\"_blank\">deep-paper-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/twy7lejy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/twy7lejy</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=21.8731 acc=0.4264 | val_loss=1.0743 acc=0.4317 | time=38.6s\n",
            "Epoch 002 | train_loss=21.0796 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=38.5s\n",
            "Epoch 003 | train_loss=20.3695 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=38.6s\n",
            "Epoch 004 | train_loss=19.7420 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=38.7s\n",
            "Epoch 005 | train_loss=19.1872 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=38.7s\n",
            "Epoch 006 | train_loss=18.7003 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=38.7s\n",
            "Epoch 007 | train_loss=18.2626 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=38.7s\n",
            "Epoch 008 | train_loss=17.8673 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=38.8s\n",
            "Epoch 009 | train_loss=17.5146 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=38.9s\n",
            "Epoch 010 | train_loss=17.2010 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=38.7s\n",
            "Epoch 011 | train_loss=16.9267 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=38.5s\n",
            "Epoch 012 | train_loss=16.6762 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=38.6s\n",
            "Epoch 013 | train_loss=16.4449 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=38.6s\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_accuracy</td><td>▁████████████</td></tr><tr><td>train_loss</td><td>█▇▆▅▅▄▃▃▂▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▆▁█▃▃█▆▅▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>16.44486</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07455</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deep-paper-9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/twy7lejy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/twy7lejy</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_075354-twy7lejy/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 08:02:57,386] Trial 8 pruned. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "▸ Trial 8 pruned at epoch 14\n",
            "\n",
            "===== Trial 9 =====\n",
            " lr=3.74e-05, wd=2.44e-05, L1=off, blocks=3, heads=2, segs=10, step_size=20, gamma=0.17\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_080257-yv536skf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yv536skf' target=\"_blank\">astral-shape-10</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yv536skf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yv536skf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0724 acc=0.4210 | val_loss=1.0745 acc=0.4317 | time=31.6s\n",
            "Epoch 002 | train_loss=1.0672 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=31.3s\n",
            "Epoch 003 | train_loss=1.0669 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.5s\n",
            "Epoch 004 | train_loss=1.0658 acc=0.4311 | val_loss=1.0768 acc=0.4317 | time=31.5s\n",
            "Epoch 005 | train_loss=1.0661 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.3s\n",
            "Epoch 006 | train_loss=1.0667 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.4s\n",
            "Epoch 007 | train_loss=1.0669 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.4s\n",
            "Epoch 008 | train_loss=1.0672 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=31.4s\n",
            "Epoch 009 | train_loss=1.0676 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.4s\n",
            "Epoch 010 | train_loss=1.0668 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=31.6s\n",
            "Epoch 011 | train_loss=1.0673 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.3s\n",
            "Epoch 012 | train_loss=1.0659 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.3s\n",
            "Epoch 013 | train_loss=1.0668 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.3s\n",
            "Epoch 014 | train_loss=1.0668 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.4s\n",
            "Epoch 015 | train_loss=1.0669 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.5s\n",
            "Epoch 016 | train_loss=1.0666 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.4s\n",
            "Epoch 017 | train_loss=1.0668 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.4s\n",
            "Epoch 018 | train_loss=1.0659 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.5s\n",
            "Epoch 019 | train_loss=1.0666 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.5s\n",
            "Epoch 020 | train_loss=1.0663 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=31.4s\n",
            "Epoch 021 | train_loss=1.0658 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.3s\n",
            "Epoch 022 | train_loss=1.0654 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.4s\n",
            "Epoch 023 | train_loss=1.0666 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=31.5s\n",
            "Epoch 024 | train_loss=1.0656 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.5s\n",
            "Epoch 025 | train_loss=1.0654 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.4s\n",
            "★ Early stopping at epoch 25\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁████████████████████████</td></tr><tr><td>train_loss</td><td>█▃▂▁▂▂▃▃▃▂▃▁▂▂▂▂▂▁▂▂▁▁▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▆▂█▁▄▃▂▃▅▃▂▃▂▁▃▃▄▂▄▃▃▄▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>25</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06545</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07488</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">astral-shape-10</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yv536skf' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yv536skf</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_080257-yv536skf/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 08:16:05,851] Trial 9 finished with value: 1.074171318894341 and parameters: {'lr': 3.742882361019872e-05, 'weight_decay': 2.439192182545922e-05, 'num_blocks': 3, 'num_heads': 2, 'num_segments': 10, 'use_l1': False, 'step_size': 20, 'gamma': 0.16647853546069247}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 10 =====\n",
            " lr=7.67e-04, wd=8.45e-06, L1=on(6.07e-04), blocks=1, heads=4, segs=5, step_size=30, gamma=0.89\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_081606-njm5djbp</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/njm5djbp' target=\"_blank\">brisk-sea-11</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/njm5djbp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/njm5djbp</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=273.9746 acc=0.4194 | val_loss=1.0753 acc=0.4317 | time=20.2s\n",
            "Epoch 002 | train_loss=242.1309 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.4s\n",
            "Epoch 003 | train_loss=221.1341 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.4s\n",
            "Epoch 004 | train_loss=202.2783 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.5s\n",
            "Epoch 005 | train_loss=184.9904 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.1s\n",
            "Epoch 006 | train_loss=169.1731 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=20.3s\n",
            "Epoch 007 | train_loss=154.4483 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.3s\n",
            "Epoch 008 | train_loss=140.6719 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.4s\n",
            "Epoch 009 | train_loss=127.8163 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.5s\n",
            "Epoch 010 | train_loss=115.8559 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.3s\n",
            "Epoch 011 | train_loss=104.7579 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.4s\n",
            "Epoch 012 | train_loss=94.4891 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=20.5s\n",
            "Epoch 013 | train_loss=85.0169 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.3s\n",
            "Epoch 014 | train_loss=76.2999 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.3s\n",
            "Epoch 015 | train_loss=68.3040 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.3s\n",
            "Epoch 016 | train_loss=60.9901 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.3s\n",
            "Epoch 017 | train_loss=54.3378 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.5s\n",
            "Epoch 018 | train_loss=48.3084 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.4s\n",
            "Epoch 019 | train_loss=42.8459 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.4s\n",
            "Epoch 020 | train_loss=37.9118 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.4s\n",
            "Epoch 021 | train_loss=33.4706 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.4s\n",
            "Epoch 022 | train_loss=29.4886 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.5s\n",
            "Epoch 023 | train_loss=25.9264 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.2s\n",
            "Epoch 024 | train_loss=22.7489 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.3s\n",
            "Epoch 025 | train_loss=19.9248 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.5s\n",
            "Epoch 026 | train_loss=17.4246 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.4s\n",
            "Epoch 027 | train_loss=15.2170 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=20.5s\n",
            "Epoch 028 | train_loss=13.2716 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.4s\n",
            "Epoch 029 | train_loss=11.5628 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=20.4s\n",
            "Epoch 030 | train_loss=10.0672 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.6s\n",
            "Epoch 031 | train_loss=8.8169 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.3s\n",
            "Epoch 032 | train_loss=7.7922 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.4s\n",
            "Epoch 033 | train_loss=6.8894 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.5s\n",
            "Epoch 034 | train_loss=6.0972 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=20.4s\n",
            "★ Early stopping at epoch 34\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁█████████████████████████████████</td></tr><tr><td>train_loss</td><td>█▇▇▆▆▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▄▃▃▅▁▂▄▄▂▅▂▁▂▂▃▁▂▄▁▂▃▃▄▃▄▂▆▂▃▄▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>6.09721</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07471</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">brisk-sea-11</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/njm5djbp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/njm5djbp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_081606-njm5djbp/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-05-01 08:27:42,032] Trial 10 finished with value: 1.0742883057821364 and parameters: {'lr': 0.0007669393343338705, 'weight_decay': 8.450671040376852e-06, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5, 'use_l1': True, 'l1_lambda': 0.0006073678327728755, 'step_size': 30, 'gamma': 0.892573883329919}. Best is trial 1 with value: 0.8029196219784873.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 11 =====\n",
            " lr=5.94e-04, wd=1.14e-04, L1=off, blocks=1, heads=4, segs=5, step_size=30, gamma=0.18\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_082742-r096fvi0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/r096fvi0' target=\"_blank\">polished-night-12</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/r096fvi0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/r096fvi0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0729 acc=0.4128 | val_loss=1.0742 acc=0.4317 | time=20.3s\n",
            "Epoch 002 | train_loss=1.0698 acc=0.4198 | val_loss=1.0841 acc=0.4317 | time=20.4s\n",
            "Epoch 003 | train_loss=1.0677 acc=0.4252 | val_loss=1.0843 acc=0.4317 | time=20.3s\n",
            "Epoch 004 | train_loss=1.0685 acc=0.4214 | val_loss=1.0786 acc=0.4317 | time=20.3s\n",
            "Epoch 005 | train_loss=1.0679 acc=0.4311 | val_loss=1.0769 acc=0.4317 | time=20.4s\n",
            "Epoch 006 | train_loss=1.0677 acc=0.4311 | val_loss=1.0709 acc=0.4317 | time=20.3s\n",
            "Epoch 007 | train_loss=0.9980 acc=0.5254 | val_loss=0.9644 acc=0.5590 | time=20.4s\n",
            "Epoch 008 | train_loss=0.9389 acc=0.5852 | val_loss=0.9560 acc=0.5512 | time=20.3s\n",
            "Epoch 009 | train_loss=0.9268 acc=0.5860 | val_loss=0.9426 acc=0.5637 | time=20.4s\n",
            "Epoch 010 | train_loss=0.8981 acc=0.6016 | val_loss=0.9455 acc=0.6056 | time=20.2s\n",
            "Epoch 011 | train_loss=0.8621 acc=0.6303 | val_loss=0.8939 acc=0.6227 | time=20.3s\n",
            "Epoch 012 | train_loss=0.8332 acc=0.6466 | val_loss=0.9771 acc=0.6289 | time=20.3s\n",
            "Epoch 013 | train_loss=0.7941 acc=0.6590 | val_loss=0.9014 acc=0.5932 | time=20.4s\n",
            "Epoch 014 | train_loss=0.7607 acc=0.6835 | val_loss=0.7509 acc=0.6832 | time=20.3s\n",
            "Epoch 015 | train_loss=0.7060 acc=0.6986 | val_loss=0.8440 acc=0.6661 | time=20.3s\n",
            "Epoch 016 | train_loss=0.6512 acc=0.7359 | val_loss=0.7154 acc=0.7096 | time=20.3s\n",
            "Epoch 017 | train_loss=0.6063 acc=0.7604 | val_loss=0.6968 acc=0.6925 | time=20.4s\n",
            "Epoch 018 | train_loss=0.5607 acc=0.7705 | val_loss=0.6841 acc=0.6925 | time=20.3s\n",
            "Epoch 019 | train_loss=0.5326 acc=0.7833 | val_loss=0.6867 acc=0.6972 | time=20.3s\n",
            "Epoch 020 | train_loss=0.4621 acc=0.8140 | val_loss=0.7138 acc=0.6739 | time=20.3s\n",
            "Epoch 021 | train_loss=0.4314 acc=0.8283 | val_loss=0.7140 acc=0.6957 | time=20.3s\n",
            "Epoch 022 | train_loss=0.3565 acc=0.8664 | val_loss=0.8000 acc=0.7034 | time=20.4s\n",
            "Epoch 023 | train_loss=0.3021 acc=0.8858 | val_loss=1.1481 acc=0.6863 | time=20.2s\n",
            "Epoch 024 | train_loss=0.2522 acc=0.9099 | val_loss=1.0473 acc=0.6786 | time=20.1s\n",
            "Epoch 025 | train_loss=0.2239 acc=0.9165 | val_loss=0.9707 acc=0.6988 | time=20.4s\n",
            "Epoch 026 | train_loss=0.1486 acc=0.9464 | val_loss=1.3897 acc=0.6832 | time=20.4s\n",
            "Epoch 027 | train_loss=0.1831 acc=0.9278 | val_loss=1.5789 acc=0.6879 | time=20.3s\n",
            "Epoch 028 | train_loss=0.2157 acc=0.9258 | val_loss=1.0220 acc=0.6925 | time=20.2s\n",
            "Epoch 029 | train_loss=0.1193 acc=0.9573 | val_loss=1.5579 acc=0.6817 | time=20.3s\n",
            "Epoch 030 | train_loss=0.1085 acc=0.9573 | val_loss=1.6865 acc=0.6941 | time=20.4s\n",
            "Epoch 031 | train_loss=0.0513 acc=0.9825 | val_loss=1.5791 acc=0.6770 | time=20.3s\n",
            "Epoch 032 | train_loss=0.0343 acc=0.9911 | val_loss=1.7826 acc=0.6708 | time=20.5s\n",
            "Epoch 033 | train_loss=0.0292 acc=0.9922 | val_loss=1.8416 acc=0.6646 | time=20.3s\n",
            "Epoch 034 | train_loss=0.0228 acc=0.9961 | val_loss=2.0111 acc=0.6708 | time=20.3s\n",
            "Epoch 035 | train_loss=0.0205 acc=0.9946 | val_loss=2.1101 acc=0.6646 | time=20.4s\n",
            "Epoch 036 | train_loss=0.0181 acc=0.9957 | val_loss=2.2095 acc=0.6646 | time=20.3s\n",
            "Epoch 037 | train_loss=0.0144 acc=0.9965 | val_loss=2.2336 acc=0.6677 | time=20.3s\n",
            "Epoch 038 | train_loss=0.0134 acc=0.9973 | val_loss=2.3766 acc=0.6661 | time=20.2s\n",
            "★ Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇▇▇██████████</td></tr><tr><td>train_loss</td><td>███████▇▇▇▇▆▆▆▆▅▅▅▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▄▄▄▅▆▆▅▇▇████▇██▇▇█▇▇█▇█▇▇▇▇▇▇▇▇</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▂▂▂▂▂▂▂▁▂▁▁▁▁▁▁▁▃▃▂▄▅▂▅▅▅▆▆▆▇▇▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_accuracy</td><td>0.99728</td></tr><tr><td>train_loss</td><td>0.01344</td></tr><tr><td>validation_accuracy</td><td>0.66615</td></tr><tr><td>validation_loss</td><td>2.37659</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-night-12</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/r096fvi0' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/r096fvi0</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_082742-r096fvi0/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 08:40:37,807] Trial 11 finished with value: 0.6841289514587039 and parameters: {'lr': 0.0005937996982252374, 'weight_decay': 0.00011437784562484397, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5, 'use_l1': False, 'step_size': 30, 'gamma': 0.18108826448896542}. Best is trial 11 with value: 0.6841289514587039.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 12 =====\n",
            " lr=1.30e-03, wd=6.93e-05, L1=off, blocks=1, heads=4, segs=5, step_size=30, gamma=0.30\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_084038-yrmwxhhp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yrmwxhhp' target=\"_blank\">rosy-grass-13</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yrmwxhhp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yrmwxhhp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0727 acc=0.4272 | val_loss=1.0828 acc=0.4317 | time=20.5s\n",
            "Epoch 002 | train_loss=1.0710 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=20.1s\n",
            "Epoch 003 | train_loss=1.0686 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=20.2s\n",
            "Epoch 004 | train_loss=1.0690 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=20.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁███</td></tr><tr><td>train_loss</td><td>█▅▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06896</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07512</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rosy-grass-13</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yrmwxhhp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/yrmwxhhp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_084038-yrmwxhhp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 08:42:25,655] Trial 12 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 12 pruned at epoch 5\n",
            "\n",
            "===== Trial 13 =====\n",
            " lr=1.14e-03, wd=1.96e-04, L1=off, blocks=1, heads=4, segs=5, step_size=30, gamma=0.33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_084225-hg9amd93</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/hg9amd93' target=\"_blank\">exalted-dew-14</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/hg9amd93' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/hg9amd93</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0736 acc=0.4171 | val_loss=1.0809 acc=0.4317 | time=20.4s\n",
            "Epoch 002 | train_loss=1.0690 acc=0.4311 | val_loss=1.0779 acc=0.4317 | time=20.3s\n",
            "Epoch 003 | train_loss=1.0674 acc=0.4311 | val_loss=1.0723 acc=0.4317 | time=20.2s\n",
            "Epoch 004 | train_loss=1.0326 acc=0.4862 | val_loss=1.0437 acc=0.5342 | time=20.3s\n",
            "Epoch 005 | train_loss=0.9423 acc=0.5856 | val_loss=0.9551 acc=0.5683 | time=20.4s\n",
            "Epoch 006 | train_loss=0.8944 acc=0.6132 | val_loss=0.9267 acc=0.5823 | time=20.5s\n",
            "Epoch 007 | train_loss=0.8811 acc=0.6140 | val_loss=0.9465 acc=0.5776 | time=20.5s\n",
            "Epoch 008 | train_loss=0.8429 acc=0.6346 | val_loss=0.9277 acc=0.5854 | time=20.4s\n",
            "Epoch 009 | train_loss=0.7974 acc=0.6532 | val_loss=0.9905 acc=0.5994 | time=20.6s\n",
            "Epoch 010 | train_loss=0.7792 acc=0.6621 | val_loss=0.9284 acc=0.5994 | time=20.7s\n",
            "Epoch 011 | train_loss=0.7715 acc=0.6649 | val_loss=1.0118 acc=0.5854 | time=20.6s\n",
            "Epoch 012 | train_loss=0.7126 acc=0.6850 | val_loss=0.9501 acc=0.6025 | time=20.4s\n",
            "Epoch 013 | train_loss=0.6726 acc=0.7200 | val_loss=1.0843 acc=0.6118 | time=20.7s\n",
            "Epoch 014 | train_loss=0.6511 acc=0.7262 | val_loss=0.8961 acc=0.6102 | time=20.5s\n",
            "Epoch 015 | train_loss=0.6462 acc=0.7254 | val_loss=0.8829 acc=0.5683 | time=20.5s\n",
            "Epoch 016 | train_loss=0.6139 acc=0.7406 | val_loss=0.9763 acc=0.6382 | time=20.4s\n",
            "Epoch 017 | train_loss=0.5756 acc=0.7596 | val_loss=1.0562 acc=0.6134 | time=20.5s\n",
            "Epoch 018 | train_loss=0.5820 acc=0.7577 | val_loss=0.8516 acc=0.6196 | time=20.7s\n",
            "Epoch 019 | train_loss=0.5641 acc=0.7654 | val_loss=0.9227 acc=0.5994 | time=20.5s\n",
            "Epoch 020 | train_loss=0.5559 acc=0.7689 | val_loss=0.9948 acc=0.6382 | time=20.4s\n",
            "Epoch 021 | train_loss=0.4722 acc=0.8047 | val_loss=0.9083 acc=0.6134 | time=20.6s\n",
            "Epoch 022 | train_loss=0.4489 acc=0.8179 | val_loss=1.0007 acc=0.6134 | time=20.4s\n",
            "Epoch 023 | train_loss=0.4354 acc=0.8210 | val_loss=1.1402 acc=0.6351 | time=20.6s\n",
            "Epoch 024 | train_loss=0.3696 acc=0.8466 | val_loss=1.3689 acc=0.6165 | time=20.6s\n",
            "Epoch 025 | train_loss=0.3402 acc=0.8637 | val_loss=1.5254 acc=0.6227 | time=20.6s\n",
            "Epoch 026 | train_loss=0.2957 acc=0.8854 | val_loss=1.5147 acc=0.6040 | time=20.7s\n",
            "Epoch 027 | train_loss=0.2356 acc=0.9076 | val_loss=1.9615 acc=0.6118 | time=20.6s\n",
            "Epoch 028 | train_loss=0.2591 acc=0.8986 | val_loss=1.6976 acc=0.5963 | time=20.5s\n",
            "Epoch 029 | train_loss=0.2764 acc=0.9002 | val_loss=1.4200 acc=0.6009 | time=20.7s\n",
            "Epoch 030 | train_loss=0.2554 acc=0.9072 | val_loss=1.3605 acc=0.6134 | time=20.6s\n",
            "Epoch 031 | train_loss=0.2208 acc=0.9157 | val_loss=1.5473 acc=0.6149 | time=20.6s\n",
            "Epoch 032 | train_loss=0.1544 acc=0.9433 | val_loss=1.8740 acc=0.6102 | time=20.8s\n",
            "Epoch 033 | train_loss=0.1171 acc=0.9584 | val_loss=2.2006 acc=0.6071 | time=20.4s\n",
            "Epoch 034 | train_loss=0.0831 acc=0.9697 | val_loss=2.5388 acc=0.5994 | time=20.5s\n",
            "Epoch 035 | train_loss=0.0777 acc=0.9720 | val_loss=2.4234 acc=0.5807 | time=20.6s\n",
            "Epoch 036 | train_loss=0.0539 acc=0.9852 | val_loss=3.0150 acc=0.5963 | time=20.5s\n",
            "Epoch 037 | train_loss=0.0420 acc=0.9868 | val_loss=3.0301 acc=0.5963 | time=20.5s\n",
            "Epoch 038 | train_loss=0.0651 acc=0.9783 | val_loss=3.0037 acc=0.5870 | time=20.5s\n",
            "★ Early stopping at epoch 38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▂▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>████▇▇▇▆▆▆▆▆▅▅▅▅▅▅▅▄▄▄▄▃▃▃▂▂▃▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▄▆▆▆▆▇▇▆▇▇▇▆█▇▇▇█▇▇█▇▇▇▇▇▇▇▇▇▇▇▆▇▇▆</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▁▁▁▁▁▁▂▁▂▁▁▁▂▁▁▁▁▁▂▃▃▃▅▄▃▃▃▄▅▆▆███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>38</td></tr><tr><td>train_accuracy</td><td>0.97825</td></tr><tr><td>train_loss</td><td>0.0651</td></tr><tr><td>validation_accuracy</td><td>0.58696</td></tr><tr><td>validation_loss</td><td>3.00367</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">exalted-dew-14</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/hg9amd93' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/hg9amd93</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_084225-hg9amd93/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 08:55:33,519] Trial 13 finished with value: 0.8515792347135998 and parameters: {'lr': 0.0011374342496391653, 'weight_decay': 0.00019628505332231725, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5, 'use_l1': False, 'step_size': 30, 'gamma': 0.3314326940086906}. Best is trial 11 with value: 0.6841289514587039.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 14 =====\n",
            " lr=3.26e-04, wd=8.69e-04, L1=on(1.72e-07), blocks=1, heads=4, segs=5, step_size=30, gamma=0.12\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_085533-05jadd9o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/05jadd9o' target=\"_blank\">gentle-star-15</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/05jadd9o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/05jadd9o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.1564 acc=0.4132 | val_loss=1.0746 acc=0.4317 | time=20.5s\n",
            "Epoch 002 | train_loss=1.1496 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.5s\n",
            "Epoch 003 | train_loss=1.1464 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=20.5s\n",
            "Epoch 004 | train_loss=1.1445 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=20.4s\n",
            "Epoch 005 | train_loss=1.1424 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=20.3s\n",
            "Epoch 006 | train_loss=1.1399 acc=0.4311 | val_loss=1.0719 acc=0.4317 | time=20.6s\n",
            "Epoch 007 | train_loss=1.0876 acc=0.5344 | val_loss=0.9845 acc=0.5714 | time=20.3s\n",
            "Epoch 008 | train_loss=1.0611 acc=0.5530 | val_loss=0.9821 acc=0.5528 | time=20.4s\n",
            "Epoch 009 | train_loss=1.0161 acc=0.5891 | val_loss=1.0412 acc=0.4969 | time=20.3s\n",
            "Epoch 010 | train_loss=1.0031 acc=0.5872 | val_loss=0.9763 acc=0.5792 | time=20.3s\n",
            "Epoch 011 | train_loss=0.9849 acc=0.6008 | val_loss=0.9596 acc=0.5512 | time=20.4s\n",
            "Epoch 012 | train_loss=0.9720 acc=0.6089 | val_loss=0.9630 acc=0.5559 | time=20.5s\n",
            "Epoch 013 | train_loss=0.9461 acc=0.6167 | val_loss=0.9524 acc=0.5714 | time=20.4s\n",
            "Epoch 014 | train_loss=0.9377 acc=0.6299 | val_loss=1.0005 acc=0.5637 | time=20.5s\n",
            "Epoch 015 | train_loss=0.8965 acc=0.6450 | val_loss=0.9808 acc=0.5575 | time=20.4s\n",
            "Epoch 016 | train_loss=0.8829 acc=0.6528 | val_loss=0.9739 acc=0.5652 | time=20.5s\n",
            "Epoch 017 | train_loss=0.8788 acc=0.6520 | val_loss=0.9857 acc=0.5807 | time=20.2s\n",
            "Epoch 018 | train_loss=0.8422 acc=0.6683 | val_loss=1.0284 acc=0.5823 | time=20.3s\n",
            "Epoch 019 | train_loss=0.8163 acc=0.6819 | val_loss=1.0759 acc=0.5575 | time=20.6s\n",
            "Epoch 020 | train_loss=0.7925 acc=0.6948 | val_loss=1.0390 acc=0.5559 | time=20.3s\n",
            "Epoch 021 | train_loss=0.7801 acc=0.7006 | val_loss=1.0448 acc=0.5668 | time=20.3s\n",
            "Epoch 022 | train_loss=0.7371 acc=0.7247 | val_loss=1.1451 acc=0.5606 | time=20.4s\n",
            "Epoch 023 | train_loss=0.7068 acc=0.7417 | val_loss=1.1625 acc=0.5497 | time=20.3s\n",
            "Epoch 024 | train_loss=0.6820 acc=0.7515 | val_loss=1.2283 acc=0.5637 | time=20.6s\n",
            "Epoch 025 | train_loss=0.6951 acc=0.7433 | val_loss=1.2200 acc=0.5466 | time=20.4s\n",
            "Epoch 026 | train_loss=0.6139 acc=0.7829 | val_loss=1.2962 acc=0.5652 | time=20.3s\n",
            "Epoch 027 | train_loss=0.6059 acc=0.7852 | val_loss=1.2711 acc=0.5388 | time=20.4s\n",
            "Epoch 028 | train_loss=0.5731 acc=0.8000 | val_loss=1.6512 acc=0.5450 | time=20.4s\n",
            "Epoch 029 | train_loss=0.5433 acc=0.8159 | val_loss=1.5010 acc=0.5652 | time=20.5s\n",
            "Epoch 030 | train_loss=0.4950 acc=0.8330 | val_loss=1.4720 acc=0.5280 | time=20.3s\n",
            "Epoch 031 | train_loss=0.3949 acc=0.8804 | val_loss=1.7854 acc=0.5404 | time=20.4s\n",
            "Epoch 032 | train_loss=0.3456 acc=0.9060 | val_loss=1.9398 acc=0.5404 | time=20.4s\n",
            "Epoch 033 | train_loss=0.3239 acc=0.9107 | val_loss=2.0661 acc=0.5388 | time=20.2s\n",
            "★ Early stopping at epoch 33\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇███</td></tr><tr><td>train_loss</td><td>██████▇▇▇▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▂▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▇▇▄█▇▇▇▇▇▇██▇▇▇▇▆▇▆▇▆▆▇▅▆▆▆</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▁▁▂▁▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▅▄▄▆▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>train_accuracy</td><td>0.91068</td></tr><tr><td>train_loss</td><td>0.32389</td></tr><tr><td>validation_accuracy</td><td>0.53882</td></tr><tr><td>validation_loss</td><td>2.06606</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gentle-star-15</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/05jadd9o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/05jadd9o</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_085533-05jadd9o/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 09:06:54,383] Trial 14 finished with value: 0.952350948538099 and parameters: {'lr': 0.0003264831943129236, 'weight_decay': 0.0008686858820868705, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5, 'use_l1': True, 'l1_lambda': 1.7178879551866324e-07, 'step_size': 30, 'gamma': 0.12018926331640767}. Best is trial 11 with value: 0.6841289514587039.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 15 =====\n",
            " lr=1.44e-05, wd=2.85e-05, L1=off, blocks=1, heads=2, segs=5, step_size=30, gamma=0.27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_090654-u3nz6erz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/u3nz6erz' target=\"_blank\">winter-voice-16</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/u3nz6erz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/u3nz6erz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0812 acc=0.3697 | val_loss=1.0771 acc=0.4317 | time=17.5s\n",
            "Epoch 002 | train_loss=1.0689 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=17.2s\n",
            "Epoch 003 | train_loss=1.0673 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.3s\n",
            "Epoch 004 | train_loss=1.0661 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.5s\n",
            "Epoch 005 | train_loss=1.0664 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.5s\n",
            "Epoch 006 | train_loss=1.0659 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.3s\n",
            "Epoch 007 | train_loss=1.0667 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.3s\n",
            "Epoch 008 | train_loss=1.0668 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=17.4s\n",
            "Epoch 009 | train_loss=1.0670 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.4s\n",
            "Epoch 010 | train_loss=1.0665 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.5s\n",
            "Epoch 011 | train_loss=1.0663 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.3s\n",
            "Epoch 012 | train_loss=1.0662 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.2s\n",
            "Epoch 013 | train_loss=1.0664 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.4s\n",
            "Epoch 014 | train_loss=1.0660 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.6s\n",
            "Epoch 015 | train_loss=1.0668 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=17.4s\n",
            "Epoch 016 | train_loss=1.0656 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.5s\n",
            "Epoch 017 | train_loss=1.0670 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.3s\n",
            "Epoch 018 | train_loss=1.0660 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=17.4s\n",
            "Epoch 019 | train_loss=1.0669 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=17.4s\n",
            "Epoch 020 | train_loss=1.0656 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=17.4s\n",
            "Epoch 021 | train_loss=1.0671 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.4s\n",
            "Epoch 022 | train_loss=1.0658 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=17.2s\n",
            "★ Early stopping at epoch 22\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁█████████████████████</td></tr><tr><td>train_loss</td><td>█▂▂▁▁▁▁▂▂▁▁▁▁▁▂▁▂▁▂▁▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▁▂▂▂▃▃▃▃▃▂▃▃▃▄▂▃▃▃▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06581</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07471</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">winter-voice-16</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/u3nz6erz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/u3nz6erz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_090654-u3nz6erz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 09:13:24,118] Trial 15 finished with value: 1.0742352292651223 and parameters: {'lr': 1.4403036813086675e-05, 'weight_decay': 2.8450636938411763e-05, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 5, 'use_l1': False, 'step_size': 30, 'gamma': 0.26892398845800014}. Best is trial 11 with value: 0.6841289514587039.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 16 =====\n",
            " lr=1.89e-03, wd=5.98e-06, L1=off, blocks=2, heads=2, segs=5, step_size=30, gamma=0.40\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_091324-o8onh50u</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/o8onh50u' target=\"_blank\">winter-wind-17</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/o8onh50u' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/o8onh50u</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0704 acc=0.4233 | val_loss=1.0786 acc=0.4317 | time=23.3s\n",
            "Epoch 002 | train_loss=1.0698 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=23.2s\n",
            "Epoch 003 | train_loss=1.0700 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=23.2s\n",
            "Epoch 004 | train_loss=1.0687 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=23.4s\n",
            "Epoch 005 | train_loss=1.0677 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=23.5s\n",
            "Epoch 006 | train_loss=1.0676 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=23.5s\n",
            "Epoch 007 | train_loss=1.0701 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=23.2s\n",
            "Epoch 008 | train_loss=1.0669 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=23.2s\n",
            "Epoch 009 | train_loss=1.0675 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=23.3s\n",
            "Epoch 010 | train_loss=1.0665 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=23.1s\n",
            "Epoch 011 | train_loss=1.0676 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=23.2s\n",
            "Epoch 012 | train_loss=1.0668 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=23.4s\n",
            "Epoch 013 | train_loss=1.0671 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=23.4s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▂▃▃▄▅▅▆▆▇▇█</td></tr><tr><td>train_accuracy</td><td>▁████████████</td></tr><tr><td>train_loss</td><td>█▇▇▅▃▃▇▂▃▁▃▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▄▃▁▄▁▄▂▂▂▄▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>13</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.0671</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07559</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">winter-wind-17</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/o8onh50u' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/o8onh50u</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_091324-o8onh50u/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 09:18:56,906] Trial 16 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 16 pruned at epoch 14\n",
            "\n",
            "===== Trial 17 =====\n",
            " lr=6.61e-04, wd=5.31e-05, L1=off, blocks=1, heads=4, segs=5, step_size=20, gamma=0.21\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_091857-1cbedwbx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/1cbedwbx' target=\"_blank\">visionary-dawn-18</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/1cbedwbx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/1cbedwbx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0709 acc=0.4245 | val_loss=1.0811 acc=0.4317 | time=20.4s\n",
            "Epoch 002 | train_loss=1.0691 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=20.3s\n",
            "Epoch 003 | train_loss=1.0703 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=20.5s\n",
            "Epoch 004 | train_loss=1.0680 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=20.3s\n",
            "Epoch 005 | train_loss=1.0683 acc=0.4311 | val_loss=1.0834 acc=0.4317 | time=20.3s\n",
            "Epoch 006 | train_loss=1.0688 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.4s\n",
            "Epoch 007 | train_loss=1.0681 acc=0.4311 | val_loss=1.0772 acc=0.4317 | time=20.3s\n",
            "Epoch 008 | train_loss=1.0676 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=20.4s\n",
            "Epoch 009 | train_loss=1.0668 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=20.3s\n",
            "Epoch 010 | train_loss=1.0675 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=20.3s\n",
            "Epoch 011 | train_loss=1.0658 acc=0.4311 | val_loss=1.0711 acc=0.4317 | time=20.4s\n",
            "Epoch 012 | train_loss=1.0330 acc=0.4924 | val_loss=0.9821 acc=0.5295 | time=20.2s\n",
            "Epoch 013 | train_loss=0.9601 acc=0.5732 | val_loss=0.9721 acc=0.5714 | time=20.3s\n",
            "Epoch 014 | train_loss=0.9331 acc=0.5880 | val_loss=0.9605 acc=0.5776 | time=20.4s\n",
            "Epoch 015 | train_loss=0.9274 acc=0.5876 | val_loss=0.9848 acc=0.5792 | time=20.2s\n",
            "Epoch 016 | train_loss=0.8988 acc=0.6050 | val_loss=0.9418 acc=0.5870 | time=20.5s\n",
            "Epoch 017 | train_loss=0.8698 acc=0.6132 | val_loss=0.9344 acc=0.5885 | time=20.2s\n",
            "Epoch 018 | train_loss=0.8379 acc=0.6287 | val_loss=0.9207 acc=0.5854 | time=20.2s\n",
            "Epoch 019 | train_loss=0.8232 acc=0.6338 | val_loss=0.8977 acc=0.5932 | time=20.2s\n",
            "Epoch 020 | train_loss=0.7918 acc=0.6416 | val_loss=0.8915 acc=0.5963 | time=20.1s\n",
            "Epoch 021 | train_loss=0.7339 acc=0.6680 | val_loss=0.8991 acc=0.5885 | time=20.4s\n",
            "Epoch 022 | train_loss=0.7175 acc=0.6788 | val_loss=0.8777 acc=0.5947 | time=20.1s\n",
            "Epoch 023 | train_loss=0.7047 acc=0.6920 | val_loss=0.8743 acc=0.6165 | time=20.3s\n",
            "Epoch 024 | train_loss=0.6876 acc=0.6986 | val_loss=0.9017 acc=0.5994 | time=20.2s\n",
            "Epoch 025 | train_loss=0.6725 acc=0.7049 | val_loss=0.8728 acc=0.6025 | time=20.2s\n",
            "Epoch 026 | train_loss=0.6553 acc=0.7115 | val_loss=0.8690 acc=0.5963 | time=20.4s\n",
            "Epoch 027 | train_loss=0.6482 acc=0.7150 | val_loss=0.8913 acc=0.6056 | time=20.2s\n",
            "Epoch 028 | train_loss=0.6365 acc=0.7231 | val_loss=0.8606 acc=0.6025 | time=20.1s\n",
            "Epoch 029 | train_loss=0.6155 acc=0.7336 | val_loss=0.8796 acc=0.6102 | time=20.1s\n",
            "Epoch 030 | train_loss=0.6073 acc=0.7383 | val_loss=0.8710 acc=0.6118 | time=20.3s\n",
            "Epoch 031 | train_loss=0.5987 acc=0.7449 | val_loss=0.8570 acc=0.6258 | time=20.5s\n",
            "Epoch 032 | train_loss=0.5738 acc=0.7503 | val_loss=0.8559 acc=0.6242 | time=20.4s\n",
            "Epoch 033 | train_loss=0.5650 acc=0.7577 | val_loss=0.8934 acc=0.6180 | time=20.2s\n",
            "Epoch 034 | train_loss=0.5593 acc=0.7682 | val_loss=0.8862 acc=0.6242 | time=20.4s\n",
            "Epoch 035 | train_loss=0.5483 acc=0.7713 | val_loss=0.8778 acc=0.6258 | time=20.3s\n",
            "Epoch 036 | train_loss=0.5210 acc=0.7817 | val_loss=0.9368 acc=0.6304 | time=20.5s\n",
            "Epoch 037 | train_loss=0.5014 acc=0.7903 | val_loss=0.9054 acc=0.6444 | time=20.4s\n",
            "Epoch 038 | train_loss=0.4935 acc=0.7981 | val_loss=0.8123 acc=0.6491 | time=20.4s\n",
            "Epoch 039 | train_loss=0.4742 acc=0.8074 | val_loss=0.8602 acc=0.6444 | time=20.4s\n",
            "Epoch 040 | train_loss=0.4690 acc=0.8070 | val_loss=0.9333 acc=0.6553 | time=20.2s\n",
            "Epoch 041 | train_loss=0.4154 acc=0.8384 | val_loss=0.9103 acc=0.6615 | time=20.3s\n",
            "Epoch 042 | train_loss=0.4044 acc=0.8381 | val_loss=0.9149 acc=0.6661 | time=20.2s\n",
            "Epoch 043 | train_loss=0.3980 acc=0.8361 | val_loss=0.9236 acc=0.6630 | time=20.2s\n",
            "Epoch 044 | train_loss=0.3967 acc=0.8443 | val_loss=0.9288 acc=0.6615 | time=20.5s\n",
            "Epoch 045 | train_loss=0.3925 acc=0.8462 | val_loss=0.9542 acc=0.6646 | time=20.2s\n",
            "Epoch 046 | train_loss=0.3838 acc=0.8470 | val_loss=0.9385 acc=0.6708 | time=20.3s\n",
            "Epoch 047 | train_loss=0.3823 acc=0.8478 | val_loss=0.9353 acc=0.6661 | time=20.2s\n",
            "Epoch 048 | train_loss=0.3735 acc=0.8513 | val_loss=0.9413 acc=0.6630 | time=20.3s\n",
            "Epoch 049 | train_loss=0.3748 acc=0.8458 | val_loss=0.9599 acc=0.6677 | time=20.5s\n",
            "Epoch 050 | train_loss=0.3683 acc=0.8540 | val_loss=0.9755 acc=0.6724 | time=20.2s\n",
            "Epoch 051 | train_loss=0.3620 acc=0.8583 | val_loss=0.9678 acc=0.6708 | time=20.4s\n",
            "Epoch 052 | train_loss=0.3558 acc=0.8606 | val_loss=0.9691 acc=0.6677 | time=20.3s\n",
            "Epoch 053 | train_loss=0.3554 acc=0.8629 | val_loss=0.9653 acc=0.6770 | time=20.4s\n",
            "Epoch 054 | train_loss=0.3535 acc=0.8610 | val_loss=0.9998 acc=0.6708 | time=20.4s\n",
            "Epoch 055 | train_loss=0.3479 acc=0.8656 | val_loss=0.9822 acc=0.6739 | time=20.3s\n",
            "Epoch 056 | train_loss=0.3381 acc=0.8703 | val_loss=0.9839 acc=0.6739 | time=20.3s\n",
            "Epoch 057 | train_loss=0.3341 acc=0.8722 | val_loss=1.0141 acc=0.6708 | time=20.3s\n",
            "Epoch 058 | train_loss=0.3334 acc=0.8680 | val_loss=0.9991 acc=0.6739 | time=20.4s\n",
            "★ Early stopping at epoch 58\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▂▄▄▄▄▄▅▅▆▆▆▆▆▆▆▆▇▇▇█▇██████████</td></tr><tr><td>train_loss</td><td>█████████▇▇▆▆▆▆▅▅▄▄▄▄▄▄▄▃▃▃▃▃▂▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▄▅▅▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇██████████</td></tr><tr><td>validation_loss</td><td>██████▅▅▅▄▄▃▃▃▃▃▃▂▃▂▂▃▃▃▄▁▂▄▄▄▅▄▄▄▅▅▅▅▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>58</td></tr><tr><td>train_accuracy</td><td>0.86796</td></tr><tr><td>train_loss</td><td>0.33339</td></tr><tr><td>validation_accuracy</td><td>0.67391</td></tr><tr><td>validation_loss</td><td>0.99908</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">visionary-dawn-18</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/1cbedwbx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/1cbedwbx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_091857-1cbedwbx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 09:38:43,220] Trial 17 finished with value: 0.8123467592965989 and parameters: {'lr': 0.0006613322502150471, 'weight_decay': 5.306026913460125e-05, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 5, 'use_l1': False, 'step_size': 20, 'gamma': 0.20561411467734086}. Best is trial 11 with value: 0.6841289514587039.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 18 =====\n",
            " lr=1.90e-04, wd=1.72e-04, L1=on(1.48e-05), blocks=1, heads=2, segs=10, step_size=10, gamma=0.39\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_093843-zqrysdx3</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/zqrysdx3' target=\"_blank\">rose-monkey-19</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/zqrysdx3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-3/runs/zqrysdx3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=8.1850 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.3s\n",
            "Epoch 002 | train_loss=7.9208 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=17.3s\n",
            "Epoch 003 | train_loss=7.6896 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.5s\n",
            "Epoch 004 | train_loss=7.4814 acc=0.4311 | val_loss=1.0813 acc=0.4317 | time=17.5s\n",
            "Epoch 005 | train_loss=7.2928 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=17.6s\n",
            "Epoch 006 | train_loss=7.1234 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.5s\n",
            "Epoch 007 | train_loss=6.9673 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=17.3s\n",
            "Epoch 008 | train_loss=6.8240 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=17.6s\n",
            "Epoch 009 | train_loss=6.6866 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.5s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "faa73G8rQeOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oQi8vmlVQeTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OA10eebXAvwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXuRNllZAvy2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-4, 1e-3\n",
        "WD_MIN, WD_MAX     = 1e-5, 1e-3\n",
        "NUM_FILTERS        = 120\n",
        "NUM_BLOCK_CHOICES  = [1, 2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 3, 4]\n",
        "SEGMENT_CHOICES    = [5, 10, 15]\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "MAX_EPOCHS  = 200\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # 1) Sample hyperparameters\n",
        "    lr            = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay  = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_blocks    = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads     = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments  = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "    # New: sample scheduler hyperparameters\n",
        "    step_size     = trial.suggest_int(\"step_size\", 10, 30, step=10)\n",
        "    gamma         = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, blocks={num_blocks}, \"\n",
        "        f\"heads={num_heads}, segs={num_segments}, \"\n",
        "        f\"step_size={step_size}, gamma={gamma:.2f}\"\n",
        "    )\n",
        "\n",
        "    # 2) Load metadata and build dataset\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta  = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels      = [d[\"label\"] for d in train_meta]\n",
        "    n_samples   = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 3) Single hold-out split (80/20), stratified\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # 4) W&B init\n",
        "    wandb.init(project=\"eeg-holdout-tuning-1\", config=trial.params)\n",
        "\n",
        "    # 5) Build model, optimizer, loss\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 6) Scheduler with sampled step_size & gamma\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=step_size,\n",
        "        gamma=gamma\n",
        "    )\n",
        "\n",
        "    # 7) Training loop with early stopping & pruning\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "    # store best-epoch metrics\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss    += loss.item()\n",
        "                vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # report & prune\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        # — log all four metrics to wandb for graphing\n",
        "        wandb.log({\n",
        "            \"epoch\": epoch,\n",
        "            \"train_loss\": train_loss,\n",
        "            \"train_accuracy\": train_acc,\n",
        "            \"validation_loss\": val_loss,\n",
        "            \"validation_accuracy\": val_acc,\n",
        "        }, step=epoch)\n",
        "\n",
        "        # step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # early stopping & record best\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best-epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_trial-1\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_holdout-1.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout, n_trials=30)\n",
        "\n",
        "    # print best trial & metrics\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"best_val_acc    = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(f\"best_train_loss = {best.user_attrs['best_train_loss']:.4f}\")\n",
        "    print(f\"best_train_acc  = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "jrMfCm9fv0FY",
        "outputId": "c3363713-0c9f-4f1a-d776-5d3e400d43b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 22:40:14,173] Using an existing study with name 'eeg_holdout_trial-1' instead of creating a new one.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=2.50e-04, wd=1.45e-04, blocks=1, heads=4, segs=10, step_size=20, gamma=0.70\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁█</td></tr><tr><td>train_accuracy</td><td>▁█</td></tr><tr><td>train_loss</td><td>▁█</td></tr><tr><td>validation_accuracy</td><td>▁█</td></tr><tr><td>validation_loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>2</td></tr><tr><td>train_accuracy</td><td>0.42951</td></tr><tr><td>train_loss</td><td>1.07145</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07453</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">treasured-durian-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ex4hdmue' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ex4hdmue</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_223826-ex4hdmue/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_224014-bgsp85nb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/bgsp85nb' target=\"_blank\">sage-armadillo-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/bgsp85nb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/bgsp85nb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0706 acc=0.4260 | val_loss=1.0761 acc=0.4317 | time=20.7s\n",
            "Epoch 002 | train_loss=1.0684 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=20.4s\n",
            "Epoch 003 | train_loss=1.0676 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=20.3s\n",
            "Epoch 004 | train_loss=1.0671 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=20.5s\n",
            "Epoch 005 | train_loss=1.0679 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=20.4s\n",
            "Epoch 006 | train_loss=1.0684 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=20.5s\n",
            "Epoch 007 | train_loss=1.0676 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.4s\n",
            "Epoch 008 | train_loss=1.0669 acc=0.4311 | val_loss=1.0765 acc=0.4317 | time=20.5s\n",
            "Epoch 009 | train_loss=1.0672 acc=0.4311 | val_loss=1.0836 acc=0.4317 | time=20.4s\n",
            "Epoch 010 | train_loss=1.0709 acc=0.4249 | val_loss=1.0759 acc=0.4317 | time=20.4s\n",
            "Epoch 011 | train_loss=1.0673 acc=0.4311 | val_loss=1.0768 acc=0.4317 | time=20.6s\n",
            "Epoch 012 | train_loss=1.0681 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=20.3s\n",
            "Epoch 013 | train_loss=1.0671 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=20.5s\n",
            "Epoch 014 | train_loss=1.0674 acc=0.4311 | val_loss=1.0736 acc=0.4317 | time=20.6s\n",
            "Epoch 015 | train_loss=1.0288 acc=0.4928 | val_loss=0.9818 acc=0.5637 | time=20.5s\n",
            "Epoch 016 | train_loss=0.9662 acc=0.5806 | val_loss=0.9770 acc=0.5699 | time=20.4s\n",
            "Epoch 017 | train_loss=0.9275 acc=0.5930 | val_loss=0.9434 acc=0.5745 | time=20.3s\n",
            "Epoch 018 | train_loss=0.9029 acc=0.6093 | val_loss=0.9975 acc=0.5854 | time=20.6s\n",
            "Epoch 019 | train_loss=0.8997 acc=0.6101 | val_loss=0.9359 acc=0.5745 | time=20.5s\n",
            "Epoch 020 | train_loss=0.8723 acc=0.6198 | val_loss=0.9274 acc=0.5745 | time=20.5s\n",
            "Epoch 021 | train_loss=0.8288 acc=0.6431 | val_loss=0.9365 acc=0.5994 | time=20.6s\n",
            "Epoch 022 | train_loss=0.7984 acc=0.6571 | val_loss=0.9030 acc=0.6087 | time=20.5s\n",
            "Epoch 023 | train_loss=0.7720 acc=0.6625 | val_loss=0.9430 acc=0.5963 | time=20.4s\n",
            "Epoch 024 | train_loss=0.7445 acc=0.6788 | val_loss=0.9197 acc=0.6071 | time=20.4s\n",
            "Epoch 025 | train_loss=0.6857 acc=0.7091 | val_loss=0.9007 acc=0.6242 | time=20.4s\n",
            "Epoch 026 | train_loss=0.6331 acc=0.7336 | val_loss=0.9001 acc=0.6211 | time=20.6s\n",
            "Epoch 027 | train_loss=0.5628 acc=0.7674 | val_loss=0.9093 acc=0.6025 | time=20.2s\n",
            "Epoch 028 | train_loss=0.5453 acc=0.7790 | val_loss=0.9273 acc=0.6258 | time=20.5s\n",
            "Epoch 029 | train_loss=0.4939 acc=0.8047 | val_loss=0.8924 acc=0.6242 | time=20.7s\n",
            "Epoch 030 | train_loss=0.4368 acc=0.8338 | val_loss=1.0068 acc=0.6211 | time=20.5s\n",
            "Epoch 031 | train_loss=0.4030 acc=0.8497 | val_loss=0.9769 acc=0.6335 | time=20.6s\n",
            "Epoch 032 | train_loss=0.3371 acc=0.8715 | val_loss=0.9827 acc=0.6460 | time=20.4s\n",
            "Epoch 033 | train_loss=0.3090 acc=0.8800 | val_loss=1.0454 acc=0.6382 | time=20.5s\n",
            "Epoch 034 | train_loss=0.2722 acc=0.8986 | val_loss=1.2302 acc=0.6630 | time=20.5s\n",
            "Epoch 035 | train_loss=0.2352 acc=0.9177 | val_loss=1.0928 acc=0.6615 | time=20.4s\n",
            "Epoch 036 | train_loss=0.2257 acc=0.9142 | val_loss=1.1442 acc=0.6599 | time=20.7s\n",
            "Epoch 037 | train_loss=0.1723 acc=0.9398 | val_loss=1.3590 acc=0.6708 | time=20.4s\n",
            "Epoch 038 | train_loss=0.1901 acc=0.9305 | val_loss=1.3417 acc=0.6630 | time=20.4s\n",
            "Epoch 039 | train_loss=0.1497 acc=0.9542 | val_loss=1.5891 acc=0.6537 | time=20.5s\n",
            "Epoch 040 | train_loss=0.1444 acc=0.9542 | val_loss=1.3638 acc=0.6832 | time=20.4s\n",
            "Epoch 041 | train_loss=0.1188 acc=0.9666 | val_loss=1.5061 acc=0.6475 | time=20.5s\n",
            "Epoch 042 | train_loss=0.1052 acc=0.9650 | val_loss=1.6578 acc=0.6786 | time=20.5s\n",
            "Epoch 043 | train_loss=0.0571 acc=0.9833 | val_loss=1.9880 acc=0.6599 | time=20.4s\n",
            "Epoch 044 | train_loss=0.0685 acc=0.9783 | val_loss=1.9858 acc=0.6568 | time=20.4s\n",
            "Epoch 045 | train_loss=0.0724 acc=0.9771 | val_loss=1.8051 acc=0.6630 | time=20.4s\n",
            "Epoch 046 | train_loss=0.0442 acc=0.9883 | val_loss=2.1083 acc=0.6786 | time=20.4s\n",
            "Epoch 047 | train_loss=0.0361 acc=0.9911 | val_loss=2.2433 acc=0.6599 | time=20.3s\n",
            "Epoch 048 | train_loss=0.0470 acc=0.9856 | val_loss=2.2155 acc=0.6677 | time=20.5s\n",
            "Epoch 049 | train_loss=0.0987 acc=0.9717 | val_loss=2.0565 acc=0.6786 | time=20.5s\n",
            "★ Early stopping at epoch 49\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▃▃▄▄▄▅▅▅▅▆▆▇▇▇▇▇█████████</td></tr><tr><td>train_loss</td><td>█████████████▇▇▇▇▆▆▆▅▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▁▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇█▇█▇▇█▇█</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▂▃▂▂▃▅▃▄▅▇▆▇█▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>49</td></tr><tr><td>train_accuracy</td><td>0.97165</td></tr><tr><td>train_loss</td><td>0.09872</td></tr><tr><td>validation_accuracy</td><td>0.67857</td></tr><tr><td>validation_loss</td><td>2.05648</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sage-armadillo-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/bgsp85nb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/bgsp85nb</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_224014-bgsp85nb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 22:57:00,480] Trial 1 finished with value: 0.8923814424446651 and parameters: {'lr': 0.000249828961706864, 'weight_decay': 0.000144630365763184, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 10, 'step_size': 20, 'gamma': 0.6986434256347877}. Best is trial 1 with value: 0.8923814424446651.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=4.82e-04, wd=7.72e-05, blocks=1, heads=2, segs=10, step_size=20, gamma=0.70\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_225700-6x2ppggg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/6x2ppggg' target=\"_blank\">worthy-capybara-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/6x2ppggg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/6x2ppggg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0710 acc=0.4256 | val_loss=1.0749 acc=0.4317 | time=17.7s\n",
            "Epoch 002 | train_loss=1.0690 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=17.2s\n",
            "Epoch 003 | train_loss=1.0680 acc=0.4311 | val_loss=1.0782 acc=0.4317 | time=17.6s\n",
            "Epoch 004 | train_loss=1.0697 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=17.6s\n",
            "Epoch 005 | train_loss=1.0682 acc=0.4311 | val_loss=1.0781 acc=0.4317 | time=17.7s\n",
            "Epoch 006 | train_loss=1.0679 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.6s\n",
            "Epoch 007 | train_loss=1.0316 acc=0.4792 | val_loss=0.9652 acc=0.5575 | time=17.4s\n",
            "Epoch 008 | train_loss=0.9506 acc=0.5833 | val_loss=0.9921 acc=0.5140 | time=17.4s\n",
            "Epoch 009 | train_loss=0.9262 acc=0.5922 | val_loss=0.9432 acc=0.5528 | time=17.6s\n",
            "Epoch 010 | train_loss=0.9066 acc=0.6035 | val_loss=0.9319 acc=0.5714 | time=17.2s\n",
            "Epoch 011 | train_loss=0.8757 acc=0.6167 | val_loss=0.9201 acc=0.5745 | time=17.8s\n",
            "Epoch 012 | train_loss=0.8435 acc=0.6311 | val_loss=0.9452 acc=0.5870 | time=17.4s\n",
            "Epoch 013 | train_loss=0.8139 acc=0.6450 | val_loss=0.9736 acc=0.6040 | time=17.4s\n",
            "Epoch 014 | train_loss=0.7711 acc=0.6668 | val_loss=0.8810 acc=0.6398 | time=17.9s\n",
            "Epoch 015 | train_loss=0.7388 acc=0.6897 | val_loss=1.0841 acc=0.5901 | time=17.6s\n",
            "Epoch 016 | train_loss=0.7123 acc=0.6928 | val_loss=1.0663 acc=0.5916 | time=17.5s\n",
            "Epoch 017 | train_loss=0.7186 acc=0.6979 | val_loss=0.9242 acc=0.6118 | time=17.6s\n",
            "Epoch 018 | train_loss=0.6140 acc=0.7452 | val_loss=1.0962 acc=0.6258 | time=17.3s\n",
            "Epoch 019 | train_loss=0.5763 acc=0.7600 | val_loss=1.0696 acc=0.6273 | time=17.3s\n",
            "Epoch 020 | train_loss=0.5289 acc=0.7864 | val_loss=1.1517 acc=0.6289 | time=17.3s\n",
            "Epoch 021 | train_loss=0.4728 acc=0.8171 | val_loss=1.1765 acc=0.6320 | time=17.3s\n",
            "Epoch 022 | train_loss=0.4316 acc=0.8365 | val_loss=1.1845 acc=0.6165 | time=17.5s\n",
            "Epoch 023 | train_loss=0.3771 acc=0.8598 | val_loss=1.4052 acc=0.6351 | time=17.2s\n",
            "Epoch 024 | train_loss=0.3636 acc=0.8664 | val_loss=1.4346 acc=0.6165 | time=17.5s\n",
            "Epoch 025 | train_loss=0.3321 acc=0.8777 | val_loss=1.3930 acc=0.6320 | time=17.9s\n",
            "Epoch 026 | train_loss=0.2645 acc=0.9080 | val_loss=1.8547 acc=0.6335 | time=17.4s\n",
            "Epoch 027 | train_loss=0.2507 acc=0.9115 | val_loss=1.6180 acc=0.6180 | time=17.4s\n",
            "Epoch 028 | train_loss=0.2089 acc=0.9262 | val_loss=1.8465 acc=0.6149 | time=17.7s\n",
            "Epoch 029 | train_loss=0.1804 acc=0.9390 | val_loss=1.9074 acc=0.6320 | time=17.7s\n",
            "Epoch 030 | train_loss=0.1854 acc=0.9340 | val_loss=1.8764 acc=0.6165 | time=17.5s\n",
            "Epoch 031 | train_loss=0.1468 acc=0.9546 | val_loss=2.1843 acc=0.6165 | time=18.0s\n",
            "Epoch 032 | train_loss=0.1423 acc=0.9491 | val_loss=1.9960 acc=0.6087 | time=17.4s\n",
            "Epoch 033 | train_loss=0.1015 acc=0.9682 | val_loss=2.2191 acc=0.6382 | time=17.4s\n",
            "Epoch 034 | train_loss=0.0781 acc=0.9736 | val_loss=2.5938 acc=0.6351 | time=17.7s\n",
            "★ Early stopping at epoch 34\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▃▃▃▄▄▄▄▄▄▅▅▆▆▆▇▇▇▇▇▇█▇████</td></tr><tr><td>train_loss</td><td>███████▇▇▇▇▆▆▆▆▅▆▅▅▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▅▄▅▆▆▆▇█▆▆▇████▇█▇██▇▇█▇▇▇██</td></tr><tr><td>validation_loss</td><td>▂▂▂▂▂▂▁▁▁▁▁▁▁▁▂▂▁▂▂▂▂▂▃▃▃▅▄▅▅▅▆▆▆█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>34</td></tr><tr><td>train_accuracy</td><td>0.97359</td></tr><tr><td>train_loss</td><td>0.07808</td></tr><tr><td>validation_accuracy</td><td>0.63509</td></tr><tr><td>validation_loss</td><td>2.59377</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">worthy-capybara-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/6x2ppggg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/6x2ppggg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_225700-6x2ppggg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 23:06:59,494] Trial 2 finished with value: 0.8809579866273063 and parameters: {'lr': 0.00048240685668179294, 'weight_decay': 7.722789242670862e-05, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 10, 'step_size': 20, 'gamma': 0.703019033811063}. Best is trial 2 with value: 0.8809579866273063.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=5.76e-04, wd=3.83e-05, blocks=2, heads=4, segs=10, step_size=20, gamma=0.11\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_230659-u24fs56c</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/u24fs56c' target=\"_blank\">autumn-moon-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/u24fs56c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/u24fs56c</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0753 acc=0.4151 | val_loss=1.0790 acc=0.4317 | time=33.8s\n",
            "Epoch 002 | train_loss=1.0717 acc=0.4252 | val_loss=1.0744 acc=0.4317 | time=33.4s\n",
            "Epoch 003 | train_loss=1.0686 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=33.6s\n",
            "Epoch 004 | train_loss=1.0700 acc=0.4311 | val_loss=1.0785 acc=0.4317 | time=33.7s\n",
            "Epoch 005 | train_loss=1.0672 acc=0.4210 | val_loss=1.0869 acc=0.4317 | time=33.4s\n",
            "Epoch 006 | train_loss=1.0672 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=33.5s\n",
            "Epoch 007 | train_loss=1.0667 acc=0.4311 | val_loss=1.0808 acc=0.4317 | time=33.6s\n",
            "Epoch 008 | train_loss=1.0678 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=33.6s\n",
            "Epoch 009 | train_loss=1.0676 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=33.5s\n",
            "Epoch 010 | train_loss=1.0617 acc=0.4392 | val_loss=1.0224 acc=0.5730 | time=33.4s\n",
            "Epoch 011 | train_loss=0.9791 acc=0.5616 | val_loss=0.9815 acc=0.5217 | time=33.6s\n",
            "Epoch 012 | train_loss=0.9443 acc=0.5852 | val_loss=1.0092 acc=0.5683 | time=33.5s\n",
            "Epoch 013 | train_loss=0.9215 acc=0.5903 | val_loss=0.9524 acc=0.5621 | time=33.6s\n",
            "Epoch 014 | train_loss=0.9141 acc=0.5996 | val_loss=1.0065 acc=0.5466 | time=33.5s\n",
            "Epoch 015 | train_loss=0.8990 acc=0.6101 | val_loss=0.9530 acc=0.5590 | time=33.5s\n",
            "Epoch 016 | train_loss=0.8725 acc=0.6214 | val_loss=0.9226 acc=0.5901 | time=33.5s\n",
            "Epoch 017 | train_loss=0.8546 acc=0.6303 | val_loss=0.9228 acc=0.5901 | time=33.4s\n",
            "Epoch 018 | train_loss=0.8494 acc=0.6334 | val_loss=0.9027 acc=0.5978 | time=33.7s\n",
            "Epoch 019 | train_loss=0.8118 acc=0.6447 | val_loss=0.9823 acc=0.6056 | time=33.3s\n",
            "Epoch 020 | train_loss=0.8040 acc=0.6482 | val_loss=0.8952 acc=0.5947 | time=33.3s\n",
            "Epoch 021 | train_loss=0.7549 acc=0.6730 | val_loss=0.8867 acc=0.6149 | time=33.6s\n",
            "Epoch 022 | train_loss=0.7334 acc=0.6850 | val_loss=0.8881 acc=0.6165 | time=33.4s\n",
            "Epoch 023 | train_loss=0.7262 acc=0.6920 | val_loss=0.8775 acc=0.6180 | time=33.5s\n",
            "Epoch 024 | train_loss=0.7208 acc=0.6928 | val_loss=0.8881 acc=0.6180 | time=33.5s\n",
            "Epoch 025 | train_loss=0.7090 acc=0.7017 | val_loss=0.8810 acc=0.6071 | time=33.6s\n",
            "Epoch 026 | train_loss=0.7013 acc=0.7072 | val_loss=0.9055 acc=0.6149 | time=33.6s\n",
            "Epoch 027 | train_loss=0.6919 acc=0.7083 | val_loss=0.9026 acc=0.6118 | time=33.6s\n",
            "Epoch 028 | train_loss=0.6788 acc=0.7157 | val_loss=0.9066 acc=0.6227 | time=33.5s\n",
            "Epoch 029 | train_loss=0.6723 acc=0.7173 | val_loss=0.9082 acc=0.6087 | time=33.6s\n",
            "Epoch 030 | train_loss=0.6558 acc=0.7247 | val_loss=0.9071 acc=0.6025 | time=33.4s\n",
            "Epoch 031 | train_loss=0.6469 acc=0.7351 | val_loss=0.9137 acc=0.6056 | time=33.5s\n",
            "Epoch 032 | train_loss=0.6357 acc=0.7441 | val_loss=0.9034 acc=0.5932 | time=33.6s\n",
            "Epoch 033 | train_loss=0.6170 acc=0.7476 | val_loss=0.9211 acc=0.5870 | time=33.5s\n",
            "Epoch 034 | train_loss=0.6373 acc=0.7313 | val_loss=0.9348 acc=0.6196 | time=33.4s\n",
            "Epoch 035 | train_loss=0.6201 acc=0.7367 | val_loss=0.9648 acc=0.6196 | time=33.7s\n",
            "Epoch 036 | train_loss=0.5943 acc=0.7569 | val_loss=0.9423 acc=0.6165 | time=33.7s\n",
            "Epoch 037 | train_loss=0.5778 acc=0.7670 | val_loss=1.0426 acc=0.6149 | time=33.4s\n",
            "Epoch 038 | train_loss=0.5605 acc=0.7755 | val_loss=0.9921 acc=0.6071 | time=33.5s\n",
            "Epoch 039 | train_loss=0.5394 acc=0.7864 | val_loss=0.9948 acc=0.6149 | time=33.5s\n",
            "Epoch 040 | train_loss=0.5250 acc=0.7957 | val_loss=1.0202 acc=0.6165 | time=33.6s\n",
            "Epoch 041 | train_loss=0.4970 acc=0.8070 | val_loss=1.0125 acc=0.6180 | time=33.6s\n",
            "Epoch 042 | train_loss=0.4859 acc=0.8113 | val_loss=1.0353 acc=0.6102 | time=33.5s\n",
            "Epoch 043 | train_loss=0.4859 acc=0.8163 | val_loss=1.0393 acc=0.6149 | time=33.5s\n",
            "★ Early stopping at epoch 43\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▁▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇███</td></tr><tr><td>train_loss</td><td>██████████▇▆▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁▆▄▆▆▆▇▇▇▇▇████████▇▇▇▇█████████</td></tr><tr><td>validation_loss</td><td>█████████▆▄▅▄▄▃▃▂▅▂▁▁▁▁▁▂▂▂▂▂▂▂▃▄▃▇▅▅▆▆▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>43</td></tr><tr><td>train_accuracy</td><td>0.81631</td></tr><tr><td>train_loss</td><td>0.48586</td></tr><tr><td>validation_accuracy</td><td>0.61491</td></tr><tr><td>validation_loss</td><td>1.03933</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">autumn-moon-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/u24fs56c' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/u24fs56c</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_230659-u24fs56c/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 23:31:03,649] Trial 3 finished with value: 0.87745715039117 and parameters: {'lr': 0.0005763140134110753, 'weight_decay': 3.829160944186297e-05, 'num_blocks': 2, 'num_heads': 4, 'num_segments': 10, 'step_size': 20, 'gamma': 0.10570697111390635}. Best is trial 3 with value: 0.87745715039117.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=9.67e-04, wd=1.00e-04, blocks=3, heads=2, segs=5, step_size=30, gamma=0.46\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_233103-v2qmlyvj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/v2qmlyvj' target=\"_blank\">stellar-valley-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/v2qmlyvj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/v2qmlyvj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0718 acc=0.4148 | val_loss=1.0756 acc=0.4317 | time=31.3s\n",
            "Epoch 002 | train_loss=1.0684 acc=0.4311 | val_loss=1.0816 acc=0.4317 | time=31.3s\n",
            "Epoch 003 | train_loss=1.0679 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.3s\n",
            "Epoch 004 | train_loss=1.0679 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.4s\n",
            "Epoch 005 | train_loss=1.0676 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.2s\n",
            "Epoch 006 | train_loss=1.0670 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=31.3s\n",
            "Epoch 007 | train_loss=1.0677 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.2s\n",
            "Epoch 008 | train_loss=1.0672 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.5s\n",
            "Epoch 009 | train_loss=1.0669 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=31.3s\n",
            "Epoch 010 | train_loss=1.0675 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.4s\n",
            "Epoch 011 | train_loss=1.0667 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=31.2s\n",
            "Epoch 012 | train_loss=1.0674 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.3s\n",
            "Epoch 013 | train_loss=1.0631 acc=0.4311 | val_loss=1.0900 acc=0.4317 | time=31.3s\n",
            "Epoch 014 | train_loss=1.0087 acc=0.5243 | val_loss=0.9604 acc=0.5606 | time=31.3s\n",
            "Epoch 015 | train_loss=0.9391 acc=0.5876 | val_loss=0.9566 acc=0.5668 | time=31.8s\n",
            "Epoch 016 | train_loss=0.9094 acc=0.5969 | val_loss=0.9335 acc=0.5947 | time=31.6s\n",
            "Epoch 017 | train_loss=0.8919 acc=0.6047 | val_loss=0.9505 acc=0.5885 | time=31.8s\n",
            "Epoch 018 | train_loss=0.8741 acc=0.6202 | val_loss=0.9296 acc=0.5963 | time=31.8s\n",
            "Epoch 019 | train_loss=0.8498 acc=0.6311 | val_loss=0.9003 acc=0.6040 | time=31.8s\n",
            "Epoch 020 | train_loss=0.8243 acc=0.6373 | val_loss=0.8828 acc=0.6056 | time=31.8s\n",
            "Epoch 021 | train_loss=0.8133 acc=0.6544 | val_loss=0.8769 acc=0.6071 | time=31.6s\n",
            "Epoch 022 | train_loss=0.7911 acc=0.6649 | val_loss=0.8882 acc=0.6102 | time=31.7s\n",
            "Epoch 023 | train_loss=0.7931 acc=0.6656 | val_loss=0.8599 acc=0.6460 | time=31.8s\n",
            "Epoch 024 | train_loss=0.7766 acc=0.6757 | val_loss=0.8430 acc=0.6320 | time=31.7s\n",
            "Epoch 025 | train_loss=0.7250 acc=0.7025 | val_loss=0.8008 acc=0.6444 | time=31.7s\n",
            "Epoch 026 | train_loss=0.6984 acc=0.7134 | val_loss=0.7899 acc=0.6553 | time=31.8s\n",
            "Epoch 027 | train_loss=0.7064 acc=0.7091 | val_loss=0.8254 acc=0.6351 | time=31.9s\n",
            "Epoch 028 | train_loss=0.6795 acc=0.7196 | val_loss=0.7700 acc=0.6630 | time=32.0s\n",
            "Epoch 029 | train_loss=0.6602 acc=0.7320 | val_loss=0.7473 acc=0.6832 | time=31.8s\n",
            "Epoch 030 | train_loss=0.6570 acc=0.7301 | val_loss=0.7521 acc=0.6863 | time=32.0s\n",
            "Epoch 031 | train_loss=0.6088 acc=0.7495 | val_loss=0.7577 acc=0.6615 | time=31.8s\n",
            "Epoch 032 | train_loss=0.5874 acc=0.7584 | val_loss=0.7361 acc=0.6863 | time=32.2s\n",
            "Epoch 033 | train_loss=0.5823 acc=0.7515 | val_loss=0.7117 acc=0.6755 | time=31.9s\n",
            "Epoch 034 | train_loss=0.5662 acc=0.7612 | val_loss=0.6898 acc=0.6925 | time=32.1s\n",
            "Epoch 035 | train_loss=0.5669 acc=0.7608 | val_loss=0.7236 acc=0.6941 | time=31.8s\n",
            "Epoch 036 | train_loss=0.5324 acc=0.7724 | val_loss=0.6593 acc=0.6941 | time=31.9s\n",
            "Epoch 037 | train_loss=0.5151 acc=0.7926 | val_loss=0.6964 acc=0.7050 | time=31.9s\n",
            "Epoch 038 | train_loss=0.4998 acc=0.7849 | val_loss=0.8144 acc=0.7003 | time=31.9s\n",
            "Epoch 039 | train_loss=0.4963 acc=0.7852 | val_loss=0.6533 acc=0.7127 | time=31.7s\n",
            "Epoch 040 | train_loss=0.4722 acc=0.8054 | val_loss=0.6987 acc=0.7050 | time=32.0s\n",
            "Epoch 041 | train_loss=0.4592 acc=0.8035 | val_loss=0.6434 acc=0.7096 | time=31.9s\n",
            "Epoch 042 | train_loss=0.4252 acc=0.8229 | val_loss=0.6246 acc=0.7298 | time=31.9s\n",
            "Epoch 043 | train_loss=0.4525 acc=0.8101 | val_loss=0.6608 acc=0.7143 | time=31.8s\n",
            "Epoch 044 | train_loss=0.4398 acc=0.8171 | val_loss=0.6267 acc=0.7267 | time=31.8s\n",
            "Epoch 045 | train_loss=0.4733 acc=0.8012 | val_loss=0.6697 acc=0.7081 | time=31.6s\n",
            "Epoch 046 | train_loss=0.4790 acc=0.7969 | val_loss=0.5970 acc=0.7329 | time=31.7s\n",
            "Epoch 047 | train_loss=0.4536 acc=0.8085 | val_loss=0.6567 acc=0.7283 | time=31.7s\n",
            "Epoch 048 | train_loss=0.4165 acc=0.8245 | val_loss=0.7016 acc=0.7345 | time=31.8s\n",
            "Epoch 049 | train_loss=0.3959 acc=0.8392 | val_loss=0.6051 acc=0.7283 | time=31.6s\n",
            "Epoch 050 | train_loss=0.3959 acc=0.8412 | val_loss=0.6580 acc=0.7438 | time=31.9s\n",
            "Epoch 051 | train_loss=0.3960 acc=0.8326 | val_loss=0.6459 acc=0.7438 | time=31.7s\n",
            "Epoch 052 | train_loss=0.3707 acc=0.8497 | val_loss=0.5963 acc=0.7593 | time=31.8s\n",
            "Epoch 053 | train_loss=0.3575 acc=0.8520 | val_loss=0.6283 acc=0.7484 | time=31.6s\n",
            "Epoch 054 | train_loss=0.3534 acc=0.8482 | val_loss=0.6104 acc=0.7671 | time=31.8s\n",
            "Epoch 055 | train_loss=0.3451 acc=0.8551 | val_loss=0.6680 acc=0.7500 | time=31.6s\n",
            "Epoch 056 | train_loss=0.3559 acc=0.8598 | val_loss=0.5999 acc=0.7624 | time=31.7s\n",
            "Epoch 057 | train_loss=0.3402 acc=0.8695 | val_loss=0.6528 acc=0.7655 | time=31.6s\n",
            "Epoch 058 | train_loss=0.3237 acc=0.8691 | val_loss=0.6367 acc=0.7562 | time=31.7s\n",
            "Epoch 059 | train_loss=0.3076 acc=0.8816 | val_loss=0.5753 acc=0.7671 | time=31.6s\n",
            "Epoch 060 | train_loss=0.3023 acc=0.8819 | val_loss=0.6484 acc=0.7748 | time=31.6s\n",
            "Epoch 061 | train_loss=0.2623 acc=0.8975 | val_loss=0.5988 acc=0.7748 | time=31.6s\n",
            "Epoch 062 | train_loss=0.2451 acc=0.9122 | val_loss=0.6922 acc=0.7733 | time=31.6s\n",
            "Epoch 063 | train_loss=0.2357 acc=0.9111 | val_loss=0.6138 acc=0.7904 | time=31.7s\n",
            "Epoch 064 | train_loss=0.2453 acc=0.9091 | val_loss=0.6133 acc=0.7935 | time=31.7s\n",
            "Epoch 065 | train_loss=0.2210 acc=0.9184 | val_loss=0.6498 acc=0.7888 | time=31.6s\n",
            "Epoch 066 | train_loss=0.2098 acc=0.9239 | val_loss=0.6177 acc=0.7935 | time=31.9s\n",
            "Epoch 067 | train_loss=0.2067 acc=0.9250 | val_loss=0.7136 acc=0.7811 | time=31.9s\n",
            "Epoch 068 | train_loss=0.2172 acc=0.9216 | val_loss=0.6329 acc=0.7811 | time=31.8s\n",
            "Epoch 069 | train_loss=0.2107 acc=0.9184 | val_loss=0.6309 acc=0.7888 | time=32.0s\n",
            "Epoch 070 | train_loss=0.2066 acc=0.9208 | val_loss=0.6868 acc=0.7935 | time=31.7s\n",
            "Epoch 071 | train_loss=0.1849 acc=0.9328 | val_loss=0.6811 acc=0.7935 | time=31.8s\n",
            "Epoch 072 | train_loss=0.1894 acc=0.9282 | val_loss=0.6822 acc=0.7857 | time=31.8s\n",
            "Epoch 073 | train_loss=0.1831 acc=0.9344 | val_loss=0.6776 acc=0.8012 | time=31.7s\n",
            "Epoch 074 | train_loss=0.1833 acc=0.9371 | val_loss=0.7031 acc=0.7919 | time=32.0s\n",
            "Epoch 075 | train_loss=0.1692 acc=0.9417 | val_loss=0.6835 acc=0.7981 | time=32.0s\n",
            "Epoch 076 | train_loss=0.1585 acc=0.9429 | val_loss=0.6761 acc=0.8012 | time=31.8s\n",
            "Epoch 077 | train_loss=0.1511 acc=0.9472 | val_loss=0.7185 acc=0.8152 | time=31.9s\n",
            "Epoch 078 | train_loss=0.1549 acc=0.9456 | val_loss=0.8373 acc=0.7811 | time=31.8s\n",
            "Epoch 079 | train_loss=0.1602 acc=0.9375 | val_loss=0.7720 acc=0.7966 | time=31.8s\n",
            "★ Early stopping at epoch 79\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▂▃▄▄▄▄▄▅▅▅▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>████████▇▇▆▆▆▅▅▅▄▄▄▄▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▃▃▄▄▄▄▄▅▅▅▆▅▆▅▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇█████</td></tr><tr><td>validation_loss</td><td>███████▆▆▅▅▅▄▄▄▃▃▃▃▄▃▂▂▂▁▁▂▂▂▁▁▂▂▁▂▂▂▃▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>79</td></tr><tr><td>train_accuracy</td><td>0.93748</td></tr><tr><td>train_loss</td><td>0.16023</td></tr><tr><td>validation_accuracy</td><td>0.79658</td></tr><tr><td>validation_loss</td><td>0.77196</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-valley-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/v2qmlyvj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/v2qmlyvj</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_233103-v2qmlyvj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:12:52,337] Trial 4 finished with value: 0.5753395855426788 and parameters: {'lr': 0.0009667592898435953, 'weight_decay': 9.995420002148595e-05, 'num_blocks': 3, 'num_heads': 2, 'num_segments': 5, 'step_size': 30, 'gamma': 0.4573463771689297}. Best is trial 4 with value: 0.5753395855426788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=6.22e-04, wd=4.54e-05, blocks=2, heads=2, segs=5, step_size=20, gamma=0.71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_001252-e4hu7u4o</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/e4hu7u4o' target=\"_blank\">prime-bird-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/e4hu7u4o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/e4hu7u4o</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0704 acc=0.4190 | val_loss=1.0747 acc=0.4317 | time=23.6s\n",
            "Epoch 002 | train_loss=1.0710 acc=0.4082 | val_loss=1.0748 acc=0.4317 | time=23.5s\n",
            "Epoch 003 | train_loss=1.0687 acc=0.4311 | val_loss=1.0779 acc=0.4317 | time=23.3s\n",
            "Epoch 004 | train_loss=1.0686 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=23.4s\n",
            "Epoch 005 | train_loss=1.0671 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=23.4s\n",
            "Epoch 006 | train_loss=1.0664 acc=0.4311 | val_loss=1.0767 acc=0.4317 | time=23.5s\n",
            "Epoch 007 | train_loss=1.0655 acc=0.4311 | val_loss=1.0824 acc=0.4317 | time=23.6s\n",
            "Epoch 008 | train_loss=1.0107 acc=0.5219 | val_loss=0.9660 acc=0.5776 | time=23.6s\n",
            "Epoch 009 | train_loss=0.9627 acc=0.5670 | val_loss=0.9528 acc=0.5714 | time=23.7s\n",
            "Epoch 010 | train_loss=0.9302 acc=0.5891 | val_loss=0.9506 acc=0.5606 | time=23.4s\n",
            "Epoch 011 | train_loss=0.9221 acc=0.5953 | val_loss=1.0528 acc=0.5435 | time=23.5s\n",
            "Epoch 012 | train_loss=0.9045 acc=0.6050 | val_loss=0.9833 acc=0.5839 | time=23.7s\n",
            "Epoch 013 | train_loss=0.8916 acc=0.6082 | val_loss=0.9218 acc=0.5807 | time=23.6s\n",
            "Epoch 014 | train_loss=0.8554 acc=0.6315 | val_loss=0.9337 acc=0.6102 | time=23.5s\n",
            "Epoch 015 | train_loss=0.8542 acc=0.6280 | val_loss=0.9323 acc=0.5932 | time=23.6s\n",
            "Epoch 016 | train_loss=0.8359 acc=0.6276 | val_loss=0.9161 acc=0.6025 | time=23.7s\n",
            "Epoch 017 | train_loss=0.7825 acc=0.6497 | val_loss=0.9142 acc=0.6180 | time=23.5s\n",
            "Epoch 018 | train_loss=0.7695 acc=0.6579 | val_loss=0.9927 acc=0.5823 | time=23.8s\n",
            "Epoch 019 | train_loss=0.7450 acc=0.6617 | val_loss=0.9042 acc=0.6149 | time=23.8s\n",
            "Epoch 020 | train_loss=0.7166 acc=0.6816 | val_loss=0.8658 acc=0.6196 | time=23.6s\n",
            "Epoch 021 | train_loss=0.6828 acc=0.6839 | val_loss=0.9230 acc=0.6056 | time=23.7s\n",
            "Epoch 022 | train_loss=0.6317 acc=0.7258 | val_loss=0.8702 acc=0.5932 | time=23.7s\n",
            "Epoch 023 | train_loss=0.6254 acc=0.7285 | val_loss=0.9251 acc=0.6211 | time=23.7s\n",
            "Epoch 024 | train_loss=0.5828 acc=0.7441 | val_loss=0.9329 acc=0.5807 | time=23.6s\n",
            "Epoch 025 | train_loss=0.5426 acc=0.7689 | val_loss=1.0293 acc=0.6196 | time=23.8s\n",
            "Epoch 026 | train_loss=0.5277 acc=0.7751 | val_loss=0.9671 acc=0.5854 | time=23.8s\n",
            "Epoch 027 | train_loss=0.4728 acc=0.8070 | val_loss=1.1542 acc=0.5994 | time=23.7s\n",
            "Epoch 028 | train_loss=0.4655 acc=0.8050 | val_loss=1.0341 acc=0.5978 | time=23.8s\n",
            "Epoch 029 | train_loss=0.4650 acc=0.8066 | val_loss=1.0288 acc=0.5885 | time=23.5s\n",
            "Epoch 030 | train_loss=0.4160 acc=0.8423 | val_loss=1.1040 acc=0.6196 | time=23.5s\n",
            "Epoch 031 | train_loss=0.3709 acc=0.8517 | val_loss=1.2427 acc=0.6056 | time=23.7s\n",
            "Epoch 032 | train_loss=0.3759 acc=0.8517 | val_loss=1.1276 acc=0.6134 | time=23.9s\n",
            "Epoch 033 | train_loss=0.3035 acc=0.8858 | val_loss=1.2771 acc=0.6025 | time=23.7s\n",
            "Epoch 034 | train_loss=0.3016 acc=0.8878 | val_loss=1.3548 acc=0.6056 | time=23.5s\n",
            "Epoch 035 | train_loss=0.2770 acc=0.8936 | val_loss=1.5585 acc=0.6025 | time=23.7s\n",
            "Epoch 036 | train_loss=0.2759 acc=0.8944 | val_loss=1.3420 acc=0.6165 | time=23.7s\n",
            "Epoch 037 | train_loss=0.2469 acc=0.9052 | val_loss=1.2864 acc=0.5978 | time=23.6s\n",
            "Epoch 038 | train_loss=0.2292 acc=0.9130 | val_loss=1.6117 acc=0.6429 | time=23.9s\n",
            "Epoch 039 | train_loss=0.2015 acc=0.9254 | val_loss=1.6402 acc=0.6056 | time=23.7s\n",
            "Epoch 040 | train_loss=0.2071 acc=0.9270 | val_loss=1.5961 acc=0.6009 | time=23.8s\n",
            "★ Early stopping at epoch 40\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▃▃▃▄▄▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>████████▇▇▇▇▇▆▆▆▆▆▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▆▆▅▅▆▆▇▆▇▇▆▇▇▇▆▇▆▇▆▇▇▆▇▇▇▇▇▇▇▇█▇▇</td></tr><tr><td>validation_loss</td><td>▃▃▃▃▃▃▃▂▂▂▃▂▂▂▂▁▁▂▁▁▂▁▂▂▂▂▄▃▂▃▄▃▅▅▇▅▅███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>40</td></tr><tr><td>train_accuracy</td><td>0.92699</td></tr><tr><td>train_loss</td><td>0.20714</td></tr><tr><td>validation_accuracy</td><td>0.60093</td></tr><tr><td>validation_loss</td><td>1.59606</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">prime-bird-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/e4hu7u4o' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/e4hu7u4o</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_001252-e4hu7u4o/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:28:41,345] Trial 5 finished with value: 0.8657736324128651 and parameters: {'lr': 0.0006224513223071905, 'weight_decay': 4.540866158772019e-05, 'num_blocks': 2, 'num_heads': 2, 'num_segments': 5, 'step_size': 20, 'gamma': 0.7083541661273333}. Best is trial 4 with value: 0.5753395855426788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 6 =====\n",
            " lr=5.21e-04, wd=1.63e-04, blocks=2, heads=4, segs=15, step_size=10, gamma=0.76\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_002841-f8zmtrod</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/f8zmtrod' target=\"_blank\">eager-monkey-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/f8zmtrod' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/f8zmtrod</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0724 acc=0.4287 | val_loss=1.0797 acc=0.4317 | time=33.5s\n",
            "Epoch 002 | train_loss=1.0711 acc=0.4311 | val_loss=1.0772 acc=0.4317 | time=33.8s\n",
            "Epoch 003 | train_loss=1.0683 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=33.9s\n",
            "Epoch 004 | train_loss=1.0680 acc=0.4311 | val_loss=1.0775 acc=0.4317 | time=33.9s\n",
            "Epoch 005 | train_loss=1.0689 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=33.9s\n",
            "Epoch 006 | train_loss=1.0672 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=34.0s\n",
            "Epoch 007 | train_loss=1.0683 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=33.9s\n",
            "Epoch 008 | train_loss=1.0674 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=34.0s\n",
            "Epoch 009 | train_loss=1.0668 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=34.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:34:21,823] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 6 pruned at epoch 10\n",
            "\n",
            "===== Trial 7 =====\n",
            " lr=2.61e-04, wd=4.44e-04, blocks=2, heads=4, segs=5, step_size=30, gamma=0.88\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁████████</td></tr><tr><td>train_loss</td><td>█▆▃▂▄▁▃▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▅▃▅▁▁▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06678</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07437</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-monkey-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/f8zmtrod' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/f8zmtrod</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_002841-f8zmtrod/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_003421-q3g3damt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/q3g3damt' target=\"_blank\">treasured-thunder-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/q3g3damt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/q3g3damt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0728 acc=0.4264 | val_loss=1.0748 acc=0.4317 | time=33.8s\n",
            "Epoch 002 | train_loss=1.0692 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=33.7s\n",
            "Epoch 003 | train_loss=1.0681 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=33.5s\n",
            "Epoch 004 | train_loss=1.0677 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=33.5s\n",
            "Epoch 005 | train_loss=1.0690 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=33.6s\n",
            "Epoch 006 | train_loss=1.0676 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=33.4s\n",
            "Epoch 007 | train_loss=1.0681 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=33.7s\n",
            "Epoch 008 | train_loss=1.0682 acc=0.4311 | val_loss=1.0804 acc=0.4317 | time=33.5s\n",
            "Epoch 009 | train_loss=1.0682 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=33.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:39:59,635] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 7 pruned at epoch 10\n",
            "\n",
            "===== Trial 8 =====\n",
            " lr=1.94e-04, wd=2.01e-05, blocks=2, heads=4, segs=5, step_size=30, gamma=0.86\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁████████</td></tr><tr><td>train_loss</td><td>█▃▂▁▃▁▂▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▂▂▂▁▁▁▂█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06825</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07467</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">treasured-thunder-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/q3g3damt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/q3g3damt</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_003421-q3g3damt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_003959-jcjticnl</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/jcjticnl' target=\"_blank\">happy-flower-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/jcjticnl' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/jcjticnl</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0716 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=33.7s\n",
            "Epoch 002 | train_loss=1.0663 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=33.7s\n",
            "Epoch 003 | train_loss=1.0671 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=33.7s\n",
            "Epoch 004 | train_loss=1.0677 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=33.5s\n",
            "Epoch 005 | train_loss=1.0671 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=33.6s\n",
            "Epoch 006 | train_loss=1.0665 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=33.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:43:56,779] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 8 pruned at epoch 7\n",
            "\n",
            "===== Trial 9 =====\n",
            " lr=2.78e-04, wd=4.85e-04, blocks=2, heads=4, segs=5, step_size=30, gamma=0.56\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▁▂▃▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▇▅▂█▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06649</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07459</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">happy-flower-9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/jcjticnl' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/jcjticnl</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_003959-jcjticnl/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_004356-wvsly0z1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/wvsly0z1' target=\"_blank\">eager-bird-10</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/wvsly0z1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/wvsly0z1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0774 acc=0.4074 | val_loss=1.0744 acc=0.4317 | time=33.5s\n",
            "Epoch 002 | train_loss=1.0695 acc=0.4311 | val_loss=1.0780 acc=0.4317 | time=33.7s\n",
            "Epoch 003 | train_loss=1.0689 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=33.6s\n",
            "Epoch 004 | train_loss=1.0679 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=33.6s\n",
            "Epoch 005 | train_loss=1.0663 acc=0.4311 | val_loss=1.0799 acc=0.4317 | time=33.8s\n",
            "Epoch 006 | train_loss=1.0675 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=33.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:47:53,980] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 9 pruned at epoch 7\n",
            "\n",
            "===== Trial 10 =====\n",
            " lr=7.45e-04, wd=2.97e-04, blocks=1, heads=3, segs=15, step_size=20, gamma=0.52\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>train_accuracy</td><td>▁█████</td></tr><tr><td>train_loss</td><td>█▃▃▂▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▆▁▂█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06746</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07479</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eager-bird-10</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/wvsly0z1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/wvsly0z1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_004356-wvsly0z1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_004754-d4xqaa9r</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d4xqaa9r' target=\"_blank\">eternal-dew-11</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d4xqaa9r' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d4xqaa9r</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0703 acc=0.4276 | val_loss=1.0747 acc=0.4317 | time=18.7s\n",
            "Epoch 002 | train_loss=1.0708 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.8s\n",
            "Epoch 003 | train_loss=1.0677 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=19.1s\n",
            "Epoch 004 | train_loss=1.0702 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=19.1s\n",
            "Epoch 005 | train_loss=1.0688 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=18.8s\n",
            "Epoch 006 | train_loss=1.0679 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.7s\n",
            "Epoch 007 | train_loss=1.0674 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=18.6s\n",
            "Epoch 008 | train_loss=1.0665 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=18.9s\n",
            "Epoch 009 | train_loss=1.0204 acc=0.5021 | val_loss=1.0276 acc=0.4953 | time=18.6s\n",
            "Epoch 010 | train_loss=0.9694 acc=0.5763 | val_loss=0.9753 acc=0.5730 | time=18.9s\n",
            "Epoch 011 | train_loss=0.9385 acc=0.5891 | val_loss=0.9889 acc=0.5264 | time=18.7s\n",
            "Epoch 012 | train_loss=0.9213 acc=0.5973 | val_loss=0.9832 acc=0.5807 | time=19.0s\n",
            "Epoch 013 | train_loss=0.8915 acc=0.6082 | val_loss=0.9393 acc=0.5823 | time=18.4s\n",
            "Epoch 014 | train_loss=0.8706 acc=0.6221 | val_loss=0.9508 acc=0.5947 | time=18.7s\n",
            "Epoch 015 | train_loss=0.8465 acc=0.6307 | val_loss=0.9263 acc=0.5932 | time=18.8s\n",
            "Epoch 016 | train_loss=0.8208 acc=0.6408 | val_loss=0.9024 acc=0.6009 | time=18.8s\n",
            "Epoch 017 | train_loss=0.8020 acc=0.6450 | val_loss=0.9248 acc=0.6056 | time=19.0s\n",
            "Epoch 018 | train_loss=0.7690 acc=0.6606 | val_loss=0.8954 acc=0.5450 | time=19.0s\n",
            "Epoch 019 | train_loss=0.7463 acc=0.6769 | val_loss=0.9572 acc=0.6196 | time=19.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:54:12,505] Trial 10 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 10 pruned at epoch 20\n",
            "\n",
            "===== Trial 11 =====\n",
            " lr=9.76e-04, wd=1.63e-05, blocks=3, heads=2, segs=5, step_size=10, gamma=0.27\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▃▅▆▆▆▆▇▇▇██</td></tr><tr><td>train_loss</td><td>████████▇▆▅▅▄▄▃▃▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▃▆▅▇▇▇▇▇▇▅█</td></tr><tr><td>validation_loss</td><td>████████▆▄▅▄▃▃▂▁▂▁▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.67689</td></tr><tr><td>train_loss</td><td>0.74629</td></tr><tr><td>validation_accuracy</td><td>0.61957</td></tr><tr><td>validation_loss</td><td>0.9572</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">eternal-dew-11</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d4xqaa9r' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d4xqaa9r</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_004754-d4xqaa9r/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_005412-1airxtfy</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/1airxtfy' target=\"_blank\">icy-water-12</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/1airxtfy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/1airxtfy</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0702 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.8s\n",
            "Epoch 002 | train_loss=1.0687 acc=0.4311 | val_loss=1.0799 acc=0.4317 | time=31.7s\n",
            "Epoch 003 | train_loss=1.0665 acc=0.4311 | val_loss=1.0915 acc=0.4317 | time=31.8s\n",
            "Epoch 004 | train_loss=1.0686 acc=0.4311 | val_loss=1.0833 acc=0.4317 | time=31.4s\n",
            "Epoch 005 | train_loss=1.0680 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.7s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.5s\n",
            "Epoch 007 | train_loss=1.0663 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.7s\n",
            "Epoch 008 | train_loss=1.0669 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.5s\n",
            "Epoch 009 | train_loss=1.0684 acc=0.4311 | val_loss=1.0776 acc=0.4317 | time=31.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 00:59:30,768] Trial 11 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 11 pruned at epoch 10\n",
            "\n",
            "===== Trial 12 =====\n",
            " lr=1.25e-04, wd=5.46e-05, blocks=3, heads=2, segs=5, step_size=30, gamma=0.37\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▁▅▄▃▁▂▅</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▃█▅▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06844</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07762</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">icy-water-12</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/1airxtfy' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/1airxtfy</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_005412-1airxtfy/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_005930-uk6nzzm1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/uk6nzzm1' target=\"_blank\">polished-deluge-13</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/uk6nzzm1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/uk6nzzm1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0692 acc=0.4311 | val_loss=1.0785 acc=0.4317 | time=31.6s\n",
            "Epoch 002 | train_loss=1.0679 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=31.5s\n",
            "Epoch 003 | train_loss=1.0666 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=31.5s\n",
            "Epoch 004 | train_loss=1.0667 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.5s\n",
            "Epoch 005 | train_loss=1.0667 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.6s\n",
            "Epoch 006 | train_loss=1.0659 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=31.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:03:13,327] Trial 12 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 12 pruned at epoch 7\n",
            "\n",
            "===== Trial 13 =====\n",
            " lr=9.76e-04, wd=3.13e-05, blocks=3, heads=2, segs=5, step_size=10, gamma=0.38\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▄▅▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▂▃▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▃▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>6</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06588</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07442</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">polished-deluge-13</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/uk6nzzm1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/uk6nzzm1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_005930-uk6nzzm1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_010313-27rd76n5</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/27rd76n5' target=\"_blank\">vague-hill-14</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/27rd76n5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/27rd76n5</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0753 acc=0.4132 | val_loss=1.0776 acc=0.4317 | time=31.5s\n",
            "Epoch 002 | train_loss=1.0751 acc=0.4206 | val_loss=1.0819 acc=0.4317 | time=31.6s\n",
            "Epoch 003 | train_loss=1.0677 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.6s\n",
            "Epoch 004 | train_loss=1.0678 acc=0.4311 | val_loss=1.0771 acc=0.4317 | time=31.8s\n",
            "Epoch 005 | train_loss=1.0669 acc=0.4311 | val_loss=1.0790 acc=0.4317 | time=31.7s\n",
            "Epoch 006 | train_loss=1.0685 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.9s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=31.8s\n",
            "Epoch 008 | train_loss=1.0679 acc=0.4311 | val_loss=1.0766 acc=0.4317 | time=31.8s\n",
            "Epoch 009 | train_loss=1.0678 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=31.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:08:32,378] Trial 13 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 13 pruned at epoch 10\n",
            "\n",
            "===== Trial 14 =====\n",
            " lr=4.05e-04, wd=9.54e-04, blocks=3, heads=2, segs=5, step_size=20, gamma=0.58\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▄▅▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▄███████</td></tr><tr><td>train_loss</td><td>██▂▂▁▂▁▂▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▄█▁▄▅▁▁▃▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>9</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06784</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07433</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vague-hill-14</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/27rd76n5' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/27rd76n5</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_010313-27rd76n5/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_010832-m4n7opc2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/m4n7opc2' target=\"_blank\">quiet-thunder-15</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/m4n7opc2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/m4n7opc2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0738 acc=0.4206 | val_loss=1.0748 acc=0.4317 | time=31.9s\n",
            "Epoch 002 | train_loss=1.0714 acc=0.4252 | val_loss=1.0793 acc=0.3416 | time=31.9s\n",
            "Epoch 003 | train_loss=1.0702 acc=0.4245 | val_loss=1.0745 acc=0.4317 | time=32.1s\n",
            "Epoch 004 | train_loss=1.0684 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.9s\n",
            "Epoch 005 | train_loss=1.0672 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.7s\n",
            "Epoch 006 | train_loss=1.0671 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=31.7s\n",
            "Epoch 007 | train_loss=1.0670 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=32.0s\n",
            "Epoch 008 | train_loss=1.0661 acc=0.4311 | val_loss=1.0699 acc=0.4317 | time=31.8s\n",
            "Epoch 009 | train_loss=1.0564 acc=0.4392 | val_loss=1.0699 acc=0.4317 | time=32.1s\n",
            "Epoch 010 | train_loss=1.0097 acc=0.5320 | val_loss=0.9582 acc=0.5792 | time=31.7s\n",
            "Epoch 011 | train_loss=0.9440 acc=0.5806 | val_loss=0.9452 acc=0.5590 | time=32.0s\n",
            "Epoch 012 | train_loss=0.9116 acc=0.6031 | val_loss=0.9299 acc=0.5621 | time=31.9s\n",
            "Epoch 013 | train_loss=0.8648 acc=0.6237 | val_loss=0.9074 acc=0.5994 | time=31.7s\n",
            "Epoch 014 | train_loss=0.8649 acc=0.6252 | val_loss=0.8960 acc=0.6242 | time=31.9s\n",
            "Epoch 015 | train_loss=0.8533 acc=0.6179 | val_loss=0.8976 acc=0.6056 | time=31.9s\n",
            "Epoch 016 | train_loss=0.8359 acc=0.6412 | val_loss=0.8840 acc=0.6149 | time=31.9s\n",
            "Epoch 017 | train_loss=0.8189 acc=0.6450 | val_loss=0.8949 acc=0.5963 | time=31.9s\n",
            "Epoch 018 | train_loss=0.7822 acc=0.6551 | val_loss=0.8626 acc=0.6506 | time=31.8s\n",
            "Epoch 019 | train_loss=0.7839 acc=0.6517 | val_loss=0.8427 acc=0.6304 | time=31.9s\n",
            "Epoch 020 | train_loss=0.7529 acc=0.6680 | val_loss=0.8630 acc=0.6429 | time=31.8s\n",
            "Epoch 021 | train_loss=0.7201 acc=0.6781 | val_loss=0.8745 acc=0.6180 | time=31.7s\n",
            "Epoch 022 | train_loss=0.6624 acc=0.7056 | val_loss=0.8851 acc=0.6211 | time=31.8s\n",
            "Epoch 023 | train_loss=0.6521 acc=0.7184 | val_loss=0.8687 acc=0.6522 | time=31.8s\n",
            "Epoch 024 | train_loss=0.6350 acc=0.7231 | val_loss=0.8600 acc=0.6429 | time=31.7s\n",
            "Epoch 025 | train_loss=0.6010 acc=0.7483 | val_loss=0.9933 acc=0.6615 | time=31.9s\n",
            "Epoch 026 | train_loss=0.5914 acc=0.7425 | val_loss=0.8955 acc=0.6537 | time=31.9s\n",
            "Epoch 027 | train_loss=0.5569 acc=0.7643 | val_loss=0.8919 acc=0.6009 | time=32.1s\n",
            "Epoch 028 | train_loss=0.5718 acc=0.7596 | val_loss=0.8978 acc=0.5947 | time=31.8s\n",
            "Epoch 029 | train_loss=0.4947 acc=0.7934 | val_loss=0.9270 acc=0.6429 | time=31.7s\n",
            "Epoch 030 | train_loss=0.5025 acc=0.7969 | val_loss=0.8711 acc=0.6832 | time=31.8s\n",
            "Epoch 031 | train_loss=0.4725 acc=0.7981 | val_loss=0.8770 acc=0.6677 | time=31.7s\n",
            "Epoch 032 | train_loss=0.5075 acc=0.7872 | val_loss=0.8972 acc=0.6739 | time=31.7s\n",
            "Epoch 033 | train_loss=0.4300 acc=0.8198 | val_loss=0.9268 acc=0.6661 | time=31.6s\n",
            "Epoch 034 | train_loss=0.4312 acc=0.8233 | val_loss=0.9199 acc=0.6506 | time=31.7s\n",
            "Epoch 035 | train_loss=0.4171 acc=0.8400 | val_loss=0.9701 acc=0.6724 | time=31.7s\n",
            "Epoch 036 | train_loss=0.3799 acc=0.8404 | val_loss=1.0035 acc=0.6522 | time=31.7s\n",
            "Epoch 037 | train_loss=0.3802 acc=0.8509 | val_loss=1.0259 acc=0.6460 | time=31.5s\n",
            "Epoch 038 | train_loss=0.3360 acc=0.8691 | val_loss=1.0897 acc=0.6568 | time=31.8s\n",
            "Epoch 039 | train_loss=0.3618 acc=0.8536 | val_loss=1.1361 acc=0.6848 | time=31.4s\n",
            "★ Early stopping at epoch 39\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▁▁▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█████████▇▇▆▆▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▂▃▂▂▂▁▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▃▁▃▃▃▃▃▃▃▆▅▅▆▇▆▇▆▇▇▇▇▇▇▇█▇▆▆▇████▇█▇▇▇█</td></tr><tr><td>validation_loss</td><td>▇▇▇▇▇▇▇▆▆▄▃▃▃▂▂▂▂▁▁▁▂▂▂▁▅▂▂▂▃▂▂▂▃▃▄▅▅▇█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>39</td></tr><tr><td>train_accuracy</td><td>0.85359</td></tr><tr><td>train_loss</td><td>0.36178</td></tr><tr><td>validation_accuracy</td><td>0.68478</td></tr><tr><td>validation_loss</td><td>1.13606</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">quiet-thunder-15</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/m4n7opc2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/m4n7opc2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_010832-m4n7opc2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:29:17,070] Trial 14 finished with value: 0.8427037994066874 and parameters: {'lr': 0.00040519319762359493, 'weight_decay': 0.0009536838748084607, 'num_blocks': 3, 'num_heads': 2, 'num_segments': 5, 'step_size': 20, 'gamma': 0.5833961123553751}. Best is trial 4 with value: 0.5753395855426788.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 15 =====\n",
            " lr=3.91e-04, wd=8.56e-04, blocks=3, heads=3, segs=5, step_size=30, gamma=0.56\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_012917-b6te5bqc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b6te5bqc' target=\"_blank\">glad-bee-16</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b6te5bqc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b6te5bqc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0739 acc=0.4272 | val_loss=1.0789 acc=0.4317 | time=38.8s\n",
            "Epoch 002 | train_loss=1.0688 acc=0.4311 | val_loss=1.0767 acc=0.4317 | time=38.6s\n",
            "Epoch 003 | train_loss=1.0725 acc=0.4311 | val_loss=1.0779 acc=0.4317 | time=38.5s\n",
            "Epoch 004 | train_loss=1.0700 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=38.7s\n",
            "Epoch 005 | train_loss=1.0685 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=38.6s\n",
            "Epoch 006 | train_loss=1.0682 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=38.7s\n",
            "Epoch 007 | train_loss=1.0671 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=38.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:34:27,671] Trial 15 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 15 pruned at epoch 8\n",
            "\n",
            "===== Trial 16 =====\n",
            " lr=1.04e-04, wd=2.13e-04, blocks=3, heads=2, segs=15, step_size=20, gamma=0.41\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁██████</td></tr><tr><td>train_loss</td><td>█▃▇▄▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▅▆▂▄▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06706</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07436</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">glad-bee-16</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b6te5bqc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b6te5bqc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_012917-b6te5bqc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_013427-ibzrv46n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ibzrv46n' target=\"_blank\">clean-breeze-17</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ibzrv46n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ibzrv46n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0740 acc=0.4074 | val_loss=1.0766 acc=0.4317 | time=31.9s\n",
            "Epoch 002 | train_loss=1.0661 acc=0.4311 | val_loss=1.0775 acc=0.4317 | time=31.8s\n",
            "Epoch 003 | train_loss=1.0670 acc=0.4311 | val_loss=1.0769 acc=0.4317 | time=31.9s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=32.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:37:09,146] Trial 16 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 16 pruned at epoch 5\n",
            "\n",
            "===== Trial 17 =====\n",
            " lr=1.74e-04, wd=9.97e-04, blocks=3, heads=2, segs=5, step_size=20, gamma=0.23\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁███</td></tr><tr><td>train_loss</td><td>█▁▂▃</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▆█▆▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06823</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0747</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clean-breeze-17</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ibzrv46n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/ibzrv46n</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_013427-ibzrv46n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_013709-oxz1xyt1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/oxz1xyt1' target=\"_blank\">proud-resonance-18</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/oxz1xyt1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/oxz1xyt1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0707 acc=0.4264 | val_loss=1.0764 acc=0.4317 | time=31.3s\n",
            "Epoch 002 | train_loss=1.0700 acc=0.4229 | val_loss=1.0746 acc=0.4317 | time=31.6s\n",
            "Epoch 003 | train_loss=1.0674 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.5s\n",
            "Epoch 004 | train_loss=1.0668 acc=0.4311 | val_loss=1.0764 acc=0.4317 | time=31.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:39:48,532] Trial 17 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 17 pruned at epoch 5\n",
            "\n",
            "===== Trial 18 =====\n",
            " lr=3.84e-04, wd=1.05e-05, blocks=3, heads=2, segs=5, step_size=30, gamma=0.62\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▄▁██</td></tr><tr><td>train_loss</td><td>█▇▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06682</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07641</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">proud-resonance-18</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/oxz1xyt1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/oxz1xyt1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_013709-oxz1xyt1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_013948-b474xgmc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b474xgmc' target=\"_blank\">crimson-energy-19</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b474xgmc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b474xgmc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0707 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=31.7s\n",
            "Epoch 002 | train_loss=1.0686 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.5s\n",
            "Epoch 003 | train_loss=1.0676 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=31.7s\n",
            "Epoch 004 | train_loss=1.0672 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.7s\n",
            "Epoch 005 | train_loss=1.0675 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=31.6s\n",
            "Epoch 006 | train_loss=1.0676 acc=0.4311 | val_loss=1.0771 acc=0.4317 | time=31.6s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=31.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:44:03,264] Trial 18 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 18 pruned at epoch 8\n",
            "\n",
            "===== Trial 19 =====\n",
            " lr=7.33e-04, wd=1.01e-04, blocks=3, heads=3, segs=15, step_size=10, gamma=0.45\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▂▁▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▆▃▁▂▄█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06726</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07447</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">crimson-energy-19</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b474xgmc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/b474xgmc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_013948-b474xgmc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_014403-94zbd0ap</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/94zbd0ap' target=\"_blank\">iconic-dew-20</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/94zbd0ap' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/94zbd0ap</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0732 acc=0.4109 | val_loss=1.0793 acc=0.4317 | time=39.0s\n",
            "Epoch 002 | train_loss=1.0681 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=39.1s\n",
            "Epoch 003 | train_loss=1.0689 acc=0.4311 | val_loss=1.0766 acc=0.4317 | time=39.2s\n",
            "Epoch 004 | train_loss=1.0674 acc=0.4311 | val_loss=1.0821 acc=0.4317 | time=39.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:47:20,282] Trial 19 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 19 pruned at epoch 5\n",
            "\n",
            "===== Trial 20 =====\n",
            " lr=3.97e-04, wd=8.81e-05, blocks=3, heads=2, segs=10, step_size=20, gamma=0.28\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁███</td></tr><tr><td>train_loss</td><td>█▂▃▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▅▁▂█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06744</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.08212</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">iconic-dew-20</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/94zbd0ap' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/94zbd0ap</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_014403-94zbd0ap/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_014720-x8evlpmg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/x8evlpmg' target=\"_blank\">classic-bird-21</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/x8evlpmg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/x8evlpmg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0735 acc=0.4229 | val_loss=1.0769 acc=0.4317 | time=31.7s\n",
            "Epoch 002 | train_loss=1.0683 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=31.8s\n",
            "Epoch 003 | train_loss=1.0679 acc=0.4311 | val_loss=1.0816 acc=0.4317 | time=31.7s\n",
            "Epoch 004 | train_loss=1.0675 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=31.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:50:01,376] Trial 20 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 20 pruned at epoch 5\n",
            "\n",
            "===== Trial 21 =====\n",
            " lr=7.13e-04, wd=3.39e-04, blocks=3, heads=2, segs=5, step_size=30, gamma=0.47\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁███</td></tr><tr><td>train_loss</td><td>█▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▃▁█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06754</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07541</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">classic-bird-21</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/x8evlpmg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/x8evlpmg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_014720-x8evlpmg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_015001-5il48dtx</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/5il48dtx' target=\"_blank\">neat-dust-22</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/5il48dtx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/5il48dtx</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0734 acc=0.4190 | val_loss=1.0750 acc=0.4317 | time=31.7s\n",
            "Epoch 002 | train_loss=1.0691 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.7s\n",
            "Epoch 003 | train_loss=1.0680 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=31.5s\n",
            "Epoch 004 | train_loss=1.0683 acc=0.4287 | val_loss=1.0755 acc=0.4317 | time=31.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:52:41,351] Trial 21 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 21 pruned at epoch 5\n",
            "\n",
            "===== Trial 22 =====\n",
            " lr=6.20e-04, wd=4.70e-05, blocks=2, heads=2, segs=5, step_size=20, gamma=0.64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁██▇</td></tr><tr><td>train_loss</td><td>█▂▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▄▃█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.42874</td></tr><tr><td>train_loss</td><td>1.06832</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07554</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">neat-dust-22</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/5il48dtx' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/5il48dtx</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_015001-5il48dtx/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_015241-s8tjd6tb</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/s8tjd6tb' target=\"_blank\">clean-morning-23</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/s8tjd6tb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/s8tjd6tb</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0698 acc=0.4237 | val_loss=1.0852 acc=0.4317 | time=23.6s\n",
            "Epoch 002 | train_loss=1.0688 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=23.6s\n",
            "Epoch 003 | train_loss=1.0702 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=23.4s\n",
            "Epoch 004 | train_loss=1.0674 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=23.5s\n",
            "Epoch 005 | train_loss=1.0672 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=23.7s\n",
            "Epoch 006 | train_loss=1.0697 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=23.6s\n",
            "Epoch 007 | train_loss=1.0692 acc=0.4311 | val_loss=1.0787 acc=0.4317 | time=23.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:55:51,922] Trial 22 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 22 pruned at epoch 8\n",
            "\n",
            "===== Trial 23 =====\n",
            " lr=8.15e-04, wd=6.16e-05, blocks=2, heads=2, segs=5, step_size=20, gamma=0.80\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁██████</td></tr><tr><td>train_loss</td><td>▇▅█▂▁▇▆</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▁▁▂▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06923</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07867</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clean-morning-23</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/s8tjd6tb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/s8tjd6tb</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_015241-s8tjd6tb/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_015552-miu89e5i</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/miu89e5i' target=\"_blank\">lilac-terrain-24</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/miu89e5i' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/miu89e5i</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0716 acc=0.4287 | val_loss=1.0828 acc=0.4317 | time=23.3s\n",
            "Epoch 002 | train_loss=1.0709 acc=0.4167 | val_loss=1.0771 acc=0.4317 | time=23.5s\n",
            "Epoch 003 | train_loss=1.0686 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=23.5s\n",
            "Epoch 004 | train_loss=1.0692 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=23.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 01:57:51,459] Trial 23 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 23 pruned at epoch 5\n",
            "\n",
            "===== Trial 24 =====\n",
            " lr=4.88e-04, wd=2.76e-05, blocks=1, heads=2, segs=5, step_size=20, gamma=0.64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▇▁██</td></tr><tr><td>train_loss</td><td>█▆▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06917</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07481</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lilac-terrain-24</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/miu89e5i' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/miu89e5i</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_015552-miu89e5i/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_015751-o4wusxm8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/o4wusxm8' target=\"_blank\">dulcet-energy-25</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/o4wusxm8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/o4wusxm8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0721 acc=0.4171 | val_loss=1.0746 acc=0.4317 | time=18.4s\n",
            "Epoch 002 | train_loss=1.0701 acc=0.4210 | val_loss=1.0754 acc=0.4317 | time=18.5s\n",
            "Epoch 003 | train_loss=1.0691 acc=0.4311 | val_loss=1.0766 acc=0.4317 | time=19.0s\n",
            "Epoch 004 | train_loss=1.0680 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=18.6s\n",
            "Epoch 005 | train_loss=1.0676 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=18.4s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=18.4s\n",
            "Epoch 007 | train_loss=1.0675 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:00:21,852] Trial 24 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 24 pruned at epoch 8\n",
            "\n",
            "===== Trial 25 =====\n",
            " lr=3.46e-04, wd=1.34e-04, blocks=3, heads=2, segs=5, step_size=20, gamma=0.51\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▃█████</td></tr><tr><td>train_loss</td><td>█▅▄▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▄█▂▁▁▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.0675</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.0761</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dulcet-energy-25</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/o4wusxm8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/o4wusxm8</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_015751-o4wusxm8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_020022-afeb73xv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/afeb73xv' target=\"_blank\">confused-galaxy-26</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/afeb73xv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/afeb73xv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0702 acc=0.4311 | val_loss=1.0800 acc=0.4317 | time=31.6s\n",
            "Epoch 002 | train_loss=1.0688 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.4s\n",
            "Epoch 003 | train_loss=1.0693 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=31.8s\n",
            "Epoch 004 | train_loss=1.0672 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=31.6s\n",
            "Epoch 005 | train_loss=1.0680 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=31.2s\n",
            "Epoch 006 | train_loss=1.0670 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=31.2s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=31.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:04:35,212] Trial 25 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 25 pruned at epoch 8\n",
            "\n",
            "===== Trial 26 =====\n",
            " lr=6.24e-04, wd=2.29e-04, blocks=2, heads=3, segs=5, step_size=10, gamma=0.71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▅▆▁▃▁▂</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▂▃▁▃▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06726</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07549</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">confused-galaxy-26</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/afeb73xv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/afeb73xv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_020022-afeb73xv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_020435-zqheouxg</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/zqheouxg' target=\"_blank\">jolly-river-27</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/zqheouxg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/zqheouxg</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0736 acc=0.4066 | val_loss=1.0785 acc=0.4317 | time=28.1s\n",
            "Epoch 002 | train_loss=1.0687 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=27.8s\n",
            "Epoch 003 | train_loss=1.0693 acc=0.4237 | val_loss=1.0743 acc=0.4317 | time=28.0s\n",
            "Epoch 004 | train_loss=1.0697 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=27.8s\n",
            "Epoch 005 | train_loss=1.0668 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=28.0s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=27.8s\n",
            "Epoch 007 | train_loss=1.0693 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=27.9s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:08:20,628] Trial 26 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 26 pruned at epoch 8\n",
            "\n",
            "===== Trial 27 =====\n",
            " lr=8.31e-04, wd=7.73e-05, blocks=3, heads=2, segs=5, step_size=30, gamma=0.61\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▂▃▅▆▇█</td></tr><tr><td>train_accuracy</td><td>▁█▆████</td></tr><tr><td>train_loss</td><td>█▃▄▄▁▁▄</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>validation_loss</td><td>█▃▁▂▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>7</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06928</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07458</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jolly-river-27</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/zqheouxg' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/zqheouxg</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_020435-zqheouxg/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_020820-21orx56f</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/21orx56f' target=\"_blank\">dutiful-glitter-28</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/21orx56f' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/21orx56f</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0716 acc=0.4295 | val_loss=1.0743 acc=0.4317 | time=31.3s\n",
            "Epoch 002 | train_loss=1.0693 acc=0.4311 | val_loss=1.0799 acc=0.4317 | time=31.2s\n",
            "Epoch 003 | train_loss=1.0708 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=31.4s\n",
            "Epoch 004 | train_loss=1.0686 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=31.1s\n",
            "Epoch 005 | train_loss=1.0607 acc=0.4322 | val_loss=1.0776 acc=0.4317 | time=31.3s\n",
            "Epoch 006 | train_loss=1.0708 acc=0.4159 | val_loss=1.0837 acc=0.4317 | time=31.5s\n",
            "Epoch 007 | train_loss=1.0683 acc=0.4311 | val_loss=1.0716 acc=0.4317 | time=31.3s\n",
            "Epoch 008 | train_loss=1.0399 acc=0.4707 | val_loss=0.9963 acc=0.5388 | time=31.3s\n",
            "Epoch 009 | train_loss=0.9786 acc=0.5619 | val_loss=0.9826 acc=0.5668 | time=31.4s\n",
            "Epoch 010 | train_loss=0.9644 acc=0.5662 | val_loss=0.9586 acc=0.5683 | time=31.4s\n",
            "Epoch 011 | train_loss=0.9340 acc=0.5845 | val_loss=0.9447 acc=0.5575 | time=31.6s\n",
            "Epoch 012 | train_loss=0.9251 acc=0.6000 | val_loss=0.9461 acc=0.5730 | time=31.6s\n",
            "Epoch 013 | train_loss=0.9070 acc=0.6039 | val_loss=0.9272 acc=0.5745 | time=31.7s\n",
            "Epoch 014 | train_loss=0.8796 acc=0.6210 | val_loss=0.9744 acc=0.5419 | time=31.6s\n",
            "Epoch 015 | train_loss=0.8722 acc=0.6252 | val_loss=0.9371 acc=0.5978 | time=31.9s\n",
            "Epoch 016 | train_loss=0.8846 acc=0.6151 | val_loss=0.9271 acc=0.5963 | time=31.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:17:17,623] Trial 27 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 27 pruned at epoch 17\n",
            "\n",
            "===== Trial 28 =====\n",
            " lr=4.38e-04, wd=1.11e-04, blocks=3, heads=2, segs=5, step_size=20, gamma=0.80\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▄▄▅▅▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▂▂▂▂▁▂▃▆▆▇▇▇███</td></tr><tr><td>train_loss</td><td>███████▇▅▄▃▃▂▁▁▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▆▇▇▆▇▇▆██</td></tr><tr><td>validation_loss</td><td>██████▇▄▃▂▂▂▁▃▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>16</td></tr><tr><td>train_accuracy</td><td>0.61515</td></tr><tr><td>train_loss</td><td>0.88463</td></tr><tr><td>validation_accuracy</td><td>0.59627</td></tr><tr><td>validation_loss</td><td>0.92711</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dutiful-glitter-28</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/21orx56f' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/21orx56f</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_020820-21orx56f/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_021717-75nvweyt</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/75nvweyt' target=\"_blank\">soft-flower-29</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/75nvweyt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/75nvweyt</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0738 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.6s\n",
            "Epoch 002 | train_loss=1.0718 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=31.4s\n",
            "Epoch 003 | train_loss=1.0693 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=31.5s\n",
            "Epoch 004 | train_loss=1.0668 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=31.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:19:57,158] Trial 28 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 28 pruned at epoch 5\n",
            "\n",
            "===== Trial 29 =====\n",
            " lr=5.77e-04, wd=6.79e-04, blocks=1, heads=2, segs=15, step_size=20, gamma=0.56\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▃▆█</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁</td></tr><tr><td>train_loss</td><td>█▆▄▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁</td></tr><tr><td>validation_loss</td><td>▁▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>4</td></tr><tr><td>train_accuracy</td><td>0.43107</td></tr><tr><td>train_loss</td><td>1.06681</td></tr><tr><td>validation_accuracy</td><td>0.43168</td></tr><tr><td>validation_loss</td><td>1.07611</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">soft-flower-29</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/75nvweyt' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/75nvweyt</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_021717-75nvweyt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_021957-d77a655h</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d77a655h' target=\"_blank\">playful-dew-30</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d77a655h' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d77a655h</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0696 acc=0.4245 | val_loss=1.0744 acc=0.4317 | time=18.2s\n",
            "Epoch 002 | train_loss=1.0708 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=18.3s\n",
            "Epoch 003 | train_loss=1.0679 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=18.2s\n",
            "Epoch 004 | train_loss=1.0682 acc=0.4311 | val_loss=1.0768 acc=0.4317 | time=18.4s\n",
            "Epoch 005 | train_loss=1.0685 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=18.4s\n",
            "Epoch 006 | train_loss=1.0664 acc=0.4311 | val_loss=1.0784 acc=0.4317 | time=18.2s\n",
            "Epoch 007 | train_loss=1.0689 acc=0.4311 | val_loss=1.0796 acc=0.4317 | time=18.8s\n",
            "Epoch 008 | train_loss=1.0329 acc=0.4913 | val_loss=1.0455 acc=0.5342 | time=18.5s\n",
            "Epoch 009 | train_loss=0.9497 acc=0.5794 | val_loss=0.9591 acc=0.5668 | time=18.8s\n",
            "Epoch 010 | train_loss=0.9223 acc=0.5969 | val_loss=0.9393 acc=0.5652 | time=18.6s\n",
            "Epoch 011 | train_loss=0.9109 acc=0.6031 | val_loss=0.9785 acc=0.5745 | time=18.7s\n",
            "Epoch 012 | train_loss=0.8828 acc=0.6124 | val_loss=0.9780 acc=0.5621 | time=18.9s\n",
            "Epoch 013 | train_loss=0.8644 acc=0.6206 | val_loss=0.9114 acc=0.6009 | time=18.3s\n",
            "Epoch 014 | train_loss=0.8297 acc=0.6408 | val_loss=0.9858 acc=0.6134 | time=18.5s\n",
            "Epoch 015 | train_loss=0.8266 acc=0.6416 | val_loss=0.9162 acc=0.5932 | time=18.3s\n",
            "Epoch 016 | train_loss=0.8288 acc=0.6272 | val_loss=0.9347 acc=0.5730 | time=18.9s\n",
            "Epoch 017 | train_loss=0.7780 acc=0.6610 | val_loss=0.9345 acc=0.6087 | time=18.7s\n",
            "Epoch 018 | train_loss=0.7532 acc=0.6664 | val_loss=0.9264 acc=0.5854 | time=18.6s\n",
            "Epoch 019 | train_loss=0.7261 acc=0.6913 | val_loss=0.9425 acc=0.6211 | time=18.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:26:09,950] Trial 29 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 29 pruned at epoch 20\n",
            "\n",
            "===== Trial 30 =====\n",
            " lr=3.14e-04, wd=1.66e-04, blocks=2, heads=3, segs=10, step_size=20, gamma=0.66\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇██</td></tr><tr><td>train_accuracy</td><td>▁▁▁▁▁▁▁▃▅▆▆▆▆▇▇▆▇▇█</td></tr><tr><td>train_loss</td><td>███████▇▆▅▅▄▄▃▃▃▂▂▁</td></tr><tr><td>validation_accuracy</td><td>▁▁▁▁▁▁▁▅▆▆▆▆▇█▇▆█▇█</td></tr><tr><td>validation_loss</td><td>███████▇▃▂▄▄▁▄▁▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>19</td></tr><tr><td>train_accuracy</td><td>0.69126</td></tr><tr><td>train_loss</td><td>0.72615</td></tr><tr><td>validation_accuracy</td><td>0.62112</td></tr><tr><td>validation_loss</td><td>0.94252</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">playful-dew-30</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d77a655h' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/d77a655h</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250501_021957-d77a655h/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250501_022610-gtea7v60</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/gtea7v60' target=\"_blank\">kind-paper-31</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/gtea7v60' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning-1/runs/gtea7v60</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0702 acc=0.4249 | val_loss=1.0793 acc=0.3416 | time=28.4s\n",
            "Epoch 002 | train_loss=1.0696 acc=0.4237 | val_loss=1.0742 acc=0.4317 | time=28.6s\n",
            "Epoch 003 | train_loss=1.0681 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=28.5s\n",
            "Epoch 004 | train_loss=1.0690 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=28.7s\n",
            "Epoch 005 | train_loss=1.0667 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=28.3s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=28.2s\n",
            "Epoch 007 | train_loss=1.0668 acc=0.4311 | val_loss=1.0766 acc=0.4317 | time=28.3s\n",
            "Epoch 008 | train_loss=1.0668 acc=0.4311 | val_loss=1.0720 acc=0.4317 | time=28.5s\n",
            "Epoch 009 | train_loss=1.0378 acc=0.4819 | val_loss=0.9859 acc=0.5543 | time=28.4s\n",
            "Epoch 010 | train_loss=0.9629 acc=0.5682 | val_loss=0.9659 acc=0.5714 | time=28.3s\n",
            "Epoch 011 | train_loss=0.9239 acc=0.5988 | val_loss=0.9472 acc=0.5854 | time=28.4s\n",
            "Epoch 012 | train_loss=0.8993 acc=0.6027 | val_loss=0.9550 acc=0.5870 | time=28.5s\n",
            "Epoch 013 | train_loss=0.8992 acc=0.6058 | val_loss=0.9164 acc=0.5994 | time=28.4s\n",
            "Epoch 014 | train_loss=0.8556 acc=0.6287 | val_loss=0.9161 acc=0.5839 | time=28.4s\n",
            "Epoch 015 | train_loss=0.8464 acc=0.6338 | val_loss=0.8995 acc=0.6102 | time=28.2s\n",
            "Epoch 016 | train_loss=0.8059 acc=0.6439 | val_loss=0.9088 acc=0.5978 | time=28.6s\n",
            "Epoch 017 | train_loss=0.7655 acc=0.6625 | val_loss=0.9935 acc=0.5901 | time=28.4s\n",
            "Epoch 018 | train_loss=0.7334 acc=0.6866 | val_loss=0.9930 acc=0.5839 | time=28.4s\n",
            "Epoch 019 | train_loss=0.7185 acc=0.6870 | val_loss=1.0352 acc=0.5683 | time=28.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-01 02:35:40,692] Trial 30 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 30 pruned at epoch 20\n",
            "\n",
            "===== Best Trial =====\n",
            "best_val_loss   = 0.575340\n",
            "best_val_acc    = 0.7671\n",
            "best_train_loss = 0.3076\n",
            "best_train_acc  = 0.8816\n",
            "best params:\n",
            "  lr: 0.0009667592898435953\n",
            "  weight_decay: 9.995420002148595e-05\n",
            "  num_blocks: 3\n",
            "  num_heads: 2\n",
            "  num_segments: 5\n",
            "  step_size: 30\n",
            "  gamma: 0.4573463771689297\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"hello\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0kogiAiCv9kz",
        "outputId": "0b38821b-5679-4caf-b9a4-e05dc862673c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NNyye3mg18Yi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1cRfcWsN18ek"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "y-HJ_FkM18jk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SAv8z3Ei18lv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eb8NCZUsv0O3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-4, 1e-3\n",
        "WD_MIN, WD_MAX     = 1e-5, 1e-3\n",
        "NUM_FILTERS        = 120\n",
        "NUM_BLOCK_CHOICES  = [1, 2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 3, 4]\n",
        "SEGMENT_CHOICES    = [5, 10, 15]\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "MAX_EPOCHS  = 200\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # 1) Sample hyperparameters\n",
        "    lr           = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(f\" lr={lr:.2e}, wd={weight_decay:.2e}, blocks={num_blocks}, \"\n",
        "          f\"heads={num_heads}, segs={num_segments}\")\n",
        "\n",
        "    # 2) 데이터 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 3) 한 번의 Hold-out split (80/20)\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # 4) W&B init (optional)\n",
        "    wandb.init(project=\"eeg-holdout-tuning\", config=trial.params)\n",
        "\n",
        "    # 5) Model / Optimizer / Loss\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 6) Train with early-stopping & pruning\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "\n",
        "    # **여기에 저장용 변수를 선언**\n",
        "    best_train_loss = None\n",
        "    best_train_acc  = None\n",
        "    best_val_acc    = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward(); optimizer.step()\n",
        "            tloss     += loss.item()\n",
        "            tcorrect  += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal    += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — val —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss    += loss.item()\n",
        "                vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # pruning\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        print(f\"Epoch {epoch:03d} | \"\n",
        "              f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "              f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "              f\"time={elapsed:.1f}s\")\n",
        "\n",
        "        # early stopping & **최적시 train/val 지표 저장**\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # **최적 epoch 시 지표들을 user_attrs 에 저장**\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_trial\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_holdout.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout, n_trials=30)\n",
        "\n",
        "    # — 최종적으로 best trial 의 값과 user_attrs 를 함께 출력 —\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"best_val_acc    = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(f\"best_train_loss = {best.user_attrs['best_train_loss']:.4f}\")\n",
        "    print(f\"best_train_acc  = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GuajFZS5CHKL",
        "outputId": "37e79775-eafa-490f-e95e-dd79f788e71e"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-30 19:01:36,475] A new study created in RDB with name: eeg_holdout_trial\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 0 =====\n",
            " lr=1.10e-04, wd=1.46e-04, blocks=1, heads=2, segs=10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_190146-dq3zwe5l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/dq3zwe5l' target=\"_blank\">peachy-thunder-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/dq3zwe5l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/dq3zwe5l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0711 acc=0.4311 | val_loss=1.0785 acc=0.4317 | time=330.3s\n",
            "Epoch 002 | train_loss=1.0664 acc=0.4311 | val_loss=1.0804 acc=0.4317 | time=17.3s\n",
            "Epoch 003 | train_loss=1.0679 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=17.3s\n",
            "Epoch 004 | train_loss=1.0668 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=17.3s\n",
            "Epoch 005 | train_loss=1.0685 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=17.1s\n",
            "Epoch 006 | train_loss=1.0673 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.5s\n",
            "Epoch 007 | train_loss=1.0673 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.5s\n",
            "Epoch 008 | train_loss=1.0668 acc=0.4311 | val_loss=1.0770 acc=0.4317 | time=17.4s\n",
            "Epoch 009 | train_loss=1.0669 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=17.4s\n",
            "Epoch 010 | train_loss=1.0681 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.2s\n",
            "Epoch 011 | train_loss=1.0662 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=17.2s\n",
            "Epoch 012 | train_loss=1.0667 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=17.1s\n",
            "Epoch 013 | train_loss=1.0661 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=17.1s\n",
            "Epoch 014 | train_loss=1.0669 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=17.6s\n",
            "Epoch 015 | train_loss=1.0666 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=17.1s\n",
            "Epoch 016 | train_loss=1.0543 acc=0.4505 | val_loss=1.0318 acc=0.5373 | time=17.1s\n",
            "Epoch 017 | train_loss=0.9716 acc=0.5670 | val_loss=0.9988 acc=0.5575 | time=17.5s\n",
            "Epoch 018 | train_loss=0.9286 acc=0.5868 | val_loss=0.9473 acc=0.5590 | time=17.4s\n",
            "Epoch 019 | train_loss=0.9022 acc=0.5981 | val_loss=0.9350 acc=0.5543 | time=17.6s\n",
            "Epoch 020 | train_loss=0.8654 acc=0.6237 | val_loss=0.9620 acc=0.5870 | time=17.4s\n",
            "Epoch 021 | train_loss=0.8598 acc=0.6283 | val_loss=0.9198 acc=0.5839 | time=17.4s\n",
            "Epoch 022 | train_loss=0.8388 acc=0.6334 | val_loss=1.0459 acc=0.5590 | time=17.7s\n",
            "Epoch 023 | train_loss=0.8234 acc=0.6431 | val_loss=0.9166 acc=0.5714 | time=17.8s\n",
            "Epoch 024 | train_loss=0.8005 acc=0.6617 | val_loss=0.9822 acc=0.5761 | time=17.7s\n",
            "Epoch 025 | train_loss=0.7816 acc=0.6707 | val_loss=0.9545 acc=0.5699 | time=17.8s\n",
            "Epoch 026 | train_loss=0.7714 acc=0.6602 | val_loss=0.9510 acc=0.5776 | time=17.6s\n",
            "Epoch 027 | train_loss=0.7442 acc=0.6738 | val_loss=0.9391 acc=0.5761 | time=17.4s\n",
            "Epoch 028 | train_loss=0.7301 acc=0.6819 | val_loss=0.9793 acc=0.5683 | time=17.6s\n",
            "Epoch 029 | train_loss=0.7088 acc=0.6843 | val_loss=0.9518 acc=0.5683 | time=17.3s\n",
            "Epoch 030 | train_loss=0.6917 acc=0.7006 | val_loss=0.9469 acc=0.5870 | time=17.7s\n",
            "Epoch 031 | train_loss=0.6559 acc=0.7091 | val_loss=0.9865 acc=0.6040 | time=17.7s\n",
            "Epoch 032 | train_loss=0.6511 acc=0.7181 | val_loss=1.0182 acc=0.5947 | time=17.5s\n",
            "Epoch 033 | train_loss=0.6351 acc=0.7297 | val_loss=0.9102 acc=0.5963 | time=17.6s\n",
            "Epoch 034 | train_loss=0.6064 acc=0.7375 | val_loss=0.9416 acc=0.6149 | time=17.7s\n",
            "Epoch 035 | train_loss=0.5813 acc=0.7569 | val_loss=1.0542 acc=0.6211 | time=17.7s\n",
            "Epoch 036 | train_loss=0.5571 acc=0.7639 | val_loss=1.0731 acc=0.6149 | time=17.7s\n",
            "Epoch 037 | train_loss=0.5281 acc=0.7783 | val_loss=1.0633 acc=0.6320 | time=17.6s\n",
            "Epoch 038 | train_loss=0.4936 acc=0.7988 | val_loss=1.0245 acc=0.6289 | time=17.8s\n",
            "Epoch 039 | train_loss=0.4614 acc=0.8113 | val_loss=1.0719 acc=0.6335 | time=17.7s\n",
            "Epoch 040 | train_loss=0.4203 acc=0.8245 | val_loss=1.1026 acc=0.6118 | time=17.9s\n",
            "Epoch 041 | train_loss=0.4361 acc=0.8140 | val_loss=1.0889 acc=0.6429 | time=17.9s\n",
            "Epoch 042 | train_loss=0.3885 acc=0.8439 | val_loss=1.1269 acc=0.6289 | time=17.5s\n",
            "Epoch 043 | train_loss=0.3458 acc=0.8664 | val_loss=1.2327 acc=0.6413 | time=18.0s\n",
            "Epoch 044 | train_loss=0.3423 acc=0.8645 | val_loss=1.1952 acc=0.5963 | time=18.0s\n",
            "Epoch 045 | train_loss=0.3083 acc=0.8827 | val_loss=1.1741 acc=0.6134 | time=17.7s\n",
            "Epoch 046 | train_loss=0.3163 acc=0.8738 | val_loss=1.3201 acc=0.6599 | time=17.7s\n",
            "Epoch 047 | train_loss=0.3176 acc=0.8765 | val_loss=1.2447 acc=0.6661 | time=17.5s\n",
            "Epoch 048 | train_loss=0.2523 acc=0.9045 | val_loss=1.2705 acc=0.6599 | time=17.8s\n",
            "Epoch 049 | train_loss=0.2509 acc=0.9056 | val_loss=1.5484 acc=0.6537 | time=17.4s\n",
            "Epoch 050 | train_loss=0.2227 acc=0.9165 | val_loss=1.3767 acc=0.6460 | time=18.0s\n",
            "Epoch 051 | train_loss=0.2569 acc=0.8913 | val_loss=1.3500 acc=0.6040 | time=17.6s\n",
            "Epoch 052 | train_loss=0.1866 acc=0.9285 | val_loss=1.5709 acc=0.6444 | time=17.8s\n",
            "Epoch 053 | train_loss=0.1635 acc=0.9375 | val_loss=1.7017 acc=0.6522 | time=17.7s\n",
            "★ Early stopping at epoch 53\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">peachy-thunder-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/dq3zwe5l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/dq3zwe5l</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_190146-dq3zwe5l/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-30 19:22:37,535] Trial 0 finished with value: 0.9102347521554857 and parameters: {'lr': 0.00011016247004786675, 'weight_decay': 0.00014633689947303344, 'num_blocks': 1, 'num_heads': 2, 'num_segments': 10}. Best is trial 0 with value: 0.9102347521554857.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 1 =====\n",
            " lr=1.05e-04, wd=6.82e-05, blocks=3, heads=2, segs=10\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_192237-hnxhteiu</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/hnxhteiu' target=\"_blank\">dauntless-sponge-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/hnxhteiu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/hnxhteiu</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 001 | train_loss=1.0713 acc=0.4287 | val_loss=1.0756 acc=0.4317 | time=31.5s\n",
            "Epoch 002 | train_loss=1.0664 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.7s\n",
            "Epoch 003 | train_loss=1.0672 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=31.6s\n",
            "Epoch 004 | train_loss=1.0668 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.5s\n",
            "Epoch 005 | train_loss=1.0674 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=31.5s\n",
            "Epoch 006 | train_loss=1.0669 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.6s\n",
            "Epoch 007 | train_loss=1.0678 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=31.4s\n",
            "Epoch 008 | train_loss=1.0674 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=31.5s\n",
            "Epoch 009 | train_loss=1.0678 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=31.4s\n",
            "Epoch 010 | train_loss=1.0668 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=31.7s\n",
            "Epoch 011 | train_loss=1.0672 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=31.4s\n",
            "Epoch 012 | train_loss=1.0668 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=31.5s\n",
            "Epoch 013 | train_loss=1.0667 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=31.3s\n",
            "Epoch 014 | train_loss=1.0663 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=31.6s\n",
            "Epoch 015 | train_loss=1.0629 acc=0.4295 | val_loss=1.0768 acc=0.4006 | time=31.2s\n",
            "Epoch 016 | train_loss=1.0583 acc=0.4384 | val_loss=1.0835 acc=0.4146 | time=31.6s\n",
            "Epoch 017 | train_loss=1.0574 acc=0.4252 | val_loss=1.0855 acc=0.4317 | time=31.5s\n",
            "Epoch 018 | train_loss=1.0485 acc=0.4408 | val_loss=1.0883 acc=0.3913 | time=31.7s\n",
            "Epoch 019 | train_loss=1.0463 acc=0.4462 | val_loss=1.0930 acc=0.3913 | time=31.4s\n",
            "Epoch 020 | train_loss=1.0397 acc=0.4517 | val_loss=1.0994 acc=0.4006 | time=31.6s\n",
            "Epoch 021 | train_loss=1.0360 acc=0.4602 | val_loss=1.1016 acc=0.4255 | time=31.3s\n",
            "Epoch 022 | train_loss=1.0321 acc=0.4555 | val_loss=1.1121 acc=0.3556 | time=31.7s\n",
            "Epoch 023 | train_loss=1.0296 acc=0.4664 | val_loss=1.1067 acc=0.3680 | time=31.4s\n",
            "★ Early stopping at epoch 23\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dauntless-sponge-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/hnxhteiu' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/hnxhteiu</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_192237-hnxhteiu/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-30 19:34:44,533] Trial 1 finished with value: 1.0745091551826114 and parameters: {'lr': 0.00010539653046890808, 'weight_decay': 6.824872535774929e-05, 'num_blocks': 3, 'num_heads': 2, 'num_segments': 10}. Best is trial 0 with value: 0.9102347521554857.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 2 =====\n",
            " lr=4.00e-04, wd=1.87e-04, blocks=2, heads=4, segs=15\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_193444-qkbritka</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/qkbritka' target=\"_blank\">fragrant-spaceship-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/qkbritka' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/qkbritka</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0720 acc=0.4276 | val_loss=1.0744 acc=0.4317 | time=33.6s\n",
            "Epoch 002 | train_loss=1.0686 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=33.7s\n",
            "Epoch 003 | train_loss=1.0692 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=33.8s\n",
            "Epoch 004 | train_loss=1.0685 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=33.9s\n",
            "Epoch 005 | train_loss=1.0695 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=33.6s\n",
            "Epoch 006 | train_loss=1.0677 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=33.7s\n",
            "Epoch 007 | train_loss=1.0697 acc=0.4311 | val_loss=1.0771 acc=0.4317 | time=33.7s\n",
            "Epoch 008 | train_loss=1.0683 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=33.7s\n",
            "Epoch 009 | train_loss=1.0697 acc=0.4311 | val_loss=1.0793 acc=0.4317 | time=33.5s\n",
            "Epoch 010 | train_loss=1.0679 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=33.9s\n",
            "Epoch 011 | train_loss=1.0676 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=33.7s\n",
            "Epoch 012 | train_loss=1.0467 acc=0.4664 | val_loss=0.9727 acc=0.5652 | time=33.8s\n",
            "Epoch 013 | train_loss=0.9624 acc=0.5748 | val_loss=0.9689 acc=0.5621 | time=33.8s\n",
            "Epoch 014 | train_loss=0.9421 acc=0.5821 | val_loss=0.9674 acc=0.5326 | time=33.7s\n",
            "Epoch 015 | train_loss=0.9284 acc=0.5864 | val_loss=1.0253 acc=0.5714 | time=33.6s\n",
            "Epoch 016 | train_loss=0.9015 acc=0.6039 | val_loss=0.9293 acc=0.5792 | time=33.7s\n",
            "Epoch 017 | train_loss=0.8761 acc=0.6210 | val_loss=0.9484 acc=0.5668 | time=33.7s\n",
            "Epoch 018 | train_loss=0.8314 acc=0.6416 | val_loss=0.8905 acc=0.6087 | time=33.6s\n",
            "Epoch 019 | train_loss=0.7886 acc=0.6699 | val_loss=0.8178 acc=0.6475 | time=33.7s\n",
            "Epoch 020 | train_loss=0.7309 acc=0.6975 | val_loss=0.8612 acc=0.6304 | time=33.6s\n",
            "Epoch 021 | train_loss=0.6403 acc=0.7340 | val_loss=0.7324 acc=0.6770 | time=33.7s\n",
            "Epoch 022 | train_loss=0.6311 acc=0.7371 | val_loss=0.7765 acc=0.6739 | time=33.5s\n",
            "Epoch 023 | train_loss=0.5994 acc=0.7515 | val_loss=0.8415 acc=0.6630 | time=33.8s\n",
            "Epoch 024 | train_loss=0.5209 acc=0.7783 | val_loss=0.7657 acc=0.6693 | time=33.6s\n",
            "Epoch 025 | train_loss=0.4904 acc=0.7977 | val_loss=0.7257 acc=0.6910 | time=33.7s\n",
            "Epoch 026 | train_loss=0.4466 acc=0.8198 | val_loss=0.7218 acc=0.7019 | time=33.9s\n",
            "Epoch 027 | train_loss=0.4113 acc=0.8307 | val_loss=0.7881 acc=0.6957 | time=33.6s\n",
            "Epoch 028 | train_loss=0.3750 acc=0.8458 | val_loss=0.7152 acc=0.6941 | time=33.6s\n",
            "Epoch 029 | train_loss=0.3936 acc=0.8392 | val_loss=0.7189 acc=0.6910 | time=33.8s\n",
            "Epoch 030 | train_loss=0.3288 acc=0.8629 | val_loss=0.8463 acc=0.6786 | time=33.7s\n",
            "Epoch 031 | train_loss=0.3076 acc=0.8734 | val_loss=0.8178 acc=0.7158 | time=33.6s\n",
            "Epoch 032 | train_loss=0.2535 acc=0.8936 | val_loss=0.9825 acc=0.6832 | time=33.7s\n",
            "Epoch 033 | train_loss=0.2215 acc=0.9099 | val_loss=1.3376 acc=0.6894 | time=33.6s\n",
            "Epoch 034 | train_loss=0.2131 acc=0.9192 | val_loss=1.2953 acc=0.7158 | time=33.8s\n",
            "Epoch 035 | train_loss=0.1819 acc=0.9289 | val_loss=1.1481 acc=0.6894 | time=33.6s\n",
            "Epoch 036 | train_loss=0.1482 acc=0.9414 | val_loss=1.5718 acc=0.7065 | time=33.7s\n",
            "Epoch 037 | train_loss=0.1289 acc=0.9495 | val_loss=1.5137 acc=0.6988 | time=33.7s\n",
            "Epoch 038 | train_loss=0.0926 acc=0.9670 | val_loss=1.6247 acc=0.6879 | time=33.8s\n",
            "Epoch 039 | train_loss=0.0946 acc=0.9647 | val_loss=1.8255 acc=0.6925 | time=33.7s\n",
            "Epoch 040 | train_loss=0.0587 acc=0.9806 | val_loss=1.8637 acc=0.7019 | time=33.6s\n",
            "Epoch 041 | train_loss=0.0576 acc=0.9806 | val_loss=1.8098 acc=0.6817 | time=33.5s\n",
            "Epoch 042 | train_loss=0.0997 acc=0.9627 | val_loss=1.9592 acc=0.6972 | time=33.8s\n",
            "Epoch 043 | train_loss=0.0905 acc=0.9678 | val_loss=1.8117 acc=0.6894 | time=33.7s\n",
            "Epoch 044 | train_loss=0.0598 acc=0.9794 | val_loss=1.8157 acc=0.7220 | time=33.6s\n",
            "Epoch 045 | train_loss=0.0631 acc=0.9786 | val_loss=1.9345 acc=0.6708 | time=33.8s\n",
            "Epoch 046 | train_loss=0.0476 acc=0.9825 | val_loss=2.0996 acc=0.6848 | time=33.6s\n",
            "Epoch 047 | train_loss=0.0155 acc=0.9961 | val_loss=2.0677 acc=0.7050 | time=33.4s\n",
            "Epoch 048 | train_loss=0.0182 acc=0.9942 | val_loss=2.4231 acc=0.7081 | time=33.7s\n",
            "★ Early stopping at epoch 48\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">fragrant-spaceship-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/qkbritka' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/qkbritka</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_193444-qkbritka/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 20:01:49,060] Trial 2 finished with value: 0.7152097299695015 and parameters: {'lr': 0.0003996888097196801, 'weight_decay': 0.00018677630423799053, 'num_blocks': 2, 'num_heads': 4, 'num_segments': 15}. Best is trial 2 with value: 0.7152097299695015.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 3 =====\n",
            " lr=6.44e-04, wd=7.44e-04, blocks=3, heads=4, segs=10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_200149-wszqovbe</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wszqovbe' target=\"_blank\">royal-bird-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wszqovbe' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wszqovbe</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0720 acc=0.4245 | val_loss=1.0756 acc=0.4317 | time=46.9s\n",
            "Epoch 002 | train_loss=1.0688 acc=0.4245 | val_loss=1.0752 acc=0.4317 | time=46.7s\n",
            "Epoch 003 | train_loss=1.0689 acc=0.4202 | val_loss=1.0744 acc=0.4317 | time=46.7s\n",
            "Epoch 004 | train_loss=1.0678 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=46.8s\n",
            "Epoch 005 | train_loss=1.0678 acc=0.4311 | val_loss=1.0757 acc=0.4317 | time=46.6s\n",
            "Epoch 006 | train_loss=1.0683 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=46.7s\n",
            "Epoch 007 | train_loss=1.0681 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.8s\n",
            "Epoch 008 | train_loss=1.0670 acc=0.4311 | val_loss=1.0762 acc=0.4317 | time=46.7s\n",
            "Epoch 009 | train_loss=1.0675 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=46.7s\n",
            "Epoch 010 | train_loss=1.0665 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=46.8s\n",
            "Epoch 011 | train_loss=1.0671 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.7s\n",
            "Epoch 012 | train_loss=1.0666 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.8s\n",
            "Epoch 013 | train_loss=1.0671 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.7s\n",
            "Epoch 014 | train_loss=1.0675 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=46.7s\n",
            "Epoch 015 | train_loss=1.0673 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.7s\n",
            "Epoch 016 | train_loss=1.0668 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=46.7s\n",
            "Epoch 017 | train_loss=1.0679 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=46.8s\n",
            "Epoch 018 | train_loss=1.0661 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.7s\n",
            "Epoch 019 | train_loss=1.0666 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=46.8s\n",
            "Epoch 020 | train_loss=1.0671 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=46.7s\n",
            "Epoch 021 | train_loss=1.0664 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=46.8s\n",
            "Epoch 022 | train_loss=1.0670 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=46.7s\n",
            "Epoch 023 | train_loss=1.0669 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=46.7s\n",
            "Epoch 024 | train_loss=1.0670 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=46.7s\n",
            "Epoch 025 | train_loss=1.0665 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=46.7s\n",
            "Epoch 026 | train_loss=1.0673 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=46.8s\n",
            "Epoch 027 | train_loss=1.0655 acc=0.4315 | val_loss=1.0655 acc=0.5155 | time=46.8s\n",
            "Epoch 028 | train_loss=1.0010 acc=0.5421 | val_loss=1.0037 acc=0.5450 | time=46.7s\n",
            "Epoch 029 | train_loss=0.9449 acc=0.5713 | val_loss=0.9996 acc=0.5093 | time=46.9s\n",
            "Epoch 030 | train_loss=0.9249 acc=0.5806 | val_loss=0.9357 acc=0.5652 | time=46.6s\n",
            "Epoch 031 | train_loss=0.8880 acc=0.5957 | val_loss=0.9186 acc=0.5699 | time=46.8s\n",
            "Epoch 032 | train_loss=0.8809 acc=0.6089 | val_loss=0.9148 acc=0.5761 | time=46.7s\n",
            "Epoch 033 | train_loss=0.8525 acc=0.6148 | val_loss=0.9120 acc=0.5885 | time=46.8s\n",
            "Epoch 034 | train_loss=0.8242 acc=0.6245 | val_loss=0.9394 acc=0.5714 | time=46.7s\n",
            "Epoch 035 | train_loss=0.8248 acc=0.6369 | val_loss=0.8817 acc=0.5792 | time=46.8s\n",
            "Epoch 036 | train_loss=0.7995 acc=0.6466 | val_loss=0.8767 acc=0.5823 | time=46.6s\n",
            "Epoch 037 | train_loss=0.7845 acc=0.6583 | val_loss=0.9382 acc=0.6118 | time=46.7s\n",
            "Epoch 038 | train_loss=0.7735 acc=0.6493 | val_loss=0.8996 acc=0.6071 | time=46.7s\n",
            "Epoch 039 | train_loss=0.7721 acc=0.6563 | val_loss=0.9291 acc=0.6196 | time=46.8s\n",
            "Epoch 040 | train_loss=0.7483 acc=0.6672 | val_loss=0.8640 acc=0.6335 | time=46.8s\n",
            "Epoch 041 | train_loss=0.7140 acc=0.6909 | val_loss=0.8883 acc=0.6366 | time=47.0s\n",
            "Epoch 042 | train_loss=0.6988 acc=0.6971 | val_loss=0.8687 acc=0.6553 | time=46.9s\n",
            "Epoch 043 | train_loss=0.6805 acc=0.7021 | val_loss=0.8511 acc=0.6444 | time=46.9s\n",
            "Epoch 044 | train_loss=0.6386 acc=0.7305 | val_loss=0.8875 acc=0.6460 | time=47.0s\n",
            "Epoch 045 | train_loss=0.6249 acc=0.7270 | val_loss=0.8665 acc=0.6537 | time=47.0s\n",
            "Epoch 046 | train_loss=0.6074 acc=0.7414 | val_loss=0.8560 acc=0.6165 | time=46.9s\n",
            "Epoch 047 | train_loss=0.5536 acc=0.7557 | val_loss=0.8369 acc=0.6615 | time=47.2s\n",
            "Epoch 048 | train_loss=0.5243 acc=0.7740 | val_loss=0.9909 acc=0.6522 | time=47.3s\n",
            "Epoch 049 | train_loss=0.5076 acc=0.7942 | val_loss=0.9385 acc=0.6537 | time=47.2s\n",
            "Epoch 050 | train_loss=0.4710 acc=0.8019 | val_loss=0.9441 acc=0.6429 | time=47.2s\n",
            "Epoch 051 | train_loss=0.4754 acc=0.7953 | val_loss=0.8160 acc=0.6910 | time=47.1s\n",
            "Epoch 052 | train_loss=0.4132 acc=0.8175 | val_loss=1.1203 acc=0.6646 | time=47.0s\n",
            "Epoch 053 | train_loss=0.3637 acc=0.8447 | val_loss=1.0107 acc=0.6724 | time=47.1s\n",
            "Epoch 054 | train_loss=0.3401 acc=0.8590 | val_loss=0.9885 acc=0.6801 | time=47.2s\n",
            "Epoch 055 | train_loss=0.3455 acc=0.8625 | val_loss=1.0460 acc=0.6646 | time=47.2s\n",
            "Epoch 056 | train_loss=0.3099 acc=0.8742 | val_loss=1.2029 acc=0.6506 | time=47.0s\n",
            "Epoch 057 | train_loss=0.2748 acc=0.8866 | val_loss=1.2133 acc=0.6537 | time=47.3s\n",
            "Epoch 058 | train_loss=0.2745 acc=0.8967 | val_loss=1.2314 acc=0.6630 | time=47.1s\n",
            "Epoch 059 | train_loss=0.2385 acc=0.9091 | val_loss=1.3417 acc=0.6584 | time=47.0s\n",
            "Epoch 060 | train_loss=0.2237 acc=0.9118 | val_loss=1.3843 acc=0.6553 | time=47.0s\n",
            "Epoch 061 | train_loss=0.1860 acc=0.9313 | val_loss=1.6602 acc=0.6258 | time=47.2s\n",
            "Epoch 062 | train_loss=0.2177 acc=0.9122 | val_loss=1.5820 acc=0.6568 | time=47.1s\n",
            "Epoch 063 | train_loss=0.1867 acc=0.9247 | val_loss=1.7152 acc=0.6413 | time=47.1s\n",
            "Epoch 064 | train_loss=0.1620 acc=0.9390 | val_loss=1.8302 acc=0.6304 | time=47.1s\n",
            "Epoch 065 | train_loss=0.1270 acc=0.9526 | val_loss=2.2602 acc=0.6258 | time=47.2s\n",
            "Epoch 066 | train_loss=0.1263 acc=0.9522 | val_loss=2.2894 acc=0.6413 | time=47.2s\n",
            "Epoch 067 | train_loss=0.1590 acc=0.9406 | val_loss=1.8991 acc=0.6289 | time=47.1s\n",
            "Epoch 068 | train_loss=0.0989 acc=0.9627 | val_loss=2.5244 acc=0.6289 | time=47.0s\n",
            "Epoch 069 | train_loss=0.0904 acc=0.9650 | val_loss=2.7360 acc=0.6413 | time=47.1s\n",
            "Epoch 070 | train_loss=0.1198 acc=0.9588 | val_loss=2.0672 acc=0.6506 | time=47.3s\n",
            "Epoch 071 | train_loss=0.0997 acc=0.9647 | val_loss=2.5318 acc=0.6444 | time=47.1s\n",
            "★ Early stopping at epoch 71\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">royal-bird-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wszqovbe' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wszqovbe</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_200149-wszqovbe/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 20:57:26,827] Trial 3 finished with value: 0.8160391095138732 and parameters: {'lr': 0.0006437813768189071, 'weight_decay': 0.0007437632759498076, 'num_blocks': 3, 'num_heads': 4, 'num_segments': 10}. Best is trial 2 with value: 0.7152097299695015.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 4 =====\n",
            " lr=1.44e-04, wd=6.77e-04, blocks=3, heads=4, segs=5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_205726-k29aef5a</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/k29aef5a' target=\"_blank\">bright-snowflake-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/k29aef5a' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/k29aef5a</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0684 acc=0.4318 | val_loss=1.0804 acc=0.4317 | time=46.6s\n",
            "Epoch 002 | train_loss=1.0682 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=46.6s\n",
            "Epoch 003 | train_loss=1.0670 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=46.6s\n",
            "Epoch 004 | train_loss=1.0673 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=46.6s\n",
            "Epoch 005 | train_loss=1.0672 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.6s\n",
            "Epoch 006 | train_loss=1.0682 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=46.5s\n",
            "Epoch 007 | train_loss=1.0665 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=46.6s\n",
            "Epoch 008 | train_loss=1.0670 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=46.5s\n",
            "Epoch 009 | train_loss=1.0677 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=46.7s\n",
            "Epoch 010 | train_loss=1.0674 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=46.6s\n",
            "Epoch 011 | train_loss=1.0663 acc=0.4311 | val_loss=1.0767 acc=0.4317 | time=46.5s\n",
            "Epoch 012 | train_loss=1.0675 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=46.5s\n",
            "Epoch 013 | train_loss=1.0661 acc=0.4311 | val_loss=1.0771 acc=0.4317 | time=46.5s\n",
            "Epoch 014 | train_loss=1.0669 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.5s\n",
            "Epoch 015 | train_loss=1.0661 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=46.5s\n",
            "Epoch 016 | train_loss=1.0668 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.5s\n",
            "Epoch 017 | train_loss=1.0667 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=46.5s\n",
            "Epoch 018 | train_loss=1.0669 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.6s\n",
            "Epoch 019 | train_loss=1.0668 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=46.5s\n",
            "Epoch 020 | train_loss=1.0668 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=46.5s\n",
            "Epoch 021 | train_loss=1.0672 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=46.6s\n",
            "Epoch 022 | train_loss=1.0666 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.6s\n",
            "Epoch 023 | train_loss=1.0668 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=46.5s\n",
            "Epoch 024 | train_loss=1.0659 acc=0.4311 | val_loss=1.0790 acc=0.4317 | time=46.5s\n",
            "★ Early stopping at epoch 24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">bright-snowflake-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/k29aef5a' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/k29aef5a</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_205726-k29aef5a/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 21:16:11,024] Trial 4 finished with value: 1.0742442863328117 and parameters: {'lr': 0.00014358978517331993, 'weight_decay': 0.0006773170584205079, 'num_blocks': 3, 'num_heads': 4, 'num_segments': 5}. Best is trial 2 with value: 0.7152097299695015.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 5 =====\n",
            " lr=1.14e-04, wd=5.87e-04, blocks=3, heads=4, segs=10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_211611-sor32p9g</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/sor32p9g' target=\"_blank\">scarlet-pond-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/sor32p9g' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/sor32p9g</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0696 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=46.8s\n",
            "Epoch 002 | train_loss=1.0675 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.6s\n",
            "Epoch 003 | train_loss=1.0678 acc=0.4311 | val_loss=1.0747 acc=0.4317 | time=46.8s\n",
            "Epoch 004 | train_loss=1.0673 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=46.7s\n",
            "Epoch 005 | train_loss=1.0675 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=46.7s\n",
            "Epoch 006 | train_loss=1.0677 acc=0.4311 | val_loss=1.0745 acc=0.4317 | time=46.7s\n",
            "Epoch 007 | train_loss=1.0677 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=46.8s\n",
            "Epoch 008 | train_loss=1.0674 acc=0.4311 | val_loss=1.0751 acc=0.4317 | time=46.6s\n",
            "Epoch 009 | train_loss=1.0672 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=46.8s\n",
            "Epoch 010 | train_loss=1.0672 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=46.7s\n",
            "Epoch 011 | train_loss=1.0669 acc=0.4311 | val_loss=1.0763 acc=0.4317 | time=46.7s\n",
            "Epoch 012 | train_loss=1.0667 acc=0.4311 | val_loss=1.0761 acc=0.4317 | time=46.6s\n",
            "Epoch 013 | train_loss=1.0673 acc=0.4311 | val_loss=1.0750 acc=0.4317 | time=46.7s\n",
            "Epoch 014 | train_loss=1.0666 acc=0.4311 | val_loss=1.0755 acc=0.4317 | time=46.8s\n",
            "Epoch 015 | train_loss=1.0662 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.7s\n",
            "Epoch 016 | train_loss=1.0669 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=46.7s\n",
            "Epoch 017 | train_loss=1.0676 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=46.7s\n",
            "Epoch 018 | train_loss=1.0650 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=46.6s\n",
            "Epoch 019 | train_loss=1.0619 acc=0.4311 | val_loss=1.0784 acc=0.4317 | time=46.7s\n",
            "Epoch 020 | train_loss=1.0619 acc=0.4280 | val_loss=1.0800 acc=0.4286 | time=46.6s\n",
            "Epoch 021 | train_loss=1.0562 acc=0.4342 | val_loss=1.0901 acc=0.4317 | time=46.7s\n",
            "Epoch 022 | train_loss=1.0516 acc=0.4264 | val_loss=1.0921 acc=0.3742 | time=46.7s\n",
            "Epoch 023 | train_loss=1.0509 acc=0.4342 | val_loss=1.0928 acc=0.4084 | time=46.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 21:34:58,325] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "▸ Trial 5 pruned at epoch 24\n",
            "\n",
            "===== Trial 6 =====\n",
            " lr=9.64e-04, wd=3.60e-05, blocks=1, heads=4, segs=10\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">scarlet-pond-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/sor32p9g' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/sor32p9g</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_211611-sor32p9g/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_213458-cyqxyt15</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/cyqxyt15' target=\"_blank\">expert-galaxy-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/cyqxyt15' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/cyqxyt15</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0708 acc=0.4217 | val_loss=1.0762 acc=0.4317 | time=20.5s\n",
            "Epoch 002 | train_loss=1.0692 acc=0.4241 | val_loss=1.0773 acc=0.4317 | time=20.4s\n",
            "Epoch 003 | train_loss=1.0696 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=20.5s\n",
            "Epoch 004 | train_loss=1.0670 acc=0.4311 | val_loss=1.0760 acc=0.4317 | time=20.4s\n",
            "Epoch 005 | train_loss=1.0269 acc=0.4843 | val_loss=0.9679 acc=0.5590 | time=20.4s\n",
            "Epoch 006 | train_loss=0.9568 acc=0.5814 | val_loss=0.9761 acc=0.5388 | time=20.5s\n",
            "Epoch 007 | train_loss=0.9153 acc=0.6016 | val_loss=0.9511 acc=0.5947 | time=20.4s\n",
            "Epoch 008 | train_loss=0.9045 acc=0.6047 | val_loss=0.9230 acc=0.5870 | time=20.6s\n",
            "Epoch 009 | train_loss=0.8755 acc=0.6210 | val_loss=0.9090 acc=0.5932 | time=20.6s\n",
            "Epoch 010 | train_loss=0.8425 acc=0.6315 | val_loss=0.9071 acc=0.6165 | time=20.6s\n",
            "Epoch 011 | train_loss=0.8397 acc=0.6400 | val_loss=0.9020 acc=0.6134 | time=20.6s\n",
            "Epoch 012 | train_loss=0.8089 acc=0.6571 | val_loss=0.8799 acc=0.6196 | time=20.4s\n",
            "Epoch 013 | train_loss=0.8038 acc=0.6548 | val_loss=0.9377 acc=0.6118 | time=20.6s\n",
            "Epoch 014 | train_loss=0.7643 acc=0.6715 | val_loss=0.8976 acc=0.6118 | time=20.7s\n",
            "Epoch 015 | train_loss=0.7230 acc=0.6940 | val_loss=0.9231 acc=0.5714 | time=20.5s\n",
            "Epoch 016 | train_loss=0.7133 acc=0.6917 | val_loss=0.8959 acc=0.6196 | time=20.6s\n",
            "Epoch 017 | train_loss=0.6799 acc=0.7157 | val_loss=0.8690 acc=0.6335 | time=20.7s\n",
            "Epoch 018 | train_loss=0.6535 acc=0.7250 | val_loss=0.8898 acc=0.6102 | time=20.5s\n",
            "Epoch 019 | train_loss=0.6034 acc=0.7518 | val_loss=0.9608 acc=0.6165 | time=20.7s\n",
            "Epoch 020 | train_loss=0.5835 acc=0.7623 | val_loss=0.8566 acc=0.6568 | time=20.7s\n",
            "Epoch 021 | train_loss=0.5487 acc=0.7783 | val_loss=0.8604 acc=0.6537 | time=20.6s\n",
            "Epoch 022 | train_loss=0.5154 acc=0.7981 | val_loss=0.9271 acc=0.6506 | time=20.7s\n",
            "Epoch 023 | train_loss=0.5096 acc=0.7977 | val_loss=0.9868 acc=0.6335 | time=20.4s\n",
            "Epoch 024 | train_loss=0.4736 acc=0.8159 | val_loss=0.9278 acc=0.6258 | time=20.6s\n",
            "Epoch 025 | train_loss=0.4484 acc=0.8249 | val_loss=0.9476 acc=0.6134 | time=20.9s\n",
            "Epoch 026 | train_loss=0.4057 acc=0.8466 | val_loss=0.9415 acc=0.6304 | time=20.6s\n",
            "Epoch 027 | train_loss=0.3699 acc=0.8594 | val_loss=1.0993 acc=0.6351 | time=20.6s\n",
            "Epoch 028 | train_loss=0.3517 acc=0.8656 | val_loss=0.8778 acc=0.6568 | time=20.7s\n",
            "Epoch 029 | train_loss=0.3275 acc=0.8746 | val_loss=1.0307 acc=0.6568 | time=20.7s\n",
            "Epoch 030 | train_loss=0.2909 acc=0.8874 | val_loss=1.2798 acc=0.6708 | time=20.5s\n",
            "Epoch 031 | train_loss=0.2836 acc=0.8928 | val_loss=1.1434 acc=0.6258 | time=20.7s\n",
            "Epoch 032 | train_loss=0.2354 acc=0.9196 | val_loss=1.3695 acc=0.6506 | time=20.8s\n",
            "Epoch 033 | train_loss=0.2061 acc=0.9227 | val_loss=1.3638 acc=0.6491 | time=20.8s\n",
            "Epoch 034 | train_loss=0.1847 acc=0.9371 | val_loss=1.4711 acc=0.6584 | time=20.6s\n",
            "Epoch 035 | train_loss=0.2022 acc=0.9247 | val_loss=1.4320 acc=0.6491 | time=20.7s\n",
            "Epoch 036 | train_loss=0.1713 acc=0.9417 | val_loss=1.7849 acc=0.6475 | time=20.8s\n",
            "Epoch 037 | train_loss=0.1375 acc=0.9495 | val_loss=1.7933 acc=0.6646 | time=20.6s\n",
            "Epoch 038 | train_loss=0.1395 acc=0.9530 | val_loss=1.5331 acc=0.6304 | time=20.6s\n",
            "Epoch 039 | train_loss=0.0951 acc=0.9689 | val_loss=2.4033 acc=0.6196 | time=20.8s\n",
            "Epoch 040 | train_loss=0.1123 acc=0.9643 | val_loss=2.0536 acc=0.6273 | time=20.6s\n",
            "★ Early stopping at epoch 40\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">expert-galaxy-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/cyqxyt15' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/cyqxyt15</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_213458-cyqxyt15/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 21:48:50,815] Trial 6 finished with value: 0.8566069319134667 and parameters: {'lr': 0.0009643783862149072, 'weight_decay': 3.604873795102363e-05, 'num_blocks': 1, 'num_heads': 4, 'num_segments': 10}. Best is trial 2 with value: 0.7152097299695015.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 7 =====\n",
            " lr=2.04e-04, wd=1.79e-04, blocks=3, heads=4, segs=5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_214850-wgcubhm8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wgcubhm8' target=\"_blank\">misty-armadillo-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wgcubhm8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wgcubhm8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0691 acc=0.4311 | val_loss=1.0758 acc=0.4317 | time=46.5s\n",
            "Epoch 002 | train_loss=1.0674 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=46.5s\n",
            "Epoch 003 | train_loss=1.0681 acc=0.4311 | val_loss=1.0742 acc=0.4317 | time=46.6s\n",
            "Epoch 004 | train_loss=1.0661 acc=0.4311 | val_loss=1.0774 acc=0.4317 | time=46.5s\n",
            "Epoch 005 | train_loss=1.0684 acc=0.4311 | val_loss=1.0743 acc=0.4317 | time=46.5s\n",
            "Epoch 006 | train_loss=1.0674 acc=0.4311 | val_loss=1.0748 acc=0.4317 | time=46.6s\n",
            "Epoch 007 | train_loss=1.0674 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=46.6s\n",
            "Epoch 008 | train_loss=1.0677 acc=0.4311 | val_loss=1.0749 acc=0.4317 | time=46.5s\n",
            "Epoch 009 | train_loss=1.0664 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=46.5s\n",
            "Epoch 010 | train_loss=1.0664 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=46.5s\n",
            "Epoch 011 | train_loss=1.0660 acc=0.4311 | val_loss=1.0754 acc=0.4317 | time=46.6s\n",
            "Epoch 012 | train_loss=1.0665 acc=0.4311 | val_loss=1.0756 acc=0.4317 | time=46.5s\n",
            "Epoch 013 | train_loss=1.0669 acc=0.4311 | val_loss=1.0746 acc=0.4317 | time=46.6s\n",
            "Epoch 014 | train_loss=1.0674 acc=0.4311 | val_loss=1.0739 acc=0.4317 | time=46.5s\n",
            "Epoch 015 | train_loss=1.0667 acc=0.4311 | val_loss=1.0706 acc=0.4317 | time=46.5s\n",
            "Epoch 016 | train_loss=1.0402 acc=0.4683 | val_loss=1.0246 acc=0.4876 | time=46.5s\n",
            "Epoch 017 | train_loss=0.9592 acc=0.5701 | val_loss=0.9728 acc=0.5792 | time=46.6s\n",
            "Epoch 018 | train_loss=0.9216 acc=0.5981 | val_loss=0.9487 acc=0.5481 | time=46.5s\n",
            "Epoch 019 | train_loss=0.9025 acc=0.5996 | val_loss=0.9396 acc=0.5823 | time=46.5s\n",
            "Epoch 020 | train_loss=0.8882 acc=0.6159 | val_loss=0.9334 acc=0.5994 | time=46.6s\n",
            "Epoch 021 | train_loss=0.8684 acc=0.6198 | val_loss=0.9670 acc=0.5885 | time=46.5s\n",
            "Epoch 022 | train_loss=0.8530 acc=0.6264 | val_loss=1.0220 acc=0.5761 | time=46.5s\n",
            "Epoch 023 | train_loss=0.8391 acc=0.6256 | val_loss=0.9312 acc=0.6025 | time=46.5s\n",
            "Epoch 024 | train_loss=0.8167 acc=0.6416 | val_loss=0.9435 acc=0.5373 | time=46.6s\n",
            "Epoch 025 | train_loss=0.8043 acc=0.6400 | val_loss=0.8782 acc=0.6040 | time=46.5s\n",
            "Epoch 026 | train_loss=0.7949 acc=0.6509 | val_loss=0.9444 acc=0.5870 | time=46.5s\n",
            "Epoch 027 | train_loss=0.7592 acc=0.6683 | val_loss=0.8622 acc=0.6242 | time=46.5s\n",
            "Epoch 028 | train_loss=0.7418 acc=0.6726 | val_loss=0.8766 acc=0.6134 | time=46.6s\n",
            "Epoch 029 | train_loss=0.6983 acc=0.6948 | val_loss=0.8695 acc=0.6351 | time=46.6s\n",
            "Epoch 030 | train_loss=0.6681 acc=0.7118 | val_loss=0.8216 acc=0.6646 | time=46.6s\n",
            "Epoch 031 | train_loss=0.6369 acc=0.7216 | val_loss=0.8961 acc=0.6522 | time=46.5s\n",
            "Epoch 032 | train_loss=0.6096 acc=0.7460 | val_loss=0.8351 acc=0.6708 | time=46.5s\n",
            "Epoch 033 | train_loss=0.5685 acc=0.7604 | val_loss=0.8492 acc=0.6724 | time=46.6s\n",
            "Epoch 034 | train_loss=0.5265 acc=0.7891 | val_loss=0.7677 acc=0.7050 | time=46.5s\n",
            "Epoch 035 | train_loss=0.4778 acc=0.8066 | val_loss=0.7662 acc=0.6786 | time=46.5s\n",
            "Epoch 036 | train_loss=0.4622 acc=0.8113 | val_loss=0.8502 acc=0.7019 | time=46.5s\n",
            "Epoch 037 | train_loss=0.4335 acc=0.8315 | val_loss=0.8123 acc=0.7065 | time=46.5s\n",
            "Epoch 038 | train_loss=0.3858 acc=0.8482 | val_loss=0.8138 acc=0.7019 | time=46.5s\n",
            "Epoch 039 | train_loss=0.3734 acc=0.8610 | val_loss=0.7562 acc=0.7019 | time=46.6s\n",
            "Epoch 040 | train_loss=0.3398 acc=0.8649 | val_loss=0.8493 acc=0.6910 | time=46.6s\n",
            "Epoch 041 | train_loss=0.3162 acc=0.8726 | val_loss=0.9147 acc=0.7205 | time=46.5s\n",
            "Epoch 042 | train_loss=0.2887 acc=0.8994 | val_loss=0.9134 acc=0.7034 | time=46.5s\n",
            "Epoch 043 | train_loss=0.2503 acc=0.9014 | val_loss=0.9604 acc=0.7127 | time=46.5s\n",
            "Epoch 044 | train_loss=0.2466 acc=0.9115 | val_loss=0.8319 acc=0.7143 | time=46.6s\n",
            "Epoch 045 | train_loss=0.2430 acc=0.9072 | val_loss=0.9113 acc=0.7158 | time=46.6s\n",
            "Epoch 046 | train_loss=0.2092 acc=0.9301 | val_loss=0.8949 acc=0.7236 | time=46.6s\n",
            "Epoch 047 | train_loss=0.1856 acc=0.9305 | val_loss=1.1017 acc=0.7252 | time=46.5s\n",
            "Epoch 048 | train_loss=0.1875 acc=0.9340 | val_loss=0.9059 acc=0.7050 | time=46.5s\n",
            "Epoch 049 | train_loss=0.1506 acc=0.9456 | val_loss=1.0657 acc=0.7314 | time=46.5s\n",
            "Epoch 050 | train_loss=0.1696 acc=0.9394 | val_loss=1.0194 acc=0.7283 | time=46.6s\n",
            "Epoch 051 | train_loss=0.1376 acc=0.9507 | val_loss=1.0021 acc=0.7345 | time=46.6s\n",
            "Epoch 052 | train_loss=0.1412 acc=0.9511 | val_loss=1.1262 acc=0.7112 | time=46.5s\n",
            "Epoch 053 | train_loss=0.1069 acc=0.9627 | val_loss=1.2024 acc=0.7283 | time=46.6s\n",
            "Epoch 054 | train_loss=0.1148 acc=0.9588 | val_loss=1.0934 acc=0.7127 | time=46.6s\n",
            "Epoch 055 | train_loss=0.1031 acc=0.9643 | val_loss=1.1673 acc=0.7065 | time=46.5s\n",
            "Epoch 056 | train_loss=0.0864 acc=0.9685 | val_loss=1.2901 acc=0.7019 | time=46.5s\n",
            "Epoch 057 | train_loss=0.1094 acc=0.9600 | val_loss=1.3199 acc=0.7220 | time=46.5s\n",
            "Epoch 058 | train_loss=0.1146 acc=0.9588 | val_loss=1.1444 acc=0.7236 | time=46.6s\n",
            "Epoch 059 | train_loss=0.0976 acc=0.9666 | val_loss=1.2204 acc=0.7019 | time=46.6s\n",
            "★ Early stopping at epoch 59\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">misty-armadillo-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wgcubhm8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/wgcubhm8</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_214850-wgcubhm8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 22:34:44,697] Trial 7 finished with value: 0.7561915225925899 and parameters: {'lr': 0.00020419417762750276, 'weight_decay': 0.00017949537675946488, 'num_blocks': 3, 'num_heads': 4, 'num_segments': 5}. Best is trial 2 with value: 0.7152097299695015.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "===== Trial 8 =====\n",
            " lr=2.63e-04, wd=6.35e-04, blocks=2, heads=3, segs=5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_223444-1dqbbceq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/1dqbbceq' target=\"_blank\">leafy-smoke-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/1dqbbceq' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-holdout-tuning/runs/1dqbbceq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 001 | train_loss=1.0713 acc=0.4311 | val_loss=1.0744 acc=0.4317 | time=27.8s\n",
            "Epoch 002 | train_loss=1.0657 acc=0.4311 | val_loss=1.0765 acc=0.4317 | time=28.2s\n",
            "Epoch 003 | train_loss=1.0693 acc=0.4311 | val_loss=1.0759 acc=0.4317 | time=27.8s\n",
            "Epoch 004 | train_loss=1.0677 acc=0.4311 | val_loss=1.0752 acc=0.4317 | time=27.7s\n",
            "Epoch 005 | train_loss=1.0678 acc=0.4311 | val_loss=1.0753 acc=0.4317 | time=27.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-04-30 22:37:25,936] Trial 8 failed with parameters: {'lr': 0.0002627823846413076, 'weight_decay': 0.0006345325565238551, 'num_blocks': 2, 'num_heads': 3, 'num_segments': 5} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-f3a55d92be6f>\", line 111, in objective_holdout\n",
            "    tloss     += loss.item()\n",
            "                 ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-04-30 22:37:25,938] Trial 8 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-f3a55d92be6f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     )\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective_holdout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;31m# — 최종적으로 best trial 의 값과 user_attrs 를 함께 출력 —\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f3a55d92be6f>\u001b[0m in \u001b[0;36mobjective_holdout\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0mloss\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0mtloss\u001b[0m     \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0mtcorrect\u001b[0m  \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mttotal\u001b[0m    \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Search the best Hyperparameter using 5-Fold Cross-Validation"
      ],
      "metadata": {
        "id": "shb5wwkJbihV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-4, 1e-3\n",
        "WD_MIN, WD_MAX     = 1e-5, 1e-3\n",
        "NUM_FILTERS        = 120\n",
        "NUM_BLOCK_CHOICES  = [1, 2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 3, 4]\n",
        "SEGMENT_CHOICES    = [5, 10, 15]\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "MAX_EPOCHS  = 200\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def objective_holdout(trial):\n",
        "    # 1) Sample hyperparameters\n",
        "    lr            = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay  = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_blocks    = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads     = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments  = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "    # New: sample scheduler hyperparameters\n",
        "    step_size     = trial.suggest_int(\"step_size\", 10, 100, step=10)\n",
        "    gamma         = trial.suggest_float(\"gamma\", 0.1, 0.9)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} =====\")\n",
        "    print(\n",
        "        f\" lr={lr:.2e}, wd={weight_decay:.2e}, blocks={num_blocks}, \"\n",
        "        f\"heads={num_heads}, segs={num_segments}, \"\n",
        "        f\"step_size={step_size}, gamma={gamma:.2f}\"\n",
        "    )\n",
        "\n",
        "    # 2) Load metadata and build dataset\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta  = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels      = [d[\"label\"] for d in train_meta]\n",
        "    n_samples   = len(full_ds)\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 3) Single hold-out split (80/20), stratified\n",
        "    train_idx, val_idx = train_test_split(\n",
        "        list(range(n_samples)),\n",
        "        test_size=0.2,\n",
        "        stratify=labels,\n",
        "        random_state=42\n",
        "    )\n",
        "    train_loader = DataLoader(\n",
        "        Subset(full_ds, train_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        Subset(full_ds, val_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # 4) W&B init\n",
        "    wandb.init(project=\"eeg-holdout-tuning\", config=trial.params)\n",
        "\n",
        "    # 5) Build model, optimizer, loss\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = num_heads,\n",
        "        num_blocks   = num_blocks,\n",
        "        num_segments = num_segments,\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 6) Scheduler with sampled step_size & gamma\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(\n",
        "        optimizer,\n",
        "        step_size=step_size,\n",
        "        gamma=gamma\n",
        "    )\n",
        "\n",
        "    # 7) Training loop with early stopping & pruning\n",
        "    best_val_loss     = float(\"inf\")\n",
        "    epochs_no_improve = 0\n",
        "    # store best‐epoch metrics\n",
        "    best_train_loss = best_train_acc = best_val_acc = None\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "\n",
        "        # — train —\n",
        "        model.train()\n",
        "        tloss = tcorrect = ttotal = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tloss    += loss.item()\n",
        "            tcorrect += (logits.argmax(1) == y).sum().item()\n",
        "            ttotal   += y.size(0)\n",
        "        train_loss = tloss / len(train_loader)\n",
        "        train_acc  = tcorrect / ttotal\n",
        "\n",
        "        # — validate —\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                vloss    += loss.item()\n",
        "                vcorrect += (logits.argmax(1) == y).sum().item()\n",
        "                vtotal   += y.size(0)\n",
        "        val_loss = vloss / len(val_loader)\n",
        "        val_acc  = vcorrect / vtotal\n",
        "        elapsed  = time.time() - t0\n",
        "\n",
        "        # report & prune\n",
        "        trial.report(val_loss, epoch)\n",
        "        if trial.should_prune():\n",
        "            print(f\"▸ Trial {trial.number} pruned at epoch {epoch}\")\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        # step the scheduler\n",
        "        scheduler.step()\n",
        "\n",
        "        # early stopping & record best\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss     = val_loss\n",
        "            epochs_no_improve = 0\n",
        "            best_train_loss   = train_loss\n",
        "            best_train_acc    = train_acc\n",
        "            best_val_acc      = val_acc\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= PATIENCE:\n",
        "                print(f\"★ Early stopping at epoch {epoch}\")\n",
        "                break\n",
        "\n",
        "    # store best‐epoch metrics\n",
        "    trial.set_user_attr(\"best_train_loss\", best_train_loss)\n",
        "    trial.set_user_attr(\"best_train_acc\",  best_train_acc)\n",
        "    trial.set_user_attr(\"best_val_acc\",    best_val_acc)\n",
        "\n",
        "    wandb.finish()\n",
        "    gc.collect()\n",
        "    return best_val_loss\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5),\n",
        "        study_name=\"eeg_holdout_trial\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_holdout.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective_holdout, n_trials=30)\n",
        "\n",
        "    # print best trial & metrics\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"best_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"best_val_acc    = {best.user_attrs['best_val_acc']:.4f}\")\n",
        "    print(f\"best_train_loss = {best.user_attrs['best_train_loss']:.4f}\")\n",
        "    print(f\"best_train_acc  = {best.user_attrs['best_train_acc']:.4f}\")\n",
        "    print(\"best params:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n"
      ],
      "metadata": {
        "id": "zik1qaDHtrT8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Hyperparameter ranges and fixed settings ─────────────────────────\n",
        "LR_MIN, LR_MAX     = 1e-4, 1e-3             # learning rate search bounds\n",
        "WD_MIN, WD_MAX     = 1e-5, 1e-3             # weight decay search bounds\n",
        "NUM_FILTERS        = 120                    # fixed number of convolutional filters\n",
        "NUM_BLOCK_CHOICES = [2, 3]\n",
        "NUM_HEAD_CHOICES   = [2, 3, 4]                 # choices for attention heads\n",
        "SEGMENT_CHOICES    = [5, 10, 15]                    # choices for segments (fixed here)\n",
        "\n",
        "# ─── Training configuration ─────────────────────────\n",
        "N_FOLDS     = 5                             # number of CV folds\n",
        "MAX_EPOCHS  = 100                           # maximal epochs per fold\n",
        "PATIENCE    = 20                            # early stopping patience\n",
        "BATCH_SIZE  = 32                            # batch size\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))  # data loader workers\n",
        "\n",
        "# ─── Data paths & device ─────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = \"labels.json\"\n",
        "DEVICE     = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Optuna objective function: samples hyperparams, runs K-fold CV,\n",
        "    reports validation loss for pruning, and returns average val_loss.\n",
        "    \"\"\"\n",
        "    # 1) Sample hyperparameters from defined ranges\n",
        "    lr           = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_filters  = NUM_FILTERS\n",
        "    num_blocks   = trial.suggest_categorical(\"num_blocks\", NUM_BLOCK_CHOICES)\n",
        "    num_heads    = trial.suggest_categorical(\"num_heads\", NUM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    print(f\"\\n===== Trial {trial.number} start =====\")\n",
        "    print(f\"  lr={lr:.2e}, weight_decay={weight_decay:.2e}, \"\n",
        "          f\"num_blocks={num_blocks}, num_heads={num_heads}, num_segments={num_segments}\")\n",
        "\n",
        "    # 2) Load metadata and build the full dataset to infer input_length\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds    = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels     = [d[\"label\"] for d in train_meta]\n",
        "    n_samples  = len(full_ds)\n",
        "\n",
        "    # Determine original sequence length before any convolution\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # 3) Initialize Weights & Biases for logging (one run per trial)\n",
        "    wandb.init(project=\"eeg-cv-tuning-trial-3\", config=trial.params)\n",
        "\n",
        "    # 4) Set up Stratified K-Fold cross-validation\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    # Dictionary to collect per-fold metrics\n",
        "    fold_metrics = {k: [] for k in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"]}\n",
        "\n",
        "    # Loop over each fold\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(range(n_samples), labels)):\n",
        "        print(f\"\\n--- Fold {fold} ---\")\n",
        "\n",
        "        # Create DataLoaders for this fold\n",
        "        train_loader = DataLoader(\n",
        "            Subset(full_ds, train_idx),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=True,\n",
        "            num_workers=NUM_WORKERS\n",
        "        )\n",
        "        val_loader = DataLoader(\n",
        "            Subset(full_ds, val_idx),\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        # 4a) Instantiate model, optimizer, and loss\n",
        "        model = EEGformer(\n",
        "            in_channels  = 19,\n",
        "            input_length = input_length,\n",
        "            kernel_size  = 10,\n",
        "            num_filters  = num_filters,\n",
        "            num_heads    = num_heads,\n",
        "            num_blocks   = num_blocks,\n",
        "            num_segments = num_segments,\n",
        "            num_classes  = 3\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Variables for early stopping\n",
        "        best_val_loss     = float(\"inf\")\n",
        "        epochs_no_improve = 0\n",
        "        best_epoch        = 0\n",
        "        best_train_l = best_train_a = best_val_a = None\n",
        "\n",
        "        # 4b) Training loop per epoch\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            t0 = time.time()\n",
        "\n",
        "            # — Training phase —\n",
        "            model.train()\n",
        "            train_loss_sum = train_correct = train_total = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss   = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                train_loss_sum += loss.item()\n",
        "                train_correct  += (logits.argmax(1) == y).sum().item()\n",
        "                train_total    += y.size(0)\n",
        "\n",
        "            train_loss = train_loss_sum / len(train_loader)\n",
        "            train_acc  = train_correct / train_total\n",
        "\n",
        "            # — Validation phase —\n",
        "            model.eval()\n",
        "            val_loss_sum = val_correct = val_total = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss   = criterion(logits, y)\n",
        "                    val_loss_sum += loss.item()\n",
        "                    val_correct  += (logits.argmax(1) == y).sum().item()\n",
        "                    val_total    += y.size(0)\n",
        "\n",
        "            val_loss = val_loss_sum / len(val_loader)\n",
        "            val_acc  = val_correct / val_total\n",
        "            epoch_time = time.time() - t0\n",
        "\n",
        "            # Report intermediate objective value to Optuna for pruning\n",
        "            step = fold * MAX_EPOCHS + epoch\n",
        "            trial.report(val_loss, step=step)\n",
        "            if trial.should_prune():\n",
        "                print(f\"▸ Trial pruned at fold {fold}, epoch {epoch}\")\n",
        "                raise optuna.TrialPruned()\n",
        "\n",
        "            # Log metrics and epoch time\n",
        "            print(\n",
        "                f\"[Fold {fold}] Epoch {epoch:03d} \"\n",
        "                f\"train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
        "                f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | \"\n",
        "                f\"time={epoch_time:.1f}s\"\n",
        "            )\n",
        "\n",
        "            # Early stopping check\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss     = val_loss\n",
        "                best_epoch        = epoch\n",
        "                epochs_no_improve = 0\n",
        "                best_train_l      = train_loss\n",
        "                best_train_a      = train_acc\n",
        "                best_val_a        = val_acc\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= PATIENCE:\n",
        "                    print(f\"[Fold {fold}] early stopping at epoch {epoch}\")\n",
        "                    raise optuna.TrialPruned()\n",
        "\n",
        "        # Record this fold's best metrics\n",
        "        for key, value in zip(\n",
        "            [\"train_loss\",\"train_acc\",\"val_loss\",\"val_acc\",\"best_epoch\"],\n",
        "            [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]\n",
        "        ):\n",
        "            fold_metrics[key].append(value)\n",
        "            trial.set_user_attr(f\"fold{fold}_{key}\", value)\n",
        "\n",
        "        # Free GPU memory before next fold\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # 5) Compute average metrics across folds and finish the trial\n",
        "    avg = lambda key: sum(fold_metrics[key]) / N_FOLDS\n",
        "    for key in fold_metrics:\n",
        "        trial.set_user_attr(f\"avg_{key}\", avg(key))\n",
        "\n",
        "    wandb.finish()\n",
        "    return avg(\"val_loss\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Allow safe multiprocessing on Windows\n",
        "    multiprocessing.freeze_support()\n",
        "\n",
        "    # Create or load an Optuna study\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "        study_name=\"eegformer_optuna_cv_trial_3\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eeg_optuna_trial_3.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    # Run hyperparameter optimization\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Print out best trial results\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial =====\")\n",
        "    print(f\"avg_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"avg_train_acc  = {best.user_attrs['avg_train_acc']:.4f}\")\n",
        "    print(f\"avg_val_acc    = {best.user_attrs['avg_val_acc']:.4f}\")\n",
        "    print(\"best hyperparameters:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "    # ─── Retrain on full training data using best hyperparameters ─────────────────────────\n",
        "    print(\"\\nRetraining on full TRAIN set with best hyperparams…\")\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    full_meta   = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, full_meta)\n",
        "    full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # Determine full input length\n",
        "    input_length = full_ds[0][0].shape[-1]\n",
        "\n",
        "    # Re-instantiate model with best params\n",
        "    model = EEGformer(\n",
        "        in_channels  = 19,\n",
        "        input_length = input_length,\n",
        "        kernel_size  = 10,\n",
        "        num_filters  = NUM_FILTERS,\n",
        "        num_heads    = best.params[\"num_heads\"],\n",
        "        num_blocks   = best.params[\"num_blocks\"],\n",
        "        num_segments = best.params[\"num_segments\"],\n",
        "        num_classes  = 3\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=best.params[\"lr\"],\n",
        "        weight_decay=best.params[\"weight_decay\"]\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Final full-dataset training loop\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        t0 = time.time()\n",
        "        model.train()\n",
        "        loss_sum = correct = total = 0\n",
        "        for X, y in full_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += y.size(0)\n",
        "\n",
        "        avg_loss = loss_sum / len(full_loader)\n",
        "        acc      = correct / total\n",
        "        epoch_time = time.time() - t0\n",
        "        print(f\"[Full Train] Epoch {epoch:03d} | loss={avg_loss:.4f} | acc={acc:.4f} | time={epoch_time:.1f}s\")\n",
        "\n",
        "    # Save the final checkpoint\n",
        "    ckpt_dir = '/content/drive/MyDrive/2025_Lab_Research/checkpoints'\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(ckpt_dir, 'eegformer_best_3.pth')\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"💾 Saved best model to {ckpt_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5BkAh1_RhPhq",
        "outputId": "62078fa6-9519-4dff-cd62-d0df676f0301"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-30 09:49:03,595] A new study created in RDB with name: eegformer_optuna_cv_trial_3\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "===== Trial 0 start =====\n",
            "  lr=3.15e-05, weight_decay=1.45e-06, num_blocks=3, num_heads=3, num_segments=25\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">prime-lake-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-2/runs/p1dnkwzc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-2/runs/p1dnkwzc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-2</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_093704-p1dnkwzc/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_094903-mo7cwl89</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/mo7cwl89' target=\"_blank\">good-valley-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/mo7cwl89' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/mo7cwl89</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0734 train_acc=0.4311 | val_loss=1.0814 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 002 train_loss=1.0672 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 003 train_loss=1.0674 train_acc=0.4311 | val_loss=1.0846 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 004 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0832 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 005 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 006 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 007 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0855 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 008 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 009 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 010 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 011 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0824 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 012 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0841 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 013 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 014 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 015 train_loss=1.0660 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 016 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 017 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 018 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 019 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0851 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 020 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.7s\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-30 10:02:44,729] Trial 0 pruned. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Fold 0] Epoch 021 train_loss=1.0671 train_acc=0.4311 | val_loss=1.0846 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] early stopping at epoch 21\n",
            "\n",
            "===== Trial 1 start =====\n",
            "  lr=1.40e-05, weight_decay=5.99e-05, num_blocks=3, num_heads=3, num_segments=25\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">good-valley-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/mo7cwl89' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/mo7cwl89</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_094903-mo7cwl89/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_100244-si2tcytb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/si2tcytb' target=\"_blank\">serene-salad-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/si2tcytb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/si2tcytb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0761 train_acc=0.4221 | val_loss=1.0810 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 002 train_loss=1.0676 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 003 train_loss=1.0675 train_acc=0.4311 | val_loss=1.0843 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 004 train_loss=1.0679 train_acc=0.4311 | val_loss=1.0841 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 005 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0831 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 006 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 007 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 008 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 009 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0846 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 010 train_loss=1.0664 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 011 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 012 train_loss=1.0660 train_acc=0.4311 | val_loss=1.0848 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 013 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 014 train_loss=1.0660 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 015 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 016 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0839 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 017 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0846 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 018 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 019 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 020 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=39.0s\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[I 2025-04-30 10:16:25,412] Trial 1 pruned. \n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Fold 0] Epoch 021 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] early stopping at epoch 21\n",
            "\n",
            "===== Trial 2 start =====\n",
            "  lr=8.77e-04, weight_decay=6.29e-05, num_blocks=3, num_heads=3, num_segments=5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">serene-salad-2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/si2tcytb' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/si2tcytb</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_100244-si2tcytb/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_101625-sunwszju</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/sunwszju' target=\"_blank\">ruby-rain-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/sunwszju' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/sunwszju</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0719 train_acc=0.4276 | val_loss=1.0924 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 002 train_loss=1.0687 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 003 train_loss=1.0702 train_acc=0.4217 | val_loss=1.0813 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 004 train_loss=1.0696 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 005 train_loss=1.0676 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 006 train_loss=1.0687 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 007 train_loss=1.0682 train_acc=0.4311 | val_loss=1.0850 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 008 train_loss=1.0674 train_acc=0.4311 | val_loss=1.0890 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 009 train_loss=1.0664 train_acc=0.4311 | val_loss=1.0878 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 010 train_loss=1.0664 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 011 train_loss=1.0682 train_acc=0.4311 | val_loss=1.0807 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 012 train_loss=1.0673 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 013 train_loss=1.0682 train_acc=0.4311 | val_loss=1.0805 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 014 train_loss=1.0676 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 015 train_loss=1.0519 train_acc=0.4520 | val_loss=0.9980 val_acc=0.5714 | time=38.4s\n",
            "[Fold 0] Epoch 016 train_loss=0.9764 train_acc=0.5654 | val_loss=0.9418 val_acc=0.5730 | time=38.4s\n",
            "[Fold 0] Epoch 017 train_loss=0.9459 train_acc=0.5744 | val_loss=0.9934 val_acc=0.5450 | time=38.5s\n",
            "[Fold 0] Epoch 018 train_loss=0.9348 train_acc=0.5810 | val_loss=0.9058 val_acc=0.6025 | time=38.6s\n",
            "[Fold 0] Epoch 019 train_loss=0.8923 train_acc=0.6113 | val_loss=0.8734 val_acc=0.6413 | time=38.7s\n",
            "[Fold 0] Epoch 020 train_loss=0.8553 train_acc=0.6318 | val_loss=0.8428 val_acc=0.6553 | time=38.7s\n",
            "[Fold 0] Epoch 021 train_loss=0.8423 train_acc=0.6342 | val_loss=0.8625 val_acc=0.6522 | time=38.8s\n",
            "[Fold 0] Epoch 022 train_loss=0.8291 train_acc=0.6439 | val_loss=0.8529 val_acc=0.6273 | time=38.7s\n",
            "[Fold 0] Epoch 023 train_loss=0.7993 train_acc=0.6583 | val_loss=0.8008 val_acc=0.6708 | time=39.0s\n",
            "[Fold 0] Epoch 024 train_loss=0.7896 train_acc=0.6606 | val_loss=0.7765 val_acc=0.6786 | time=39.1s\n",
            "[Fold 0] Epoch 025 train_loss=0.7751 train_acc=0.6641 | val_loss=0.7783 val_acc=0.6661 | time=39.2s\n",
            "[Fold 0] Epoch 026 train_loss=0.7323 train_acc=0.6909 | val_loss=0.7696 val_acc=0.6630 | time=38.9s\n",
            "[Fold 0] Epoch 027 train_loss=0.7110 train_acc=0.6959 | val_loss=0.7450 val_acc=0.6630 | time=39.2s\n",
            "[Fold 0] Epoch 028 train_loss=0.6808 train_acc=0.7056 | val_loss=0.7230 val_acc=0.6677 | time=39.0s\n",
            "[Fold 0] Epoch 029 train_loss=0.6673 train_acc=0.7091 | val_loss=0.8045 val_acc=0.6366 | time=38.8s\n",
            "[Fold 0] Epoch 030 train_loss=0.6379 train_acc=0.7235 | val_loss=0.7409 val_acc=0.6925 | time=38.8s\n",
            "[Fold 0] Epoch 031 train_loss=0.6088 train_acc=0.7348 | val_loss=0.7027 val_acc=0.7065 | time=39.0s\n",
            "[Fold 0] Epoch 032 train_loss=0.6259 train_acc=0.7410 | val_loss=0.7725 val_acc=0.6894 | time=38.9s\n",
            "[Fold 0] Epoch 033 train_loss=0.6968 train_acc=0.6998 | val_loss=0.6835 val_acc=0.7034 | time=39.3s\n",
            "[Fold 0] Epoch 034 train_loss=0.6193 train_acc=0.7495 | val_loss=0.6762 val_acc=0.7143 | time=39.3s\n",
            "[Fold 0] Epoch 035 train_loss=0.5681 train_acc=0.7588 | val_loss=0.6638 val_acc=0.6957 | time=39.2s\n",
            "[Fold 0] Epoch 036 train_loss=0.5618 train_acc=0.7744 | val_loss=0.6656 val_acc=0.6988 | time=39.3s\n",
            "[Fold 0] Epoch 037 train_loss=0.5556 train_acc=0.7647 | val_loss=0.7117 val_acc=0.6739 | time=39.3s\n",
            "[Fold 0] Epoch 038 train_loss=0.5385 train_acc=0.7736 | val_loss=0.7535 val_acc=0.7019 | time=39.2s\n",
            "[Fold 0] Epoch 039 train_loss=0.5166 train_acc=0.7903 | val_loss=0.7571 val_acc=0.7112 | time=39.1s\n",
            "[Fold 0] Epoch 040 train_loss=0.5305 train_acc=0.7802 | val_loss=0.6741 val_acc=0.7205 | time=39.1s\n",
            "[Fold 0] Epoch 041 train_loss=0.5091 train_acc=0.7977 | val_loss=0.6508 val_acc=0.7143 | time=39.1s\n",
            "[Fold 0] Epoch 042 train_loss=0.5283 train_acc=0.7911 | val_loss=0.8375 val_acc=0.6910 | time=39.2s\n",
            "[Fold 0] Epoch 043 train_loss=0.4835 train_acc=0.8074 | val_loss=0.7331 val_acc=0.7050 | time=39.0s\n",
            "[Fold 0] Epoch 044 train_loss=0.4471 train_acc=0.8252 | val_loss=0.7521 val_acc=0.7127 | time=39.0s\n",
            "[Fold 0] Epoch 045 train_loss=0.4571 train_acc=0.8190 | val_loss=0.7079 val_acc=0.7252 | time=39.1s\n",
            "[Fold 0] Epoch 046 train_loss=0.4237 train_acc=0.8369 | val_loss=0.7302 val_acc=0.7096 | time=39.0s\n",
            "[Fold 0] Epoch 047 train_loss=0.4234 train_acc=0.8416 | val_loss=0.6731 val_acc=0.7252 | time=39.2s\n",
            "[Fold 0] Epoch 048 train_loss=0.4288 train_acc=0.8303 | val_loss=0.7765 val_acc=0.7174 | time=39.2s\n",
            "[Fold 0] Epoch 049 train_loss=0.4063 train_acc=0.8334 | val_loss=0.8105 val_acc=0.6615 | time=39.3s\n",
            "[Fold 0] Epoch 050 train_loss=0.4070 train_acc=0.8431 | val_loss=0.6895 val_acc=0.7283 | time=39.1s\n",
            "[Fold 0] Epoch 051 train_loss=0.3578 train_acc=0.8614 | val_loss=0.6833 val_acc=0.6988 | time=39.1s\n",
            "[Fold 0] Epoch 052 train_loss=0.3458 train_acc=0.8656 | val_loss=0.9259 val_acc=0.7329 | time=39.0s\n",
            "[Fold 0] Epoch 053 train_loss=0.3671 train_acc=0.8617 | val_loss=0.7783 val_acc=0.7034 | time=39.0s\n",
            "[Fold 0] Epoch 054 train_loss=0.3338 train_acc=0.8730 | val_loss=0.8001 val_acc=0.7127 | time=39.1s\n",
            "[Fold 0] Epoch 055 train_loss=0.3153 train_acc=0.8784 | val_loss=0.8273 val_acc=0.7283 | time=39.1s\n",
            "[Fold 0] Epoch 056 train_loss=0.3222 train_acc=0.8761 | val_loss=0.7742 val_acc=0.7050 | time=39.0s\n",
            "[Fold 0] Epoch 057 train_loss=0.3631 train_acc=0.8571 | val_loss=0.7965 val_acc=0.7189 | time=39.1s\n",
            "[Fold 0] Epoch 058 train_loss=0.3113 train_acc=0.8742 | val_loss=0.8175 val_acc=0.7189 | time=39.3s\n",
            "[Fold 0] Epoch 059 train_loss=0.3001 train_acc=0.8823 | val_loss=0.8037 val_acc=0.7422 | time=39.3s\n",
            "[Fold 0] Epoch 060 train_loss=0.2965 train_acc=0.8839 | val_loss=0.7646 val_acc=0.7314 | time=39.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 10:55:59,622] Trial 2 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 061 train_loss=0.2811 train_acc=0.8936 | val_loss=0.9505 val_acc=0.7438 | time=39.2s\n",
            "[Fold 0] early stopping at epoch 61\n",
            "\n",
            "===== Trial 3 start =====\n",
            "  lr=1.01e-04, weight_decay=3.52e-05, num_blocks=3, num_heads=3, num_segments=5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ruby-rain-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/sunwszju' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/sunwszju</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_101625-sunwszju/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_105559-gpvkbg06</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/gpvkbg06' target=\"_blank\">sunny-river-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/gpvkbg06' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/gpvkbg06</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0695 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 002 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 003 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0843 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 004 train_loss=1.0671 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 005 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 006 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0821 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 007 train_loss=1.0672 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 008 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 009 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 010 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=38.0s\n",
            "[Fold 0] Epoch 011 train_loss=1.0675 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 012 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 013 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.0s\n",
            "[Fold 0] Epoch 014 train_loss=1.0656 train_acc=0.4311 | val_loss=1.0852 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 015 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0839 val_acc=0.4317 | time=38.0s\n",
            "[Fold 0] Epoch 016 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=37.9s\n",
            "[Fold 0] Epoch 017 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.0s\n",
            "[Fold 0] Epoch 018 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 019 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 020 train_loss=1.0672 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 021 train_loss=1.0660 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 022 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 023 train_loss=1.0649 train_acc=0.4311 | val_loss=1.0790 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 024 train_loss=1.0558 train_acc=0.4470 | val_loss=1.0697 val_acc=0.4829 | time=38.4s\n",
            "[Fold 0] Epoch 025 train_loss=0.9883 train_acc=0.5569 | val_loss=0.9817 val_acc=0.5652 | time=38.2s\n",
            "[Fold 0] Epoch 026 train_loss=0.9455 train_acc=0.5775 | val_loss=0.9728 val_acc=0.5823 | time=38.3s\n",
            "[Fold 0] Epoch 027 train_loss=0.9346 train_acc=0.5833 | val_loss=0.9620 val_acc=0.5668 | time=38.1s\n",
            "[Fold 0] Epoch 028 train_loss=0.9191 train_acc=0.5969 | val_loss=0.9757 val_acc=0.5745 | time=38.0s\n",
            "[Fold 0] Epoch 029 train_loss=0.9038 train_acc=0.6000 | val_loss=0.9578 val_acc=0.5745 | time=37.9s\n",
            "[Fold 0] Epoch 030 train_loss=0.8850 train_acc=0.6136 | val_loss=0.9837 val_acc=0.5823 | time=38.0s\n",
            "[Fold 0] Epoch 031 train_loss=0.8721 train_acc=0.6190 | val_loss=0.9425 val_acc=0.6087 | time=38.0s\n",
            "[Fold 0] Epoch 032 train_loss=0.8412 train_acc=0.6311 | val_loss=0.9426 val_acc=0.6087 | time=38.0s\n",
            "[Fold 0] Epoch 033 train_loss=0.8415 train_acc=0.6334 | val_loss=0.9854 val_acc=0.5621 | time=38.1s\n",
            "[Fold 0] Epoch 034 train_loss=0.8118 train_acc=0.6427 | val_loss=0.9495 val_acc=0.5916 | time=38.0s\n",
            "[Fold 0] Epoch 035 train_loss=0.7851 train_acc=0.6586 | val_loss=1.0002 val_acc=0.5621 | time=38.1s\n",
            "[Fold 0] Epoch 036 train_loss=0.8042 train_acc=0.6482 | val_loss=0.9526 val_acc=0.5668 | time=38.0s\n",
            "[Fold 0] Epoch 037 train_loss=0.7689 train_acc=0.6699 | val_loss=0.9771 val_acc=0.6071 | time=38.2s\n",
            "[Fold 0] Epoch 038 train_loss=0.7325 train_acc=0.6831 | val_loss=1.0029 val_acc=0.5823 | time=38.2s\n",
            "[Fold 0] Epoch 039 train_loss=0.7176 train_acc=0.6986 | val_loss=0.9739 val_acc=0.5543 | time=38.2s\n",
            "[Fold 0] Epoch 040 train_loss=0.7003 train_acc=0.7157 | val_loss=1.0216 val_acc=0.5745 | time=38.1s\n",
            "[Fold 0] Epoch 041 train_loss=0.6682 train_acc=0.7289 | val_loss=1.0064 val_acc=0.5559 | time=38.2s\n",
            "[Fold 0] Epoch 042 train_loss=0.6441 train_acc=0.7332 | val_loss=1.0363 val_acc=0.5963 | time=38.2s\n",
            "[Fold 0] Epoch 043 train_loss=0.6413 train_acc=0.7367 | val_loss=1.0885 val_acc=0.5699 | time=38.3s\n",
            "[Fold 0] Epoch 044 train_loss=0.6357 train_acc=0.7421 | val_loss=1.0329 val_acc=0.5637 | time=38.3s\n",
            "[Fold 0] Epoch 045 train_loss=0.5923 train_acc=0.7647 | val_loss=1.0570 val_acc=0.5575 | time=38.2s\n",
            "[Fold 0] Epoch 046 train_loss=0.5732 train_acc=0.7720 | val_loss=1.0356 val_acc=0.5994 | time=38.1s\n",
            "[Fold 0] Epoch 047 train_loss=0.5560 train_acc=0.7794 | val_loss=1.1420 val_acc=0.5714 | time=38.1s\n",
            "[Fold 0] Epoch 048 train_loss=0.5496 train_acc=0.7802 | val_loss=1.1098 val_acc=0.5528 | time=38.2s\n",
            "[Fold 0] Epoch 049 train_loss=0.4822 train_acc=0.8000 | val_loss=1.2214 val_acc=0.5839 | time=38.0s\n",
            "[Fold 0] Epoch 050 train_loss=0.4571 train_acc=0.8136 | val_loss=1.2245 val_acc=0.5745 | time=38.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 11:28:33,788] Trial 3 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 051 train_loss=0.4530 train_acc=0.8315 | val_loss=1.1364 val_acc=0.5683 | time=38.2s\n",
            "[Fold 0] early stopping at epoch 51\n",
            "\n",
            "===== Trial 4 start =====\n",
            "  lr=8.06e-05, weight_decay=3.80e-05, num_blocks=3, num_heads=3, num_segments=25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sunny-river-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/gpvkbg06' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/gpvkbg06</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_105559-gpvkbg06/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_112833-9z1ddfbz</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/9z1ddfbz' target=\"_blank\">rich-moon-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/9z1ddfbz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/9z1ddfbz</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0725 train_acc=0.4318 | val_loss=1.0820 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 002 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 003 train_loss=1.0677 train_acc=0.4311 | val_loss=1.0885 val_acc=0.4317 | time=38.6s\n",
            "[Fold 0] Epoch 004 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0848 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 005 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 006 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 007 train_loss=1.0674 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 008 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0832 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 009 train_loss=1.0676 train_acc=0.4311 | val_loss=1.0815 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 010 train_loss=1.0676 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 011 train_loss=1.0673 train_acc=0.4311 | val_loss=1.0816 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 012 train_loss=1.0677 train_acc=0.4311 | val_loss=1.0863 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 013 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0859 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 014 train_loss=1.0673 train_acc=0.4311 | val_loss=1.0872 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 015 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0846 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 016 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 017 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0871 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 018 train_loss=1.0642 train_acc=0.4307 | val_loss=1.0850 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 019 train_loss=1.0594 train_acc=0.4334 | val_loss=1.0923 val_acc=0.4208 | time=38.9s\n",
            "[Fold 0] Epoch 020 train_loss=1.0569 train_acc=0.4322 | val_loss=1.0972 val_acc=0.4224 | time=38.9s\n",
            "[Fold 0] Epoch 021 train_loss=1.0510 train_acc=0.4404 | val_loss=1.0986 val_acc=0.4146 | time=38.8s\n",
            "[Fold 0] Epoch 022 train_loss=1.0461 train_acc=0.4544 | val_loss=1.1033 val_acc=0.4115 | time=38.7s\n",
            "[Fold 0] Epoch 023 train_loss=1.0387 train_acc=0.4563 | val_loss=1.1202 val_acc=0.4022 | time=38.9s\n",
            "[Fold 0] Epoch 024 train_loss=1.0281 train_acc=0.4598 | val_loss=1.1241 val_acc=0.3991 | time=38.7s\n",
            "[Fold 0] Epoch 025 train_loss=1.0201 train_acc=0.4730 | val_loss=1.1350 val_acc=0.3727 | time=38.7s\n",
            "[Fold 0] Epoch 026 train_loss=1.0104 train_acc=0.4850 | val_loss=1.1379 val_acc=0.3789 | time=38.6s\n",
            "[Fold 0] Epoch 027 train_loss=0.9965 train_acc=0.4835 | val_loss=1.1682 val_acc=0.3634 | time=38.8s\n",
            "[Fold 0] Epoch 028 train_loss=0.9799 train_acc=0.5122 | val_loss=1.1670 val_acc=0.3804 | time=39.0s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 11:47:27,443] Trial 4 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 029 train_loss=0.9629 train_acc=0.5196 | val_loss=1.1921 val_acc=0.3773 | time=38.9s\n",
            "[Fold 0] early stopping at epoch 29\n",
            "\n",
            "===== Trial 5 start =====\n",
            "  lr=1.57e-04, weight_decay=4.96e-05, num_blocks=3, num_heads=3, num_segments=5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-moon-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/9z1ddfbz' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/9z1ddfbz</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_112833-9z1ddfbz/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_114727-vxzf4v99</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/vxzf4v99' target=\"_blank\">stellar-fog-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/vxzf4v99' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/vxzf4v99</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0707 train_acc=0.4303 | val_loss=1.0817 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 002 train_loss=1.0683 train_acc=0.4311 | val_loss=1.0859 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 003 train_loss=1.0677 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 004 train_loss=1.0682 train_acc=0.4311 | val_loss=1.0855 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 005 train_loss=1.0678 train_acc=0.4311 | val_loss=1.0862 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 006 train_loss=1.0673 train_acc=0.4311 | val_loss=1.0831 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 007 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 008 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 009 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 010 train_loss=1.0674 train_acc=0.4311 | val_loss=1.0850 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 011 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 012 train_loss=1.0679 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 013 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0847 val_acc=0.4317 | time=38.0s\n",
            "[Fold 0] Epoch 014 train_loss=1.0323 train_acc=0.5025 | val_loss=1.0751 val_acc=0.4565 | time=38.1s\n",
            "[Fold 0] Epoch 015 train_loss=1.0056 train_acc=0.5305 | val_loss=1.0479 val_acc=0.4829 | time=38.0s\n",
            "[Fold 0] Epoch 016 train_loss=0.9825 train_acc=0.5577 | val_loss=0.9738 val_acc=0.5885 | time=38.0s\n",
            "[Fold 0] Epoch 017 train_loss=0.9526 train_acc=0.5732 | val_loss=0.9895 val_acc=0.5559 | time=38.0s\n",
            "[Fold 0] Epoch 018 train_loss=0.9482 train_acc=0.5736 | val_loss=0.9317 val_acc=0.5947 | time=38.0s\n",
            "[Fold 0] Epoch 019 train_loss=0.9275 train_acc=0.5849 | val_loss=0.9420 val_acc=0.5870 | time=38.0s\n",
            "[Fold 0] Epoch 020 train_loss=0.9105 train_acc=0.6004 | val_loss=0.9130 val_acc=0.6009 | time=38.0s\n",
            "[Fold 0] Epoch 021 train_loss=0.9250 train_acc=0.5915 | val_loss=0.9119 val_acc=0.6118 | time=38.0s\n",
            "[Fold 0] Epoch 022 train_loss=0.9180 train_acc=0.5950 | val_loss=0.9078 val_acc=0.6211 | time=38.1s\n",
            "[Fold 0] Epoch 023 train_loss=0.8894 train_acc=0.6113 | val_loss=0.8761 val_acc=0.6273 | time=38.1s\n",
            "[Fold 0] Epoch 024 train_loss=0.8638 train_acc=0.6225 | val_loss=0.8774 val_acc=0.6304 | time=38.1s\n",
            "[Fold 0] Epoch 025 train_loss=0.8402 train_acc=0.6318 | val_loss=0.8563 val_acc=0.6413 | time=38.2s\n",
            "[Fold 0] Epoch 026 train_loss=0.8322 train_acc=0.6396 | val_loss=0.8516 val_acc=0.6398 | time=38.2s\n",
            "[Fold 0] Epoch 027 train_loss=0.8261 train_acc=0.6443 | val_loss=0.8630 val_acc=0.6382 | time=38.2s\n",
            "[Fold 0] Epoch 028 train_loss=0.8149 train_acc=0.6505 | val_loss=0.8592 val_acc=0.6320 | time=38.1s\n",
            "[Fold 0] Epoch 029 train_loss=0.7631 train_acc=0.6680 | val_loss=0.8820 val_acc=0.6196 | time=38.2s\n",
            "[Fold 0] Epoch 030 train_loss=0.7384 train_acc=0.6839 | val_loss=0.7669 val_acc=0.6770 | time=38.2s\n",
            "[Fold 0] Epoch 031 train_loss=0.7054 train_acc=0.7064 | val_loss=0.7907 val_acc=0.6848 | time=38.0s\n",
            "[Fold 0] Epoch 032 train_loss=0.6790 train_acc=0.7161 | val_loss=0.7582 val_acc=0.7205 | time=38.2s\n",
            "[Fold 0] Epoch 033 train_loss=0.6222 train_acc=0.7445 | val_loss=0.7206 val_acc=0.7158 | time=38.0s\n",
            "[Fold 0] Epoch 034 train_loss=0.5953 train_acc=0.7534 | val_loss=0.7258 val_acc=0.7158 | time=38.2s\n",
            "[Fold 0] Epoch 035 train_loss=0.5735 train_acc=0.7670 | val_loss=0.6906 val_acc=0.7329 | time=38.1s\n",
            "[Fold 0] Epoch 036 train_loss=0.5213 train_acc=0.7868 | val_loss=0.7031 val_acc=0.7174 | time=38.0s\n",
            "[Fold 0] Epoch 037 train_loss=0.4983 train_acc=0.7942 | val_loss=0.6977 val_acc=0.6957 | time=38.0s\n",
            "[Fold 0] Epoch 038 train_loss=0.4779 train_acc=0.8066 | val_loss=0.7713 val_acc=0.7407 | time=38.2s\n",
            "[Fold 0] Epoch 039 train_loss=0.4405 train_acc=0.8167 | val_loss=0.7130 val_acc=0.7267 | time=38.0s\n",
            "[Fold 0] Epoch 040 train_loss=0.3920 train_acc=0.8369 | val_loss=0.7626 val_acc=0.7453 | time=38.2s\n",
            "[Fold 0] Epoch 041 train_loss=0.4011 train_acc=0.8353 | val_loss=0.8034 val_acc=0.7205 | time=38.1s\n",
            "[Fold 0] Epoch 042 train_loss=0.3935 train_acc=0.8435 | val_loss=0.6370 val_acc=0.7609 | time=38.2s\n",
            "[Fold 0] Epoch 043 train_loss=0.3484 train_acc=0.8602 | val_loss=0.7389 val_acc=0.7096 | time=38.2s\n",
            "[Fold 0] Epoch 044 train_loss=0.3130 train_acc=0.8707 | val_loss=0.7327 val_acc=0.7609 | time=38.3s\n",
            "[Fold 0] Epoch 045 train_loss=0.3398 train_acc=0.8699 | val_loss=0.6461 val_acc=0.7422 | time=38.3s\n",
            "[Fold 0] Epoch 046 train_loss=0.3156 train_acc=0.8742 | val_loss=0.7601 val_acc=0.7438 | time=38.3s\n",
            "[Fold 0] Epoch 047 train_loss=0.2737 train_acc=0.8944 | val_loss=0.8985 val_acc=0.7267 | time=38.3s\n",
            "[Fold 0] Epoch 048 train_loss=0.2247 train_acc=0.9037 | val_loss=0.8255 val_acc=0.7205 | time=38.3s\n",
            "[Fold 0] Epoch 049 train_loss=0.2785 train_acc=0.8858 | val_loss=0.8549 val_acc=0.7360 | time=38.3s\n",
            "[Fold 0] Epoch 050 train_loss=0.2105 train_acc=0.9184 | val_loss=0.9269 val_acc=0.7407 | time=38.1s\n",
            "[Fold 0] Epoch 051 train_loss=0.1999 train_acc=0.9247 | val_loss=0.9562 val_acc=0.7562 | time=38.0s\n",
            "[Fold 0] Epoch 052 train_loss=0.1865 train_acc=0.9282 | val_loss=0.9498 val_acc=0.7174 | time=38.0s\n",
            "[Fold 0] Epoch 053 train_loss=0.1837 train_acc=0.9282 | val_loss=1.0828 val_acc=0.7034 | time=38.1s\n",
            "[Fold 0] Epoch 054 train_loss=0.1694 train_acc=0.9348 | val_loss=1.0120 val_acc=0.6817 | time=38.1s\n",
            "[Fold 0] Epoch 055 train_loss=0.1603 train_acc=0.9429 | val_loss=0.9086 val_acc=0.7640 | time=38.1s\n",
            "[Fold 0] Epoch 056 train_loss=0.1477 train_acc=0.9480 | val_loss=0.9892 val_acc=0.7609 | time=38.1s\n",
            "[Fold 0] Epoch 057 train_loss=0.1542 train_acc=0.9421 | val_loss=1.0275 val_acc=0.7143 | time=38.0s\n",
            "[Fold 0] Epoch 058 train_loss=0.1234 train_acc=0.9565 | val_loss=1.1025 val_acc=0.7220 | time=38.5s\n",
            "[Fold 0] Epoch 059 train_loss=0.1124 train_acc=0.9623 | val_loss=1.1631 val_acc=0.7531 | time=38.6s\n",
            "[Fold 0] Epoch 060 train_loss=0.1270 train_acc=0.9534 | val_loss=1.3041 val_acc=0.7484 | time=38.9s\n",
            "[Fold 0] Epoch 061 train_loss=0.0819 train_acc=0.9709 | val_loss=1.2070 val_acc=0.7143 | time=38.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 12:27:04,585] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 062 train_loss=0.0862 train_acc=0.9682 | val_loss=1.3088 val_acc=0.7050 | time=39.1s\n",
            "[Fold 0] early stopping at epoch 62\n",
            "\n",
            "===== Trial 6 start =====\n",
            "  lr=5.74e-06, weight_decay=7.79e-06, num_blocks=3, num_heads=3, num_segments=5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-fog-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/vxzf4v99' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/vxzf4v99</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_114727-vxzf4v99/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_122705-uiyll8b4</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/uiyll8b4' target=\"_blank\">olive-sky-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/uiyll8b4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/uiyll8b4</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0975 train_acc=0.3724 | val_loss=1.0935 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 002 train_loss=1.0897 train_acc=0.4311 | val_loss=1.0886 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 003 train_loss=1.0832 train_acc=0.4311 | val_loss=1.0854 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 004 train_loss=1.0792 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=38.5s\n",
            "[Fold 0] Epoch 005 train_loss=1.0766 train_acc=0.4311 | val_loss=1.0821 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 006 train_loss=1.0742 train_acc=0.4311 | val_loss=1.0815 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 007 train_loss=1.0725 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 008 train_loss=1.0714 train_acc=0.4311 | val_loss=1.0808 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 009 train_loss=1.0703 train_acc=0.4311 | val_loss=1.0806 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 010 train_loss=1.0686 train_acc=0.4311 | val_loss=1.0806 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 011 train_loss=1.0679 train_acc=0.4311 | val_loss=1.0807 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 012 train_loss=1.0680 train_acc=0.4311 | val_loss=1.0808 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 013 train_loss=1.0682 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 014 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0812 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 015 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0814 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 016 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0816 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 017 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0818 val_acc=0.4317 | time=38.1s\n",
            "[Fold 0] Epoch 018 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0820 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 019 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 020 train_loss=1.0657 train_acc=0.4311 | val_loss=1.0824 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 021 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 022 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 023 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.4s\n",
            "[Fold 0] Epoch 024 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 025 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.3s\n",
            "[Fold 0] Epoch 026 train_loss=1.0657 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 027 train_loss=1.0655 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] Epoch 028 train_loss=1.0660 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=38.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 12:45:43,104] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 029 train_loss=1.0657 train_acc=0.4311 | val_loss=1.0832 val_acc=0.4317 | time=38.2s\n",
            "[Fold 0] early stopping at epoch 29\n",
            "\n",
            "===== Trial 7 start =====\n",
            "  lr=1.05e-06, weight_decay=1.58e-05, num_blocks=3, num_heads=3, num_segments=25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">olive-sky-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/uiyll8b4' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/uiyll8b4</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_122705-uiyll8b4/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_124543-lg5bn1ih</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/lg5bn1ih' target=\"_blank\">lemon-voice-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/lg5bn1ih' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/lg5bn1ih</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0914 train_acc=0.4140 | val_loss=1.0914 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 002 train_loss=1.0860 train_acc=0.4311 | val_loss=1.0881 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 003 train_loss=1.0823 train_acc=0.4311 | val_loss=1.0858 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 004 train_loss=1.0789 train_acc=0.4311 | val_loss=1.0841 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 005 train_loss=1.0765 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 006 train_loss=1.0744 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 007 train_loss=1.0730 train_acc=0.4311 | val_loss=1.0817 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 008 train_loss=1.0715 train_acc=0.4311 | val_loss=1.0814 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 009 train_loss=1.0713 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 010 train_loss=1.0703 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 011 train_loss=1.0694 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 012 train_loss=1.0685 train_acc=0.4311 | val_loss=1.0809 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 013 train_loss=1.0687 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 014 train_loss=1.0681 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 015 train_loss=1.0676 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 016 train_loss=1.0672 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 017 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 018 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0814 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 019 train_loss=1.0675 train_acc=0.4311 | val_loss=1.0816 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 020 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0816 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 021 train_loss=1.0672 train_acc=0.4311 | val_loss=1.0818 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 022 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0820 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 023 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0821 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 024 train_loss=1.0664 train_acc=0.4311 | val_loss=1.0822 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 025 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 026 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 027 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 028 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 029 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 030 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 031 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 13:06:37,715] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 032 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] early stopping at epoch 32\n",
            "\n",
            "===== Trial 8 start =====\n",
            "  lr=7.76e-06, weight_decay=4.45e-05, num_blocks=3, num_heads=3, num_segments=25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lemon-voice-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/lg5bn1ih' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/lg5bn1ih</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_124543-lg5bn1ih/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_130637-t3fv3yjw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/t3fv3yjw' target=\"_blank\">dark-sky-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/t3fv3yjw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/t3fv3yjw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0812 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 002 train_loss=1.0698 train_acc=0.4311 | val_loss=1.0812 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 003 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0824 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 004 train_loss=1.0661 train_acc=0.4311 | val_loss=1.0832 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 005 train_loss=1.0664 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 006 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 007 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 008 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 009 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 010 train_loss=1.0670 train_acc=0.4311 | val_loss=1.0839 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 011 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 012 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 013 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=39.2s\n",
            "[Fold 0] Epoch 014 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=39.2s\n",
            "[Fold 0] Epoch 015 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 016 train_loss=1.0667 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 017 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 018 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 019 train_loss=1.0669 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 020 train_loss=1.0666 train_acc=0.4311 | val_loss=1.0838 val_acc=0.4317 | time=38.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 13:20:23,847] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 021 train_loss=1.0659 train_acc=0.4311 | val_loss=1.0839 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] early stopping at epoch 21\n",
            "\n",
            "===== Trial 9 start =====\n",
            "  lr=2.54e-06, weight_decay=1.58e-05, num_blocks=3, num_heads=3, num_segments=25\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dark-sky-9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/t3fv3yjw' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/t3fv3yjw</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250430_130637-t3fv3yjw/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250430_132023-clyvckei</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/clyvckei' target=\"_blank\">bright-donkey-10</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/clyvckei' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial-3/runs/clyvckei</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Fold 0 ---\n",
            "[Fold 0] Epoch 001 train_loss=1.0822 train_acc=0.4311 | val_loss=1.0845 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 002 train_loss=1.0763 train_acc=0.4311 | val_loss=1.0824 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 003 train_loss=1.0735 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 004 train_loss=1.0708 train_acc=0.4311 | val_loss=1.0809 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 005 train_loss=1.0691 train_acc=0.4311 | val_loss=1.0808 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 006 train_loss=1.0681 train_acc=0.4311 | val_loss=1.0809 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 007 train_loss=1.0680 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 008 train_loss=1.0673 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=38.7s\n",
            "[Fold 0] Epoch 009 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0815 val_acc=0.4317 | time=38.6s\n",
            "[Fold 0] Epoch 010 train_loss=1.0672 train_acc=0.4311 | val_loss=1.0817 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 011 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0819 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 012 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0821 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 013 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 014 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=39.2s\n",
            "[Fold 0] Epoch 015 train_loss=1.0658 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 016 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 017 train_loss=1.0663 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 018 train_loss=1.0657 train_acc=0.4311 | val_loss=1.0831 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 019 train_loss=1.0660 train_acc=0.4311 | val_loss=1.0832 val_acc=0.4317 | time=38.9s\n",
            "[Fold 0] Epoch 020 train_loss=1.0657 train_acc=0.4311 | val_loss=1.0832 val_acc=0.4317 | time=38.8s\n",
            "[Fold 0] Epoch 021 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] Epoch 022 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=39.1s\n",
            "[Fold 0] Epoch 023 train_loss=1.0662 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=39.3s\n",
            "[Fold 0] Epoch 024 train_loss=1.0665 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=39.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-30 13:36:46,965] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 025 train_loss=1.0668 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=39.0s\n",
            "[Fold 0] early stopping at epoch 25\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Record does not exist.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-a6f77100315a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;31m# Print out best trial results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n===== Best Trial =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"avg_val_loss   = {best.value:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36mbest_trial\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    160\u001b[0m             )\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         \u001b[0mbest_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_study_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0;31m# If the trial with the best value is infeasible, select the best trial from all feasible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_cached_storage.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy_id\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mFrozenTrial\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_best_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     def set_trial_state_values(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_rdb/storage.py\u001b[0m in \u001b[0;36mget_best_trial\u001b[0;34m(self, study_id)\u001b[0m\n\u001b[1;32m    907\u001b[0m                 \u001b[0mtrial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_max_value_trial_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                 \u001b[0mtrial_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_min_value_trial_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/storages/_rdb/models.py\u001b[0m in \u001b[0;36mfind_min_value_trial_id\u001b[0;34m(cls, study_id, objective, session)\u001b[0m\n\u001b[1;32m    234\u001b[0m         )\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNOT_FOUND_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    237\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Record does not exist."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lp2Yj3VUl2iN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oCKoxdflhPjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5rqRKvTWhPlf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ss5M6mYchPnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PkDZ8VrVhPp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Zckbs6iWhPsN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5JyFKgkfYvq6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D2Y4PY25Yvs3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3y2oEdFYvu3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Constants ─────────────────────────\n",
        "LR_MIN, LR_MAX = 2e-5, 8e-5\n",
        "WD_MIN, WD_MAX = 3e-5, 1e-4\n",
        "FILTER_MIN, FILTER_MAX, FILTER_STEP = 60, 180, 60\n",
        "HEAD_CHOICES    = [2, 4]\n",
        "SEGMENT_CHOICES = [5, 15, 25]\n",
        "\n",
        "N_FOLDS     = 5\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 30\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = \"labels.json\"\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space setup\n",
        "    lr = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_filters = trial.suggest_int(\"num_filters\", FILTER_MIN, FILTER_MAX, step=FILTER_STEP)\n",
        "    rtm_blocks = trial.suggest_int(\"rtm_blocks\", 1, 3)\n",
        "    stm_blocks = trial.suggest_int(\"stm_blocks\", 1, 3)\n",
        "    ttm_blocks = trial.suggest_int(\"ttm_blocks\", 1, 3)\n",
        "    rtm_heads = trial.suggest_categorical(\"rtm_heads\", HEAD_CHOICES)\n",
        "    stm_heads = trial.suggest_categorical(\"stm_heads\", HEAD_CHOICES)\n",
        "    ttm_heads = trial.suggest_categorical(\"ttm_heads\", HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # Pruning condition: only proceed if num_filters is divisible by heads\n",
        "    for h in (rtm_heads, stm_heads, ttm_heads):\n",
        "        if num_filters % h != 0:\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"eeg-cv-tuning-trial_9\", config=trial.params)\n",
        "\n",
        "    print(f\"\\n========================= Trial {trial.number} =========================\")\n",
        "    print(f\"Testing with hyperparameters: {trial.params}\")\n",
        "\n",
        "    # Data preparation\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    n_samples = len(full_ds)\n",
        "\n",
        "    # StratifiedKFold setup\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    fold_metrics = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"best_epoch\": []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(range(n_samples), labels)):\n",
        "        # Fold separation\n",
        "        print(f\"\\n========================= Fold {fold} =========================\")\n",
        "\n",
        "        # Data loader setup\n",
        "        train_loader = DataLoader(Subset(full_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(Subset(full_ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # Model and optimizer setup\n",
        "        model = EEGformer(\n",
        "            num_classes=3,\n",
        "            in_channels=19,\n",
        "            kernel_size=10,\n",
        "            num_filters=num_filters,\n",
        "            rtm_blocks=rtm_blocks,\n",
        "            stm_blocks=stm_blocks,\n",
        "            ttm_blocks=ttm_blocks,\n",
        "            rtm_heads=rtm_heads,\n",
        "            stm_heads=stm_heads,\n",
        "            ttm_heads=ttm_heads,\n",
        "            num_segments=num_segments\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        epochs_no_improve = 0\n",
        "        best_epoch = 0\n",
        "        best_train_l = best_train_a = best_val_a = None\n",
        "        last_log_time = time.time()\n",
        "\n",
        "        # Epoch-wise training\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            model.train()\n",
        "            tl_sum = t_corr = t_tot = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tl_sum += loss.item()\n",
        "                t_corr += (logits.argmax(1) == y).sum().item()\n",
        "                t_tot += y.size(0)\n",
        "\n",
        "            train_loss = tl_sum / len(train_loader)\n",
        "            train_acc = t_corr / t_tot\n",
        "\n",
        "            model.eval()\n",
        "            vl_sum = v_corr = v_tot = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss = criterion(logits, y)\n",
        "                    vl_sum += loss.item()\n",
        "                    v_corr += (logits.argmax(1) == y).sum().item()\n",
        "                    v_tot += y.size(0)\n",
        "\n",
        "            val_loss = vl_sum / len(val_loader)\n",
        "            val_acc = v_corr / v_tot\n",
        "\n",
        "            step = fold * MAX_EPOCHS + epoch\n",
        "            trial.report(val_loss, step=step)\n",
        "\n",
        "            # Pruning check\n",
        "            if trial.should_prune():\n",
        "                print(f\"\\u274c Trial {trial.number} pruned at fold {fold}, epoch {epoch}\")\n",
        "                # Report the metrics before returning early\n",
        "                for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                 [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                    fold_metrics[k].append(v)\n",
        "                    trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                raise optuna.TrialPruned()  # End trial completely if pruned\n",
        "\n",
        "            now = time.time()\n",
        "            print(f\"[Fold {fold}] Epoch {epoch:03d} | train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
        "                  f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={now - last_log_time:.1f}s\")\n",
        "            last_log_time = now\n",
        "\n",
        "            # Early stopping: if validation loss does not improve\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_epoch = epoch\n",
        "                epochs_no_improve = 0\n",
        "                best_train_l = train_loss\n",
        "                best_train_a = train_acc\n",
        "                best_val_a = val_acc\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= PATIENCE:\n",
        "                    print(f\"[Fold {fold}] Early stopping at epoch {epoch}, best was {best_epoch}\")\n",
        "                    # Report the metrics before returning early\n",
        "                    for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                     [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                        fold_metrics[k].append(v)\n",
        "                        trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                    return best_val_loss  # End trial completely if early stopping\n",
        "\n",
        "        # Record results for the fold\n",
        "        for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                         [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "            fold_metrics[k].append(v)\n",
        "            trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        torch.mps.empty_cache() if DEVICE.type == \"mps\" else torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg = lambda k: sum(fold_metrics[k]) / N_FOLDS\n",
        "    for key in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"]:\n",
        "        trial.set_user_attr(f\"avg_{key}\", avg(key))\n",
        "\n",
        "    wandb.finish()\n",
        "    return avg(\"val_loss\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "        study_name=\"eegformer_optuna_cv_3\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eegformer_optuna_cv_3.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective, n_trials=8)\n",
        "\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial Results =====\")\n",
        "    print(f\"avg_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"avg_train_loss = {best.user_attrs['avg_train_loss']:.6f}\")\n",
        "    print(f\"avg_train_acc  = {best.user_attrs['avg_train_acc']:.4f}\")\n",
        "    print(f\"avg_val_acc    = {best.user_attrs['avg_val_acc']:.4f}\")\n",
        "    print(f\"avg_best_epoch = {best.user_attrs['avg_best_epoch']:.1f}\")\n",
        "    print(\"best hyperparameters:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(\"per-fold best metrics:\")\n",
        "    for f in range(N_FOLDS):\n",
        "        print(\n",
        "            f\"  Fold {f}: epoch={best.user_attrs[f'fold{f}_best_epoch']}, \"\n",
        "            f\"t_loss={best.user_attrs[f'fold{f}_train_loss']:.4f}, \"\n",
        "            f\"t_acc={best.user_attrs[f'fold{f}_train_acc']:.4f}, \"\n",
        "            f\"v_loss={best.user_attrs[f'fold{f}_val_loss']:.4f}, \"\n",
        "            f\"v_acc={best.user_attrs[f'fold{f}_val_acc']:.4f}\"\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Pa2PxOUVEPC5",
        "outputId": "956e5b1f-b50f-40c9-be1a-050bd8b635b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 01:54:11,503] A new study created in RDB with name: eegformer_optuna_cv_3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_015411-bmz541r9</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/bmz541r9' target=\"_blank\">vital-deluge-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/bmz541r9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/bmz541r9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 0 =========================\n",
            "Testing with hyperparameters: {'lr': 6.273724067568952e-05, 'weight_decay': 5.8339962733943407e-05, 'num_filters': 120, 'rtm_blocks': 1, 'stm_blocks': 3, 'ttm_blocks': 1, 'rtm_heads': 4, 'stm_heads': 2, 'ttm_heads': 2, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0687 train_acc=0.4318 | val_loss=1.0820 val_acc=0.4317 | time=1308.5s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0534 train_acc=0.4590 | val_loss=1.0284 val_acc=0.5435 | time=61.5s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0157 train_acc=0.5153 | val_loss=1.0012 val_acc=0.5481 | time=64.1s\n",
            "[Fold 0] Epoch 004 | train_loss=0.9863 train_acc=0.5480 | val_loss=0.9923 val_acc=0.5528 | time=62.7s\n",
            "[Fold 0] Epoch 005 | train_loss=0.9767 train_acc=0.5515 | val_loss=0.9871 val_acc=0.5652 | time=60.3s\n",
            "[Fold 0] Epoch 006 | train_loss=0.9585 train_acc=0.5666 | val_loss=0.9754 val_acc=0.5652 | time=60.8s\n",
            "[Fold 0] Epoch 007 | train_loss=0.9426 train_acc=0.5689 | val_loss=0.9539 val_acc=0.5668 | time=61.8s\n",
            "[Fold 0] Epoch 008 | train_loss=0.9240 train_acc=0.5887 | val_loss=0.9100 val_acc=0.6118 | time=61.9s\n",
            "[Fold 0] Epoch 009 | train_loss=0.9148 train_acc=0.5942 | val_loss=0.8942 val_acc=0.6165 | time=61.9s\n",
            "[Fold 0] Epoch 010 | train_loss=0.9040 train_acc=0.6066 | val_loss=0.8891 val_acc=0.6273 | time=60.6s\n",
            "[Fold 0] Epoch 011 | train_loss=0.8838 train_acc=0.6186 | val_loss=0.8948 val_acc=0.6304 | time=61.1s\n",
            "[Fold 0] Epoch 012 | train_loss=0.8668 train_acc=0.6221 | val_loss=0.8626 val_acc=0.6413 | time=61.9s\n",
            "[Fold 0] Epoch 013 | train_loss=0.8542 train_acc=0.6307 | val_loss=0.8835 val_acc=0.6320 | time=61.3s\n",
            "[Fold 0] Epoch 014 | train_loss=0.8390 train_acc=0.6357 | val_loss=0.8207 val_acc=0.6724 | time=61.0s\n",
            "[Fold 0] Epoch 015 | train_loss=0.8298 train_acc=0.6416 | val_loss=0.8336 val_acc=0.6584 | time=62.2s\n",
            "[Fold 0] Epoch 016 | train_loss=0.8076 train_acc=0.6509 | val_loss=0.8532 val_acc=0.6568 | time=60.8s\n",
            "[Fold 0] Epoch 017 | train_loss=0.7793 train_acc=0.6590 | val_loss=0.7925 val_acc=0.6972 | time=59.9s\n",
            "[Fold 0] Epoch 018 | train_loss=0.7643 train_acc=0.6656 | val_loss=0.7707 val_acc=0.7003 | time=60.3s\n",
            "[Fold 0] Epoch 019 | train_loss=0.7317 train_acc=0.6827 | val_loss=0.7630 val_acc=0.6863 | time=60.3s\n",
            "[Fold 0] Epoch 020 | train_loss=0.7150 train_acc=0.6986 | val_loss=0.7484 val_acc=0.7019 | time=61.5s\n",
            "[Fold 0] Epoch 021 | train_loss=0.6691 train_acc=0.7200 | val_loss=0.9672 val_acc=0.5916 | time=60.8s\n",
            "[Fold 0] Epoch 022 | train_loss=0.7004 train_acc=0.7060 | val_loss=0.6889 val_acc=0.7360 | time=60.4s\n",
            "[Fold 0] Epoch 023 | train_loss=0.6614 train_acc=0.7247 | val_loss=0.7166 val_acc=0.7314 | time=60.9s\n",
            "[Fold 0] Epoch 024 | train_loss=0.6470 train_acc=0.7359 | val_loss=0.6494 val_acc=0.7578 | time=61.5s\n",
            "[Fold 0] Epoch 025 | train_loss=0.6075 train_acc=0.7464 | val_loss=0.6469 val_acc=0.7469 | time=62.1s\n",
            "[Fold 0] Epoch 026 | train_loss=0.6042 train_acc=0.7557 | val_loss=0.6866 val_acc=0.7516 | time=60.8s\n",
            "[Fold 0] Epoch 027 | train_loss=0.5617 train_acc=0.7682 | val_loss=0.6825 val_acc=0.6786 | time=61.0s\n",
            "[Fold 0] Epoch 028 | train_loss=0.5392 train_acc=0.7829 | val_loss=0.6448 val_acc=0.7624 | time=60.8s\n",
            "[Fold 0] Epoch 029 | train_loss=0.5136 train_acc=0.7911 | val_loss=0.6208 val_acc=0.7966 | time=62.2s\n",
            "[Fold 0] Epoch 030 | train_loss=0.5138 train_acc=0.7895 | val_loss=0.5855 val_acc=0.7904 | time=60.4s\n",
            "[Fold 0] Epoch 031 | train_loss=0.5007 train_acc=0.7988 | val_loss=0.6720 val_acc=0.7360 | time=60.9s\n",
            "[Fold 0] Epoch 032 | train_loss=0.4945 train_acc=0.8004 | val_loss=0.5844 val_acc=0.7981 | time=61.2s\n",
            "[Fold 0] Epoch 033 | train_loss=0.4423 train_acc=0.8291 | val_loss=0.5692 val_acc=0.7842 | time=61.5s\n",
            "[Fold 0] Epoch 034 | train_loss=0.4383 train_acc=0.8272 | val_loss=0.5439 val_acc=0.7717 | time=60.9s\n",
            "[Fold 0] Epoch 035 | train_loss=0.4491 train_acc=0.8264 | val_loss=0.5505 val_acc=0.8106 | time=61.3s\n",
            "[Fold 0] Epoch 036 | train_loss=0.4106 train_acc=0.8408 | val_loss=0.5127 val_acc=0.8059 | time=61.1s\n",
            "[Fold 0] Epoch 037 | train_loss=0.3927 train_acc=0.8435 | val_loss=0.5238 val_acc=0.7935 | time=62.1s\n",
            "[Fold 0] Epoch 038 | train_loss=0.3665 train_acc=0.8621 | val_loss=0.6537 val_acc=0.7640 | time=61.0s\n",
            "[Fold 0] Epoch 039 | train_loss=0.3750 train_acc=0.8586 | val_loss=0.5482 val_acc=0.7811 | time=60.9s\n",
            "[Fold 0] Epoch 040 | train_loss=0.3732 train_acc=0.8563 | val_loss=0.5636 val_acc=0.7981 | time=60.8s\n",
            "[Fold 0] Epoch 041 | train_loss=0.3508 train_acc=0.8672 | val_loss=0.5784 val_acc=0.8168 | time=62.1s\n",
            "[Fold 0] Epoch 042 | train_loss=0.3108 train_acc=0.8831 | val_loss=0.5698 val_acc=0.8012 | time=61.3s\n",
            "[Fold 0] Epoch 043 | train_loss=0.2921 train_acc=0.8928 | val_loss=0.5387 val_acc=0.8137 | time=61.3s\n",
            "[Fold 0] Epoch 044 | train_loss=0.2989 train_acc=0.8870 | val_loss=0.6251 val_acc=0.7966 | time=61.3s\n",
            "[Fold 0] Epoch 045 | train_loss=0.3106 train_acc=0.8823 | val_loss=0.5888 val_acc=0.8059 | time=62.5s\n",
            "[Fold 0] Epoch 046 | train_loss=0.2707 train_acc=0.8975 | val_loss=0.5166 val_acc=0.8106 | time=61.2s\n",
            "[Fold 0] Epoch 047 | train_loss=0.2423 train_acc=0.9138 | val_loss=0.5472 val_acc=0.8075 | time=61.4s\n",
            "[Fold 0] Epoch 048 | train_loss=0.2640 train_acc=0.9052 | val_loss=0.6522 val_acc=0.8090 | time=61.6s\n",
            "[Fold 0] Epoch 049 | train_loss=0.2277 train_acc=0.9169 | val_loss=0.5179 val_acc=0.8261 | time=63.2s\n",
            "[Fold 0] Epoch 050 | train_loss=0.2056 train_acc=0.9285 | val_loss=0.5140 val_acc=0.8152 | time=61.5s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-04-28 03:07:14,457] Trial 0 failed with parameters: {'lr': 6.273724067568952e-05, 'weight_decay': 5.8339962733943407e-05, 'num_filters': 120, 'rtm_blocks': 1, 'stm_blocks': 3, 'ttm_blocks': 1, 'rtm_heads': 4, 'stm_heads': 2, 'ttm_heads': 2, 'num_segments': 5} because of the following error: The value None could not be cast to float..\n",
            "[W 2025-04-28 03:07:14,458] Trial 0 failed with value None.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 051 | train_loss=0.2137 train_acc=0.9227 | val_loss=0.5547 val_acc=0.8199 | time=61.4s\n",
            "[Fold 0] Early stopping at epoch 51, best was 36\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vital-deluge-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/bmz541r9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/bmz541r9</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_015411-bmz541r9/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_030714-g6z2cdoc</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/g6z2cdoc' target=\"_blank\">major-voice-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/g6z2cdoc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/g6z2cdoc</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 1 =========================\n",
            "Testing with hyperparameters: {'lr': 2.1557163203444662e-05, 'weight_decay': 0.000159517315156258, 'num_filters': 120, 'rtm_blocks': 1, 'stm_blocks': 1, 'ttm_blocks': 3, 'rtm_heads': 2, 'stm_heads': 4, 'ttm_heads': 4, 'num_segments': 25}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0685 train_acc=0.4311 | val_loss=1.0841 val_acc=0.4317 | time=59.4s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0668 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=60.0s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0662 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=62.5s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0651 train_acc=0.4311 | val_loss=1.0864 val_acc=0.4317 | time=61.6s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0668 train_acc=0.4311 | val_loss=1.0853 val_acc=0.4317 | time=59.7s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0657 train_acc=0.4311 | val_loss=1.0818 val_acc=0.4317 | time=60.3s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0655 train_acc=0.4311 | val_loss=1.0818 val_acc=0.4317 | time=61.5s\n",
            "[Fold 0] Epoch 008 | train_loss=1.0634 train_acc=0.4311 | val_loss=1.0783 val_acc=0.4317 | time=61.6s\n",
            "[Fold 0] Epoch 009 | train_loss=1.0643 train_acc=0.4357 | val_loss=1.0748 val_acc=0.4332 | time=61.6s\n",
            "[Fold 0] Epoch 010 | train_loss=1.0291 train_acc=0.5017 | val_loss=1.0003 val_acc=0.5901 | time=61.5s\n",
            "[Fold 0] Epoch 011 | train_loss=0.9849 train_acc=0.5612 | val_loss=0.9643 val_acc=0.5932 | time=61.3s\n",
            "[Fold 0] Epoch 012 | train_loss=0.9512 train_acc=0.5837 | val_loss=0.9665 val_acc=0.5932 | time=61.1s\n",
            "[Fold 0] Epoch 013 | train_loss=0.9542 train_acc=0.5775 | val_loss=0.9467 val_acc=0.5963 | time=59.4s\n",
            "[Fold 0] Epoch 014 | train_loss=0.9307 train_acc=0.5887 | val_loss=0.9322 val_acc=0.5994 | time=59.4s\n",
            "[Fold 0] Epoch 015 | train_loss=0.9227 train_acc=0.5915 | val_loss=0.9154 val_acc=0.6071 | time=59.4s\n",
            "[Fold 0] Epoch 016 | train_loss=0.9076 train_acc=0.5992 | val_loss=0.9170 val_acc=0.6102 | time=59.2s\n",
            "[Fold 0] Epoch 017 | train_loss=0.8998 train_acc=0.6155 | val_loss=0.9003 val_acc=0.6118 | time=60.1s\n",
            "[Fold 0] Epoch 018 | train_loss=0.8876 train_acc=0.6140 | val_loss=0.8895 val_acc=0.6273 | time=59.3s\n",
            "[Fold 0] Epoch 019 | train_loss=0.8753 train_acc=0.6229 | val_loss=0.8704 val_acc=0.6289 | time=59.1s\n",
            "[Fold 0] Epoch 020 | train_loss=0.8578 train_acc=0.6299 | val_loss=0.8656 val_acc=0.6413 | time=59.5s\n",
            "[Fold 0] Epoch 021 | train_loss=0.8503 train_acc=0.6423 | val_loss=0.8610 val_acc=0.6289 | time=59.6s\n",
            "[Fold 0] Epoch 022 | train_loss=0.8391 train_acc=0.6447 | val_loss=0.8436 val_acc=0.6553 | time=59.1s\n",
            "[Fold 0] Epoch 023 | train_loss=0.8394 train_acc=0.6392 | val_loss=0.8367 val_acc=0.6335 | time=59.3s\n",
            "[Fold 0] Epoch 024 | train_loss=0.8332 train_acc=0.6416 | val_loss=0.8151 val_acc=0.6599 | time=59.3s\n",
            "[Fold 0] Epoch 025 | train_loss=0.8111 train_acc=0.6579 | val_loss=0.8004 val_acc=0.6724 | time=59.4s\n",
            "[Fold 0] Epoch 026 | train_loss=0.7971 train_acc=0.6660 | val_loss=0.8103 val_acc=0.6708 | time=60.2s\n",
            "[Fold 0] Epoch 027 | train_loss=0.7988 train_acc=0.6668 | val_loss=0.8033 val_acc=0.6755 | time=59.6s\n",
            "[Fold 0] Epoch 028 | train_loss=0.7787 train_acc=0.6711 | val_loss=0.7855 val_acc=0.6677 | time=59.2s\n",
            "[Fold 0] Epoch 029 | train_loss=0.7619 train_acc=0.6808 | val_loss=0.7756 val_acc=0.6770 | time=58.8s\n",
            "[Fold 0] Epoch 030 | train_loss=0.7579 train_acc=0.6823 | val_loss=0.7643 val_acc=0.6755 | time=61.5s\n",
            "[Fold 0] Epoch 031 | train_loss=0.7364 train_acc=0.6951 | val_loss=0.7212 val_acc=0.7189 | time=61.6s\n",
            "[Fold 0] Epoch 032 | train_loss=0.7269 train_acc=0.6955 | val_loss=0.7116 val_acc=0.7267 | time=60.5s\n",
            "[Fold 0] Epoch 033 | train_loss=0.7245 train_acc=0.6951 | val_loss=0.7475 val_acc=0.7236 | time=60.4s\n",
            "[Fold 0] Epoch 034 | train_loss=0.6997 train_acc=0.7138 | val_loss=0.7455 val_acc=0.7220 | time=60.9s\n",
            "[Fold 0] Epoch 035 | train_loss=0.6941 train_acc=0.7122 | val_loss=0.6946 val_acc=0.7158 | time=59.3s\n",
            "[Fold 0] Epoch 036 | train_loss=0.6972 train_acc=0.7080 | val_loss=0.7111 val_acc=0.7127 | time=58.7s\n",
            "[Fold 0] Epoch 037 | train_loss=0.6749 train_acc=0.7142 | val_loss=0.7514 val_acc=0.7267 | time=58.1s\n",
            "[Fold 0] Epoch 038 | train_loss=0.6539 train_acc=0.7328 | val_loss=0.6617 val_acc=0.7500 | time=57.9s\n",
            "[Fold 0] Epoch 039 | train_loss=0.6443 train_acc=0.7363 | val_loss=0.6971 val_acc=0.7345 | time=58.9s\n",
            "[Fold 0] Epoch 040 | train_loss=0.6232 train_acc=0.7402 | val_loss=0.6876 val_acc=0.7329 | time=58.2s\n",
            "[Fold 0] Epoch 041 | train_loss=0.6187 train_acc=0.7425 | val_loss=0.6945 val_acc=0.7516 | time=57.7s\n",
            "[Fold 0] Epoch 042 | train_loss=0.6077 train_acc=0.7538 | val_loss=0.7317 val_acc=0.7127 | time=57.2s\n",
            "[Fold 0] Epoch 043 | train_loss=0.6014 train_acc=0.7550 | val_loss=0.6538 val_acc=0.7609 | time=57.4s\n",
            "[Fold 0] Epoch 044 | train_loss=0.5768 train_acc=0.7720 | val_loss=0.6989 val_acc=0.7516 | time=58.4s\n",
            "[Fold 0] Epoch 045 | train_loss=0.5583 train_acc=0.7705 | val_loss=0.7003 val_acc=0.7531 | time=58.6s\n",
            "[Fold 0] Epoch 046 | train_loss=0.5798 train_acc=0.7592 | val_loss=0.7481 val_acc=0.7143 | time=57.5s\n",
            "[Fold 0] Epoch 047 | train_loss=0.5406 train_acc=0.7849 | val_loss=0.7161 val_acc=0.7438 | time=57.8s\n",
            "[Fold 0] Epoch 048 | train_loss=0.5349 train_acc=0.7887 | val_loss=0.6605 val_acc=0.7422 | time=58.1s\n",
            "[Fold 0] Epoch 049 | train_loss=0.5421 train_acc=0.7856 | val_loss=0.6488 val_acc=0.7811 | time=59.2s\n",
            "[Fold 0] Epoch 050 | train_loss=0.5312 train_acc=0.7876 | val_loss=0.6329 val_acc=0.7826 | time=59.1s\n",
            "[Fold 0] Epoch 051 | train_loss=0.5172 train_acc=0.7957 | val_loss=0.6619 val_acc=0.7360 | time=59.6s\n",
            "[Fold 0] Epoch 052 | train_loss=0.4727 train_acc=0.8089 | val_loss=0.6585 val_acc=0.7686 | time=59.7s\n",
            "[Fold 0] Epoch 053 | train_loss=0.4828 train_acc=0.8054 | val_loss=0.6643 val_acc=0.7407 | time=60.3s\n",
            "[Fold 0] Epoch 054 | train_loss=0.4567 train_acc=0.8190 | val_loss=0.6397 val_acc=0.7950 | time=58.4s\n",
            "[Fold 0] Epoch 055 | train_loss=0.4487 train_acc=0.8268 | val_loss=0.7423 val_acc=0.7609 | time=59.7s\n",
            "[Fold 0] Epoch 056 | train_loss=0.4386 train_acc=0.8295 | val_loss=0.6191 val_acc=0.7857 | time=58.5s\n",
            "[Fold 0] Epoch 057 | train_loss=0.4379 train_acc=0.8322 | val_loss=0.6184 val_acc=0.7609 | time=59.1s\n",
            "[Fold 0] Epoch 058 | train_loss=0.4029 train_acc=0.8435 | val_loss=0.6942 val_acc=0.7469 | time=58.9s\n",
            "[Fold 0] Epoch 059 | train_loss=0.3948 train_acc=0.8416 | val_loss=0.6296 val_acc=0.8012 | time=58.9s\n",
            "[Fold 0] Epoch 060 | train_loss=0.4184 train_acc=0.8326 | val_loss=0.6039 val_acc=0.7842 | time=58.5s\n",
            "[Fold 0] Epoch 061 | train_loss=0.3872 train_acc=0.8501 | val_loss=0.6383 val_acc=0.7888 | time=57.3s\n",
            "[Fold 0] Epoch 062 | train_loss=0.3813 train_acc=0.8478 | val_loss=0.6548 val_acc=0.7547 | time=57.2s\n",
            "[Fold 0] Epoch 063 | train_loss=0.3396 train_acc=0.8730 | val_loss=0.6476 val_acc=0.8028 | time=58.4s\n",
            "[Fold 0] Epoch 064 | train_loss=0.3479 train_acc=0.8750 | val_loss=0.5887 val_acc=0.7966 | time=60.1s\n",
            "[Fold 0] Epoch 065 | train_loss=0.3430 train_acc=0.8652 | val_loss=0.5832 val_acc=0.7981 | time=75.0s\n",
            "[Fold 0] Epoch 066 | train_loss=0.3530 train_acc=0.8633 | val_loss=0.6816 val_acc=0.7671 | time=60.4s\n",
            "[Fold 0] Epoch 067 | train_loss=0.3326 train_acc=0.8730 | val_loss=0.6906 val_acc=0.7888 | time=57.9s\n",
            "[Fold 0] Epoch 068 | train_loss=0.3158 train_acc=0.8734 | val_loss=0.5956 val_acc=0.7997 | time=57.2s\n",
            "[Fold 0] Epoch 069 | train_loss=0.3074 train_acc=0.8847 | val_loss=0.6061 val_acc=0.7686 | time=57.1s\n",
            "[Fold 0] Epoch 070 | train_loss=0.2810 train_acc=0.8975 | val_loss=0.5834 val_acc=0.8028 | time=56.9s\n",
            "[Fold 0] Epoch 071 | train_loss=0.2702 train_acc=0.9006 | val_loss=0.5977 val_acc=0.8183 | time=57.6s\n",
            "[Fold 0] Epoch 072 | train_loss=0.2683 train_acc=0.8998 | val_loss=0.5743 val_acc=0.7966 | time=56.8s\n",
            "[Fold 0] Epoch 073 | train_loss=0.2758 train_acc=0.8983 | val_loss=0.6068 val_acc=0.8090 | time=56.6s\n",
            "[Fold 0] Epoch 074 | train_loss=0.2580 train_acc=0.8979 | val_loss=0.6469 val_acc=0.7919 | time=56.9s\n",
            "[Fold 0] Epoch 075 | train_loss=0.2603 train_acc=0.9045 | val_loss=0.5707 val_acc=0.8261 | time=56.6s\n",
            "[Fold 0] Epoch 076 | train_loss=0.2340 train_acc=0.9146 | val_loss=0.6084 val_acc=0.8090 | time=57.1s\n",
            "[Fold 0] Epoch 077 | train_loss=0.2486 train_acc=0.9037 | val_loss=0.6379 val_acc=0.7919 | time=57.1s\n",
            "[Fold 0] Epoch 078 | train_loss=0.2226 train_acc=0.9204 | val_loss=0.5865 val_acc=0.8043 | time=57.0s\n",
            "[Fold 0] Epoch 079 | train_loss=0.2149 train_acc=0.9282 | val_loss=0.5658 val_acc=0.8183 | time=57.1s\n",
            "[Fold 0] Epoch 080 | train_loss=0.2024 train_acc=0.9285 | val_loss=0.6470 val_acc=0.7997 | time=56.9s\n",
            "[Fold 0] Epoch 081 | train_loss=0.2287 train_acc=0.9153 | val_loss=0.6036 val_acc=0.8152 | time=56.9s\n",
            "[Fold 0] Epoch 082 | train_loss=0.2155 train_acc=0.9188 | val_loss=0.6036 val_acc=0.7966 | time=57.1s\n",
            "[Fold 0] Epoch 083 | train_loss=0.2283 train_acc=0.9083 | val_loss=0.6514 val_acc=0.7717 | time=57.2s\n",
            "[Fold 0] Epoch 084 | train_loss=0.1886 train_acc=0.9309 | val_loss=0.6079 val_acc=0.8230 | time=56.4s\n",
            "[Fold 0] Epoch 085 | train_loss=0.1767 train_acc=0.9355 | val_loss=0.6731 val_acc=0.7981 | time=56.8s\n",
            "[Fold 0] Epoch 086 | train_loss=0.1756 train_acc=0.9309 | val_loss=0.5961 val_acc=0.8323 | time=56.4s\n",
            "[Fold 0] Epoch 087 | train_loss=0.1549 train_acc=0.9483 | val_loss=0.6244 val_acc=0.8339 | time=56.7s\n",
            "[Fold 0] Epoch 088 | train_loss=0.1799 train_acc=0.9320 | val_loss=0.6151 val_acc=0.8307 | time=56.5s\n",
            "[Fold 0] Epoch 089 | train_loss=0.1374 train_acc=0.9495 | val_loss=0.6524 val_acc=0.7935 | time=57.0s\n",
            "[Fold 0] Epoch 090 | train_loss=0.1498 train_acc=0.9472 | val_loss=0.6301 val_acc=0.8090 | time=57.7s\n",
            "[Fold 0] Epoch 091 | train_loss=0.1350 train_acc=0.9511 | val_loss=0.8087 val_acc=0.7717 | time=57.0s\n",
            "[Fold 0] Epoch 092 | train_loss=0.2191 train_acc=0.9208 | val_loss=0.6655 val_acc=0.8059 | time=56.6s\n",
            "[Fold 0] Epoch 093 | train_loss=0.1497 train_acc=0.9429 | val_loss=0.7325 val_acc=0.7717 | time=57.1s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-04-28 04:39:30,295] Trial 1 failed with parameters: {'lr': 2.1557163203444662e-05, 'weight_decay': 0.000159517315156258, 'num_filters': 120, 'rtm_blocks': 1, 'stm_blocks': 1, 'ttm_blocks': 3, 'rtm_heads': 2, 'stm_heads': 4, 'ttm_heads': 4, 'num_segments': 25} because of the following error: The value None could not be cast to float..\n",
            "[W 2025-04-28 04:39:30,297] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 094 | train_loss=0.1312 train_acc=0.9546 | val_loss=0.6095 val_acc=0.8199 | time=56.7s\n",
            "[Fold 0] Early stopping at epoch 94, best was 79\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">major-voice-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/g6z2cdoc' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/g6z2cdoc</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_030714-g6z2cdoc/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_043930-yscfkzo8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/yscfkzo8' target=\"_blank\">colorful-jazz-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/yscfkzo8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_9/runs/yscfkzo8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 2 =========================\n",
            "Testing with hyperparameters: {'lr': 1.2400547483173527e-05, 'weight_decay': 7.79511927028319e-05, 'num_filters': 120, 'rtm_blocks': 3, 'stm_blocks': 3, 'ttm_blocks': 1, 'rtm_heads': 2, 'stm_heads': 4, 'ttm_heads': 4, 'num_segments': 15}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0852 train_acc=0.4155 | val_loss=1.0800 val_acc=0.4317 | time=72.3s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0672 train_acc=0.4311 | val_loss=1.0815 val_acc=0.4317 | time=72.3s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0667 train_acc=0.4311 | val_loss=1.0817 val_acc=0.4317 | time=72.3s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0649 train_acc=0.4311 | val_loss=1.0811 val_acc=0.4317 | time=71.7s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0639 train_acc=0.4311 | val_loss=1.0798 val_acc=0.4317 | time=72.2s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0617 train_acc=0.4330 | val_loss=1.0696 val_acc=0.4317 | time=71.7s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0396 train_acc=0.4913 | val_loss=1.0129 val_acc=0.5668 | time=72.2s\n",
            "[Fold 0] Epoch 008 | train_loss=1.0156 train_acc=0.5181 | val_loss=1.0159 val_acc=0.5590 | time=72.8s\n",
            "[Fold 0] Epoch 009 | train_loss=1.0136 train_acc=0.5134 | val_loss=1.0245 val_acc=0.5326 | time=73.3s\n",
            "[Fold 0] Epoch 010 | train_loss=0.9928 train_acc=0.5383 | val_loss=0.9985 val_acc=0.5559 | time=72.8s\n",
            "[Fold 0] Epoch 011 | train_loss=0.9933 train_acc=0.5313 | val_loss=0.9733 val_acc=0.5870 | time=72.5s\n",
            "[Fold 0] Epoch 012 | train_loss=0.9763 train_acc=0.5530 | val_loss=0.9535 val_acc=0.5947 | time=72.0s\n",
            "[Fold 0] Epoch 013 | train_loss=0.9585 train_acc=0.5705 | val_loss=0.9426 val_acc=0.6087 | time=71.6s\n",
            "[Fold 0] Epoch 014 | train_loss=0.9463 train_acc=0.5790 | val_loss=0.9482 val_acc=0.5947 | time=71.8s\n",
            "[Fold 0] Epoch 015 | train_loss=0.9526 train_acc=0.5717 | val_loss=0.9570 val_acc=0.5901 | time=71.8s\n",
            "[Fold 0] Epoch 016 | train_loss=0.9477 train_acc=0.5740 | val_loss=0.9378 val_acc=0.6009 | time=71.8s\n",
            "[Fold 0] Epoch 017 | train_loss=0.9567 train_acc=0.5650 | val_loss=0.9451 val_acc=0.5994 | time=71.9s\n",
            "[Fold 0] Epoch 018 | train_loss=0.9453 train_acc=0.5736 | val_loss=0.9246 val_acc=0.6134 | time=71.7s\n",
            "[Fold 0] Epoch 019 | train_loss=0.9414 train_acc=0.5806 | val_loss=0.9220 val_acc=0.6289 | time=72.1s\n",
            "[Fold 0] Epoch 020 | train_loss=0.9240 train_acc=0.5845 | val_loss=0.9154 val_acc=0.6196 | time=72.2s\n",
            "[Fold 0] Epoch 021 | train_loss=0.9209 train_acc=0.5891 | val_loss=0.9139 val_acc=0.6382 | time=75.8s\n",
            "[Fold 0] Epoch 022 | train_loss=0.9291 train_acc=0.5922 | val_loss=0.9141 val_acc=0.6180 | time=73.7s\n",
            "[Fold 0] Epoch 023 | train_loss=0.9115 train_acc=0.5880 | val_loss=0.9260 val_acc=0.6087 | time=72.9s\n",
            "[Fold 0] Epoch 024 | train_loss=0.9151 train_acc=0.5891 | val_loss=0.9026 val_acc=0.6211 | time=72.4s\n",
            "[Fold 0] Epoch 025 | train_loss=0.9021 train_acc=0.6062 | val_loss=0.8914 val_acc=0.6335 | time=72.3s\n",
            "[Fold 0] Epoch 026 | train_loss=0.9003 train_acc=0.6089 | val_loss=0.8890 val_acc=0.6491 | time=72.4s\n",
            "[Fold 0] Epoch 027 | train_loss=0.8951 train_acc=0.6078 | val_loss=0.8899 val_acc=0.6320 | time=72.2s\n",
            "[Fold 0] Epoch 028 | train_loss=0.8987 train_acc=0.6085 | val_loss=0.9130 val_acc=0.6320 | time=72.7s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xrT1tbFI_NmR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kGHGCc4m_N93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3dERy5yJ_OWk",
        "outputId": "05f1c1bf-76f8-4d4e-a1c5-588175e2a12e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Constants ─────────────────────────\n",
        "LR_MIN, LR_MAX = 2e-5, 8e-5\n",
        "WD_MIN, WD_MAX = 3e-5, 1e-4\n",
        "FILTER_MIN = FILTER_MAX = 120\n",
        "\n",
        "# blocks -> [1,2,3] / heads -> [2,3]\n",
        "RTM_BLOCK_CHOICES = [1, 2, 3]\n",
        "STM_BLOCK_CHOICES = [1, 2, 3]\n",
        "TTM_BLOCK_CHOICES = [1, 2, 3]\n",
        "\n",
        "RTM_HEAD_CHOICES = [2, 3, 4]\n",
        "STM_HEAD_CHOICES = [2, 3, 4]\n",
        "TTM_HEAD_CHOICES = [2, 3, 4]\n",
        "\n",
        "SEGMENT_CHOICES = [5]\n",
        "\n",
        "N_FOLDS     = 5\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = \"labels.json\"\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space setup\n",
        "    lr = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_filters = 120\n",
        "    rtm_blocks = trial.suggest_categorical(\"rtm_blocks\", RTM_BLOCK_CHOICES)\n",
        "    stm_blocks = trial.suggest_categorical(\"stm_blocks\", STM_BLOCK_CHOICES)\n",
        "    ttm_blocks = trial.suggest_categorical(\"ttm_blocks\", TTM_BLOCK_CHOICES)\n",
        "    rtm_heads = trial.suggest_categorical(\"rtm_heads\", RTM_HEAD_CHOICES)\n",
        "    stm_heads = trial.suggest_categorical(\"stm_heads\", STM_HEAD_CHOICES)\n",
        "    ttm_heads = trial.suggest_categorical(\"ttm_heads\", TTM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # Pruning condition: only proceed if num_filters is divisible by heads\n",
        "    for h in (rtm_heads, stm_heads, ttm_heads):\n",
        "        if num_filters % h != 0:\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"eeg-cv-tuning-trial_10\", config=trial.params)\n",
        "\n",
        "    print(f\"\\n========================= Trial {trial.number} =========================\")\n",
        "    print(f\"Testing with hyperparameters: {trial.params}\")\n",
        "\n",
        "    # Data preparation\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    n_samples = len(full_ds)\n",
        "\n",
        "    # StratifiedKFold setup\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    fold_metrics = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"best_epoch\": []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(range(n_samples), labels)):\n",
        "        # Fold separation\n",
        "        print(f\"\\n========================= Fold {fold} =========================\")\n",
        "\n",
        "        # Data loader setup\n",
        "        train_loader = DataLoader(Subset(full_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(Subset(full_ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # Model and optimizer setup\n",
        "        model = EEGformer(\n",
        "            num_classes=3,\n",
        "            in_channels=19,\n",
        "            kernel_size=10,\n",
        "            num_filters=num_filters,\n",
        "            rtm_blocks=rtm_blocks,\n",
        "            stm_blocks=stm_blocks,\n",
        "            ttm_blocks=ttm_blocks,\n",
        "            rtm_heads=rtm_heads,\n",
        "            stm_heads=stm_heads,\n",
        "            ttm_heads=ttm_heads,\n",
        "            num_segments=num_segments\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        epochs_no_improve = 0\n",
        "        best_epoch = 0\n",
        "        best_train_l = best_train_a = best_val_a = None\n",
        "        last_log_time = time.time()\n",
        "\n",
        "        # Epoch-wise training\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            model.train()\n",
        "            tl_sum = t_corr = t_tot = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tl_sum += loss.item()\n",
        "                t_corr += (logits.argmax(1) == y).sum().item()\n",
        "                t_tot += y.size(0)\n",
        "\n",
        "            train_loss = tl_sum / len(train_loader)\n",
        "            train_acc = t_corr / t_tot\n",
        "\n",
        "            model.eval()\n",
        "            vl_sum = v_corr = v_tot = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss = criterion(logits, y)\n",
        "                    vl_sum += loss.item()\n",
        "                    v_corr += (logits.argmax(1) == y).sum().item()\n",
        "                    v_tot += y.size(0)\n",
        "\n",
        "            val_loss = vl_sum / len(val_loader)\n",
        "            val_acc = v_corr / v_tot\n",
        "\n",
        "            step = fold * MAX_EPOCHS + epoch\n",
        "            trial.report(val_loss, step=step)\n",
        "\n",
        "            # Pruning check\n",
        "            if trial.should_prune():\n",
        "                print(f\"\\u274c Trial {trial.number} pruned at fold {fold}, epoch {epoch}\")\n",
        "                # Report the metrics before returning early\n",
        "                for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                 [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                    fold_metrics[k].append(v)\n",
        "                    trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                raise optuna.TrialPruned()  # End trial completely if pruned\n",
        "\n",
        "            now = time.time()\n",
        "            print(f\"[Fold {fold}] Epoch {epoch:03d} | train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
        "                  f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={now - last_log_time:.1f}s\")\n",
        "            last_log_time = now\n",
        "\n",
        "            # Early stopping: if validation loss does not improve\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_epoch = epoch\n",
        "                epochs_no_improve = 0\n",
        "                best_train_l = train_loss\n",
        "                best_train_a = train_acc\n",
        "                best_val_a = val_acc\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= PATIENCE:\n",
        "                    print(f\"[Fold {fold}] Early stopping at epoch {epoch}, best was {best_epoch}\")\n",
        "                    # Report the metrics before returning early\n",
        "                    for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                     [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                        fold_metrics[k].append(v)\n",
        "                        trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                    return best_val_loss  # End trial completely if early stopping\n",
        "\n",
        "        # Record results for the fold\n",
        "        for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                         [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "            fold_metrics[k].append(v)\n",
        "            trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        torch.mps.empty_cache() if DEVICE.type == \"mps\" else torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg = lambda k: sum(fold_metrics[k]) / N_FOLDS\n",
        "    for key in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"]:\n",
        "        trial.set_user_attr(f\"avg_{key}\", avg(key))\n",
        "\n",
        "    wandb.finish()\n",
        "    return avg(\"val_loss\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "        study_name=\"eegformer_optuna_cv_4\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eegformer_optuna_cv_4.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial Results =====\")\n",
        "    print(f\"avg_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"avg_train_loss = {best.user_attrs['avg_train_loss']:.6f}\")\n",
        "    print(f\"avg_train_acc  = {best.user_attrs['avg_train_acc']:.4f}\")\n",
        "    print(f\"avg_val_acc    = {best.user_attrs['avg_val_acc']:.4f}\")\n",
        "    print(f\"avg_best_epoch = {best.user_attrs['avg_best_epoch']:.1f}\")\n",
        "    print(\"best hyperparameters:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(\"per-fold best metrics:\")\n",
        "    for f in range(N_FOLDS):\n",
        "        print(\n",
        "            f\"  Fold {f}: epoch={best.user_attrs[f'fold{f}_best_epoch']}, \"\n",
        "            f\"t_loss={best.user_attrs[f'fold{f}_train_loss']:.4f}, \"\n",
        "            f\"t_acc={best.user_attrs[f'fold{f}_train_acc']:.4f}, \"\n",
        "            f\"v_loss={best.user_attrs[f'fold{f}_val_loss']:.4f}, \"\n",
        "            f\"v_acc={best.user_attrs[f'fold{f}_val_acc']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # ─── 전체 데이터(train)로 재학습 + loss·accuracy 출력 + 저장 ─────────────────\n",
        "    print(\"\\nRetraining on full TRAIN dataset with best params…\")\n",
        "\n",
        "    # 1) train 메타만 골라서 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    full_meta   = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, full_meta)\n",
        "    full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # 2) 모델·옵티마이저 재설정\n",
        "    model = EEGformer(\n",
        "        num_classes=3,\n",
        "        in_channels=19,\n",
        "        kernel_size=10,\n",
        "        num_filters=120,\n",
        "        rtm_blocks=best.params[\"rtm_blocks\"],\n",
        "        stm_blocks=best.params[\"stm_blocks\"],\n",
        "        ttm_blocks=best.params[\"ttm_blocks\"],\n",
        "        rtm_heads= best.params[\"rtm_heads\"],\n",
        "        stm_heads= best.params[\"stm_heads\"],\n",
        "        ttm_heads= best.params[\"ttm_heads\"],\n",
        "        num_segments=best.params[\"num_segments\"]\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=best.params[\"lr\"],\n",
        "        weight_decay=best.params[\"weight_decay\"]\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 3) MAX_EPOCHS 만큼 전체 학습하며 loss·accuracy 출력\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        model.train()\n",
        "        loss_sum = 0.0\n",
        "        correct  = 0\n",
        "        total    = 0\n",
        "        for X, y in full_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += y.size(0)\n",
        "\n",
        "        avg_loss = loss_sum / len(full_loader)\n",
        "        acc      = correct / total\n",
        "        print(f\"[Full Train] Epoch {epoch:03d} | loss={avg_loss:.4f} | acc={acc:.4f}\")\n",
        "\n",
        "    # 4) 체크포인트 저장\n",
        "    ckpt_dir = '/content/drive/MyDrive/2025_Lab_Research/checkpoints'\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(ckpt_dir, 'eegformer_best.pth')\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"💾 Saved best model to {ckpt_path}\")\n"
      ],
      "metadata": {
        "id": "_c_ihAlwGEjK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "20c9610a-f5b6-42b8-f43f-269ebbaf875e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 07:20:44,501] A new study created in RDB with name: eegformer_optuna_cv_4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_072044-v8wubgjv</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/v8wubgjv' target=\"_blank\">effortless-valley-3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/v8wubgjv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/v8wubgjv</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 0 =========================\n",
            "Testing with hyperparameters: {'lr': 7.00045782400034e-05, 'weight_decay': 8.734906501291418e-05, 'rtm_blocks': 1, 'stm_blocks': 2, 'ttm_blocks': 2, 'rtm_heads': 2, 'stm_heads': 3, 'ttm_heads': 2, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0691 train_acc=0.4225 | val_loss=1.0870 val_acc=0.4317 | time=726.1s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0673 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=18.7s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0675 train_acc=0.4311 | val_loss=1.0879 val_acc=0.4317 | time=18.4s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0671 train_acc=0.4311 | val_loss=1.0844 val_acc=0.4317 | time=18.6s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0672 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0672 train_acc=0.4311 | val_loss=1.0854 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0665 train_acc=0.4311 | val_loss=1.0836 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 008 | train_loss=1.0662 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 009 | train_loss=1.0673 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=18.9s\n",
            "[Fold 0] Epoch 010 | train_loss=1.0659 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=18.3s\n",
            "[Fold 0] Epoch 011 | train_loss=1.0631 train_acc=0.4404 | val_loss=1.0773 val_acc=0.4596 | time=18.5s\n",
            "[Fold 0] Epoch 012 | train_loss=1.0160 train_acc=0.5052 | val_loss=0.9982 val_acc=0.5388 | time=18.4s\n",
            "[Fold 0] Epoch 013 | train_loss=0.9250 train_acc=0.5868 | val_loss=0.9582 val_acc=0.5714 | time=18.6s\n",
            "[Fold 0] Epoch 014 | train_loss=0.8997 train_acc=0.6050 | val_loss=0.9010 val_acc=0.6273 | time=18.4s\n",
            "[Fold 0] Epoch 015 | train_loss=0.8745 train_acc=0.6214 | val_loss=0.9215 val_acc=0.5994 | time=18.2s\n",
            "[Fold 0] Epoch 016 | train_loss=0.8476 train_acc=0.6283 | val_loss=0.8775 val_acc=0.6335 | time=18.6s\n",
            "[Fold 0] Epoch 017 | train_loss=0.8643 train_acc=0.6198 | val_loss=0.8656 val_acc=0.6289 | time=18.1s\n",
            "[Fold 0] Epoch 018 | train_loss=0.8108 train_acc=0.6489 | val_loss=0.8690 val_acc=0.6382 | time=18.7s\n",
            "[Fold 0] Epoch 019 | train_loss=0.8212 train_acc=0.6392 | val_loss=0.8627 val_acc=0.6413 | time=18.4s\n",
            "[Fold 0] Epoch 020 | train_loss=0.7925 train_acc=0.6614 | val_loss=0.8602 val_acc=0.6429 | time=18.4s\n",
            "[Fold 0] Epoch 021 | train_loss=0.7883 train_acc=0.6617 | val_loss=0.8632 val_acc=0.6522 | time=18.1s\n",
            "[Fold 0] Epoch 022 | train_loss=0.7640 train_acc=0.6722 | val_loss=0.9015 val_acc=0.6335 | time=18.4s\n",
            "[Fold 0] Epoch 023 | train_loss=0.7516 train_acc=0.6777 | val_loss=0.8151 val_acc=0.6568 | time=18.5s\n",
            "[Fold 0] Epoch 024 | train_loss=0.7207 train_acc=0.6924 | val_loss=0.8927 val_acc=0.6615 | time=18.1s\n",
            "[Fold 0] Epoch 025 | train_loss=0.7242 train_acc=0.6874 | val_loss=0.8356 val_acc=0.6724 | time=18.6s\n",
            "[Fold 0] Epoch 026 | train_loss=0.6987 train_acc=0.6955 | val_loss=0.8615 val_acc=0.6444 | time=18.5s\n",
            "[Fold 0] Epoch 027 | train_loss=0.6913 train_acc=0.6944 | val_loss=0.8404 val_acc=0.6475 | time=18.2s\n",
            "[Fold 0] Epoch 028 | train_loss=0.6804 train_acc=0.7072 | val_loss=0.8084 val_acc=0.6553 | time=18.3s\n",
            "[Fold 0] Epoch 029 | train_loss=0.6427 train_acc=0.7204 | val_loss=0.8605 val_acc=0.6755 | time=18.3s\n",
            "[Fold 0] Epoch 030 | train_loss=0.6502 train_acc=0.7177 | val_loss=0.8178 val_acc=0.6972 | time=18.7s\n",
            "[Fold 0] Epoch 031 | train_loss=0.6247 train_acc=0.7309 | val_loss=0.8724 val_acc=0.6724 | time=18.6s\n",
            "[Fold 0] Epoch 032 | train_loss=0.6083 train_acc=0.7410 | val_loss=0.8617 val_acc=0.6537 | time=18.5s\n",
            "[Fold 0] Epoch 033 | train_loss=0.6099 train_acc=0.7390 | val_loss=0.8031 val_acc=0.6941 | time=18.4s\n",
            "[Fold 0] Epoch 034 | train_loss=0.5931 train_acc=0.7534 | val_loss=0.8162 val_acc=0.6863 | time=18.2s\n",
            "[Fold 0] Epoch 035 | train_loss=0.5804 train_acc=0.7495 | val_loss=0.8468 val_acc=0.7050 | time=18.3s\n",
            "[Fold 0] Epoch 036 | train_loss=0.5663 train_acc=0.7627 | val_loss=0.8301 val_acc=0.6817 | time=18.3s\n",
            "[Fold 0] Epoch 037 | train_loss=0.5438 train_acc=0.7612 | val_loss=0.9175 val_acc=0.6398 | time=18.2s\n",
            "[Fold 0] Epoch 038 | train_loss=0.5556 train_acc=0.7650 | val_loss=0.8186 val_acc=0.6801 | time=18.2s\n",
            "[Fold 0] Epoch 039 | train_loss=0.5081 train_acc=0.7802 | val_loss=0.8848 val_acc=0.6941 | time=18.7s\n",
            "[Fold 0] Epoch 040 | train_loss=0.5123 train_acc=0.7724 | val_loss=0.8005 val_acc=0.6832 | time=18.7s\n",
            "[Fold 0] Epoch 041 | train_loss=0.5057 train_acc=0.7841 | val_loss=0.8684 val_acc=0.6941 | time=18.6s\n",
            "[Fold 0] Epoch 042 | train_loss=0.5000 train_acc=0.7814 | val_loss=0.8102 val_acc=0.6972 | time=18.4s\n",
            "[Fold 0] Epoch 043 | train_loss=0.4699 train_acc=0.7961 | val_loss=0.8070 val_acc=0.7003 | time=18.5s\n",
            "[Fold 0] Epoch 044 | train_loss=0.4423 train_acc=0.8085 | val_loss=0.8333 val_acc=0.6910 | time=18.5s\n",
            "[Fold 0] Epoch 045 | train_loss=0.4346 train_acc=0.8124 | val_loss=0.7910 val_acc=0.7050 | time=18.4s\n",
            "[Fold 0] Epoch 046 | train_loss=0.4310 train_acc=0.8148 | val_loss=0.8862 val_acc=0.7019 | time=18.7s\n",
            "[Fold 0] Epoch 047 | train_loss=0.4142 train_acc=0.8155 | val_loss=0.9747 val_acc=0.6615 | time=18.5s\n",
            "[Fold 0] Epoch 048 | train_loss=0.3956 train_acc=0.8291 | val_loss=0.9678 val_acc=0.7050 | time=18.5s\n",
            "[Fold 0] Epoch 049 | train_loss=0.3755 train_acc=0.8416 | val_loss=0.9535 val_acc=0.7112 | time=18.5s\n",
            "[Fold 0] Epoch 050 | train_loss=0.3780 train_acc=0.8454 | val_loss=0.8556 val_acc=0.6786 | time=18.6s\n",
            "[Fold 0] Epoch 051 | train_loss=0.3717 train_acc=0.8330 | val_loss=0.9794 val_acc=0.6941 | time=18.1s\n",
            "[Fold 0] Epoch 052 | train_loss=0.3378 train_acc=0.8497 | val_loss=0.9206 val_acc=0.7112 | time=18.2s\n",
            "[Fold 0] Epoch 053 | train_loss=0.3169 train_acc=0.8691 | val_loss=0.9749 val_acc=0.7143 | time=18.2s\n",
            "[Fold 0] Epoch 054 | train_loss=0.3318 train_acc=0.8586 | val_loss=0.9890 val_acc=0.6972 | time=18.3s\n",
            "[Fold 0] Epoch 055 | train_loss=0.3118 train_acc=0.8703 | val_loss=0.9112 val_acc=0.7220 | time=18.5s\n",
            "[Fold 0] Epoch 056 | train_loss=0.2842 train_acc=0.8847 | val_loss=0.9027 val_acc=0.7189 | time=18.3s\n",
            "[Fold 0] Epoch 057 | train_loss=0.2740 train_acc=0.8940 | val_loss=1.0368 val_acc=0.7158 | time=18.6s\n",
            "[Fold 0] Epoch 058 | train_loss=0.2660 train_acc=0.8816 | val_loss=0.9936 val_acc=0.7143 | time=18.2s\n",
            "[Fold 0] Epoch 059 | train_loss=0.3013 train_acc=0.8781 | val_loss=0.8915 val_acc=0.7003 | time=18.6s\n",
            "[Fold 0] Epoch 060 | train_loss=0.2554 train_acc=0.8955 | val_loss=1.0126 val_acc=0.7267 | time=18.5s\n",
            "[Fold 0] Epoch 061 | train_loss=0.2140 train_acc=0.9107 | val_loss=1.1070 val_acc=0.7158 | time=18.3s\n",
            "[Fold 0] Epoch 062 | train_loss=0.2545 train_acc=0.8998 | val_loss=1.1526 val_acc=0.6848 | time=18.8s\n",
            "[Fold 0] Epoch 063 | train_loss=0.2350 train_acc=0.9021 | val_loss=1.1374 val_acc=0.7050 | time=17.5s\n",
            "[Fold 0] Epoch 064 | train_loss=0.2491 train_acc=0.9006 | val_loss=0.9956 val_acc=0.7189 | time=17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 07:52:34,910] Trial 0 finished with value: 0.7909612684022813 and parameters: {'lr': 7.00045782400034e-05, 'weight_decay': 8.734906501291418e-05, 'rtm_blocks': 1, 'stm_blocks': 2, 'ttm_blocks': 2, 'rtm_heads': 2, 'stm_heads': 3, 'ttm_heads': 2, 'num_segments': 5}. Best is trial 0 with value: 0.7909612684022813.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 065 | train_loss=0.1856 train_acc=0.9262 | val_loss=1.1634 val_acc=0.7189 | time=17.5s\n",
            "[Fold 0] Early stopping at epoch 65, best was 45\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">effortless-valley-3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/v8wubgjv' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/v8wubgjv</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_072044-v8wubgjv/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_075235-yo0ckwsp</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yo0ckwsp' target=\"_blank\">absurd-sunset-4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yo0ckwsp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yo0ckwsp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 1 =========================\n",
            "Testing with hyperparameters: {'lr': 4.9554878139823925e-05, 'weight_decay': 8.001690176093237e-05, 'rtm_blocks': 1, 'stm_blocks': 3, 'ttm_blocks': 2, 'rtm_heads': 2, 'stm_heads': 2, 'ttm_heads': 2, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0684 train_acc=0.4311 | val_loss=1.0891 val_acc=0.4317 | time=17.9s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0669 train_acc=0.4311 | val_loss=1.0846 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0657 train_acc=0.4311 | val_loss=1.0803 val_acc=0.4317 | time=17.6s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0626 train_acc=0.4338 | val_loss=1.0817 val_acc=0.4317 | time=17.9s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0621 train_acc=0.4361 | val_loss=1.0733 val_acc=0.4410 | time=17.8s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0556 train_acc=0.4408 | val_loss=1.0692 val_acc=0.4394 | time=17.7s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0537 train_acc=0.4396 | val_loss=1.0701 val_acc=0.4410 | time=17.8s\n",
            "[Fold 0] Epoch 008 | train_loss=1.0549 train_acc=0.4381 | val_loss=1.0671 val_acc=0.4503 | time=17.5s\n",
            "[Fold 0] Epoch 009 | train_loss=1.0423 train_acc=0.4583 | val_loss=1.0769 val_acc=0.4363 | time=17.7s\n",
            "[Fold 0] Epoch 010 | train_loss=1.0249 train_acc=0.4940 | val_loss=1.0712 val_acc=0.4550 | time=17.8s\n",
            "[Fold 0] Epoch 011 | train_loss=1.0168 train_acc=0.5064 | val_loss=1.0415 val_acc=0.4876 | time=17.7s\n",
            "[Fold 0] Epoch 012 | train_loss=0.9934 train_acc=0.5266 | val_loss=0.9795 val_acc=0.5730 | time=18.1s\n",
            "[Fold 0] Epoch 013 | train_loss=0.9698 train_acc=0.5538 | val_loss=0.9624 val_acc=0.5714 | time=17.5s\n",
            "[Fold 0] Epoch 014 | train_loss=0.9427 train_acc=0.5783 | val_loss=0.9956 val_acc=0.5621 | time=18.0s\n",
            "[Fold 0] Epoch 015 | train_loss=0.9114 train_acc=0.5977 | val_loss=0.9121 val_acc=0.6134 | time=17.7s\n",
            "[Fold 0] Epoch 016 | train_loss=0.9033 train_acc=0.5996 | val_loss=0.8916 val_acc=0.6196 | time=17.7s\n",
            "[Fold 0] Epoch 017 | train_loss=0.8736 train_acc=0.6287 | val_loss=0.9138 val_acc=0.6149 | time=18.0s\n",
            "[Fold 0] Epoch 018 | train_loss=0.8658 train_acc=0.6303 | val_loss=0.8673 val_acc=0.6491 | time=17.7s\n",
            "[Fold 0] Epoch 019 | train_loss=0.8583 train_acc=0.6353 | val_loss=0.8687 val_acc=0.6506 | time=17.8s\n",
            "[Fold 0] Epoch 020 | train_loss=0.8413 train_acc=0.6400 | val_loss=0.8591 val_acc=0.6382 | time=17.9s\n",
            "[Fold 0] Epoch 021 | train_loss=0.8272 train_acc=0.6478 | val_loss=0.8561 val_acc=0.6491 | time=17.3s\n",
            "[Fold 0] Epoch 022 | train_loss=0.8182 train_acc=0.6524 | val_loss=0.8414 val_acc=0.6522 | time=17.9s\n",
            "[Fold 0] Epoch 023 | train_loss=0.8067 train_acc=0.6610 | val_loss=0.8214 val_acc=0.6693 | time=17.9s\n",
            "[Fold 0] Epoch 024 | train_loss=0.7974 train_acc=0.6645 | val_loss=0.8542 val_acc=0.6568 | time=17.6s\n",
            "[Fold 0] Epoch 025 | train_loss=0.7948 train_acc=0.6614 | val_loss=0.8265 val_acc=0.6661 | time=17.7s\n",
            "[Fold 0] Epoch 026 | train_loss=0.7727 train_acc=0.6730 | val_loss=0.8147 val_acc=0.6708 | time=17.6s\n",
            "[Fold 0] Epoch 027 | train_loss=0.7812 train_acc=0.6637 | val_loss=0.8084 val_acc=0.6693 | time=17.8s\n",
            "[Fold 0] Epoch 028 | train_loss=0.7595 train_acc=0.6773 | val_loss=0.8250 val_acc=0.6568 | time=17.8s\n",
            "[Fold 0] Epoch 029 | train_loss=0.7471 train_acc=0.6777 | val_loss=0.7986 val_acc=0.6584 | time=17.7s\n",
            "[Fold 0] Epoch 030 | train_loss=0.7429 train_acc=0.6738 | val_loss=0.8744 val_acc=0.6429 | time=18.1s\n",
            "[Fold 0] Epoch 031 | train_loss=0.7272 train_acc=0.6800 | val_loss=0.7667 val_acc=0.7065 | time=17.7s\n",
            "[Fold 0] Epoch 032 | train_loss=0.7093 train_acc=0.6983 | val_loss=0.7806 val_acc=0.6630 | time=17.6s\n",
            "[Fold 0] Epoch 033 | train_loss=0.6967 train_acc=0.7029 | val_loss=0.8069 val_acc=0.6848 | time=17.7s\n",
            "[Fold 0] Epoch 034 | train_loss=0.6846 train_acc=0.7115 | val_loss=0.8012 val_acc=0.6863 | time=17.6s\n",
            "[Fold 0] Epoch 035 | train_loss=0.6950 train_acc=0.6948 | val_loss=0.7760 val_acc=0.6739 | time=17.7s\n",
            "[Fold 0] Epoch 036 | train_loss=0.6755 train_acc=0.7153 | val_loss=0.7817 val_acc=0.7034 | time=17.7s\n",
            "[Fold 0] Epoch 037 | train_loss=0.6483 train_acc=0.7282 | val_loss=0.7688 val_acc=0.7003 | time=17.9s\n",
            "[Fold 0] Epoch 038 | train_loss=0.6383 train_acc=0.7216 | val_loss=0.7117 val_acc=0.7158 | time=17.9s\n",
            "[Fold 0] Epoch 039 | train_loss=0.6408 train_acc=0.7297 | val_loss=0.7555 val_acc=0.7081 | time=17.7s\n",
            "[Fold 0] Epoch 040 | train_loss=0.6305 train_acc=0.7320 | val_loss=0.7911 val_acc=0.6894 | time=17.6s\n",
            "[Fold 0] Epoch 041 | train_loss=0.6407 train_acc=0.7235 | val_loss=0.7365 val_acc=0.7112 | time=17.7s\n",
            "[Fold 0] Epoch 042 | train_loss=0.6321 train_acc=0.7297 | val_loss=0.7470 val_acc=0.7065 | time=17.6s\n",
            "[Fold 0] Epoch 043 | train_loss=0.6037 train_acc=0.7417 | val_loss=0.7712 val_acc=0.7034 | time=17.7s\n",
            "[Fold 0] Epoch 044 | train_loss=0.6269 train_acc=0.7285 | val_loss=0.7068 val_acc=0.7283 | time=17.7s\n",
            "[Fold 0] Epoch 045 | train_loss=0.6017 train_acc=0.7499 | val_loss=0.7348 val_acc=0.7158 | time=17.5s\n",
            "[Fold 0] Epoch 046 | train_loss=0.5855 train_acc=0.7538 | val_loss=0.7556 val_acc=0.7158 | time=17.9s\n",
            "[Fold 0] Epoch 047 | train_loss=0.5820 train_acc=0.7581 | val_loss=0.7072 val_acc=0.7407 | time=17.8s\n",
            "[Fold 0] Epoch 048 | train_loss=0.5684 train_acc=0.7616 | val_loss=0.7425 val_acc=0.7065 | time=17.9s\n",
            "[Fold 0] Epoch 049 | train_loss=0.5499 train_acc=0.7732 | val_loss=0.7794 val_acc=0.7034 | time=17.7s\n",
            "[Fold 0] Epoch 050 | train_loss=0.5789 train_acc=0.7608 | val_loss=0.7100 val_acc=0.7189 | time=17.6s\n",
            "[Fold 0] Epoch 051 | train_loss=0.5583 train_acc=0.7643 | val_loss=0.6972 val_acc=0.7096 | time=17.9s\n",
            "[Fold 0] Epoch 052 | train_loss=0.5469 train_acc=0.7732 | val_loss=0.7001 val_acc=0.7252 | time=17.7s\n",
            "[Fold 0] Epoch 053 | train_loss=0.5361 train_acc=0.7763 | val_loss=0.7183 val_acc=0.7283 | time=17.4s\n",
            "[Fold 0] Epoch 054 | train_loss=0.5540 train_acc=0.7771 | val_loss=0.7293 val_acc=0.7345 | time=17.8s\n",
            "[Fold 0] Epoch 055 | train_loss=0.5293 train_acc=0.7841 | val_loss=0.7488 val_acc=0.7158 | time=17.9s\n",
            "[Fold 0] Epoch 056 | train_loss=0.5092 train_acc=0.7825 | val_loss=0.6714 val_acc=0.7484 | time=17.6s\n",
            "[Fold 0] Epoch 057 | train_loss=0.5131 train_acc=0.7883 | val_loss=0.7762 val_acc=0.7112 | time=17.8s\n",
            "[Fold 0] Epoch 058 | train_loss=0.5023 train_acc=0.7988 | val_loss=0.6869 val_acc=0.7329 | time=17.5s\n",
            "[Fold 0] Epoch 059 | train_loss=0.5038 train_acc=0.7961 | val_loss=0.7014 val_acc=0.7236 | time=17.8s\n",
            "[Fold 0] Epoch 060 | train_loss=0.4984 train_acc=0.7934 | val_loss=0.7730 val_acc=0.7345 | time=17.7s\n",
            "[Fold 0] Epoch 061 | train_loss=0.4714 train_acc=0.8113 | val_loss=0.7771 val_acc=0.7003 | time=17.7s\n",
            "[Fold 0] Epoch 062 | train_loss=0.5392 train_acc=0.7790 | val_loss=0.7142 val_acc=0.7283 | time=17.8s\n",
            "[Fold 0] Epoch 063 | train_loss=0.4610 train_acc=0.8113 | val_loss=0.6741 val_acc=0.7453 | time=17.6s\n",
            "[Fold 0] Epoch 064 | train_loss=0.4712 train_acc=0.8074 | val_loss=0.6598 val_acc=0.7453 | time=18.0s\n",
            "[Fold 0] Epoch 065 | train_loss=0.4516 train_acc=0.8151 | val_loss=0.7919 val_acc=0.7360 | time=17.6s\n",
            "[Fold 0] Epoch 066 | train_loss=0.4798 train_acc=0.8012 | val_loss=0.7496 val_acc=0.7205 | time=17.7s\n",
            "[Fold 0] Epoch 067 | train_loss=0.4362 train_acc=0.8315 | val_loss=0.6825 val_acc=0.7453 | time=17.8s\n",
            "[Fold 0] Epoch 068 | train_loss=0.4264 train_acc=0.8276 | val_loss=0.6871 val_acc=0.7391 | time=17.6s\n",
            "[Fold 0] Epoch 069 | train_loss=0.4153 train_acc=0.8404 | val_loss=0.7538 val_acc=0.7391 | time=17.6s\n",
            "[Fold 0] Epoch 070 | train_loss=0.4415 train_acc=0.8249 | val_loss=0.7042 val_acc=0.7267 | time=17.6s\n",
            "[Fold 0] Epoch 071 | train_loss=0.3973 train_acc=0.8404 | val_loss=0.6839 val_acc=0.7438 | time=17.6s\n",
            "[Fold 0] Epoch 072 | train_loss=0.4039 train_acc=0.8450 | val_loss=0.7776 val_acc=0.7189 | time=18.0s\n",
            "[Fold 0] Epoch 073 | train_loss=0.3888 train_acc=0.8416 | val_loss=0.8382 val_acc=0.7345 | time=17.9s\n",
            "[Fold 0] Epoch 074 | train_loss=0.3843 train_acc=0.8489 | val_loss=0.7053 val_acc=0.7453 | time=17.6s\n",
            "[Fold 0] Epoch 075 | train_loss=0.3898 train_acc=0.8458 | val_loss=0.9349 val_acc=0.6863 | time=18.0s\n",
            "[Fold 0] Epoch 076 | train_loss=0.3616 train_acc=0.8583 | val_loss=0.7790 val_acc=0.7298 | time=17.6s\n",
            "[Fold 0] Epoch 077 | train_loss=0.3549 train_acc=0.8625 | val_loss=0.7820 val_acc=0.7360 | time=17.6s\n",
            "[Fold 0] Epoch 078 | train_loss=0.3688 train_acc=0.8579 | val_loss=0.7525 val_acc=0.7407 | time=17.7s\n",
            "[Fold 0] Epoch 079 | train_loss=0.3582 train_acc=0.8625 | val_loss=0.7837 val_acc=0.7438 | time=17.7s\n",
            "[Fold 0] Epoch 080 | train_loss=0.3547 train_acc=0.8614 | val_loss=0.8036 val_acc=0.7236 | time=18.0s\n",
            "[Fold 0] Epoch 081 | train_loss=0.3348 train_acc=0.8726 | val_loss=0.8500 val_acc=0.7050 | time=17.9s\n",
            "[Fold 0] Epoch 082 | train_loss=0.3219 train_acc=0.8773 | val_loss=0.8200 val_acc=0.7252 | time=17.9s\n",
            "[Fold 0] Epoch 083 | train_loss=0.2966 train_acc=0.8866 | val_loss=0.8061 val_acc=0.7252 | time=17.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 08:17:28,502] Trial 1 finished with value: 0.659756180076372 and parameters: {'lr': 4.9554878139823925e-05, 'weight_decay': 8.001690176093237e-05, 'rtm_blocks': 1, 'stm_blocks': 3, 'ttm_blocks': 2, 'rtm_heads': 2, 'stm_heads': 2, 'ttm_heads': 2, 'num_segments': 5}. Best is trial 1 with value: 0.659756180076372.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 084 | train_loss=0.3070 train_acc=0.8819 | val_loss=0.7979 val_acc=0.7438 | time=17.8s\n",
            "[Fold 0] Early stopping at epoch 84, best was 64\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">absurd-sunset-4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yo0ckwsp' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yo0ckwsp</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_075235-yo0ckwsp/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_081728-vq3z1a5l</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/vq3z1a5l' target=\"_blank\">faithful-universe-5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/vq3z1a5l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/vq3z1a5l</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 2 =========================\n",
            "Testing with hyperparameters: {'lr': 4.573338352305618e-05, 'weight_decay': 6.452939826551289e-05, 'rtm_blocks': 1, 'stm_blocks': 2, 'ttm_blocks': 3, 'rtm_heads': 2, 'stm_heads': 2, 'ttm_heads': 3, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0695 train_acc=0.4272 | val_loss=1.0843 val_acc=0.4317 | time=18.0s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0662 train_acc=0.4311 | val_loss=1.0843 val_acc=0.4317 | time=18.1s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0664 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=17.9s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0670 train_acc=0.4311 | val_loss=1.0826 val_acc=0.4317 | time=17.6s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0650 train_acc=0.4330 | val_loss=1.0809 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0398 train_acc=0.4835 | val_loss=0.9997 val_acc=0.5730 | time=17.5s\n",
            "[Fold 0] Epoch 007 | train_loss=0.9702 train_acc=0.5588 | val_loss=0.9545 val_acc=0.5885 | time=17.4s\n",
            "[Fold 0] Epoch 008 | train_loss=0.9485 train_acc=0.5775 | val_loss=0.9155 val_acc=0.6180 | time=17.6s\n",
            "[Fold 0] Epoch 009 | train_loss=0.9252 train_acc=0.5930 | val_loss=0.9166 val_acc=0.6102 | time=17.6s\n",
            "[Fold 0] Epoch 010 | train_loss=0.9086 train_acc=0.6004 | val_loss=0.8954 val_acc=0.6180 | time=17.5s\n",
            "[Fold 0] Epoch 011 | train_loss=0.8863 train_acc=0.6105 | val_loss=0.9018 val_acc=0.6149 | time=17.7s\n",
            "[Fold 0] Epoch 012 | train_loss=0.8772 train_acc=0.6186 | val_loss=0.8998 val_acc=0.6258 | time=17.5s\n",
            "[Fold 0] Epoch 013 | train_loss=0.8612 train_acc=0.6217 | val_loss=0.8478 val_acc=0.6320 | time=17.3s\n",
            "[Fold 0] Epoch 014 | train_loss=0.8572 train_acc=0.6237 | val_loss=0.9029 val_acc=0.6460 | time=17.9s\n",
            "[Fold 0] Epoch 015 | train_loss=0.8471 train_acc=0.6245 | val_loss=0.8272 val_acc=0.6335 | time=17.6s\n",
            "[Fold 0] Epoch 016 | train_loss=0.8297 train_acc=0.6299 | val_loss=0.8142 val_acc=0.6351 | time=17.3s\n",
            "[Fold 0] Epoch 017 | train_loss=0.8214 train_acc=0.6357 | val_loss=0.7951 val_acc=0.6475 | time=17.6s\n",
            "[Fold 0] Epoch 018 | train_loss=0.8169 train_acc=0.6400 | val_loss=0.8397 val_acc=0.6537 | time=17.6s\n",
            "[Fold 0] Epoch 019 | train_loss=0.7915 train_acc=0.6489 | val_loss=0.8149 val_acc=0.6693 | time=17.4s\n",
            "[Fold 0] Epoch 020 | train_loss=0.7823 train_acc=0.6493 | val_loss=0.7895 val_acc=0.6646 | time=17.5s\n",
            "[Fold 0] Epoch 021 | train_loss=0.7863 train_acc=0.6450 | val_loss=0.7911 val_acc=0.6630 | time=17.5s\n",
            "[Fold 0] Epoch 022 | train_loss=0.7695 train_acc=0.6470 | val_loss=0.7575 val_acc=0.6770 | time=17.9s\n",
            "[Fold 0] Epoch 023 | train_loss=0.7547 train_acc=0.6637 | val_loss=0.7616 val_acc=0.6677 | time=17.5s\n",
            "[Fold 0] Epoch 024 | train_loss=0.7338 train_acc=0.6773 | val_loss=0.7429 val_acc=0.6832 | time=17.4s\n",
            "[Fold 0] Epoch 025 | train_loss=0.7259 train_acc=0.6683 | val_loss=0.7416 val_acc=0.6941 | time=17.7s\n",
            "[Fold 0] Epoch 026 | train_loss=0.7267 train_acc=0.6718 | val_loss=0.7398 val_acc=0.6770 | time=17.6s\n",
            "[Fold 0] Epoch 027 | train_loss=0.7129 train_acc=0.6792 | val_loss=0.7067 val_acc=0.6910 | time=17.4s\n",
            "[Fold 0] Epoch 028 | train_loss=0.6989 train_acc=0.6823 | val_loss=0.6989 val_acc=0.6894 | time=17.5s\n",
            "[Fold 0] Epoch 029 | train_loss=0.7065 train_acc=0.6878 | val_loss=0.7124 val_acc=0.6879 | time=17.5s\n",
            "[Fold 0] Epoch 030 | train_loss=0.6790 train_acc=0.6948 | val_loss=0.6753 val_acc=0.7096 | time=17.8s\n",
            "[Fold 0] Epoch 031 | train_loss=0.6593 train_acc=0.7025 | val_loss=0.7220 val_acc=0.6817 | time=17.4s\n",
            "[Fold 0] Epoch 032 | train_loss=0.6455 train_acc=0.7099 | val_loss=0.6635 val_acc=0.7019 | time=17.6s\n",
            "[Fold 0] Epoch 033 | train_loss=0.6225 train_acc=0.7204 | val_loss=0.6856 val_acc=0.7252 | time=17.9s\n",
            "[Fold 0] Epoch 034 | train_loss=0.6243 train_acc=0.7142 | val_loss=0.6926 val_acc=0.7003 | time=17.5s\n",
            "[Fold 0] Epoch 035 | train_loss=0.6166 train_acc=0.7161 | val_loss=0.7186 val_acc=0.6848 | time=17.5s\n",
            "[Fold 0] Epoch 036 | train_loss=0.6209 train_acc=0.7184 | val_loss=0.7322 val_acc=0.6894 | time=17.6s\n",
            "[Fold 0] Epoch 037 | train_loss=0.5976 train_acc=0.7340 | val_loss=0.6654 val_acc=0.7174 | time=17.3s\n",
            "[Fold 0] Epoch 038 | train_loss=0.5774 train_acc=0.7421 | val_loss=0.6537 val_acc=0.7329 | time=17.6s\n",
            "[Fold 0] Epoch 039 | train_loss=0.5815 train_acc=0.7336 | val_loss=0.6955 val_acc=0.7252 | time=17.2s\n",
            "[Fold 0] Epoch 040 | train_loss=0.5684 train_acc=0.7495 | val_loss=0.6732 val_acc=0.7283 | time=17.7s\n",
            "[Fold 0] Epoch 041 | train_loss=0.5408 train_acc=0.7546 | val_loss=0.6988 val_acc=0.7314 | time=17.7s\n",
            "[Fold 0] Epoch 042 | train_loss=0.5462 train_acc=0.7511 | val_loss=0.7610 val_acc=0.7096 | time=17.7s\n",
            "[Fold 0] Epoch 043 | train_loss=0.5374 train_acc=0.7581 | val_loss=0.7119 val_acc=0.7267 | time=17.5s\n",
            "[Fold 0] Epoch 044 | train_loss=0.5260 train_acc=0.7678 | val_loss=0.6736 val_acc=0.7298 | time=17.6s\n",
            "[Fold 0] Epoch 045 | train_loss=0.5401 train_acc=0.7650 | val_loss=0.6529 val_acc=0.7391 | time=17.3s\n",
            "[Fold 0] Epoch 046 | train_loss=0.5244 train_acc=0.7604 | val_loss=0.6761 val_acc=0.7283 | time=17.7s\n",
            "[Fold 0] Epoch 047 | train_loss=0.5080 train_acc=0.7724 | val_loss=0.6499 val_acc=0.7236 | time=17.6s\n",
            "[Fold 0] Epoch 048 | train_loss=0.4965 train_acc=0.7720 | val_loss=0.6576 val_acc=0.7407 | time=17.5s\n",
            "[Fold 0] Epoch 049 | train_loss=0.4813 train_acc=0.7891 | val_loss=0.6319 val_acc=0.6941 | time=17.8s\n",
            "[Fold 0] Epoch 050 | train_loss=0.4852 train_acc=0.7794 | val_loss=0.7412 val_acc=0.6894 | time=17.7s\n",
            "[Fold 0] Epoch 051 | train_loss=0.4620 train_acc=0.7876 | val_loss=0.6710 val_acc=0.7453 | time=17.6s\n",
            "[Fold 0] Epoch 052 | train_loss=0.4790 train_acc=0.7849 | val_loss=0.6144 val_acc=0.7391 | time=17.7s\n",
            "[Fold 0] Epoch 053 | train_loss=0.4622 train_acc=0.7926 | val_loss=0.6315 val_acc=0.7345 | time=17.6s\n",
            "[Fold 0] Epoch 054 | train_loss=0.4458 train_acc=0.8008 | val_loss=0.6710 val_acc=0.7376 | time=17.7s\n",
            "[Fold 0] Epoch 055 | train_loss=0.4372 train_acc=0.8062 | val_loss=0.6500 val_acc=0.7205 | time=17.5s\n",
            "[Fold 0] Epoch 056 | train_loss=0.4404 train_acc=0.8027 | val_loss=0.6709 val_acc=0.7143 | time=17.6s\n",
            "[Fold 0] Epoch 057 | train_loss=0.4244 train_acc=0.8132 | val_loss=0.7048 val_acc=0.7500 | time=17.5s\n",
            "[Fold 0] Epoch 058 | train_loss=0.3897 train_acc=0.8252 | val_loss=0.7320 val_acc=0.7314 | time=17.4s\n",
            "[Fold 0] Epoch 059 | train_loss=0.4176 train_acc=0.8120 | val_loss=0.7365 val_acc=0.7360 | time=17.3s\n",
            "[Fold 0] Epoch 060 | train_loss=0.4165 train_acc=0.8198 | val_loss=0.7113 val_acc=0.7484 | time=17.8s\n",
            "[Fold 0] Epoch 061 | train_loss=0.4089 train_acc=0.8233 | val_loss=0.7118 val_acc=0.7034 | time=17.3s\n",
            "[Fold 0] Epoch 062 | train_loss=0.4012 train_acc=0.8318 | val_loss=0.7162 val_acc=0.7531 | time=17.6s\n",
            "[Fold 0] Epoch 063 | train_loss=0.3765 train_acc=0.8338 | val_loss=0.6447 val_acc=0.7562 | time=17.8s\n",
            "[Fold 0] Epoch 064 | train_loss=0.3749 train_acc=0.8392 | val_loss=0.7196 val_acc=0.7578 | time=17.6s\n",
            "[Fold 0] Epoch 065 | train_loss=0.3764 train_acc=0.8365 | val_loss=0.7551 val_acc=0.7283 | time=17.8s\n",
            "[Fold 0] Epoch 066 | train_loss=0.3476 train_acc=0.8427 | val_loss=0.8245 val_acc=0.7422 | time=17.7s\n",
            "[Fold 0] Epoch 067 | train_loss=0.3667 train_acc=0.8419 | val_loss=0.7041 val_acc=0.7267 | time=17.8s\n",
            "[Fold 0] Epoch 068 | train_loss=0.3785 train_acc=0.8377 | val_loss=0.7447 val_acc=0.7562 | time=18.2s\n",
            "[Fold 0] Epoch 069 | train_loss=0.3197 train_acc=0.8579 | val_loss=0.7168 val_acc=0.7578 | time=17.8s\n",
            "[Fold 0] Epoch 070 | train_loss=0.3123 train_acc=0.8695 | val_loss=0.6786 val_acc=0.7391 | time=17.6s\n",
            "[Fold 0] Epoch 071 | train_loss=0.3014 train_acc=0.8687 | val_loss=0.7348 val_acc=0.7469 | time=17.6s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 08:38:38,644] Trial 2 finished with value: 0.6143529968602317 and parameters: {'lr': 4.573338352305618e-05, 'weight_decay': 6.452939826551289e-05, 'rtm_blocks': 1, 'stm_blocks': 2, 'ttm_blocks': 3, 'rtm_heads': 2, 'stm_heads': 2, 'ttm_heads': 3, 'num_segments': 5}. Best is trial 2 with value: 0.6143529968602317.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 072 | train_loss=0.2957 train_acc=0.8734 | val_loss=0.7236 val_acc=0.7609 | time=17.2s\n",
            "[Fold 0] Early stopping at epoch 72, best was 52\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">faithful-universe-5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/vq3z1a5l' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/vq3z1a5l</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_081728-vq3z1a5l/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_083838-dea00sy2</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/dea00sy2' target=\"_blank\">stellar-plant-6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/dea00sy2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/dea00sy2</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 3 =========================\n",
            "Testing with hyperparameters: {'lr': 4.6507943057617634e-05, 'weight_decay': 5.965386496463809e-05, 'rtm_blocks': 2, 'stm_blocks': 2, 'ttm_blocks': 1, 'rtm_heads': 2, 'stm_heads': 3, 'ttm_heads': 3, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0724 train_acc=0.4237 | val_loss=1.0827 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0665 train_acc=0.4311 | val_loss=1.0806 val_acc=0.4317 | time=17.9s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0670 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=18.0s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0665 train_acc=0.4346 | val_loss=1.0801 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0662 train_acc=0.4311 | val_loss=1.0803 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0669 train_acc=0.4311 | val_loss=1.0760 val_acc=0.4317 | time=17.8s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0528 train_acc=0.4641 | val_loss=1.0627 val_acc=0.4736 | time=17.7s\n",
            "[Fold 0] Epoch 008 | train_loss=1.0134 train_acc=0.5309 | val_loss=0.9895 val_acc=0.5776 | time=18.0s\n",
            "[Fold 0] Epoch 009 | train_loss=0.9951 train_acc=0.5421 | val_loss=0.9741 val_acc=0.5792 | time=17.8s\n",
            "[Fold 0] Epoch 010 | train_loss=0.9721 train_acc=0.5530 | val_loss=0.9821 val_acc=0.5637 | time=17.7s\n",
            "[Fold 0] Epoch 011 | train_loss=0.9532 train_acc=0.5678 | val_loss=1.0092 val_acc=0.5373 | time=18.2s\n",
            "[Fold 0] Epoch 012 | train_loss=0.9491 train_acc=0.5802 | val_loss=0.9337 val_acc=0.6102 | time=18.0s\n",
            "[Fold 0] Epoch 013 | train_loss=0.9177 train_acc=0.5953 | val_loss=0.8961 val_acc=0.6211 | time=18.2s\n",
            "[Fold 0] Epoch 014 | train_loss=0.8866 train_acc=0.6128 | val_loss=0.8595 val_acc=0.6413 | time=17.8s\n",
            "[Fold 0] Epoch 015 | train_loss=0.8979 train_acc=0.5942 | val_loss=0.8555 val_acc=0.6335 | time=18.1s\n",
            "[Fold 0] Epoch 016 | train_loss=0.8858 train_acc=0.6128 | val_loss=0.8461 val_acc=0.6289 | time=17.9s\n",
            "[Fold 0] Epoch 017 | train_loss=0.8554 train_acc=0.6241 | val_loss=0.8337 val_acc=0.6398 | time=17.7s\n",
            "[Fold 0] Epoch 018 | train_loss=0.8291 train_acc=0.6450 | val_loss=0.8202 val_acc=0.6568 | time=18.4s\n",
            "[Fold 0] Epoch 019 | train_loss=0.8159 train_acc=0.6474 | val_loss=0.8353 val_acc=0.6429 | time=18.2s\n",
            "[Fold 0] Epoch 020 | train_loss=0.7856 train_acc=0.6629 | val_loss=0.8053 val_acc=0.6615 | time=18.0s\n",
            "[Fold 0] Epoch 021 | train_loss=0.7926 train_acc=0.6548 | val_loss=0.7741 val_acc=0.6770 | time=18.1s\n",
            "[Fold 0] Epoch 022 | train_loss=0.7672 train_acc=0.6711 | val_loss=0.7715 val_acc=0.6817 | time=17.7s\n",
            "[Fold 0] Epoch 023 | train_loss=0.7497 train_acc=0.6687 | val_loss=0.8111 val_acc=0.6568 | time=18.1s\n",
            "[Fold 0] Epoch 024 | train_loss=0.7346 train_acc=0.6664 | val_loss=0.7418 val_acc=0.7050 | time=17.9s\n",
            "[Fold 0] Epoch 025 | train_loss=0.7307 train_acc=0.6796 | val_loss=0.8217 val_acc=0.6149 | time=17.7s\n",
            "[Fold 0] Epoch 026 | train_loss=0.7168 train_acc=0.6800 | val_loss=0.7456 val_acc=0.7003 | time=17.8s\n",
            "[Fold 0] Epoch 027 | train_loss=0.6958 train_acc=0.7006 | val_loss=0.7180 val_acc=0.7065 | time=18.0s\n",
            "[Fold 0] Epoch 028 | train_loss=0.7063 train_acc=0.6839 | val_loss=0.7567 val_acc=0.6910 | time=17.8s\n",
            "[Fold 0] Epoch 029 | train_loss=0.6902 train_acc=0.6913 | val_loss=0.7572 val_acc=0.6398 | time=18.0s\n",
            "[Fold 0] Epoch 030 | train_loss=0.6802 train_acc=0.7083 | val_loss=0.7448 val_acc=0.6941 | time=17.8s\n",
            "[Fold 0] Epoch 031 | train_loss=0.6566 train_acc=0.7196 | val_loss=0.7637 val_acc=0.6832 | time=17.9s\n",
            "[Fold 0] Epoch 032 | train_loss=0.6432 train_acc=0.7138 | val_loss=0.7391 val_acc=0.7065 | time=17.7s\n",
            "[Fold 0] Epoch 033 | train_loss=0.6468 train_acc=0.7153 | val_loss=0.6927 val_acc=0.7034 | time=18.0s\n",
            "[Fold 0] Epoch 034 | train_loss=0.6417 train_acc=0.7247 | val_loss=0.6970 val_acc=0.7112 | time=17.9s\n",
            "[Fold 0] Epoch 035 | train_loss=0.6230 train_acc=0.7371 | val_loss=0.7010 val_acc=0.7252 | time=17.8s\n",
            "[Fold 0] Epoch 036 | train_loss=0.6123 train_acc=0.7386 | val_loss=0.7324 val_acc=0.7158 | time=17.9s\n",
            "[Fold 0] Epoch 037 | train_loss=0.6112 train_acc=0.7332 | val_loss=0.7506 val_acc=0.7143 | time=17.9s\n",
            "[Fold 0] Epoch 038 | train_loss=0.6111 train_acc=0.7452 | val_loss=0.7002 val_acc=0.7112 | time=17.9s\n",
            "[Fold 0] Epoch 039 | train_loss=0.5716 train_acc=0.7569 | val_loss=0.7546 val_acc=0.7252 | time=18.0s\n",
            "[Fold 0] Epoch 040 | train_loss=0.5754 train_acc=0.7495 | val_loss=0.7453 val_acc=0.7252 | time=18.1s\n",
            "[Fold 0] Epoch 041 | train_loss=0.5691 train_acc=0.7588 | val_loss=0.7106 val_acc=0.7252 | time=17.9s\n",
            "[Fold 0] Epoch 042 | train_loss=0.5714 train_acc=0.7550 | val_loss=0.8282 val_acc=0.6925 | time=17.9s\n",
            "[Fold 0] Epoch 043 | train_loss=0.5367 train_acc=0.7732 | val_loss=0.7411 val_acc=0.7065 | time=18.1s\n",
            "[Fold 0] Epoch 044 | train_loss=0.5471 train_acc=0.7736 | val_loss=0.8098 val_acc=0.7220 | time=17.7s\n",
            "[Fold 0] Epoch 045 | train_loss=0.5217 train_acc=0.7794 | val_loss=0.6999 val_acc=0.7345 | time=18.5s\n",
            "[Fold 0] Epoch 046 | train_loss=0.5204 train_acc=0.7732 | val_loss=0.7423 val_acc=0.7298 | time=18.2s\n",
            "[Fold 0] Epoch 047 | train_loss=0.4935 train_acc=0.7922 | val_loss=0.7201 val_acc=0.7407 | time=18.2s\n",
            "[Fold 0] Epoch 048 | train_loss=0.4683 train_acc=0.8128 | val_loss=0.6996 val_acc=0.7283 | time=18.3s\n",
            "[Fold 0] Epoch 049 | train_loss=0.4788 train_acc=0.7988 | val_loss=0.7659 val_acc=0.7360 | time=17.8s\n",
            "[Fold 0] Epoch 050 | train_loss=0.4750 train_acc=0.8016 | val_loss=0.7079 val_acc=0.7407 | time=18.2s\n",
            "[Fold 0] Epoch 051 | train_loss=0.4522 train_acc=0.8117 | val_loss=0.7149 val_acc=0.7484 | time=17.8s\n",
            "[Fold 0] Epoch 052 | train_loss=0.4497 train_acc=0.8214 | val_loss=0.6953 val_acc=0.7360 | time=18.0s\n",
            "[Fold 0] Epoch 053 | train_loss=0.4272 train_acc=0.8210 | val_loss=0.6842 val_acc=0.7422 | time=18.2s\n",
            "[Fold 0] Epoch 054 | train_loss=0.4222 train_acc=0.8307 | val_loss=0.7110 val_acc=0.7438 | time=17.9s\n",
            "[Fold 0] Epoch 055 | train_loss=0.4214 train_acc=0.8233 | val_loss=0.7038 val_acc=0.7578 | time=18.2s\n",
            "[Fold 0] Epoch 056 | train_loss=0.3865 train_acc=0.8466 | val_loss=0.7472 val_acc=0.7422 | time=17.5s\n",
            "[Fold 0] Epoch 057 | train_loss=0.3859 train_acc=0.8485 | val_loss=0.7886 val_acc=0.7236 | time=17.9s\n",
            "[Fold 0] Epoch 058 | train_loss=0.3657 train_acc=0.8579 | val_loss=0.8378 val_acc=0.6925 | time=18.0s\n",
            "[Fold 0] Epoch 059 | train_loss=0.3982 train_acc=0.8443 | val_loss=0.7042 val_acc=0.7391 | time=17.8s\n",
            "[Fold 0] Epoch 060 | train_loss=0.3821 train_acc=0.8462 | val_loss=0.7018 val_acc=0.7298 | time=17.9s\n",
            "[Fold 0] Epoch 061 | train_loss=0.3570 train_acc=0.8559 | val_loss=0.7255 val_acc=0.7314 | time=17.5s\n",
            "[Fold 0] Epoch 062 | train_loss=0.3494 train_acc=0.8602 | val_loss=0.7647 val_acc=0.7314 | time=17.8s\n",
            "[Fold 0] Epoch 063 | train_loss=0.3037 train_acc=0.8831 | val_loss=0.7898 val_acc=0.7531 | time=17.9s\n",
            "[Fold 0] Epoch 064 | train_loss=0.3208 train_acc=0.8753 | val_loss=0.7936 val_acc=0.7345 | time=17.9s\n",
            "[Fold 0] Epoch 065 | train_loss=0.3257 train_acc=0.8757 | val_loss=0.7838 val_acc=0.7438 | time=18.0s\n",
            "[Fold 0] Epoch 066 | train_loss=0.2995 train_acc=0.8788 | val_loss=0.8374 val_acc=0.7422 | time=18.1s\n",
            "[Fold 0] Epoch 067 | train_loss=0.2877 train_acc=0.8878 | val_loss=0.7435 val_acc=0.7562 | time=17.6s\n",
            "[Fold 0] Epoch 068 | train_loss=0.2689 train_acc=0.9002 | val_loss=0.8010 val_acc=0.7298 | time=17.9s\n",
            "[Fold 0] Epoch 069 | train_loss=0.2737 train_acc=0.9002 | val_loss=0.9143 val_acc=0.6925 | time=17.8s\n",
            "[Fold 0] Epoch 070 | train_loss=0.2596 train_acc=0.9041 | val_loss=0.8419 val_acc=0.7531 | time=18.0s\n",
            "[Fold 0] Epoch 071 | train_loss=0.2543 train_acc=0.8990 | val_loss=0.8409 val_acc=0.7205 | time=17.8s\n",
            "[Fold 0] Epoch 072 | train_loss=0.2309 train_acc=0.9161 | val_loss=0.9132 val_acc=0.7267 | time=17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 09:00:30,740] Trial 3 finished with value: 0.6841881253889629 and parameters: {'lr': 4.6507943057617634e-05, 'weight_decay': 5.965386496463809e-05, 'rtm_blocks': 2, 'stm_blocks': 2, 'ttm_blocks': 1, 'rtm_heads': 2, 'stm_heads': 3, 'ttm_heads': 3, 'num_segments': 5}. Best is trial 2 with value: 0.6143529968602317.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 0] Epoch 073 | train_loss=0.2454 train_acc=0.9076 | val_loss=0.8950 val_acc=0.7469 | time=18.0s\n",
            "[Fold 0] Early stopping at epoch 73, best was 53\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">stellar-plant-6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/dea00sy2' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/dea00sy2</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_083838-dea00sy2/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_090030-n12wu855</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/n12wu855' target=\"_blank\">icy-tree-7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/n12wu855' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/n12wu855</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 4 =========================\n",
            "Testing with hyperparameters: {'lr': 3.230793658846425e-05, 'weight_decay': 5.926736468642624e-05, 'rtm_blocks': 3, 'stm_blocks': 3, 'ttm_blocks': 3, 'rtm_heads': 3, 'stm_heads': 3, 'ttm_heads': 2, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0760 train_acc=0.4190 | val_loss=1.0810 val_acc=0.4317 | time=19.2s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0667 train_acc=0.4311 | val_loss=1.0843 val_acc=0.4317 | time=19.2s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0673 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=18.9s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0674 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=19.1s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0661 train_acc=0.4311 | val_loss=1.0794 val_acc=0.4317 | time=19.0s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0636 train_acc=0.4342 | val_loss=1.0596 val_acc=0.4876 | time=19.4s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0326 train_acc=0.5091 | val_loss=1.0104 val_acc=0.5512 | time=19.0s\n",
            "[Fold 0] Epoch 008 | train_loss=0.9775 train_acc=0.5647 | val_loss=0.9602 val_acc=0.5978 | time=19.2s\n",
            "[Fold 0] Epoch 009 | train_loss=0.9517 train_acc=0.5786 | val_loss=0.9570 val_acc=0.5994 | time=19.1s\n",
            "[Fold 0] Epoch 010 | train_loss=0.9215 train_acc=0.5973 | val_loss=0.9132 val_acc=0.6242 | time=19.3s\n",
            "[Fold 0] Epoch 011 | train_loss=0.8902 train_acc=0.6093 | val_loss=0.9094 val_acc=0.6102 | time=19.2s\n",
            "[Fold 0] Epoch 012 | train_loss=0.8697 train_acc=0.6163 | val_loss=0.9012 val_acc=0.6149 | time=19.3s\n",
            "[Fold 0] Epoch 013 | train_loss=0.8519 train_acc=0.6194 | val_loss=0.8710 val_acc=0.6444 | time=19.1s\n",
            "[Fold 0] Epoch 014 | train_loss=0.8318 train_acc=0.6388 | val_loss=0.8656 val_acc=0.6366 | time=19.1s\n",
            "[Fold 0] Epoch 015 | train_loss=0.8228 train_acc=0.6388 | val_loss=0.8658 val_acc=0.6444 | time=19.2s\n",
            "[Fold 0] Epoch 016 | train_loss=0.8022 train_acc=0.6497 | val_loss=0.8360 val_acc=0.6708 | time=19.3s\n",
            "[Fold 0] Epoch 017 | train_loss=0.7998 train_acc=0.6548 | val_loss=0.8574 val_acc=0.6351 | time=19.1s\n",
            "[Fold 0] Epoch 018 | train_loss=0.7973 train_acc=0.6548 | val_loss=0.8590 val_acc=0.6599 | time=19.2s\n",
            "[Fold 0] Epoch 019 | train_loss=0.7821 train_acc=0.6629 | val_loss=0.8574 val_acc=0.6366 | time=19.2s\n",
            "[Fold 0] Epoch 020 | train_loss=0.7668 train_acc=0.6606 | val_loss=0.8028 val_acc=0.7189 | time=19.3s\n",
            "[Fold 0] Epoch 021 | train_loss=0.7485 train_acc=0.6847 | val_loss=0.8067 val_acc=0.6801 | time=19.1s\n",
            "[Fold 0] Epoch 022 | train_loss=0.7398 train_acc=0.6788 | val_loss=0.8123 val_acc=0.6755 | time=19.2s\n",
            "[Fold 0] Epoch 023 | train_loss=0.7309 train_acc=0.6839 | val_loss=0.8315 val_acc=0.6708 | time=19.1s\n",
            "[Fold 0] Epoch 024 | train_loss=0.7231 train_acc=0.6913 | val_loss=0.7787 val_acc=0.7158 | time=19.4s\n",
            "[Fold 0] Epoch 025 | train_loss=0.7462 train_acc=0.6835 | val_loss=0.7981 val_acc=0.7019 | time=19.2s\n",
            "[Fold 0] Epoch 026 | train_loss=0.6914 train_acc=0.7068 | val_loss=0.7709 val_acc=0.7158 | time=19.3s\n",
            "[Fold 0] Epoch 027 | train_loss=0.6710 train_acc=0.7285 | val_loss=0.7608 val_acc=0.7143 | time=19.0s\n",
            "[Fold 0] Epoch 028 | train_loss=0.6674 train_acc=0.7235 | val_loss=0.7727 val_acc=0.7283 | time=19.2s\n",
            "[Fold 0] Epoch 029 | train_loss=0.6623 train_acc=0.7293 | val_loss=0.7724 val_acc=0.6988 | time=19.1s\n",
            "[Fold 0] Epoch 030 | train_loss=0.6385 train_acc=0.7375 | val_loss=0.7534 val_acc=0.7298 | time=19.2s\n",
            "[Fold 0] Epoch 031 | train_loss=0.6404 train_acc=0.7309 | val_loss=0.7206 val_acc=0.7391 | time=19.1s\n",
            "[Fold 0] Epoch 032 | train_loss=0.6371 train_acc=0.7386 | val_loss=0.7313 val_acc=0.7112 | time=19.2s\n",
            "[Fold 0] Epoch 033 | train_loss=0.6342 train_acc=0.7394 | val_loss=0.7384 val_acc=0.7500 | time=19.2s\n",
            "[Fold 0] Epoch 034 | train_loss=0.6043 train_acc=0.7565 | val_loss=0.7207 val_acc=0.7174 | time=19.2s\n",
            "[Fold 0] Epoch 035 | train_loss=0.6014 train_acc=0.7561 | val_loss=0.7439 val_acc=0.7236 | time=19.0s\n",
            "[Fold 0] Epoch 036 | train_loss=0.5742 train_acc=0.7654 | val_loss=0.7208 val_acc=0.7422 | time=19.2s\n",
            "[Fold 0] Epoch 037 | train_loss=0.5785 train_acc=0.7720 | val_loss=0.7253 val_acc=0.7407 | time=19.1s\n",
            "[Fold 0] Epoch 038 | train_loss=0.5431 train_acc=0.7814 | val_loss=0.7519 val_acc=0.7314 | time=19.2s\n",
            "[Fold 0] Epoch 039 | train_loss=0.5519 train_acc=0.7779 | val_loss=0.7207 val_acc=0.7314 | time=19.1s\n",
            "[Fold 0] Epoch 040 | train_loss=0.5292 train_acc=0.7876 | val_loss=0.7120 val_acc=0.7298 | time=19.2s\n",
            "[Fold 0] Epoch 041 | train_loss=0.5262 train_acc=0.7903 | val_loss=0.7211 val_acc=0.7298 | time=19.0s\n",
            "[Fold 0] Epoch 042 | train_loss=0.5532 train_acc=0.7744 | val_loss=0.7080 val_acc=0.7189 | time=19.3s\n",
            "[Fold 0] Epoch 043 | train_loss=0.5222 train_acc=0.7876 | val_loss=0.6930 val_acc=0.7500 | time=19.0s\n",
            "[Fold 0] Epoch 044 | train_loss=0.5050 train_acc=0.7961 | val_loss=0.6945 val_acc=0.7360 | time=19.2s\n",
            "[Fold 0] Epoch 045 | train_loss=0.5168 train_acc=0.7891 | val_loss=0.7811 val_acc=0.7236 | time=19.0s\n",
            "[Fold 0] Epoch 046 | train_loss=0.5118 train_acc=0.7961 | val_loss=0.7027 val_acc=0.7531 | time=19.2s\n",
            "[Fold 0] Epoch 047 | train_loss=0.4808 train_acc=0.8012 | val_loss=0.7418 val_acc=0.7609 | time=19.0s\n",
            "[Fold 0] Epoch 048 | train_loss=0.4786 train_acc=0.8109 | val_loss=0.7504 val_acc=0.7174 | time=19.0s\n",
            "[Fold 0] Epoch 049 | train_loss=0.4707 train_acc=0.8078 | val_loss=0.6843 val_acc=0.7391 | time=19.0s\n",
            "[Fold 0] Epoch 050 | train_loss=0.4655 train_acc=0.8159 | val_loss=0.6800 val_acc=0.7717 | time=19.7s\n",
            "[Fold 0] Epoch 051 | train_loss=0.4677 train_acc=0.8136 | val_loss=0.7219 val_acc=0.7376 | time=19.1s\n",
            "[Fold 0] Epoch 052 | train_loss=0.4571 train_acc=0.8167 | val_loss=0.7070 val_acc=0.7531 | time=20.0s\n",
            "[Fold 0] Epoch 053 | train_loss=0.4458 train_acc=0.8272 | val_loss=0.6945 val_acc=0.7267 | time=19.4s\n",
            "[Fold 0] Epoch 054 | train_loss=0.4510 train_acc=0.8175 | val_loss=0.6638 val_acc=0.7329 | time=19.2s\n",
            "[Fold 0] Epoch 055 | train_loss=0.4424 train_acc=0.8206 | val_loss=0.6913 val_acc=0.7562 | time=19.6s\n",
            "[Fold 0] Epoch 056 | train_loss=0.4173 train_acc=0.8353 | val_loss=0.6542 val_acc=0.7671 | time=19.6s\n",
            "[Fold 0] Epoch 057 | train_loss=0.4125 train_acc=0.8381 | val_loss=0.6516 val_acc=0.7671 | time=19.6s\n",
            "[Fold 0] Epoch 058 | train_loss=0.4042 train_acc=0.8400 | val_loss=0.7338 val_acc=0.7236 | time=19.9s\n",
            "[Fold 0] Epoch 059 | train_loss=0.4024 train_acc=0.8330 | val_loss=0.7408 val_acc=0.7671 | time=20.0s\n",
            "[Fold 0] Epoch 060 | train_loss=0.3950 train_acc=0.8474 | val_loss=0.6547 val_acc=0.7640 | time=20.1s\n",
            "[Fold 0] Epoch 061 | train_loss=0.3802 train_acc=0.8392 | val_loss=0.7742 val_acc=0.7578 | time=20.2s\n",
            "[Fold 0] Epoch 062 | train_loss=0.3710 train_acc=0.8571 | val_loss=0.6825 val_acc=0.7593 | time=19.9s\n",
            "[Fold 0] Epoch 063 | train_loss=0.3740 train_acc=0.8423 | val_loss=0.6706 val_acc=0.7624 | time=20.1s\n",
            "[Fold 0] Epoch 064 | train_loss=0.3504 train_acc=0.8598 | val_loss=0.6999 val_acc=0.7578 | time=20.0s\n",
            "[Fold 0] Epoch 065 | train_loss=0.3230 train_acc=0.8637 | val_loss=0.7889 val_acc=0.7640 | time=20.0s\n",
            "[Fold 0] Epoch 066 | train_loss=0.3574 train_acc=0.8637 | val_loss=0.6332 val_acc=0.7562 | time=20.1s\n",
            "[Fold 0] Epoch 067 | train_loss=0.3444 train_acc=0.8606 | val_loss=0.7364 val_acc=0.7671 | time=20.0s\n",
            "[Fold 0] Epoch 068 | train_loss=0.3182 train_acc=0.8722 | val_loss=0.7759 val_acc=0.7547 | time=20.3s\n",
            "[Fold 0] Epoch 069 | train_loss=0.3223 train_acc=0.8695 | val_loss=0.6500 val_acc=0.7748 | time=19.9s\n",
            "[Fold 0] Epoch 070 | train_loss=0.3018 train_acc=0.8812 | val_loss=0.7354 val_acc=0.7655 | time=20.1s\n",
            "[Fold 0] Epoch 071 | train_loss=0.3123 train_acc=0.8730 | val_loss=0.6577 val_acc=0.7655 | time=20.0s\n",
            "[Fold 0] Epoch 072 | train_loss=0.3044 train_acc=0.8839 | val_loss=0.7811 val_acc=0.7469 | time=20.3s\n",
            "[Fold 0] Epoch 073 | train_loss=0.3125 train_acc=0.8757 | val_loss=0.6302 val_acc=0.7780 | time=20.3s\n",
            "[Fold 0] Epoch 074 | train_loss=0.2829 train_acc=0.8909 | val_loss=0.7239 val_acc=0.7453 | time=20.1s\n",
            "[Fold 0] Epoch 075 | train_loss=0.2424 train_acc=0.9045 | val_loss=0.8806 val_acc=0.7438 | time=20.2s\n",
            "[Fold 0] Epoch 076 | train_loss=0.2606 train_acc=0.9002 | val_loss=0.6792 val_acc=0.7857 | time=20.0s\n",
            "[Fold 0] Epoch 077 | train_loss=0.2358 train_acc=0.9049 | val_loss=0.6944 val_acc=0.7593 | time=20.3s\n",
            "[Fold 0] Epoch 078 | train_loss=0.2670 train_acc=0.9002 | val_loss=0.7196 val_acc=0.7764 | time=19.9s\n",
            "[Fold 0] Epoch 079 | train_loss=0.2360 train_acc=0.9107 | val_loss=0.7396 val_acc=0.7578 | time=20.1s\n",
            "[Fold 0] Epoch 080 | train_loss=0.2436 train_acc=0.8994 | val_loss=0.8290 val_acc=0.7748 | time=20.2s\n",
            "[Fold 0] Epoch 081 | train_loss=0.2184 train_acc=0.9103 | val_loss=0.6914 val_acc=0.7780 | time=20.0s\n",
            "[Fold 0] Epoch 082 | train_loss=0.2652 train_acc=0.8944 | val_loss=0.7270 val_acc=0.7717 | time=20.2s\n",
            "[Fold 0] Epoch 083 | train_loss=0.1818 train_acc=0.9293 | val_loss=0.7678 val_acc=0.7578 | time=20.0s\n",
            "[Fold 0] Epoch 084 | train_loss=0.2226 train_acc=0.9153 | val_loss=0.6697 val_acc=0.7997 | time=20.2s\n",
            "[Fold 0] Epoch 085 | train_loss=0.2222 train_acc=0.9099 | val_loss=0.7918 val_acc=0.7826 | time=20.0s\n",
            "[Fold 0] Epoch 086 | train_loss=0.1634 train_acc=0.9355 | val_loss=0.8373 val_acc=0.7733 | time=20.2s\n",
            "[Fold 0] Epoch 087 | train_loss=0.2169 train_acc=0.9169 | val_loss=0.8619 val_acc=0.7438 | time=20.0s\n",
            "[Fold 0] Epoch 088 | train_loss=0.1661 train_acc=0.9379 | val_loss=0.7504 val_acc=0.7593 | time=20.0s\n",
            "[Fold 0] Epoch 089 | train_loss=0.1503 train_acc=0.9429 | val_loss=0.7814 val_acc=0.7795 | time=20.3s\n",
            "[Fold 0] Epoch 090 | train_loss=0.1555 train_acc=0.9406 | val_loss=0.8560 val_acc=0.7671 | time=19.9s\n",
            "[Fold 0] Epoch 091 | train_loss=0.1965 train_acc=0.9254 | val_loss=0.7944 val_acc=0.7842 | time=20.1s\n",
            "[Fold 0] Epoch 092 | train_loss=0.2469 train_acc=0.9153 | val_loss=0.6281 val_acc=0.7997 | time=19.9s\n",
            "[Fold 0] Epoch 093 | train_loss=0.1324 train_acc=0.9499 | val_loss=0.8475 val_acc=0.7873 | time=20.1s\n",
            "[Fold 0] Epoch 094 | train_loss=0.1965 train_acc=0.9231 | val_loss=0.7204 val_acc=0.7888 | time=20.1s\n",
            "[Fold 0] Epoch 095 | train_loss=0.1821 train_acc=0.9266 | val_loss=0.7589 val_acc=0.7671 | time=20.0s\n",
            "[Fold 0] Epoch 096 | train_loss=0.1230 train_acc=0.9546 | val_loss=0.8837 val_acc=0.7919 | time=20.3s\n",
            "[Fold 0] Epoch 097 | train_loss=0.0996 train_acc=0.9631 | val_loss=0.9145 val_acc=0.7950 | time=19.9s\n",
            "[Fold 0] Epoch 098 | train_loss=0.1266 train_acc=0.9518 | val_loss=0.7083 val_acc=0.8106 | time=20.2s\n",
            "[Fold 0] Epoch 099 | train_loss=0.1236 train_acc=0.9546 | val_loss=0.8237 val_acc=0.7842 | time=20.1s\n",
            "[Fold 0] Epoch 100 | train_loss=0.1028 train_acc=0.9627 | val_loss=0.8166 val_acc=0.7935 | time=20.0s\n",
            "\n",
            "========================= Fold 1 =========================\n",
            "[Fold 1] Epoch 001 | train_loss=1.0700 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=19.2s\n",
            "[Fold 1] Epoch 002 | train_loss=1.0664 train_acc=0.4311 | val_loss=1.0845 val_acc=0.4317 | time=19.2s\n",
            "[Fold 1] Epoch 003 | train_loss=1.0671 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=19.2s\n",
            "[Fold 1] Epoch 004 | train_loss=1.0664 train_acc=0.4311 | val_loss=1.0834 val_acc=0.4317 | time=19.3s\n",
            "[Fold 1] Epoch 005 | train_loss=1.0663 train_acc=0.4311 | val_loss=1.0824 val_acc=0.4317 | time=19.0s\n",
            "[Fold 1] Epoch 006 | train_loss=1.0656 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=19.3s\n",
            "[Fold 1] Epoch 007 | train_loss=1.0603 train_acc=0.4493 | val_loss=1.0561 val_acc=0.4689 | time=19.0s\n",
            "[Fold 1] Epoch 008 | train_loss=1.0082 train_acc=0.5278 | val_loss=0.9930 val_acc=0.5559 | time=19.5s\n",
            "[Fold 1] Epoch 009 | train_loss=0.9822 train_acc=0.5499 | val_loss=0.9698 val_acc=0.5714 | time=19.1s\n",
            "[Fold 1] Epoch 010 | train_loss=0.9740 train_acc=0.5511 | val_loss=0.9684 val_acc=0.5761 | time=19.3s\n",
            "[Fold 1] Epoch 011 | train_loss=0.9614 train_acc=0.5604 | val_loss=0.9436 val_acc=0.5854 | time=19.1s\n",
            "[Fold 1] Epoch 012 | train_loss=0.9430 train_acc=0.5682 | val_loss=0.9374 val_acc=0.5932 | time=19.2s\n",
            "[Fold 1] Epoch 013 | train_loss=0.9377 train_acc=0.5751 | val_loss=0.9223 val_acc=0.5932 | time=19.2s\n",
            "[Fold 1] Epoch 014 | train_loss=0.9302 train_acc=0.5821 | val_loss=0.9115 val_acc=0.5947 | time=19.2s\n",
            "[Fold 1] Epoch 015 | train_loss=0.9078 train_acc=0.5938 | val_loss=0.9426 val_acc=0.6025 | time=19.1s\n",
            "[Fold 1] Epoch 016 | train_loss=0.9058 train_acc=0.5911 | val_loss=0.9751 val_acc=0.5730 | time=19.3s\n",
            "[Fold 1] Epoch 017 | train_loss=0.8944 train_acc=0.6031 | val_loss=0.8881 val_acc=0.6149 | time=19.1s\n",
            "[Fold 1] Epoch 018 | train_loss=0.8923 train_acc=0.6089 | val_loss=0.8739 val_acc=0.6366 | time=19.3s\n",
            "[Fold 1] Epoch 019 | train_loss=0.8739 train_acc=0.6245 | val_loss=0.8846 val_acc=0.6382 | time=19.1s\n",
            "[Fold 1] Epoch 020 | train_loss=0.8811 train_acc=0.6233 | val_loss=0.8919 val_acc=0.6071 | time=19.3s\n",
            "[Fold 1] Epoch 021 | train_loss=0.8750 train_acc=0.6245 | val_loss=0.8732 val_acc=0.6320 | time=19.1s\n",
            "[Fold 1] Epoch 022 | train_loss=0.8684 train_acc=0.6264 | val_loss=0.8559 val_acc=0.6444 | time=19.3s\n",
            "[Fold 1] Epoch 023 | train_loss=0.8528 train_acc=0.6318 | val_loss=0.8619 val_acc=0.6460 | time=19.1s\n",
            "[Fold 1] Epoch 024 | train_loss=0.8481 train_acc=0.6338 | val_loss=0.8888 val_acc=0.6273 | time=19.2s\n",
            "[Fold 1] Epoch 025 | train_loss=0.8330 train_acc=0.6416 | val_loss=0.8848 val_acc=0.6211 | time=19.0s\n",
            "[Fold 1] Epoch 026 | train_loss=0.8275 train_acc=0.6450 | val_loss=0.8519 val_acc=0.6553 | time=19.3s\n",
            "[Fold 1] Epoch 027 | train_loss=0.8448 train_acc=0.6416 | val_loss=0.8437 val_acc=0.6537 | time=19.0s\n",
            "[Fold 1] Epoch 028 | train_loss=0.8239 train_acc=0.6478 | val_loss=0.8416 val_acc=0.6553 | time=19.3s\n",
            "[Fold 1] Epoch 029 | train_loss=0.8337 train_acc=0.6419 | val_loss=0.8394 val_acc=0.6506 | time=19.1s\n",
            "[Fold 1] Epoch 030 | train_loss=0.8107 train_acc=0.6548 | val_loss=0.8371 val_acc=0.6506 | time=19.4s\n",
            "[Fold 1] Epoch 031 | train_loss=0.8114 train_acc=0.6501 | val_loss=0.8446 val_acc=0.6506 | time=19.1s\n",
            "[Fold 1] Epoch 032 | train_loss=0.8067 train_acc=0.6548 | val_loss=0.8433 val_acc=0.6460 | time=19.3s\n",
            "[Fold 1] Epoch 033 | train_loss=0.8109 train_acc=0.6536 | val_loss=0.8312 val_acc=0.6661 | time=19.0s\n",
            "[Fold 1] Epoch 034 | train_loss=0.7991 train_acc=0.6555 | val_loss=0.8555 val_acc=0.6211 | time=19.4s\n",
            "[Fold 1] Epoch 035 | train_loss=0.8230 train_acc=0.6513 | val_loss=0.8779 val_acc=0.6134 | time=19.2s\n",
            "[Fold 1] Epoch 036 | train_loss=0.8058 train_acc=0.6478 | val_loss=0.8198 val_acc=0.6553 | time=19.5s\n",
            "[Fold 1] Epoch 037 | train_loss=0.7915 train_acc=0.6660 | val_loss=0.8712 val_acc=0.6351 | time=19.2s\n",
            "[Fold 1] Epoch 038 | train_loss=0.7978 train_acc=0.6586 | val_loss=0.8138 val_acc=0.6755 | time=19.3s\n",
            "[Fold 1] Epoch 039 | train_loss=0.7849 train_acc=0.6563 | val_loss=0.8330 val_acc=0.6724 | time=19.3s\n",
            "[Fold 1] Epoch 040 | train_loss=0.7778 train_acc=0.6668 | val_loss=0.8063 val_acc=0.6724 | time=19.3s\n",
            "[Fold 1] Epoch 041 | train_loss=0.7856 train_acc=0.6583 | val_loss=0.8142 val_acc=0.6568 | time=19.1s\n",
            "[Fold 1] Epoch 042 | train_loss=0.7751 train_acc=0.6602 | val_loss=0.8033 val_acc=0.6693 | time=19.4s\n",
            "[Fold 1] Epoch 043 | train_loss=0.7628 train_acc=0.6680 | val_loss=0.8686 val_acc=0.6398 | time=19.1s\n",
            "[Fold 1] Epoch 044 | train_loss=0.7501 train_acc=0.6753 | val_loss=0.7995 val_acc=0.6708 | time=19.3s\n",
            "[Fold 1] Epoch 045 | train_loss=0.7519 train_acc=0.6788 | val_loss=0.7920 val_acc=0.6879 | time=19.2s\n",
            "[Fold 1] Epoch 046 | train_loss=0.7523 train_acc=0.6792 | val_loss=0.7931 val_acc=0.6863 | time=19.6s\n",
            "[Fold 1] Epoch 047 | train_loss=0.7432 train_acc=0.6742 | val_loss=0.7873 val_acc=0.6941 | time=19.3s\n",
            "[Fold 1] Epoch 048 | train_loss=0.7441 train_acc=0.6734 | val_loss=0.7899 val_acc=0.6817 | time=19.4s\n",
            "[Fold 1] Epoch 049 | train_loss=0.7282 train_acc=0.6909 | val_loss=0.7873 val_acc=0.6599 | time=19.2s\n",
            "[Fold 1] Epoch 050 | train_loss=0.7363 train_acc=0.6831 | val_loss=0.8182 val_acc=0.6568 | time=19.4s\n",
            "[Fold 1] Epoch 051 | train_loss=0.7419 train_acc=0.6703 | val_loss=0.7703 val_acc=0.6879 | time=19.1s\n",
            "[Fold 1] Epoch 052 | train_loss=0.7163 train_acc=0.6920 | val_loss=0.7817 val_acc=0.7034 | time=19.3s\n",
            "[Fold 1] Epoch 053 | train_loss=0.7393 train_acc=0.6792 | val_loss=0.7783 val_acc=0.6941 | time=19.2s\n",
            "[Fold 1] Epoch 054 | train_loss=0.7109 train_acc=0.6920 | val_loss=0.7554 val_acc=0.6770 | time=19.3s\n",
            "[Fold 1] Epoch 055 | train_loss=0.6983 train_acc=0.6917 | val_loss=0.7607 val_acc=0.7081 | time=19.2s\n",
            "[Fold 1] Epoch 056 | train_loss=0.6862 train_acc=0.6959 | val_loss=0.7529 val_acc=0.6894 | time=19.3s\n",
            "[Fold 1] Epoch 057 | train_loss=0.6972 train_acc=0.6913 | val_loss=0.8213 val_acc=0.6460 | time=19.0s\n",
            "[Fold 1] Epoch 058 | train_loss=0.7050 train_acc=0.6955 | val_loss=0.7694 val_acc=0.6941 | time=19.3s\n",
            "[Fold 1] Epoch 059 | train_loss=0.6861 train_acc=0.7010 | val_loss=0.7661 val_acc=0.6817 | time=19.2s\n",
            "[Fold 1] Epoch 060 | train_loss=0.6889 train_acc=0.6986 | val_loss=0.7404 val_acc=0.6941 | time=19.3s\n",
            "[Fold 1] Epoch 061 | train_loss=0.6676 train_acc=0.7056 | val_loss=0.8004 val_acc=0.6801 | time=19.1s\n",
            "[Fold 1] Epoch 062 | train_loss=0.6478 train_acc=0.7130 | val_loss=0.7223 val_acc=0.7050 | time=19.3s\n",
            "[Fold 1] Epoch 063 | train_loss=0.6366 train_acc=0.7223 | val_loss=0.7286 val_acc=0.7158 | time=19.1s\n",
            "[Fold 1] Epoch 064 | train_loss=0.6596 train_acc=0.7091 | val_loss=0.7692 val_acc=0.7034 | time=19.2s\n",
            "[Fold 1] Epoch 065 | train_loss=0.6285 train_acc=0.7200 | val_loss=0.7083 val_acc=0.6801 | time=19.0s\n",
            "[Fold 1] Epoch 066 | train_loss=0.6174 train_acc=0.7313 | val_loss=0.7362 val_acc=0.7050 | time=19.3s\n",
            "[Fold 1] Epoch 067 | train_loss=0.6283 train_acc=0.7285 | val_loss=0.7068 val_acc=0.6941 | time=19.2s\n",
            "[Fold 1] Epoch 068 | train_loss=0.6247 train_acc=0.7247 | val_loss=0.7305 val_acc=0.7003 | time=19.3s\n",
            "[Fold 1] Epoch 069 | train_loss=0.6200 train_acc=0.7332 | val_loss=0.6907 val_acc=0.7081 | time=19.3s\n",
            "[Fold 1] Epoch 070 | train_loss=0.6087 train_acc=0.7313 | val_loss=0.7467 val_acc=0.6786 | time=19.3s\n",
            "[Fold 1] Epoch 071 | train_loss=0.5973 train_acc=0.7410 | val_loss=0.6958 val_acc=0.7127 | time=19.2s\n",
            "[Fold 1] Epoch 072 | train_loss=0.5840 train_acc=0.7355 | val_loss=0.7130 val_acc=0.6941 | time=19.3s\n",
            "[Fold 1] Epoch 073 | train_loss=0.5672 train_acc=0.7449 | val_loss=0.7289 val_acc=0.6739 | time=19.1s\n",
            "[Fold 1] Epoch 074 | train_loss=0.5893 train_acc=0.7437 | val_loss=0.7282 val_acc=0.7003 | time=19.2s\n",
            "[Fold 1] Epoch 075 | train_loss=0.5600 train_acc=0.7569 | val_loss=0.6866 val_acc=0.7034 | time=19.2s\n",
            "[Fold 1] Epoch 076 | train_loss=0.5460 train_acc=0.7542 | val_loss=0.7053 val_acc=0.7034 | time=19.1s\n",
            "[Fold 1] Epoch 077 | train_loss=0.5402 train_acc=0.7654 | val_loss=0.7527 val_acc=0.6972 | time=19.2s\n",
            "[Fold 1] Epoch 078 | train_loss=0.5375 train_acc=0.7565 | val_loss=0.6737 val_acc=0.7112 | time=19.4s\n",
            "[Fold 1] Epoch 079 | train_loss=0.5192 train_acc=0.7736 | val_loss=0.7080 val_acc=0.7267 | time=19.2s\n",
            "[Fold 1] Epoch 080 | train_loss=0.5187 train_acc=0.7744 | val_loss=0.6613 val_acc=0.7127 | time=19.1s\n",
            "[Fold 1] Epoch 081 | train_loss=0.5034 train_acc=0.7759 | val_loss=0.6804 val_acc=0.7391 | time=19.3s\n",
            "[Fold 1] Epoch 082 | train_loss=0.5057 train_acc=0.7720 | val_loss=0.6953 val_acc=0.7205 | time=19.0s\n",
            "[Fold 1] Epoch 083 | train_loss=0.4901 train_acc=0.7895 | val_loss=0.7729 val_acc=0.7081 | time=19.2s\n",
            "[Fold 1] Epoch 084 | train_loss=0.5211 train_acc=0.7763 | val_loss=0.6914 val_acc=0.7267 | time=19.2s\n",
            "[Fold 1] Epoch 085 | train_loss=0.4761 train_acc=0.7899 | val_loss=0.6625 val_acc=0.7360 | time=19.3s\n",
            "[Fold 1] Epoch 086 | train_loss=0.4796 train_acc=0.7922 | val_loss=0.7479 val_acc=0.7360 | time=19.1s\n",
            "[Fold 1] Epoch 087 | train_loss=0.5238 train_acc=0.7654 | val_loss=0.7162 val_acc=0.7112 | time=19.3s\n",
            "[Fold 1] Epoch 088 | train_loss=0.4928 train_acc=0.7713 | val_loss=0.6710 val_acc=0.7298 | time=19.2s\n",
            "[Fold 1] Epoch 089 | train_loss=0.4573 train_acc=0.8027 | val_loss=0.7037 val_acc=0.7143 | time=19.0s\n",
            "[Fold 1] Epoch 090 | train_loss=0.4750 train_acc=0.7887 | val_loss=0.7283 val_acc=0.6863 | time=19.1s\n",
            "[Fold 1] Epoch 091 | train_loss=0.4441 train_acc=0.7996 | val_loss=0.7023 val_acc=0.6817 | time=19.3s\n",
            "[Fold 1] Epoch 092 | train_loss=0.4540 train_acc=0.7938 | val_loss=0.6885 val_acc=0.6988 | time=19.1s\n",
            "[Fold 1] Epoch 093 | train_loss=0.4318 train_acc=0.8039 | val_loss=0.6980 val_acc=0.6879 | time=19.3s\n",
            "[Fold 1] Epoch 094 | train_loss=0.4399 train_acc=0.8047 | val_loss=0.6579 val_acc=0.7469 | time=19.2s\n",
            "[Fold 1] Epoch 095 | train_loss=0.3933 train_acc=0.8198 | val_loss=0.7593 val_acc=0.6786 | time=19.3s\n",
            "[Fold 1] Epoch 096 | train_loss=0.4233 train_acc=0.8167 | val_loss=0.6521 val_acc=0.7220 | time=19.2s\n",
            "[Fold 1] Epoch 097 | train_loss=0.3935 train_acc=0.8334 | val_loss=0.7178 val_acc=0.7220 | time=19.2s\n",
            "[Fold 1] Epoch 098 | train_loss=0.3878 train_acc=0.8287 | val_loss=0.7630 val_acc=0.7034 | time=19.1s\n",
            "[Fold 1] Epoch 099 | train_loss=0.4121 train_acc=0.8105 | val_loss=0.6741 val_acc=0.7314 | time=19.3s\n",
            "[Fold 1] Epoch 100 | train_loss=0.3910 train_acc=0.8237 | val_loss=0.8364 val_acc=0.7205 | time=19.1s\n",
            "\n",
            "========================= Fold 2 =========================\n",
            "[Fold 2] Epoch 001 | train_loss=1.0714 train_acc=0.4171 | val_loss=1.0836 val_acc=0.4301 | time=19.2s\n",
            "[Fold 2] Epoch 002 | train_loss=1.0667 train_acc=0.4315 | val_loss=1.0838 val_acc=0.4301 | time=19.1s\n",
            "[Fold 2] Epoch 003 | train_loss=1.0661 train_acc=0.4315 | val_loss=1.0849 val_acc=0.4301 | time=19.2s\n",
            "[Fold 2] Epoch 004 | train_loss=1.0667 train_acc=0.4315 | val_loss=1.0839 val_acc=0.4301 | time=19.2s\n",
            "[Fold 2] Epoch 005 | train_loss=1.0665 train_acc=0.4315 | val_loss=1.0848 val_acc=0.4301 | time=19.3s\n",
            "[Fold 2] Epoch 006 | train_loss=1.0657 train_acc=0.4315 | val_loss=1.0817 val_acc=0.4301 | time=19.0s\n",
            "[Fold 2] Epoch 007 | train_loss=1.0662 train_acc=0.4315 | val_loss=1.0832 val_acc=0.4301 | time=19.3s\n",
            "[Fold 2] Epoch 008 | train_loss=1.0662 train_acc=0.4315 | val_loss=1.0849 val_acc=0.4301 | time=19.2s\n",
            "[Fold 2] Epoch 009 | train_loss=1.0661 train_acc=0.4315 | val_loss=1.0839 val_acc=0.4301 | time=19.3s\n",
            "[Fold 2] Epoch 010 | train_loss=1.0656 train_acc=0.4315 | val_loss=1.0826 val_acc=0.4301 | time=19.1s\n",
            "[Fold 2] Epoch 011 | train_loss=1.0654 train_acc=0.4315 | val_loss=1.0826 val_acc=0.4301 | time=19.1s\n",
            "[Fold 2] Epoch 012 | train_loss=1.0632 train_acc=0.4350 | val_loss=1.0653 val_acc=0.4720 | time=19.1s\n",
            "[Fold 2] Epoch 013 | train_loss=1.0439 train_acc=0.4753 | val_loss=1.0751 val_acc=0.4301 | time=19.3s\n",
            "[Fold 2] Epoch 014 | train_loss=1.0256 train_acc=0.5049 | val_loss=1.0499 val_acc=0.5016 | time=19.2s\n",
            "[Fold 2] Epoch 015 | train_loss=0.9863 train_acc=0.5460 | val_loss=1.0044 val_acc=0.5326 | time=19.3s\n",
            "[Fold 2] Epoch 016 | train_loss=0.9382 train_acc=0.5891 | val_loss=0.9606 val_acc=0.5932 | time=19.1s\n",
            "[Fold 2] Epoch 017 | train_loss=0.9013 train_acc=0.6151 | val_loss=0.9586 val_acc=0.5978 | time=19.2s\n",
            "[Fold 2] Epoch 018 | train_loss=0.9022 train_acc=0.6027 | val_loss=0.9543 val_acc=0.5963 | time=19.1s\n",
            "[Fold 2] Epoch 019 | train_loss=0.8726 train_acc=0.6245 | val_loss=0.9156 val_acc=0.6118 | time=19.3s\n",
            "[Fold 2] Epoch 020 | train_loss=0.8601 train_acc=0.6210 | val_loss=0.8905 val_acc=0.6056 | time=19.1s\n",
            "[Fold 2] Epoch 021 | train_loss=0.8504 train_acc=0.6350 | val_loss=0.8935 val_acc=0.6056 | time=19.1s\n",
            "[Fold 2] Epoch 022 | train_loss=0.8432 train_acc=0.6322 | val_loss=0.8843 val_acc=0.6242 | time=19.0s\n",
            "[Fold 2] Epoch 023 | train_loss=0.8210 train_acc=0.6346 | val_loss=0.9355 val_acc=0.6211 | time=19.3s\n",
            "[Fold 2] Epoch 024 | train_loss=0.8336 train_acc=0.6303 | val_loss=0.8531 val_acc=0.6258 | time=19.1s\n",
            "[Fold 2] Epoch 025 | train_loss=0.8129 train_acc=0.6470 | val_loss=0.8697 val_acc=0.6196 | time=19.3s\n",
            "[Fold 2] Epoch 026 | train_loss=0.8051 train_acc=0.6505 | val_loss=0.8548 val_acc=0.6273 | time=19.0s\n",
            "[Fold 2] Epoch 027 | train_loss=0.8106 train_acc=0.6458 | val_loss=0.8399 val_acc=0.6366 | time=19.3s\n",
            "[Fold 2] Epoch 028 | train_loss=0.7763 train_acc=0.6563 | val_loss=0.8181 val_acc=0.6491 | time=19.1s\n",
            "[Fold 2] Epoch 029 | train_loss=0.7720 train_acc=0.6590 | val_loss=0.8060 val_acc=0.6460 | time=19.3s\n",
            "[Fold 2] Epoch 030 | train_loss=0.7789 train_acc=0.6447 | val_loss=0.8129 val_acc=0.6351 | time=19.2s\n",
            "[Fold 2] Epoch 031 | train_loss=0.7690 train_acc=0.6660 | val_loss=0.8072 val_acc=0.6491 | time=19.2s\n",
            "[Fold 2] Epoch 032 | train_loss=0.7560 train_acc=0.6730 | val_loss=0.8067 val_acc=0.6553 | time=19.0s\n",
            "[Fold 2] Epoch 033 | train_loss=0.7634 train_acc=0.6695 | val_loss=0.7937 val_acc=0.6646 | time=19.3s\n",
            "[Fold 2] Epoch 034 | train_loss=0.7494 train_acc=0.6695 | val_loss=0.8140 val_acc=0.6506 | time=19.2s\n",
            "[Fold 2] Epoch 035 | train_loss=0.7366 train_acc=0.6695 | val_loss=0.8420 val_acc=0.6398 | time=19.2s\n",
            "[Fold 2] Epoch 036 | train_loss=0.7437 train_acc=0.6757 | val_loss=0.7937 val_acc=0.6537 | time=19.1s\n",
            "[Fold 2] Epoch 037 | train_loss=0.7483 train_acc=0.6757 | val_loss=0.7882 val_acc=0.6693 | time=19.4s\n",
            "[Fold 2] Epoch 038 | train_loss=0.7263 train_acc=0.6862 | val_loss=0.7707 val_acc=0.6615 | time=19.1s\n",
            "[Fold 2] Epoch 039 | train_loss=0.7251 train_acc=0.6788 | val_loss=0.8097 val_acc=0.6599 | time=19.2s\n",
            "[Fold 2] Epoch 040 | train_loss=0.7320 train_acc=0.6847 | val_loss=0.7638 val_acc=0.6770 | time=19.2s\n",
            "[Fold 2] Epoch 041 | train_loss=0.7152 train_acc=0.6905 | val_loss=0.8018 val_acc=0.6366 | time=19.3s\n",
            "[Fold 2] Epoch 042 | train_loss=0.7094 train_acc=0.6823 | val_loss=0.7928 val_acc=0.6708 | time=19.3s\n",
            "[Fold 2] Epoch 043 | train_loss=0.6931 train_acc=0.6998 | val_loss=0.7976 val_acc=0.6630 | time=19.4s\n",
            "[Fold 2] Epoch 044 | train_loss=0.7150 train_acc=0.6788 | val_loss=0.7617 val_acc=0.6661 | time=19.3s\n",
            "[Fold 2] Epoch 045 | train_loss=0.7112 train_acc=0.6889 | val_loss=0.7826 val_acc=0.6615 | time=19.2s\n",
            "[Fold 2] Epoch 046 | train_loss=0.6845 train_acc=0.6971 | val_loss=0.7691 val_acc=0.6661 | time=19.0s\n",
            "[Fold 2] Epoch 047 | train_loss=0.6722 train_acc=0.7033 | val_loss=0.7913 val_acc=0.6599 | time=19.3s\n",
            "[Fold 2] Epoch 048 | train_loss=0.6742 train_acc=0.6940 | val_loss=0.7588 val_acc=0.6708 | time=19.1s\n",
            "[Fold 2] Epoch 049 | train_loss=0.6796 train_acc=0.6963 | val_loss=0.7495 val_acc=0.6708 | time=19.3s\n",
            "[Fold 2] Epoch 050 | train_loss=0.6584 train_acc=0.7049 | val_loss=0.7364 val_acc=0.6817 | time=19.2s\n",
            "[Fold 2] Epoch 051 | train_loss=0.6703 train_acc=0.7010 | val_loss=0.7606 val_acc=0.6848 | time=19.3s\n",
            "[Fold 2] Epoch 052 | train_loss=0.6627 train_acc=0.7021 | val_loss=0.7732 val_acc=0.6817 | time=19.3s\n",
            "[Fold 2] Epoch 053 | train_loss=0.6502 train_acc=0.7111 | val_loss=0.7330 val_acc=0.6910 | time=19.4s\n",
            "[Fold 2] Epoch 054 | train_loss=0.6345 train_acc=0.7177 | val_loss=0.6952 val_acc=0.6941 | time=18.9s\n",
            "[Fold 2] Epoch 055 | train_loss=0.6414 train_acc=0.7146 | val_loss=0.7186 val_acc=0.6894 | time=19.4s\n",
            "[Fold 2] Epoch 056 | train_loss=0.6464 train_acc=0.7181 | val_loss=0.7316 val_acc=0.6770 | time=19.0s\n",
            "[Fold 2] Epoch 057 | train_loss=0.6204 train_acc=0.7223 | val_loss=0.7450 val_acc=0.6848 | time=19.3s\n",
            "[Fold 2] Epoch 058 | train_loss=0.6308 train_acc=0.7177 | val_loss=0.7331 val_acc=0.6755 | time=19.2s\n",
            "[Fold 2] Epoch 059 | train_loss=0.6227 train_acc=0.7258 | val_loss=0.6840 val_acc=0.7034 | time=19.4s\n",
            "[Fold 2] Epoch 060 | train_loss=0.6071 train_acc=0.7239 | val_loss=0.7539 val_acc=0.6817 | time=19.1s\n",
            "[Fold 2] Epoch 061 | train_loss=0.5873 train_acc=0.7336 | val_loss=0.7073 val_acc=0.6941 | time=19.3s\n",
            "[Fold 2] Epoch 062 | train_loss=0.5764 train_acc=0.7449 | val_loss=0.6969 val_acc=0.7081 | time=18.9s\n",
            "[Fold 2] Epoch 063 | train_loss=0.5725 train_acc=0.7491 | val_loss=0.7194 val_acc=0.6894 | time=19.2s\n",
            "[Fold 2] Epoch 064 | train_loss=0.5684 train_acc=0.7452 | val_loss=0.6948 val_acc=0.6941 | time=19.0s\n",
            "[Fold 2] Epoch 065 | train_loss=0.5884 train_acc=0.7460 | val_loss=0.7354 val_acc=0.7003 | time=19.2s\n",
            "[Fold 2] Epoch 066 | train_loss=0.5638 train_acc=0.7503 | val_loss=0.7369 val_acc=0.6693 | time=19.2s\n",
            "[Fold 2] Epoch 067 | train_loss=0.5845 train_acc=0.7390 | val_loss=0.6893 val_acc=0.7127 | time=19.3s\n",
            "[Fold 2] Epoch 068 | train_loss=0.5308 train_acc=0.7619 | val_loss=0.7073 val_acc=0.7127 | time=19.0s\n",
            "[Fold 2] Epoch 069 | train_loss=0.5425 train_acc=0.7577 | val_loss=0.6975 val_acc=0.7189 | time=19.3s\n",
            "[Fold 2] Epoch 070 | train_loss=0.5482 train_acc=0.7561 | val_loss=0.7741 val_acc=0.6755 | time=19.2s\n",
            "[Fold 2] Epoch 071 | train_loss=0.5780 train_acc=0.7480 | val_loss=0.7061 val_acc=0.7050 | time=19.2s\n",
            "[Fold 2] Epoch 072 | train_loss=0.5215 train_acc=0.7775 | val_loss=0.7322 val_acc=0.6786 | time=19.1s\n",
            "[Fold 2] Epoch 073 | train_loss=0.5234 train_acc=0.7736 | val_loss=0.7583 val_acc=0.6848 | time=19.2s\n",
            "[Fold 2] Epoch 074 | train_loss=0.5313 train_acc=0.7623 | val_loss=0.7398 val_acc=0.7019 | time=19.2s\n",
            "[Fold 2] Epoch 075 | train_loss=0.4943 train_acc=0.7806 | val_loss=0.6885 val_acc=0.7189 | time=19.0s\n",
            "[Fold 2] Epoch 076 | train_loss=0.4972 train_acc=0.7841 | val_loss=0.6903 val_acc=0.7081 | time=19.1s\n",
            "[Fold 2] Epoch 077 | train_loss=0.4839 train_acc=0.7833 | val_loss=0.7249 val_acc=0.6817 | time=19.2s\n",
            "[Fold 2] Epoch 078 | train_loss=0.4739 train_acc=0.7922 | val_loss=0.7573 val_acc=0.6786 | time=19.3s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 10:30:31,085] Trial 4 finished with value: 0.6840153265567053 and parameters: {'lr': 3.230793658846425e-05, 'weight_decay': 5.926736468642624e-05, 'rtm_blocks': 3, 'stm_blocks': 3, 'ttm_blocks': 3, 'rtm_heads': 3, 'stm_heads': 3, 'ttm_heads': 2, 'num_segments': 5}. Best is trial 2 with value: 0.6143529968602317.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Fold 2] Epoch 079 | train_loss=0.4713 train_acc=0.7973 | val_loss=0.7145 val_acc=0.6910 | time=19.2s\n",
            "[Fold 2] Early stopping at epoch 79, best was 59\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">icy-tree-7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/n12wu855' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/n12wu855</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_090030-n12wu855/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_103031-jljgxqcn</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/jljgxqcn' target=\"_blank\">icy-fire-8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/jljgxqcn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/jljgxqcn</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 5 =========================\n",
            "Testing with hyperparameters: {'lr': 7.961654507774643e-05, 'weight_decay': 6.2742708650523e-05, 'rtm_blocks': 1, 'stm_blocks': 1, 'ttm_blocks': 3, 'rtm_heads': 2, 'stm_heads': 2, 'ttm_heads': 2, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 10:30:52,207] Trial 5 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Trial 5 pruned at fold 0, epoch 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">icy-fire-8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/jljgxqcn' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/jljgxqcn</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_103031-jljgxqcn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_103052-yrmgfj4w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yrmgfj4w' target=\"_blank\">vital-wood-9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yrmgfj4w' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yrmgfj4w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 6 =========================\n",
            "Testing with hyperparameters: {'lr': 4.900194549517511e-05, 'weight_decay': 5.1682408123998705e-05, 'rtm_blocks': 1, 'stm_blocks': 3, 'ttm_blocks': 1, 'rtm_heads': 3, 'stm_heads': 3, 'ttm_heads': 3, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 10:31:13,790] Trial 6 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Trial 6 pruned at fold 0, epoch 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vital-wood-9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yrmgfj4w' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/yrmgfj4w</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_103052-yrmgfj4w/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_103113-0a0wpgp8</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/0a0wpgp8' target=\"_blank\">lilac-firefly-10</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/0a0wpgp8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/0a0wpgp8</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 7 =========================\n",
            "Testing with hyperparameters: {'lr': 7.757394242051711e-05, 'weight_decay': 4.7076266580008185e-05, 'rtm_blocks': 1, 'stm_blocks': 2, 'ttm_blocks': 2, 'rtm_heads': 3, 'stm_heads': 2, 'ttm_heads': 3, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0689 train_acc=0.4311 | val_loss=1.0835 val_acc=0.4317 | time=17.6s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0670 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=17.7s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 10:32:10,128] Trial 7 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Trial 7 pruned at fold 0, epoch 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">lilac-firefly-10</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/0a0wpgp8' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/0a0wpgp8</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_103113-0a0wpgp8/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_103210-mul0sbu1</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/mul0sbu1' target=\"_blank\">silvery-dawn-11</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/mul0sbu1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/mul0sbu1</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 8 =========================\n",
            "Testing with hyperparameters: {'lr': 2.8656576209052045e-05, 'weight_decay': 6.975343786437316e-05, 'rtm_blocks': 2, 'stm_blocks': 2, 'ttm_blocks': 2, 'rtm_heads': 3, 'stm_heads': 3, 'ttm_heads': 3, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0683 train_acc=0.4311 | val_loss=1.0800 val_acc=0.4317 | time=18.0s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0678 train_acc=0.4311 | val_loss=1.0813 val_acc=0.4317 | time=17.8s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0669 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=18.0s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0670 train_acc=0.4315 | val_loss=1.0857 val_acc=0.4317 | time=18.1s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0650 train_acc=0.4311 | val_loss=1.0864 val_acc=0.4317 | time=18.2s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 10:34:01,857] Trial 8 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Trial 8 pruned at fold 0, epoch 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">silvery-dawn-11</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/mul0sbu1' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/mul0sbu1</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_103210-mul0sbu1/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_103401-xxzxtjm6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/xxzxtjm6' target=\"_blank\">restful-night-12</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/xxzxtjm6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_10/runs/xxzxtjm6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 9 =========================\n",
            "Testing with hyperparameters: {'lr': 5.956687649545858e-05, 'weight_decay': 3.544220567159663e-05, 'rtm_blocks': 2, 'stm_blocks': 1, 'ttm_blocks': 3, 'rtm_heads': 2, 'stm_heads': 2, 'ttm_heads': 3, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0691 train_acc=0.4311 | val_loss=1.0825 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0667 train_acc=0.4311 | val_loss=1.0808 val_acc=0.4317 | time=17.7s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0668 train_acc=0.4311 | val_loss=1.0794 val_acc=0.4317 | time=17.6s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0630 train_acc=0.4439 | val_loss=1.0588 val_acc=0.5326 | time=17.7s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0626 train_acc=0.4276 | val_loss=1.0787 val_acc=0.4317 | time=17.5s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0507 train_acc=0.4567 | val_loss=1.1327 val_acc=0.3432 | time=17.5s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0730 train_acc=0.4175 | val_loss=1.0827 val_acc=0.4317 | time=17.8s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 10:36:27,042] Trial 9 pruned. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "❌ Trial 9 pruned at fold 0, epoch 8\n",
            "\n",
            "===== Best Trial Results =====\n",
            "avg_val_loss   = 0.614353\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'avg_train_loss'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-d254eee1ac95>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n===== Best Trial Results =====\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"avg_val_loss   = {best.value:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"avg_train_loss = {best.user_attrs['avg_train_loss']:.6f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    216\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"avg_train_acc  = {best.user_attrs['avg_train_acc']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"avg_val_acc    = {best.user_attrs['avg_val_acc']:.4f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'avg_train_loss'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Constants ─────────────────────────\n",
        "LR_MIN, LR_MAX = 2e-5, 8e-5\n",
        "WD_MIN, WD_MAX = 3e-5, 1e-4\n",
        "FILTER_MIN = FILTER_MAX = 120\n",
        "\n",
        "# blocks -> [1,2,3] / heads -> [2,3,4]\n",
        "RTM_BLOCK_CHOICES = [1, 2, 3]\n",
        "STM_BLOCK_CHOICES = [1, 2, 3]\n",
        "TTM_BLOCK_CHOICES = [1, 2, 3]\n",
        "\n",
        "RTM_HEAD_CHOICES = [2, 3, 4]\n",
        "STM_HEAD_CHOICES = [2, 3, 4]\n",
        "TTM_HEAD_CHOICES = [2, 3, 4]\n",
        "\n",
        "SEGMENT_CHOICES = [5, 15]\n",
        "\n",
        "N_FOLDS     = 5\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = \"labels.json\"\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space setup\n",
        "    lr = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_filters = 120\n",
        "    rtm_blocks = trial.suggest_categorical(\"rtm_blocks\", RTM_BLOCK_CHOICES)\n",
        "    stm_blocks = trial.suggest_categorical(\"stm_blocks\", STM_BLOCK_CHOICES)\n",
        "    ttm_blocks = trial.suggest_categorical(\"ttm_blocks\", TTM_BLOCK_CHOICES)\n",
        "    rtm_heads = trial.suggest_categorical(\"rtm_heads\", RTM_HEAD_CHOICES)\n",
        "    stm_heads = trial.suggest_categorical(\"stm_heads\", STM_HEAD_CHOICES)\n",
        "    ttm_heads = trial.suggest_categorical(\"ttm_heads\", TTM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # Pruning condition: only proceed if num_filters is divisible by heads\n",
        "    for h in (rtm_heads, stm_heads, ttm_heads):\n",
        "        if num_filters % h != 0:\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"eeg-cv-tuning-trial_12\", config=trial.params)\n",
        "\n",
        "    print(f\"\\n========================= Trial {trial.number} =========================\")\n",
        "    print(f\"Testing with hyperparameters: {trial.params}\")\n",
        "\n",
        "    # Data preparation\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    n_samples = len(full_ds)\n",
        "\n",
        "    # StratifiedKFold setup\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    fold_metrics = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"best_epoch\": []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(range(n_samples), labels)):\n",
        "        # Fold separation\n",
        "        print(f\"\\n========================= Fold {fold} =========================\")\n",
        "\n",
        "        # Data loader setup\n",
        "        train_loader = DataLoader(Subset(full_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(Subset(full_ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # Model and optimizer setup\n",
        "        model = EEGformer(\n",
        "            num_classes=3,\n",
        "            in_channels=19,\n",
        "            kernel_size=10,\n",
        "            num_filters=num_filters,\n",
        "            rtm_blocks=rtm_blocks,\n",
        "            stm_blocks=stm_blocks,\n",
        "            ttm_blocks=ttm_blocks,\n",
        "            rtm_heads=rtm_heads,\n",
        "            stm_heads=stm_heads,\n",
        "            ttm_heads=ttm_heads,\n",
        "            num_segments=num_segments\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        epochs_no_improve = 0\n",
        "        best_epoch = 0\n",
        "        best_train_l = best_train_a = best_val_a = None\n",
        "        last_log_time = time.time()\n",
        "\n",
        "        # Epoch-wise training\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            model.train()\n",
        "            tl_sum = t_corr = t_tot = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tl_sum += loss.item()\n",
        "                t_corr += (logits.argmax(1) == y).sum().item()\n",
        "                t_tot += y.size(0)\n",
        "\n",
        "            train_loss = tl_sum / len(train_loader)\n",
        "            train_acc = t_corr / t_tot\n",
        "\n",
        "            model.eval()\n",
        "            vl_sum = v_corr = v_tot = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss = criterion(logits, y)\n",
        "                    vl_sum += loss.item()\n",
        "                    v_corr += (logits.argmax(1) == y).sum().item()\n",
        "                    v_tot += y.size(0)\n",
        "\n",
        "            val_loss = vl_sum / len(val_loader)\n",
        "            val_acc = v_corr / v_tot\n",
        "\n",
        "            step = fold * MAX_EPOCHS + epoch\n",
        "            trial.report(val_loss, step=step)\n",
        "\n",
        "            # Pruning check\n",
        "            if trial.should_prune():\n",
        "                print(f\"\\u274c Trial {trial.number} pruned at fold {fold}, epoch {epoch}\")\n",
        "                # Report the metrics before returning early\n",
        "                for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                 [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                    fold_metrics[k].append(v)\n",
        "                    trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                raise optuna.TrialPruned()  # End trial completely if pruned\n",
        "\n",
        "            now = time.time()\n",
        "            print(f\"[Fold {fold}] Epoch {epoch:03d} | train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
        "                  f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={now - last_log_time:.1f}s\")\n",
        "            last_log_time = now\n",
        "\n",
        "            # Early stopping: if validation loss does not improve\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_epoch = epoch\n",
        "                epochs_no_improve = 0\n",
        "                best_train_l = train_loss\n",
        "                best_train_a = train_acc\n",
        "                best_val_a = val_acc\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= PATIENCE:\n",
        "                    print(f\"[Fold {fold}] Early stopping at epoch {epoch}, best was {best_epoch}\")\n",
        "                    # Report the metrics before returning early\n",
        "                    for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                     [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                        fold_metrics[k].append(v)\n",
        "                        trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                    break  # Break if early stopping\n",
        "\n",
        "        # Record results for the fold\n",
        "        for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                         [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "            fold_metrics[k].append(v)\n",
        "            trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        torch.mps.empty_cache() if DEVICE.type == \"mps\" else torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg = lambda k: sum(fold_metrics[k]) / N_FOLDS\n",
        "    for key in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"]:\n",
        "        trial.set_user_attr(f\"avg_{key}\", avg(key))\n",
        "\n",
        "    wandb.finish()\n",
        "    return avg(\"val_loss\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "        study_name=\"eegformer_optuna_cv_4\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eegformer_optuna_cv_5.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial Results =====\")\n",
        "    print(f\"avg_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"avg_train_loss = {best.user_attrs['avg_train_loss']:.6f}\")\n",
        "    print(f\"avg_train_acc  = {best.user_attrs['avg_train_acc']:.4f}\")\n",
        "    print(f\"avg_val_acc    = {best.user_attrs['avg_val_acc']:.4f}\")\n",
        "    print(f\"avg_best_epoch = {best.user_attrs['avg_best_epoch']:.1f}\")\n",
        "    print(\"best hyperparameters:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(\"per-fold best metrics:\")\n",
        "    for f in range(N_FOLDS):\n",
        "        print(\n",
        "            f\"  Fold {f}: epoch={best.user_attrs[f'fold{f}_best_epoch']}, \"\n",
        "            f\"t_loss={best.user_attrs[f'fold{f}_train_loss']:.4f}, \"\n",
        "            f\"t_acc={best.user_attrs[f'fold{f}_train_acc']:.4f}, \"\n",
        "            f\"v_loss={best.user_attrs[f'fold{f}_val_loss']:.4f}, \"\n",
        "            f\"v_acc={best.user_attrs[f'fold{f}_val_acc']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # ─── 전체 데이터(train)로 재학습 + loss·accuracy 출력 + 저장 ─────────────────\n",
        "    print(\"\\nRetraining on full TRAIN dataset with best params…\")\n",
        "\n",
        "    # 1) train 메타만 골라서 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    full_meta   = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, full_meta)\n",
        "    full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # 2) 모델·옵티마이저 재설정\n",
        "    model = EEGformer(\n",
        "        num_classes=3,\n",
        "        in_channels=19,\n",
        "        kernel_size=10,\n",
        "        num_filters=120,\n",
        "        rtm_blocks=best.params[\"rtm_blocks\"],\n",
        "        stm_blocks=best.params[\"stm_blocks\"],\n",
        "        ttm_blocks=best.params[\"ttm_blocks\"],\n",
        "        rtm_heads= best.params[\"rtm_heads\"],\n",
        "        stm_heads= best.params[\"stm_heads\"],\n",
        "        ttm_heads= best.params[\"ttm_heads\"],\n",
        "        num_segments=best.params[\"num_segments\"]\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=best.params[\"lr\"],\n",
        "        weight_decay=best.params[\"weight_decay\"]\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 3) MAX_EPOCHS 만큼 전체 학습하며 loss·accuracy 출력\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        model.train()\n",
        "        loss_sum = 0.0\n",
        "        correct  = 0\n",
        "        total    = 0\n",
        "        for X, y in full_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += y.size(0)\n",
        "\n",
        "        avg_loss = loss_sum / len(full_loader)\n",
        "        acc      = correct / total\n",
        "        print(f\"[Full Train] Epoch {epoch:03d} | loss={avg_loss:.4f} | acc={acc:.4f}\")\n",
        "\n",
        "    # 4) 체크포인트 저장\n",
        "    ckpt_dir = '/content/drive/MyDrive/2025_Lab_Research/checkpoints'\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(ckpt_dir, 'eegformer_best.pth')\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"💾 Saved best model to {ckpt_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "OQrrRGRFNewu",
        "outputId": "d4c8b9c9-3f9c-47e4-e200-6b63499fe739"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to create new mne-python configuration file:\n",
            "/root/.mne/mne-python.json\n",
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 21:33:58,944] A new study created in RDB with name: eegformer_optuna_cv_4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_213359-x6vg01w6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/x6vg01w6' target=\"_blank\">resilient-waterfall-1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/x6vg01w6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/x6vg01w6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 0 =========================\n",
            "Testing with hyperparameters: {'lr': 3.306335426726131e-05, 'weight_decay': 6.37539648704434e-05, 'rtm_blocks': 3, 'stm_blocks': 2, 'ttm_blocks': 3, 'rtm_heads': 3, 'stm_heads': 3, 'ttm_heads': 2, 'num_segments': 15}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0683 train_acc=0.4311 | val_loss=1.0845 val_acc=0.4317 | time=250.0s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0669 train_acc=0.4311 | val_loss=1.0853 val_acc=0.4317 | time=18.0s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0669 train_acc=0.4311 | val_loss=1.0818 val_acc=0.4317 | time=18.1s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0666 train_acc=0.4311 | val_loss=1.0820 val_acc=0.4317 | time=18.1s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0663 train_acc=0.4311 | val_loss=1.0851 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 006 | train_loss=1.0663 train_acc=0.4311 | val_loss=1.0822 val_acc=0.4317 | time=18.1s\n",
            "[Fold 0] Epoch 007 | train_loss=1.0677 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=17.9s\n",
            "[Fold 0] Epoch 008 | train_loss=1.0672 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 009 | train_loss=1.0671 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=18.1s\n",
            "[Fold 0] Epoch 010 | train_loss=1.0663 train_acc=0.4311 | val_loss=1.0818 val_acc=0.4317 | time=18.7s\n",
            "[Fold 0] Epoch 011 | train_loss=1.0656 train_acc=0.4311 | val_loss=1.0862 val_acc=0.4317 | time=18.8s\n",
            "[Fold 0] Epoch 012 | train_loss=1.0659 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=18.6s\n",
            "[Fold 0] Epoch 013 | train_loss=1.0647 train_acc=0.4311 | val_loss=1.0909 val_acc=0.4317 | time=18.4s\n",
            "[Fold 0] Epoch 014 | train_loss=1.0677 train_acc=0.4311 | val_loss=1.0842 val_acc=0.4317 | time=18.0s\n",
            "[Fold 0] Epoch 015 | train_loss=1.0658 train_acc=0.4311 | val_loss=1.0804 val_acc=0.4317 | time=18.5s\n",
            "[Fold 0] Epoch 016 | train_loss=1.0635 train_acc=0.4233 | val_loss=1.0841 val_acc=0.4317 | time=18.4s\n",
            "[Fold 0] Epoch 017 | train_loss=1.0680 train_acc=0.4311 | val_loss=1.0840 val_acc=0.4317 | time=17.8s\n",
            "[Fold 0] Epoch 018 | train_loss=1.0656 train_acc=0.4311 | val_loss=1.0810 val_acc=0.4317 | time=18.4s\n",
            "[Fold 0] Epoch 019 | train_loss=1.0663 train_acc=0.4311 | val_loss=1.0833 val_acc=0.4317 | time=18.6s\n",
            "[Fold 0] Epoch 020 | train_loss=1.0642 train_acc=0.4311 | val_loss=1.0804 val_acc=0.4317 | time=18.2s\n",
            "[Fold 0] Epoch 021 | train_loss=1.0436 train_acc=0.4722 | val_loss=1.0080 val_acc=0.5543 | time=18.2s\n",
            "[Fold 0] Epoch 022 | train_loss=0.9705 train_acc=0.5503 | val_loss=0.9668 val_acc=0.5714 | time=18.2s\n",
            "[Fold 0] Epoch 023 | train_loss=0.9505 train_acc=0.5619 | val_loss=0.9534 val_acc=0.5854 | time=18.2s\n",
            "[Fold 0] Epoch 024 | train_loss=0.9149 train_acc=0.5876 | val_loss=0.9134 val_acc=0.6087 | time=17.9s\n",
            "[Fold 0] Epoch 025 | train_loss=0.8977 train_acc=0.6070 | val_loss=0.8971 val_acc=0.6258 | time=18.1s\n",
            "[Fold 0] Epoch 026 | train_loss=0.8799 train_acc=0.6194 | val_loss=0.8696 val_acc=0.6460 | time=17.9s\n",
            "[Fold 0] Epoch 027 | train_loss=0.8702 train_acc=0.6245 | val_loss=0.8915 val_acc=0.6180 | time=18.0s\n",
            "[Fold 0] Epoch 028 | train_loss=0.8535 train_acc=0.6326 | val_loss=0.8719 val_acc=0.6320 | time=18.3s\n",
            "[Fold 0] Epoch 029 | train_loss=0.8341 train_acc=0.6392 | val_loss=0.8434 val_acc=0.6320 | time=18.1s\n",
            "[Fold 0] Epoch 030 | train_loss=0.8325 train_acc=0.6427 | val_loss=0.8440 val_acc=0.6568 | time=18.2s\n",
            "[Fold 0] Epoch 031 | train_loss=0.8102 train_acc=0.6652 | val_loss=0.8030 val_acc=0.6894 | time=18.1s\n",
            "[Fold 0] Epoch 032 | train_loss=0.7920 train_acc=0.6683 | val_loss=0.7719 val_acc=0.7003 | time=18.0s\n",
            "[Fold 0] Epoch 033 | train_loss=0.7970 train_acc=0.6617 | val_loss=0.8390 val_acc=0.6770 | time=18.2s\n",
            "[Fold 0] Epoch 034 | train_loss=0.7764 train_acc=0.6750 | val_loss=0.7806 val_acc=0.6925 | time=18.1s\n",
            "[Fold 0] Epoch 035 | train_loss=0.7589 train_acc=0.6835 | val_loss=0.7647 val_acc=0.7019 | time=18.3s\n",
            "[Fold 0] Epoch 036 | train_loss=0.7560 train_acc=0.6882 | val_loss=0.7919 val_acc=0.6786 | time=18.2s\n",
            "[Fold 0] Epoch 037 | train_loss=0.7526 train_acc=0.6808 | val_loss=0.7585 val_acc=0.7127 | time=17.9s\n",
            "[Fold 0] Epoch 038 | train_loss=0.7579 train_acc=0.6769 | val_loss=0.8418 val_acc=0.6398 | time=18.1s\n",
            "[Fold 0] Epoch 039 | train_loss=0.7326 train_acc=0.6878 | val_loss=0.7491 val_acc=0.6941 | time=18.1s\n",
            "[Fold 0] Epoch 040 | train_loss=0.7194 train_acc=0.6924 | val_loss=0.7466 val_acc=0.6801 | time=18.4s\n",
            "[Fold 0] Epoch 041 | train_loss=0.7224 train_acc=0.6928 | val_loss=0.7409 val_acc=0.7096 | time=18.2s\n",
            "[Fold 0] Epoch 042 | train_loss=0.6976 train_acc=0.7025 | val_loss=0.7473 val_acc=0.6910 | time=18.1s\n",
            "[Fold 0] Epoch 043 | train_loss=0.6846 train_acc=0.7146 | val_loss=0.7549 val_acc=0.6941 | time=18.0s\n",
            "[Fold 0] Epoch 044 | train_loss=0.6914 train_acc=0.7025 | val_loss=0.7235 val_acc=0.7158 | time=18.2s\n",
            "[Fold 0] Epoch 045 | train_loss=0.6648 train_acc=0.7208 | val_loss=0.6967 val_acc=0.7252 | time=18.2s\n",
            "[Fold 0] Epoch 046 | train_loss=0.6570 train_acc=0.7239 | val_loss=0.6887 val_acc=0.7438 | time=18.0s\n",
            "[Fold 0] Epoch 047 | train_loss=0.6591 train_acc=0.7118 | val_loss=0.6778 val_acc=0.7422 | time=18.0s\n",
            "[Fold 0] Epoch 048 | train_loss=0.6341 train_acc=0.7336 | val_loss=0.6868 val_acc=0.7391 | time=18.1s\n",
            "[Fold 0] Epoch 049 | train_loss=0.6373 train_acc=0.7247 | val_loss=0.6856 val_acc=0.7267 | time=18.0s\n",
            "[Fold 0] Epoch 050 | train_loss=0.6216 train_acc=0.7355 | val_loss=0.6960 val_acc=0.7391 | time=18.3s\n",
            "[Fold 0] Epoch 051 | train_loss=0.6412 train_acc=0.7293 | val_loss=0.6525 val_acc=0.7298 | time=17.9s\n",
            "[Fold 0] Epoch 052 | train_loss=0.6175 train_acc=0.7363 | val_loss=0.7238 val_acc=0.7314 | time=17.8s\n",
            "[Fold 0] Epoch 053 | train_loss=0.6003 train_acc=0.7483 | val_loss=0.6612 val_acc=0.7422 | time=18.1s\n",
            "[Fold 0] Epoch 054 | train_loss=0.5930 train_acc=0.7417 | val_loss=0.6800 val_acc=0.7671 | time=18.2s\n",
            "[Fold 0] Epoch 055 | train_loss=0.5808 train_acc=0.7460 | val_loss=0.6555 val_acc=0.7531 | time=18.2s\n",
            "[Fold 0] Epoch 056 | train_loss=0.5896 train_acc=0.7491 | val_loss=0.6826 val_acc=0.7469 | time=18.0s\n",
            "[Fold 0] Epoch 057 | train_loss=0.5632 train_acc=0.7538 | val_loss=0.6848 val_acc=0.7422 | time=18.1s\n",
            "[Fold 0] Epoch 058 | train_loss=0.5957 train_acc=0.7417 | val_loss=0.7390 val_acc=0.7127 | time=17.8s\n",
            "[Fold 0] Epoch 059 | train_loss=0.6034 train_acc=0.7425 | val_loss=0.6422 val_acc=0.7484 | time=18.0s\n",
            "[Fold 0] Epoch 060 | train_loss=0.5451 train_acc=0.7670 | val_loss=0.6259 val_acc=0.7780 | time=18.4s\n",
            "[Fold 0] Epoch 061 | train_loss=0.5370 train_acc=0.7794 | val_loss=0.6274 val_acc=0.7562 | time=18.1s\n",
            "[Fold 0] Epoch 062 | train_loss=0.5405 train_acc=0.7709 | val_loss=0.6735 val_acc=0.7578 | time=17.9s\n",
            "[Fold 0] Epoch 063 | train_loss=0.5134 train_acc=0.7876 | val_loss=0.6594 val_acc=0.7795 | time=18.1s\n",
            "[Fold 0] Epoch 064 | train_loss=0.5401 train_acc=0.7685 | val_loss=0.6198 val_acc=0.7717 | time=18.1s\n",
            "[Fold 0] Epoch 065 | train_loss=0.5087 train_acc=0.7783 | val_loss=0.6340 val_acc=0.7624 | time=18.2s\n",
            "[Fold 0] Epoch 066 | train_loss=0.4842 train_acc=0.8023 | val_loss=0.6611 val_acc=0.7857 | time=18.1s\n",
            "[Fold 0] Epoch 067 | train_loss=0.4971 train_acc=0.7969 | val_loss=0.5918 val_acc=0.7671 | time=17.8s\n",
            "[Fold 0] Epoch 068 | train_loss=0.4788 train_acc=0.8016 | val_loss=0.6653 val_acc=0.7298 | time=18.2s\n",
            "[Fold 0] Epoch 069 | train_loss=0.4933 train_acc=0.7950 | val_loss=0.6049 val_acc=0.7702 | time=18.0s\n",
            "[Fold 0] Epoch 070 | train_loss=0.4889 train_acc=0.7915 | val_loss=0.6054 val_acc=0.7748 | time=18.0s\n",
            "[Fold 0] Epoch 071 | train_loss=0.4660 train_acc=0.8144 | val_loss=0.6481 val_acc=0.7717 | time=18.1s\n",
            "[Fold 0] Epoch 072 | train_loss=0.4804 train_acc=0.8019 | val_loss=0.6459 val_acc=0.7655 | time=18.2s\n",
            "[Fold 0] Epoch 073 | train_loss=0.4644 train_acc=0.8124 | val_loss=0.5860 val_acc=0.7780 | time=18.1s\n",
            "[Fold 0] Epoch 074 | train_loss=0.4354 train_acc=0.8214 | val_loss=0.6492 val_acc=0.7422 | time=18.1s\n",
            "[Fold 0] Epoch 075 | train_loss=0.4358 train_acc=0.8155 | val_loss=0.6231 val_acc=0.7826 | time=18.1s\n",
            "[Fold 0] Epoch 076 | train_loss=0.4237 train_acc=0.8221 | val_loss=0.6327 val_acc=0.7593 | time=18.1s\n",
            "[Fold 0] Epoch 077 | train_loss=0.4215 train_acc=0.8221 | val_loss=0.5752 val_acc=0.7717 | time=18.2s\n",
            "[Fold 0] Epoch 078 | train_loss=0.4253 train_acc=0.8283 | val_loss=0.6240 val_acc=0.7826 | time=18.1s\n",
            "[Fold 0] Epoch 079 | train_loss=0.3901 train_acc=0.8466 | val_loss=0.6199 val_acc=0.7702 | time=18.2s\n",
            "[Fold 0] Epoch 080 | train_loss=0.3912 train_acc=0.8462 | val_loss=0.6101 val_acc=0.7826 | time=18.1s\n",
            "[Fold 0] Epoch 081 | train_loss=0.3911 train_acc=0.8416 | val_loss=0.6579 val_acc=0.7655 | time=17.9s\n",
            "[Fold 0] Epoch 082 | train_loss=0.4071 train_acc=0.8326 | val_loss=0.6631 val_acc=0.7578 | time=18.1s\n",
            "[Fold 0] Epoch 083 | train_loss=0.3817 train_acc=0.8454 | val_loss=0.6262 val_acc=0.7609 | time=18.0s\n",
            "[Fold 0] Epoch 084 | train_loss=0.3918 train_acc=0.8458 | val_loss=0.6533 val_acc=0.7795 | time=17.9s\n",
            "[Fold 0] Epoch 085 | train_loss=0.3750 train_acc=0.8474 | val_loss=0.5927 val_acc=0.7764 | time=18.3s\n",
            "[Fold 0] Epoch 086 | train_loss=0.3476 train_acc=0.8629 | val_loss=0.6135 val_acc=0.7873 | time=18.2s\n",
            "[Fold 0] Epoch 087 | train_loss=0.3528 train_acc=0.8575 | val_loss=0.6201 val_acc=0.7733 | time=18.4s\n",
            "[Fold 0] Epoch 088 | train_loss=0.3443 train_acc=0.8575 | val_loss=0.6247 val_acc=0.7671 | time=18.1s\n",
            "[Fold 0] Epoch 089 | train_loss=0.3304 train_acc=0.8703 | val_loss=0.6146 val_acc=0.7888 | time=18.0s\n",
            "[Fold 0] Epoch 090 | train_loss=0.3220 train_acc=0.8757 | val_loss=0.6325 val_acc=0.7811 | time=18.4s\n",
            "[Fold 0] Epoch 091 | train_loss=0.3164 train_acc=0.8750 | val_loss=0.6456 val_acc=0.7935 | time=18.2s\n",
            "[Fold 0] Epoch 092 | train_loss=0.2895 train_acc=0.8812 | val_loss=0.6367 val_acc=0.7780 | time=18.1s\n",
            "[Fold 0] Epoch 093 | train_loss=0.3331 train_acc=0.8652 | val_loss=0.6166 val_acc=0.7717 | time=18.3s\n",
            "[Fold 0] Epoch 094 | train_loss=0.3232 train_acc=0.8707 | val_loss=0.6016 val_acc=0.7997 | time=18.0s\n",
            "[Fold 0] Epoch 095 | train_loss=0.3008 train_acc=0.8843 | val_loss=0.6639 val_acc=0.7717 | time=17.9s\n",
            "[Fold 0] Epoch 096 | train_loss=0.2939 train_acc=0.8804 | val_loss=0.6185 val_acc=0.7997 | time=17.9s\n",
            "[Fold 0] Epoch 097 | train_loss=0.2892 train_acc=0.8889 | val_loss=0.6288 val_acc=0.7733 | time=18.1s\n",
            "[Fold 0] Early stopping at epoch 97, best was 77\n",
            "\n",
            "========================= Fold 1 =========================\n",
            "[Fold 1] Epoch 001 | train_loss=1.0702 train_acc=0.4167 | val_loss=1.0852 val_acc=0.4317 | time=18.1s\n",
            "[Fold 1] Epoch 002 | train_loss=1.0660 train_acc=0.4311 | val_loss=1.0830 val_acc=0.4317 | time=18.0s\n",
            "[Fold 1] Epoch 003 | train_loss=1.0665 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=18.0s\n",
            "[Fold 1] Epoch 004 | train_loss=1.0659 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=18.0s\n",
            "[Fold 1] Epoch 005 | train_loss=1.0643 train_acc=0.4326 | val_loss=1.0787 val_acc=0.4317 | time=18.4s\n",
            "[Fold 1] Epoch 006 | train_loss=1.0514 train_acc=0.4637 | val_loss=1.0756 val_acc=0.4565 | time=18.0s\n",
            "[Fold 1] Epoch 007 | train_loss=1.0071 train_acc=0.5320 | val_loss=0.9942 val_acc=0.5668 | time=18.4s\n",
            "[Fold 1] Epoch 008 | train_loss=0.9786 train_acc=0.5557 | val_loss=1.0014 val_acc=0.5559 | time=18.2s\n",
            "[Fold 1] Epoch 009 | train_loss=0.9614 train_acc=0.5623 | val_loss=0.9624 val_acc=0.5916 | time=17.8s\n",
            "[Fold 1] Epoch 010 | train_loss=0.9429 train_acc=0.5771 | val_loss=0.9347 val_acc=0.5839 | time=18.2s\n",
            "[Fold 1] Epoch 011 | train_loss=0.9112 train_acc=0.5977 | val_loss=0.9173 val_acc=0.6040 | time=18.1s\n",
            "[Fold 1] Epoch 012 | train_loss=0.9000 train_acc=0.6004 | val_loss=0.9034 val_acc=0.6211 | time=18.0s\n",
            "[Fold 1] Epoch 013 | train_loss=0.8671 train_acc=0.6206 | val_loss=0.8780 val_acc=0.6273 | time=18.1s\n",
            "[Fold 1] Epoch 014 | train_loss=0.8487 train_acc=0.6190 | val_loss=0.9542 val_acc=0.5932 | time=18.2s\n",
            "[Fold 1] Epoch 015 | train_loss=0.8347 train_acc=0.6295 | val_loss=0.8665 val_acc=0.6320 | time=18.2s\n",
            "[Fold 1] Epoch 016 | train_loss=0.8028 train_acc=0.6528 | val_loss=0.8930 val_acc=0.5963 | time=18.1s\n",
            "[Fold 1] Epoch 017 | train_loss=0.7891 train_acc=0.6540 | val_loss=0.8252 val_acc=0.6599 | time=18.0s\n",
            "[Fold 1] Epoch 018 | train_loss=0.7590 train_acc=0.6796 | val_loss=0.7981 val_acc=0.6739 | time=17.7s\n",
            "[Fold 1] Epoch 019 | train_loss=0.7293 train_acc=0.6823 | val_loss=0.8727 val_acc=0.6320 | time=17.9s\n",
            "[Fold 1] Epoch 020 | train_loss=0.7411 train_acc=0.6839 | val_loss=0.7912 val_acc=0.6661 | time=18.2s\n",
            "[Fold 1] Epoch 021 | train_loss=0.7044 train_acc=0.6986 | val_loss=0.7671 val_acc=0.6957 | time=17.9s\n",
            "[Fold 1] Epoch 022 | train_loss=0.6957 train_acc=0.7049 | val_loss=0.7889 val_acc=0.6708 | time=18.0s\n",
            "[Fold 1] Epoch 023 | train_loss=0.7189 train_acc=0.6878 | val_loss=0.7679 val_acc=0.7050 | time=18.0s\n",
            "[Fold 1] Epoch 024 | train_loss=0.6919 train_acc=0.6948 | val_loss=0.7327 val_acc=0.6894 | time=17.9s\n",
            "[Fold 1] Epoch 025 | train_loss=0.6638 train_acc=0.7138 | val_loss=0.7276 val_acc=0.7050 | time=18.4s\n",
            "[Fold 1] Epoch 026 | train_loss=0.6702 train_acc=0.7083 | val_loss=0.7767 val_acc=0.6568 | time=17.9s\n",
            "[Fold 1] Epoch 027 | train_loss=0.6680 train_acc=0.7087 | val_loss=0.7550 val_acc=0.6599 | time=18.3s\n",
            "[Fold 1] Epoch 028 | train_loss=0.6328 train_acc=0.7278 | val_loss=0.7560 val_acc=0.6941 | time=17.9s\n",
            "[Fold 1] Epoch 029 | train_loss=0.6162 train_acc=0.7487 | val_loss=0.7394 val_acc=0.6801 | time=17.9s\n",
            "[Fold 1] Epoch 030 | train_loss=0.6447 train_acc=0.7192 | val_loss=0.7163 val_acc=0.7236 | time=18.2s\n",
            "[Fold 1] Epoch 031 | train_loss=0.6003 train_acc=0.7483 | val_loss=0.7421 val_acc=0.7019 | time=18.1s\n",
            "[Fold 1] Epoch 032 | train_loss=0.6106 train_acc=0.7379 | val_loss=0.8076 val_acc=0.6444 | time=18.2s\n",
            "[Fold 1] Epoch 033 | train_loss=0.5915 train_acc=0.7518 | val_loss=0.7030 val_acc=0.7081 | time=18.2s\n",
            "[Fold 1] Epoch 034 | train_loss=0.5729 train_acc=0.7584 | val_loss=0.6709 val_acc=0.7376 | time=18.1s\n",
            "[Fold 1] Epoch 035 | train_loss=0.5567 train_acc=0.7674 | val_loss=0.7576 val_acc=0.7050 | time=18.0s\n",
            "[Fold 1] Epoch 036 | train_loss=0.5683 train_acc=0.7627 | val_loss=0.6741 val_acc=0.7438 | time=18.1s\n",
            "[Fold 1] Epoch 037 | train_loss=0.5309 train_acc=0.7701 | val_loss=0.7020 val_acc=0.7329 | time=18.4s\n",
            "[Fold 1] Epoch 038 | train_loss=0.5013 train_acc=0.7876 | val_loss=0.7068 val_acc=0.7174 | time=18.0s\n",
            "[Fold 1] Epoch 039 | train_loss=0.5110 train_acc=0.7915 | val_loss=0.7122 val_acc=0.7267 | time=17.9s\n",
            "[Fold 1] Epoch 040 | train_loss=0.4991 train_acc=0.7969 | val_loss=0.6387 val_acc=0.7314 | time=18.2s\n",
            "[Fold 1] Epoch 041 | train_loss=0.4930 train_acc=0.8012 | val_loss=0.7480 val_acc=0.7174 | time=18.2s\n",
            "[Fold 1] Epoch 042 | train_loss=0.4873 train_acc=0.8047 | val_loss=0.6256 val_acc=0.7516 | time=18.2s\n",
            "[Fold 1] Epoch 043 | train_loss=0.4499 train_acc=0.8229 | val_loss=0.6667 val_acc=0.7391 | time=18.2s\n",
            "[Fold 1] Epoch 044 | train_loss=0.4442 train_acc=0.8198 | val_loss=0.6537 val_acc=0.7329 | time=18.1s\n",
            "[Fold 1] Epoch 045 | train_loss=0.4992 train_acc=0.7934 | val_loss=0.5910 val_acc=0.7593 | time=18.1s\n",
            "[Fold 1] Epoch 046 | train_loss=0.4242 train_acc=0.8307 | val_loss=0.6036 val_acc=0.7407 | time=18.0s\n",
            "[Fold 1] Epoch 047 | train_loss=0.4070 train_acc=0.8338 | val_loss=0.6317 val_acc=0.7484 | time=18.2s\n",
            "[Fold 1] Epoch 048 | train_loss=0.3881 train_acc=0.8466 | val_loss=0.6389 val_acc=0.7438 | time=17.8s\n",
            "[Fold 1] Epoch 049 | train_loss=0.4007 train_acc=0.8299 | val_loss=0.6302 val_acc=0.7593 | time=18.0s\n",
            "[Fold 1] Epoch 050 | train_loss=0.4104 train_acc=0.8322 | val_loss=0.7170 val_acc=0.7034 | time=18.1s\n",
            "[Fold 1] Epoch 051 | train_loss=0.4086 train_acc=0.8299 | val_loss=0.5989 val_acc=0.7500 | time=18.1s\n",
            "[Fold 1] Epoch 052 | train_loss=0.3700 train_acc=0.8454 | val_loss=0.6561 val_acc=0.7391 | time=17.9s\n",
            "[Fold 1] Epoch 053 | train_loss=0.3769 train_acc=0.8493 | val_loss=0.6594 val_acc=0.7376 | time=18.0s\n",
            "[Fold 1] Epoch 054 | train_loss=0.3498 train_acc=0.8544 | val_loss=0.5830 val_acc=0.7547 | time=17.9s\n",
            "[Fold 1] Epoch 055 | train_loss=0.3450 train_acc=0.8567 | val_loss=0.6137 val_acc=0.7547 | time=18.2s\n",
            "[Fold 1] Epoch 056 | train_loss=0.3365 train_acc=0.8660 | val_loss=0.5803 val_acc=0.7624 | time=18.0s\n",
            "[Fold 1] Epoch 057 | train_loss=0.3171 train_acc=0.8683 | val_loss=0.7038 val_acc=0.7453 | time=18.1s\n",
            "[Fold 1] Epoch 058 | train_loss=0.3660 train_acc=0.8497 | val_loss=0.6499 val_acc=0.7531 | time=18.0s\n",
            "[Fold 1] Epoch 059 | train_loss=0.3075 train_acc=0.8765 | val_loss=0.6062 val_acc=0.7717 | time=18.2s\n",
            "[Fold 1] Epoch 060 | train_loss=0.2864 train_acc=0.8804 | val_loss=0.6088 val_acc=0.7764 | time=18.2s\n",
            "[Fold 1] Epoch 061 | train_loss=0.2868 train_acc=0.8847 | val_loss=0.6913 val_acc=0.7609 | time=18.2s\n",
            "[Fold 1] Epoch 062 | train_loss=0.3173 train_acc=0.8633 | val_loss=0.5731 val_acc=0.7671 | time=18.1s\n",
            "[Fold 1] Epoch 063 | train_loss=0.2807 train_acc=0.8816 | val_loss=0.7018 val_acc=0.7671 | time=17.9s\n",
            "[Fold 1] Epoch 064 | train_loss=0.2728 train_acc=0.8913 | val_loss=0.6744 val_acc=0.7220 | time=17.9s\n",
            "[Fold 1] Epoch 065 | train_loss=0.2937 train_acc=0.8847 | val_loss=0.6333 val_acc=0.7640 | time=18.1s\n",
            "[Fold 1] Epoch 066 | train_loss=0.2590 train_acc=0.9006 | val_loss=0.6727 val_acc=0.7422 | time=18.1s\n",
            "[Fold 1] Epoch 067 | train_loss=0.3039 train_acc=0.8765 | val_loss=0.5964 val_acc=0.7702 | time=18.1s\n",
            "[Fold 1] Epoch 068 | train_loss=0.2513 train_acc=0.8990 | val_loss=0.6934 val_acc=0.7748 | time=18.2s\n",
            "[Fold 1] Epoch 069 | train_loss=0.2264 train_acc=0.9099 | val_loss=0.6639 val_acc=0.7671 | time=18.0s\n",
            "[Fold 1] Epoch 070 | train_loss=0.2224 train_acc=0.9103 | val_loss=0.6523 val_acc=0.7562 | time=18.1s\n",
            "[Fold 1] Epoch 071 | train_loss=0.2174 train_acc=0.9095 | val_loss=0.6593 val_acc=0.7904 | time=18.0s\n",
            "[Fold 1] Epoch 072 | train_loss=0.2510 train_acc=0.8990 | val_loss=0.6661 val_acc=0.7795 | time=18.2s\n",
            "[Fold 1] Epoch 073 | train_loss=0.2729 train_acc=0.8819 | val_loss=0.6742 val_acc=0.7780 | time=17.9s\n",
            "[Fold 1] Epoch 074 | train_loss=0.2228 train_acc=0.9122 | val_loss=0.6846 val_acc=0.7562 | time=18.2s\n",
            "[Fold 1] Epoch 075 | train_loss=0.2144 train_acc=0.9111 | val_loss=0.7630 val_acc=0.7640 | time=18.2s\n",
            "[Fold 1] Epoch 076 | train_loss=0.1918 train_acc=0.9243 | val_loss=0.7311 val_acc=0.7609 | time=18.2s\n",
            "[Fold 1] Epoch 077 | train_loss=0.1728 train_acc=0.9336 | val_loss=0.7307 val_acc=0.7578 | time=18.2s\n",
            "[Fold 1] Epoch 078 | train_loss=0.1828 train_acc=0.9274 | val_loss=0.8105 val_acc=0.7562 | time=18.2s\n",
            "[Fold 1] Epoch 079 | train_loss=0.2183 train_acc=0.9146 | val_loss=0.7273 val_acc=0.7764 | time=18.0s\n",
            "[Fold 1] Epoch 080 | train_loss=0.1523 train_acc=0.9425 | val_loss=0.6989 val_acc=0.7795 | time=18.0s\n",
            "[Fold 1] Epoch 081 | train_loss=0.1509 train_acc=0.9398 | val_loss=0.7561 val_acc=0.7811 | time=17.9s\n",
            "[Fold 1] Epoch 082 | train_loss=0.1548 train_acc=0.9421 | val_loss=0.8052 val_acc=0.7516 | time=18.1s\n",
            "[Fold 1] Early stopping at epoch 82, best was 62\n",
            "\n",
            "========================= Fold 2 =========================\n",
            "[Fold 2] Epoch 001 | train_loss=1.0688 train_acc=0.4307 | val_loss=1.0857 val_acc=0.4301 | time=17.9s\n",
            "[Fold 2] Epoch 002 | train_loss=1.0679 train_acc=0.4315 | val_loss=1.0809 val_acc=0.4301 | time=18.0s\n",
            "[Fold 2] Epoch 003 | train_loss=1.0673 train_acc=0.4315 | val_loss=1.0836 val_acc=0.4301 | time=18.1s\n",
            "[Fold 2] Epoch 004 | train_loss=1.0671 train_acc=0.4315 | val_loss=1.0831 val_acc=0.4301 | time=17.8s\n",
            "[Fold 2] Epoch 005 | train_loss=1.0650 train_acc=0.4315 | val_loss=1.0858 val_acc=0.4301 | time=18.1s\n",
            "[Fold 2] Epoch 006 | train_loss=1.0651 train_acc=0.4315 | val_loss=1.0832 val_acc=0.4301 | time=18.0s\n",
            "[Fold 2] Epoch 007 | train_loss=1.0636 train_acc=0.4338 | val_loss=1.0717 val_acc=0.4938 | time=17.9s\n",
            "[Fold 2] Epoch 008 | train_loss=1.0242 train_acc=0.5188 | val_loss=1.0251 val_acc=0.5202 | time=18.2s\n",
            "[Fold 2] Epoch 009 | train_loss=0.9782 train_acc=0.5600 | val_loss=0.9849 val_acc=0.5652 | time=18.0s\n",
            "[Fold 2] Epoch 010 | train_loss=0.9554 train_acc=0.5771 | val_loss=0.9606 val_acc=0.5823 | time=18.1s\n",
            "[Fold 2] Epoch 011 | train_loss=0.9439 train_acc=0.5806 | val_loss=0.9588 val_acc=0.5792 | time=18.0s\n",
            "[Fold 2] Epoch 012 | train_loss=0.9259 train_acc=0.5891 | val_loss=0.9496 val_acc=0.5839 | time=17.9s\n",
            "[Fold 2] Epoch 013 | train_loss=0.9213 train_acc=0.5961 | val_loss=0.9356 val_acc=0.5901 | time=18.1s\n",
            "[Fold 2] Epoch 014 | train_loss=0.9111 train_acc=0.5992 | val_loss=0.9306 val_acc=0.6040 | time=17.9s\n",
            "[Fold 2] Epoch 015 | train_loss=0.9116 train_acc=0.5981 | val_loss=0.9130 val_acc=0.6009 | time=18.3s\n",
            "[Fold 2] Epoch 016 | train_loss=0.8892 train_acc=0.6058 | val_loss=0.8975 val_acc=0.5947 | time=17.9s\n",
            "[Fold 2] Epoch 017 | train_loss=0.8795 train_acc=0.6085 | val_loss=0.9045 val_acc=0.5947 | time=17.7s\n",
            "[Fold 2] Epoch 018 | train_loss=0.8676 train_acc=0.6202 | val_loss=0.8959 val_acc=0.6273 | time=18.4s\n",
            "[Fold 2] Epoch 019 | train_loss=0.8635 train_acc=0.6272 | val_loss=0.9096 val_acc=0.5233 | time=18.1s\n",
            "[Fold 2] Epoch 020 | train_loss=0.8567 train_acc=0.6287 | val_loss=0.8893 val_acc=0.6227 | time=18.2s\n",
            "[Fold 2] Epoch 021 | train_loss=0.8386 train_acc=0.6396 | val_loss=0.8620 val_acc=0.6211 | time=18.1s\n",
            "[Fold 2] Epoch 022 | train_loss=0.8256 train_acc=0.6447 | val_loss=0.8479 val_acc=0.6382 | time=17.9s\n",
            "[Fold 2] Epoch 023 | train_loss=0.8033 train_acc=0.6606 | val_loss=0.8581 val_acc=0.6304 | time=17.9s\n",
            "[Fold 2] Epoch 024 | train_loss=0.8074 train_acc=0.6540 | val_loss=0.8681 val_acc=0.6242 | time=18.1s\n",
            "[Fold 2] Epoch 025 | train_loss=0.7829 train_acc=0.6680 | val_loss=0.8336 val_acc=0.6304 | time=18.3s\n",
            "[Fold 2] Epoch 026 | train_loss=0.7755 train_acc=0.6649 | val_loss=0.8264 val_acc=0.6429 | time=18.1s\n",
            "[Fold 2] Epoch 027 | train_loss=0.7494 train_acc=0.6796 | val_loss=0.8194 val_acc=0.6413 | time=18.0s\n",
            "[Fold 2] Epoch 028 | train_loss=0.7553 train_acc=0.6695 | val_loss=0.8170 val_acc=0.6366 | time=18.5s\n",
            "[Fold 2] Epoch 029 | train_loss=0.7401 train_acc=0.6862 | val_loss=0.7877 val_acc=0.6630 | time=18.1s\n",
            "[Fold 2] Epoch 030 | train_loss=0.7303 train_acc=0.6847 | val_loss=0.7838 val_acc=0.6708 | time=18.1s\n",
            "[Fold 2] Epoch 031 | train_loss=0.7177 train_acc=0.6944 | val_loss=0.7911 val_acc=0.6677 | time=18.1s\n",
            "[Fold 2] Epoch 032 | train_loss=0.7026 train_acc=0.6951 | val_loss=0.7749 val_acc=0.6693 | time=18.0s\n",
            "[Fold 2] Epoch 033 | train_loss=0.6980 train_acc=0.7045 | val_loss=0.8238 val_acc=0.6071 | time=18.1s\n",
            "[Fold 2] Epoch 034 | train_loss=0.7153 train_acc=0.6979 | val_loss=0.7972 val_acc=0.6661 | time=18.2s\n",
            "[Fold 2] Epoch 035 | train_loss=0.6771 train_acc=0.7138 | val_loss=0.8114 val_acc=0.6056 | time=18.2s\n",
            "[Fold 2] Epoch 036 | train_loss=0.7010 train_acc=0.7052 | val_loss=0.7610 val_acc=0.6786 | time=18.0s\n",
            "[Fold 2] Epoch 037 | train_loss=0.6511 train_acc=0.7293 | val_loss=0.7439 val_acc=0.6894 | time=18.0s\n",
            "[Fold 2] Epoch 038 | train_loss=0.6405 train_acc=0.7223 | val_loss=0.7317 val_acc=0.6972 | time=17.9s\n",
            "[Fold 2] Epoch 039 | train_loss=0.6352 train_acc=0.7324 | val_loss=0.7349 val_acc=0.6910 | time=18.0s\n",
            "[Fold 2] Epoch 040 | train_loss=0.6324 train_acc=0.7355 | val_loss=0.7485 val_acc=0.6615 | time=18.3s\n",
            "[Fold 2] Epoch 041 | train_loss=0.6204 train_acc=0.7371 | val_loss=0.7358 val_acc=0.6677 | time=18.2s\n",
            "[Fold 2] Epoch 042 | train_loss=0.6324 train_acc=0.7305 | val_loss=0.7411 val_acc=0.6568 | time=18.0s\n",
            "[Fold 2] Epoch 043 | train_loss=0.6005 train_acc=0.7402 | val_loss=0.6827 val_acc=0.6972 | time=17.8s\n",
            "[Fold 2] Epoch 044 | train_loss=0.6037 train_acc=0.7503 | val_loss=0.7315 val_acc=0.6988 | time=18.3s\n",
            "[Fold 2] Epoch 045 | train_loss=0.5825 train_acc=0.7487 | val_loss=0.7239 val_acc=0.7003 | time=18.4s\n",
            "[Fold 2] Epoch 046 | train_loss=0.5925 train_acc=0.7499 | val_loss=0.6741 val_acc=0.7220 | time=18.3s\n",
            "[Fold 2] Epoch 047 | train_loss=0.5679 train_acc=0.7627 | val_loss=0.7119 val_acc=0.6941 | time=18.3s\n",
            "[Fold 2] Epoch 048 | train_loss=0.5789 train_acc=0.7619 | val_loss=0.6805 val_acc=0.7174 | time=17.9s\n",
            "[Fold 2] Epoch 049 | train_loss=0.5550 train_acc=0.7678 | val_loss=0.7604 val_acc=0.7096 | time=18.0s\n",
            "[Fold 2] Epoch 050 | train_loss=0.5633 train_acc=0.7720 | val_loss=0.6774 val_acc=0.7283 | time=18.4s\n",
            "[Fold 2] Epoch 051 | train_loss=0.5219 train_acc=0.7810 | val_loss=0.6922 val_acc=0.7096 | time=18.1s\n",
            "[Fold 2] Epoch 052 | train_loss=0.5399 train_acc=0.7682 | val_loss=0.6503 val_acc=0.7283 | time=18.3s\n",
            "[Fold 2] Epoch 053 | train_loss=0.5121 train_acc=0.7907 | val_loss=0.6618 val_acc=0.7112 | time=18.0s\n",
            "[Fold 2] Epoch 054 | train_loss=0.5087 train_acc=0.7845 | val_loss=0.6388 val_acc=0.7189 | time=18.1s\n",
            "[Fold 2] Epoch 055 | train_loss=0.5010 train_acc=0.7907 | val_loss=0.6024 val_acc=0.7407 | time=18.1s\n",
            "[Fold 2] Epoch 056 | train_loss=0.4871 train_acc=0.7984 | val_loss=0.7776 val_acc=0.7127 | time=18.0s\n",
            "[Fold 2] Epoch 057 | train_loss=0.4827 train_acc=0.8016 | val_loss=0.6293 val_acc=0.7360 | time=18.4s\n",
            "[Fold 2] Epoch 058 | train_loss=0.4646 train_acc=0.8035 | val_loss=0.6800 val_acc=0.7236 | time=18.0s\n",
            "[Fold 2] Epoch 059 | train_loss=0.4478 train_acc=0.8155 | val_loss=0.5972 val_acc=0.7516 | time=17.9s\n",
            "[Fold 2] Epoch 060 | train_loss=0.4423 train_acc=0.8210 | val_loss=0.6270 val_acc=0.7438 | time=17.9s\n",
            "[Fold 2] Epoch 061 | train_loss=0.4460 train_acc=0.8151 | val_loss=0.6473 val_acc=0.7298 | time=18.2s\n",
            "[Fold 2] Epoch 062 | train_loss=0.4378 train_acc=0.8198 | val_loss=0.6271 val_acc=0.7500 | time=18.3s\n",
            "[Fold 2] Epoch 063 | train_loss=0.4381 train_acc=0.8120 | val_loss=0.6057 val_acc=0.7453 | time=18.1s\n",
            "[Fold 2] Epoch 064 | train_loss=0.4049 train_acc=0.8357 | val_loss=0.6416 val_acc=0.7189 | time=17.9s\n",
            "[Fold 2] Epoch 065 | train_loss=0.4215 train_acc=0.8307 | val_loss=0.6337 val_acc=0.7422 | time=17.8s\n",
            "[Fold 2] Epoch 066 | train_loss=0.3932 train_acc=0.8396 | val_loss=0.8263 val_acc=0.6863 | time=17.8s\n",
            "[Fold 2] Epoch 067 | train_loss=0.3943 train_acc=0.8384 | val_loss=0.7230 val_acc=0.7003 | time=18.4s\n",
            "[Fold 2] Epoch 068 | train_loss=0.3811 train_acc=0.8458 | val_loss=0.5835 val_acc=0.7578 | time=18.1s\n",
            "[Fold 2] Epoch 069 | train_loss=0.3906 train_acc=0.8454 | val_loss=0.7262 val_acc=0.7174 | time=18.0s\n",
            "[Fold 2] Epoch 070 | train_loss=0.3822 train_acc=0.8474 | val_loss=0.6296 val_acc=0.7453 | time=18.1s\n",
            "[Fold 2] Epoch 071 | train_loss=0.3488 train_acc=0.8586 | val_loss=0.7008 val_acc=0.7500 | time=18.0s\n",
            "[Fold 2] Epoch 072 | train_loss=0.3673 train_acc=0.8478 | val_loss=0.6748 val_acc=0.7422 | time=18.3s\n",
            "[Fold 2] Epoch 073 | train_loss=0.3607 train_acc=0.8617 | val_loss=0.6384 val_acc=0.7717 | time=17.8s\n",
            "[Fold 2] Epoch 074 | train_loss=0.3298 train_acc=0.8676 | val_loss=0.6291 val_acc=0.7733 | time=18.2s\n",
            "[Fold 2] Epoch 075 | train_loss=0.3253 train_acc=0.8715 | val_loss=0.8221 val_acc=0.7500 | time=17.9s\n",
            "[Fold 2] Epoch 076 | train_loss=0.3172 train_acc=0.8691 | val_loss=0.6966 val_acc=0.7609 | time=17.9s\n",
            "[Fold 2] Epoch 077 | train_loss=0.3147 train_acc=0.8750 | val_loss=0.6365 val_acc=0.7593 | time=18.2s\n",
            "[Fold 2] Epoch 078 | train_loss=0.3363 train_acc=0.8614 | val_loss=0.6847 val_acc=0.7158 | time=17.8s\n",
            "[Fold 2] Epoch 079 | train_loss=0.3012 train_acc=0.8808 | val_loss=0.7075 val_acc=0.7407 | time=18.1s\n",
            "[Fold 2] Epoch 080 | train_loss=0.2953 train_acc=0.8870 | val_loss=0.7535 val_acc=0.7562 | time=18.1s\n",
            "[Fold 2] Epoch 081 | train_loss=0.2902 train_acc=0.8858 | val_loss=0.7130 val_acc=0.7469 | time=18.0s\n",
            "[Fold 2] Epoch 082 | train_loss=0.2942 train_acc=0.8847 | val_loss=0.6330 val_acc=0.7826 | time=18.3s\n",
            "[Fold 2] Epoch 083 | train_loss=0.2577 train_acc=0.8971 | val_loss=0.6628 val_acc=0.7811 | time=18.0s\n",
            "[Fold 2] Epoch 084 | train_loss=0.2288 train_acc=0.9157 | val_loss=0.6100 val_acc=0.7857 | time=18.0s\n",
            "[Fold 2] Epoch 085 | train_loss=0.2324 train_acc=0.9165 | val_loss=0.7411 val_acc=0.7717 | time=18.1s\n",
            "[Fold 2] Epoch 086 | train_loss=0.2620 train_acc=0.8967 | val_loss=0.6391 val_acc=0.7671 | time=17.9s\n",
            "[Fold 2] Epoch 087 | train_loss=0.2708 train_acc=0.8882 | val_loss=0.6415 val_acc=0.7686 | time=18.3s\n",
            "[Fold 2] Epoch 088 | train_loss=0.2522 train_acc=0.8986 | val_loss=0.8009 val_acc=0.7298 | time=18.0s\n",
            "[Fold 2] Early stopping at epoch 88, best was 68\n",
            "\n",
            "========================= Fold 3 =========================\n",
            "[Fold 3] Epoch 001 | train_loss=1.0678 train_acc=0.4283 | val_loss=1.0818 val_acc=0.4301 | time=18.3s\n",
            "[Fold 3] Epoch 002 | train_loss=1.0664 train_acc=0.4315 | val_loss=1.0838 val_acc=0.4301 | time=17.8s\n",
            "[Fold 3] Epoch 003 | train_loss=1.0644 train_acc=0.4315 | val_loss=1.0792 val_acc=0.4301 | time=17.8s\n",
            "[Fold 3] Epoch 004 | train_loss=1.0402 train_acc=0.4823 | val_loss=1.0229 val_acc=0.5575 | time=18.2s\n",
            "[Fold 3] Epoch 005 | train_loss=0.9836 train_acc=0.5546 | val_loss=0.9871 val_acc=0.5699 | time=18.0s\n",
            "[Fold 3] Epoch 006 | train_loss=0.9528 train_acc=0.5744 | val_loss=0.9568 val_acc=0.5854 | time=18.3s\n",
            "[Fold 3] Epoch 007 | train_loss=0.9445 train_acc=0.5802 | val_loss=0.9391 val_acc=0.5901 | time=18.0s\n",
            "[Fold 3] Epoch 008 | train_loss=0.9296 train_acc=0.5911 | val_loss=0.9383 val_acc=0.5870 | time=18.0s\n",
            "[Fold 3] Epoch 009 | train_loss=0.9190 train_acc=0.5856 | val_loss=0.9086 val_acc=0.6025 | time=18.2s\n",
            "[Fold 3] Epoch 010 | train_loss=0.9011 train_acc=0.5957 | val_loss=0.8869 val_acc=0.6056 | time=18.2s\n",
            "[Fold 3] Epoch 011 | train_loss=0.9091 train_acc=0.5953 | val_loss=0.8861 val_acc=0.6009 | time=18.0s\n",
            "[Fold 3] Epoch 012 | train_loss=0.8650 train_acc=0.6155 | val_loss=0.9322 val_acc=0.5932 | time=17.9s\n",
            "[Fold 3] Epoch 013 | train_loss=0.8528 train_acc=0.6167 | val_loss=0.8348 val_acc=0.6553 | time=18.0s\n",
            "[Fold 3] Epoch 014 | train_loss=0.8215 train_acc=0.6416 | val_loss=0.8322 val_acc=0.6522 | time=18.2s\n",
            "[Fold 3] Epoch 015 | train_loss=0.8046 train_acc=0.6544 | val_loss=0.8433 val_acc=0.6429 | time=17.9s\n",
            "[Fold 3] Epoch 016 | train_loss=0.7822 train_acc=0.6614 | val_loss=0.7967 val_acc=0.6584 | time=18.2s\n",
            "[Fold 3] Epoch 017 | train_loss=0.7822 train_acc=0.6590 | val_loss=0.8218 val_acc=0.6475 | time=18.1s\n",
            "[Fold 3] Epoch 018 | train_loss=0.7660 train_acc=0.6649 | val_loss=0.7661 val_acc=0.6957 | time=17.9s\n",
            "[Fold 3] Epoch 019 | train_loss=0.7595 train_acc=0.6707 | val_loss=0.7702 val_acc=0.6755 | time=18.0s\n",
            "[Fold 3] Epoch 020 | train_loss=0.7528 train_acc=0.6711 | val_loss=0.8741 val_acc=0.6599 | time=18.1s\n",
            "[Fold 3] Epoch 021 | train_loss=0.7443 train_acc=0.6765 | val_loss=0.7488 val_acc=0.6832 | time=18.2s\n",
            "[Fold 3] Epoch 022 | train_loss=0.7134 train_acc=0.6882 | val_loss=0.7638 val_acc=0.6925 | time=17.7s\n",
            "[Fold 3] Epoch 023 | train_loss=0.7159 train_acc=0.6893 | val_loss=0.7532 val_acc=0.6615 | time=18.1s\n",
            "[Fold 3] Epoch 024 | train_loss=0.6972 train_acc=0.6963 | val_loss=0.7414 val_acc=0.6972 | time=18.1s\n",
            "[Fold 3] Epoch 025 | train_loss=0.6964 train_acc=0.6963 | val_loss=0.7758 val_acc=0.6398 | time=17.8s\n",
            "[Fold 3] Epoch 026 | train_loss=0.6861 train_acc=0.6986 | val_loss=0.7423 val_acc=0.6941 | time=18.1s\n",
            "[Fold 3] Epoch 027 | train_loss=0.6768 train_acc=0.7060 | val_loss=0.7250 val_acc=0.6910 | time=17.7s\n",
            "[Fold 3] Epoch 028 | train_loss=0.6630 train_acc=0.7076 | val_loss=0.6956 val_acc=0.7096 | time=18.0s\n",
            "[Fold 3] Epoch 029 | train_loss=0.6504 train_acc=0.7181 | val_loss=0.7651 val_acc=0.6755 | time=18.1s\n",
            "[Fold 3] Epoch 030 | train_loss=0.6320 train_acc=0.7227 | val_loss=0.6964 val_acc=0.7314 | time=18.1s\n",
            "[Fold 3] Epoch 031 | train_loss=0.6263 train_acc=0.7340 | val_loss=0.7530 val_acc=0.7158 | time=18.2s\n",
            "[Fold 3] Epoch 032 | train_loss=0.6343 train_acc=0.7165 | val_loss=0.7426 val_acc=0.7019 | time=18.1s\n",
            "[Fold 3] Epoch 033 | train_loss=0.6258 train_acc=0.7200 | val_loss=0.7074 val_acc=0.7267 | time=18.0s\n",
            "[Fold 3] Epoch 034 | train_loss=0.5993 train_acc=0.7487 | val_loss=0.7367 val_acc=0.7081 | time=18.2s\n",
            "[Fold 3] Epoch 035 | train_loss=0.5907 train_acc=0.7483 | val_loss=0.7176 val_acc=0.7236 | time=18.0s\n",
            "[Fold 3] Epoch 036 | train_loss=0.5724 train_acc=0.7491 | val_loss=0.7296 val_acc=0.7034 | time=17.7s\n",
            "[Fold 3] Epoch 037 | train_loss=0.5383 train_acc=0.7759 | val_loss=0.6791 val_acc=0.7578 | time=18.1s\n",
            "[Fold 3] Epoch 038 | train_loss=0.5510 train_acc=0.7666 | val_loss=0.7524 val_acc=0.7127 | time=18.0s\n",
            "[Fold 3] Epoch 039 | train_loss=0.5320 train_acc=0.7790 | val_loss=0.7713 val_acc=0.7019 | time=18.2s\n",
            "[Fold 3] Epoch 040 | train_loss=0.5265 train_acc=0.7783 | val_loss=0.6940 val_acc=0.7531 | time=18.0s\n",
            "[Fold 3] Epoch 041 | train_loss=0.5007 train_acc=0.7895 | val_loss=0.7660 val_acc=0.6879 | time=18.0s\n",
            "[Fold 3] Epoch 042 | train_loss=0.4865 train_acc=0.7984 | val_loss=0.6753 val_acc=0.7717 | time=18.0s\n",
            "[Fold 3] Epoch 043 | train_loss=0.4862 train_acc=0.7953 | val_loss=0.7511 val_acc=0.7283 | time=18.3s\n",
            "[Fold 3] Epoch 044 | train_loss=0.4576 train_acc=0.8144 | val_loss=0.6729 val_acc=0.7702 | time=18.4s\n",
            "[Fold 3] Epoch 045 | train_loss=0.4544 train_acc=0.8159 | val_loss=0.6423 val_acc=0.7686 | time=18.1s\n",
            "[Fold 3] Epoch 046 | train_loss=0.4450 train_acc=0.8163 | val_loss=0.6958 val_acc=0.7733 | time=18.2s\n",
            "[Fold 3] Epoch 047 | train_loss=0.4231 train_acc=0.8346 | val_loss=0.6610 val_acc=0.7640 | time=17.6s\n",
            "[Fold 3] Epoch 048 | train_loss=0.4062 train_acc=0.8357 | val_loss=0.6678 val_acc=0.7733 | time=17.8s\n",
            "[Fold 3] Epoch 049 | train_loss=0.4236 train_acc=0.8256 | val_loss=0.6912 val_acc=0.7640 | time=18.1s\n",
            "[Fold 3] Epoch 050 | train_loss=0.3956 train_acc=0.8478 | val_loss=0.6766 val_acc=0.7686 | time=18.0s\n",
            "[Fold 3] Epoch 051 | train_loss=0.3867 train_acc=0.8532 | val_loss=0.7003 val_acc=0.7391 | time=18.0s\n",
            "[Fold 3] Epoch 052 | train_loss=0.3732 train_acc=0.8524 | val_loss=0.7863 val_acc=0.7624 | time=18.0s\n",
            "[Fold 3] Epoch 053 | train_loss=0.3694 train_acc=0.8524 | val_loss=0.6527 val_acc=0.7811 | time=17.9s\n",
            "[Fold 3] Epoch 054 | train_loss=0.3623 train_acc=0.8579 | val_loss=0.6811 val_acc=0.7795 | time=18.4s\n",
            "[Fold 3] Epoch 055 | train_loss=0.3435 train_acc=0.8683 | val_loss=0.7786 val_acc=0.7360 | time=17.8s\n",
            "[Fold 3] Epoch 056 | train_loss=0.3615 train_acc=0.8602 | val_loss=0.7112 val_acc=0.7671 | time=17.9s\n",
            "[Fold 3] Epoch 057 | train_loss=0.3507 train_acc=0.8621 | val_loss=0.6817 val_acc=0.7935 | time=18.1s\n",
            "[Fold 3] Epoch 058 | train_loss=0.3123 train_acc=0.8812 | val_loss=0.7172 val_acc=0.7717 | time=18.0s\n",
            "[Fold 3] Epoch 059 | train_loss=0.3106 train_acc=0.8850 | val_loss=0.6867 val_acc=0.7811 | time=18.2s\n",
            "[Fold 3] Epoch 060 | train_loss=0.3284 train_acc=0.8757 | val_loss=0.8062 val_acc=0.7578 | time=18.0s\n",
            "[Fold 3] Epoch 061 | train_loss=0.3499 train_acc=0.8606 | val_loss=0.7170 val_acc=0.7780 | time=18.2s\n",
            "[Fold 3] Epoch 062 | train_loss=0.3219 train_acc=0.8707 | val_loss=0.6911 val_acc=0.7748 | time=18.1s\n",
            "[Fold 3] Epoch 063 | train_loss=0.2959 train_acc=0.8878 | val_loss=0.6682 val_acc=0.7795 | time=17.9s\n",
            "[Fold 3] Epoch 064 | train_loss=0.2983 train_acc=0.8874 | val_loss=0.7024 val_acc=0.7795 | time=18.1s\n",
            "[Fold 3] Epoch 065 | train_loss=0.2534 train_acc=0.9021 | val_loss=0.7229 val_acc=0.8012 | time=18.1s\n",
            "[Fold 3] Early stopping at epoch 65, best was 45\n",
            "\n",
            "========================= Fold 4 =========================\n",
            "[Fold 4] Epoch 001 | train_loss=1.0685 train_acc=0.4301 | val_loss=1.0886 val_acc=0.4323 | time=18.2s\n",
            "[Fold 4] Epoch 002 | train_loss=1.0658 train_acc=0.4309 | val_loss=1.0807 val_acc=0.4323 | time=18.0s\n",
            "[Fold 4] Epoch 003 | train_loss=1.0680 train_acc=0.4309 | val_loss=1.0836 val_acc=0.4323 | time=17.8s\n",
            "[Fold 4] Epoch 004 | train_loss=1.0674 train_acc=0.4309 | val_loss=1.0813 val_acc=0.4323 | time=18.4s\n",
            "[Fold 4] Epoch 005 | train_loss=1.0670 train_acc=0.4309 | val_loss=1.0851 val_acc=0.4323 | time=17.8s\n",
            "[Fold 4] Epoch 006 | train_loss=1.0666 train_acc=0.4309 | val_loss=1.0818 val_acc=0.4323 | time=18.0s\n",
            "[Fold 4] Epoch 007 | train_loss=1.0660 train_acc=0.4309 | val_loss=1.0810 val_acc=0.4323 | time=18.0s\n",
            "[Fold 4] Epoch 008 | train_loss=1.0658 train_acc=0.4309 | val_loss=1.0810 val_acc=0.4323 | time=18.0s\n",
            "[Fold 4] Epoch 009 | train_loss=1.0621 train_acc=0.4297 | val_loss=1.0735 val_acc=0.4386 | time=18.4s\n",
            "[Fold 4] Epoch 010 | train_loss=1.0521 train_acc=0.4480 | val_loss=1.0721 val_acc=0.4432 | time=18.1s\n",
            "[Fold 4] Epoch 011 | train_loss=1.0465 train_acc=0.4596 | val_loss=1.0610 val_acc=0.4930 | time=17.9s\n",
            "[Fold 4] Epoch 012 | train_loss=0.9968 train_acc=0.5377 | val_loss=0.9921 val_acc=0.5599 | time=18.3s\n",
            "[Fold 4] Epoch 013 | train_loss=0.9399 train_acc=0.5807 | val_loss=1.0165 val_acc=0.5381 | time=18.5s\n",
            "[Fold 4] Epoch 014 | train_loss=0.9230 train_acc=0.5955 | val_loss=1.0162 val_acc=0.5739 | time=18.2s\n",
            "[Fold 4] Epoch 015 | train_loss=0.9130 train_acc=0.6071 | val_loss=0.9346 val_acc=0.5832 | time=18.1s\n",
            "[Fold 4] Epoch 016 | train_loss=0.9008 train_acc=0.6036 | val_loss=0.9504 val_acc=0.6050 | time=18.2s\n",
            "[Fold 4] Epoch 017 | train_loss=0.8851 train_acc=0.6254 | val_loss=0.9344 val_acc=0.6065 | time=18.2s\n",
            "[Fold 4] Epoch 018 | train_loss=0.8801 train_acc=0.6227 | val_loss=0.9299 val_acc=0.6034 | time=18.1s\n",
            "[Fold 4] Epoch 019 | train_loss=0.8679 train_acc=0.6258 | val_loss=0.9299 val_acc=0.5956 | time=18.2s\n",
            "[Fold 4] Epoch 020 | train_loss=0.8580 train_acc=0.6343 | val_loss=0.9354 val_acc=0.6019 | time=18.2s\n",
            "[Fold 4] Epoch 021 | train_loss=0.8504 train_acc=0.6359 | val_loss=0.9244 val_acc=0.6050 | time=18.0s\n",
            "[Fold 4] Epoch 022 | train_loss=0.8430 train_acc=0.6413 | val_loss=0.9516 val_acc=0.6065 | time=18.2s\n",
            "[Fold 4] Epoch 023 | train_loss=0.8356 train_acc=0.6436 | val_loss=0.8939 val_acc=0.6112 | time=18.1s\n",
            "[Fold 4] Epoch 024 | train_loss=0.8379 train_acc=0.6390 | val_loss=0.8882 val_acc=0.6159 | time=18.1s\n",
            "[Fold 4] Epoch 025 | train_loss=0.8167 train_acc=0.6642 | val_loss=0.9135 val_acc=0.6174 | time=17.9s\n",
            "[Fold 4] Epoch 026 | train_loss=0.8025 train_acc=0.6530 | val_loss=0.9243 val_acc=0.6190 | time=18.4s\n",
            "[Fold 4] Epoch 027 | train_loss=0.7857 train_acc=0.6638 | val_loss=0.9107 val_acc=0.6159 | time=18.1s\n",
            "[Fold 4] Epoch 028 | train_loss=0.7932 train_acc=0.6588 | val_loss=0.8894 val_acc=0.6205 | time=17.9s\n",
            "[Fold 4] Epoch 029 | train_loss=0.7823 train_acc=0.6696 | val_loss=0.8760 val_acc=0.6330 | time=18.1s\n",
            "[Fold 4] Epoch 030 | train_loss=0.7758 train_acc=0.6770 | val_loss=0.8918 val_acc=0.6299 | time=18.3s\n",
            "[Fold 4] Epoch 031 | train_loss=0.7515 train_acc=0.6844 | val_loss=0.9515 val_acc=0.6096 | time=18.4s\n",
            "[Fold 4] Epoch 032 | train_loss=0.7597 train_acc=0.6770 | val_loss=0.9498 val_acc=0.6267 | time=17.9s\n",
            "[Fold 4] Epoch 033 | train_loss=0.7396 train_acc=0.6972 | val_loss=0.9194 val_acc=0.6283 | time=18.2s\n",
            "[Fold 4] Epoch 034 | train_loss=0.7483 train_acc=0.6836 | val_loss=0.8904 val_acc=0.6407 | time=18.1s\n",
            "[Fold 4] Epoch 035 | train_loss=0.7149 train_acc=0.6995 | val_loss=0.8538 val_acc=0.6501 | time=18.1s\n",
            "[Fold 4] Epoch 036 | train_loss=0.6993 train_acc=0.7081 | val_loss=0.8237 val_acc=0.6594 | time=18.1s\n",
            "[Fold 4] Epoch 037 | train_loss=0.7010 train_acc=0.7061 | val_loss=0.8144 val_acc=0.6579 | time=17.9s\n",
            "[Fold 4] Epoch 038 | train_loss=0.6920 train_acc=0.7139 | val_loss=0.8297 val_acc=0.6703 | time=18.1s\n",
            "[Fold 4] Epoch 039 | train_loss=0.6905 train_acc=0.7158 | val_loss=0.8366 val_acc=0.6656 | time=18.1s\n",
            "[Fold 4] Epoch 040 | train_loss=0.6744 train_acc=0.7220 | val_loss=0.8355 val_acc=0.6750 | time=18.1s\n",
            "[Fold 4] Epoch 041 | train_loss=0.6626 train_acc=0.7290 | val_loss=0.9472 val_acc=0.6361 | time=18.2s\n",
            "[Fold 4] Epoch 042 | train_loss=0.6658 train_acc=0.7240 | val_loss=0.8354 val_acc=0.6516 | time=18.0s\n",
            "[Fold 4] Epoch 043 | train_loss=0.6526 train_acc=0.7302 | val_loss=0.8338 val_acc=0.6687 | time=18.2s\n",
            "[Fold 4] Epoch 044 | train_loss=0.6581 train_acc=0.7244 | val_loss=0.8157 val_acc=0.6734 | time=17.9s\n",
            "[Fold 4] Epoch 045 | train_loss=0.6620 train_acc=0.7325 | val_loss=0.8048 val_acc=0.6719 | time=18.3s\n",
            "[Fold 4] Epoch 046 | train_loss=0.6149 train_acc=0.7465 | val_loss=0.8304 val_acc=0.6625 | time=18.3s\n",
            "[Fold 4] Epoch 047 | train_loss=0.6282 train_acc=0.7395 | val_loss=0.7972 val_acc=0.6781 | time=18.3s\n",
            "[Fold 4] Epoch 048 | train_loss=0.6107 train_acc=0.7531 | val_loss=0.8233 val_acc=0.6672 | time=18.5s\n",
            "[Fold 4] Epoch 049 | train_loss=0.6129 train_acc=0.7418 | val_loss=0.7998 val_acc=0.6750 | time=17.9s\n",
            "[Fold 4] Epoch 050 | train_loss=0.6174 train_acc=0.7469 | val_loss=0.8730 val_acc=0.6827 | time=18.0s\n",
            "[Fold 4] Epoch 051 | train_loss=0.6094 train_acc=0.7523 | val_loss=0.8413 val_acc=0.6905 | time=18.2s\n",
            "[Fold 4] Epoch 052 | train_loss=0.6067 train_acc=0.7461 | val_loss=0.8197 val_acc=0.6641 | time=18.2s\n",
            "[Fold 4] Epoch 053 | train_loss=0.5780 train_acc=0.7628 | val_loss=0.7875 val_acc=0.6470 | time=18.2s\n",
            "[Fold 4] Epoch 054 | train_loss=0.5794 train_acc=0.7698 | val_loss=0.8510 val_acc=0.6392 | time=17.9s\n",
            "[Fold 4] Epoch 055 | train_loss=0.5800 train_acc=0.7558 | val_loss=0.8299 val_acc=0.6656 | time=17.9s\n",
            "[Fold 4] Epoch 056 | train_loss=0.5836 train_acc=0.7605 | val_loss=0.8358 val_acc=0.6781 | time=18.1s\n",
            "[Fold 4] Epoch 057 | train_loss=0.5604 train_acc=0.7663 | val_loss=0.7570 val_acc=0.7030 | time=18.0s\n",
            "[Fold 4] Epoch 058 | train_loss=0.5581 train_acc=0.7741 | val_loss=0.8090 val_acc=0.6703 | time=18.0s\n",
            "[Fold 4] Epoch 059 | train_loss=0.5477 train_acc=0.7752 | val_loss=0.7785 val_acc=0.6983 | time=18.1s\n",
            "[Fold 4] Epoch 060 | train_loss=0.5550 train_acc=0.7640 | val_loss=0.8478 val_acc=0.6905 | time=18.2s\n",
            "[Fold 4] Epoch 061 | train_loss=0.5518 train_acc=0.7620 | val_loss=0.7993 val_acc=0.6641 | time=18.4s\n",
            "[Fold 4] Epoch 062 | train_loss=0.5282 train_acc=0.7822 | val_loss=0.7353 val_acc=0.7185 | time=18.2s\n",
            "[Fold 4] Epoch 063 | train_loss=0.5155 train_acc=0.7873 | val_loss=0.7929 val_acc=0.7185 | time=18.2s\n",
            "[Fold 4] Epoch 064 | train_loss=0.5144 train_acc=0.7911 | val_loss=0.7562 val_acc=0.7154 | time=18.1s\n",
            "[Fold 4] Epoch 065 | train_loss=0.5050 train_acc=0.7900 | val_loss=0.7515 val_acc=0.7030 | time=18.4s\n",
            "[Fold 4] Epoch 066 | train_loss=0.4966 train_acc=0.7946 | val_loss=0.8363 val_acc=0.6905 | time=18.2s\n",
            "[Fold 4] Epoch 067 | train_loss=0.4806 train_acc=0.7962 | val_loss=0.7351 val_acc=0.6890 | time=18.2s\n",
            "[Fold 4] Epoch 068 | train_loss=0.4858 train_acc=0.8016 | val_loss=0.8259 val_acc=0.6827 | time=18.3s\n",
            "[Fold 4] Epoch 069 | train_loss=0.4950 train_acc=0.7985 | val_loss=0.7520 val_acc=0.7232 | time=18.0s\n",
            "[Fold 4] Epoch 070 | train_loss=0.4684 train_acc=0.8055 | val_loss=0.7091 val_acc=0.7356 | time=18.4s\n",
            "[Fold 4] Epoch 071 | train_loss=0.4642 train_acc=0.8086 | val_loss=0.8029 val_acc=0.7076 | time=18.1s\n",
            "[Fold 4] Epoch 072 | train_loss=0.4573 train_acc=0.8129 | val_loss=0.7797 val_acc=0.7263 | time=18.2s\n",
            "[Fold 4] Epoch 073 | train_loss=0.4867 train_acc=0.8005 | val_loss=0.7294 val_acc=0.7123 | time=19.4s\n",
            "[Fold 4] Epoch 074 | train_loss=0.4507 train_acc=0.8086 | val_loss=0.7622 val_acc=0.7341 | time=19.3s\n",
            "[Fold 4] Epoch 075 | train_loss=0.4179 train_acc=0.8261 | val_loss=0.7366 val_acc=0.7434 | time=19.6s\n",
            "[Fold 4] Epoch 076 | train_loss=0.4517 train_acc=0.8129 | val_loss=0.7315 val_acc=0.7030 | time=19.1s\n",
            "[Fold 4] Epoch 077 | train_loss=0.4386 train_acc=0.8296 | val_loss=0.7980 val_acc=0.6843 | time=19.5s\n",
            "[Fold 4] Epoch 078 | train_loss=0.4459 train_acc=0.8078 | val_loss=0.7665 val_acc=0.7372 | time=19.4s\n",
            "[Fold 4] Epoch 079 | train_loss=0.3899 train_acc=0.8381 | val_loss=0.8195 val_acc=0.7232 | time=19.2s\n",
            "[Fold 4] Epoch 080 | train_loss=0.4173 train_acc=0.8346 | val_loss=0.7853 val_acc=0.7403 | time=19.3s\n",
            "[Fold 4] Epoch 081 | train_loss=0.4016 train_acc=0.8257 | val_loss=0.7702 val_acc=0.7372 | time=19.4s\n",
            "[Fold 4] Epoch 082 | train_loss=0.4134 train_acc=0.8292 | val_loss=0.7743 val_acc=0.7232 | time=19.4s\n",
            "[Fold 4] Epoch 083 | train_loss=0.4022 train_acc=0.8350 | val_loss=0.7565 val_acc=0.7341 | time=19.2s\n",
            "[Fold 4] Epoch 084 | train_loss=0.3978 train_acc=0.8358 | val_loss=0.7767 val_acc=0.7496 | time=19.5s\n",
            "[Fold 4] Epoch 085 | train_loss=0.3692 train_acc=0.8486 | val_loss=0.7790 val_acc=0.7512 | time=19.3s\n",
            "[Fold 4] Epoch 086 | train_loss=0.3555 train_acc=0.8536 | val_loss=0.7692 val_acc=0.7418 | time=18.9s\n",
            "[Fold 4] Epoch 087 | train_loss=0.3394 train_acc=0.8649 | val_loss=0.7726 val_acc=0.7216 | time=19.1s\n",
            "[Fold 4] Epoch 088 | train_loss=0.3514 train_acc=0.8606 | val_loss=0.8041 val_acc=0.7278 | time=19.2s\n",
            "[Fold 4] Epoch 089 | train_loss=0.3547 train_acc=0.8548 | val_loss=0.7712 val_acc=0.7527 | time=19.1s\n",
            "[Fold 4] Epoch 090 | train_loss=0.3881 train_acc=0.8370 | val_loss=0.7125 val_acc=0.7185 | time=19.0s\n",
            "[Fold 4] Early stopping at epoch 90, best was 70\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">resilient-waterfall-1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/x6vg01w6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/x6vg01w6</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250428_213359-x6vg01w6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-04-28 23:45:36,764] Trial 0 finished with value: 1.23324632417588 and parameters: {'lr': 3.306335426726131e-05, 'weight_decay': 6.37539648704434e-05, 'rtm_blocks': 3, 'stm_blocks': 2, 'ttm_blocks': 3, 'rtm_heads': 3, 'stm_heads': 3, 'ttm_heads': 2, 'num_segments': 15}. Best is trial 0 with value: 1.23324632417588.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250428_234536-48zv0fzk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/48zv0fzk' target=\"_blank\">pious-dream-2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/48zv0fzk' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-cv-tuning-trial_12/runs/48zv0fzk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "========================= Trial 1 =========================\n",
            "Testing with hyperparameters: {'lr': 5.4741533345840924e-05, 'weight_decay': 6.0702711367702194e-05, 'rtm_blocks': 1, 'stm_blocks': 1, 'ttm_blocks': 2, 'rtm_heads': 3, 'stm_heads': 4, 'ttm_heads': 2, 'num_segments': 5}\n",
            "\n",
            "========================= Fold 0 =========================\n",
            "[Fold 0] Epoch 001 | train_loss=1.0832 train_acc=0.4171 | val_loss=1.0814 val_acc=0.4317 | time=17.3s\n",
            "[Fold 0] Epoch 002 | train_loss=1.0687 train_acc=0.4276 | val_loss=1.0813 val_acc=0.4317 | time=17.2s\n",
            "[Fold 0] Epoch 003 | train_loss=1.0653 train_acc=0.4361 | val_loss=1.0778 val_acc=0.4317 | time=17.5s\n",
            "[Fold 0] Epoch 004 | train_loss=1.0536 train_acc=0.4571 | val_loss=1.0373 val_acc=0.5435 | time=17.5s\n",
            "[Fold 0] Epoch 005 | train_loss=1.0091 train_acc=0.5297 | val_loss=1.0274 val_acc=0.5233 | time=17.6s\n",
            "[Fold 0] Epoch 006 | train_loss=0.9954 train_acc=0.5371 | val_loss=0.9988 val_acc=0.5512 | time=17.8s\n",
            "[Fold 0] Epoch 007 | train_loss=0.9675 train_acc=0.5623 | val_loss=0.9701 val_acc=0.5745 | time=17.4s\n",
            "[Fold 0] Epoch 008 | train_loss=0.9667 train_acc=0.5550 | val_loss=0.9628 val_acc=0.5730 | time=17.3s\n",
            "[Fold 0] Epoch 009 | train_loss=0.9357 train_acc=0.5783 | val_loss=0.9430 val_acc=0.5916 | time=17.5s\n",
            "[Fold 0] Epoch 010 | train_loss=0.9177 train_acc=0.5946 | val_loss=0.9095 val_acc=0.6149 | time=17.5s\n",
            "[Fold 0] Epoch 011 | train_loss=0.8988 train_acc=0.6113 | val_loss=0.8987 val_acc=0.6491 | time=17.4s\n",
            "[Fold 0] Epoch 012 | train_loss=0.8801 train_acc=0.6249 | val_loss=0.9421 val_acc=0.5714 | time=17.4s\n",
            "[Fold 0] Epoch 013 | train_loss=0.8584 train_acc=0.6295 | val_loss=0.8685 val_acc=0.6227 | time=17.2s\n",
            "[Fold 0] Epoch 014 | train_loss=0.8425 train_acc=0.6388 | val_loss=0.8661 val_acc=0.6475 | time=17.5s\n",
            "[Fold 0] Epoch 015 | train_loss=0.8412 train_acc=0.6392 | val_loss=0.8444 val_acc=0.6553 | time=17.2s\n",
            "[Fold 0] Epoch 016 | train_loss=0.8199 train_acc=0.6497 | val_loss=0.8636 val_acc=0.6398 | time=17.3s\n",
            "[Fold 0] Epoch 017 | train_loss=0.8212 train_acc=0.6482 | val_loss=0.8301 val_acc=0.6568 | time=17.5s\n",
            "[Fold 0] Epoch 018 | train_loss=0.7915 train_acc=0.6617 | val_loss=0.8471 val_acc=0.6475 | time=17.2s\n",
            "[Fold 0] Epoch 019 | train_loss=0.7916 train_acc=0.6610 | val_loss=0.8405 val_acc=0.6661 | time=17.3s\n",
            "[Fold 0] Epoch 020 | train_loss=0.7800 train_acc=0.6668 | val_loss=0.8324 val_acc=0.6661 | time=17.5s\n",
            "[Fold 0] Epoch 021 | train_loss=0.7678 train_acc=0.6672 | val_loss=0.8445 val_acc=0.6537 | time=17.3s\n",
            "[Fold 0] Epoch 022 | train_loss=0.7627 train_acc=0.6660 | val_loss=0.7874 val_acc=0.7050 | time=17.5s\n",
            "[Fold 0] Epoch 023 | train_loss=0.7291 train_acc=0.6885 | val_loss=0.7889 val_acc=0.6801 | time=17.6s\n",
            "[Fold 0] Epoch 024 | train_loss=0.7249 train_acc=0.6878 | val_loss=0.9025 val_acc=0.6211 | time=17.3s\n",
            "[Fold 0] Epoch 025 | train_loss=0.7276 train_acc=0.6843 | val_loss=0.7941 val_acc=0.6801 | time=17.3s\n",
            "[Fold 0] Epoch 026 | train_loss=0.6975 train_acc=0.7017 | val_loss=0.7726 val_acc=0.6910 | time=17.7s\n",
            "[Fold 0] Epoch 027 | train_loss=0.6938 train_acc=0.7072 | val_loss=0.7852 val_acc=0.6832 | time=17.6s\n",
            "[Fold 0] Epoch 028 | train_loss=0.6768 train_acc=0.7037 | val_loss=0.7656 val_acc=0.7003 | time=17.3s\n",
            "[Fold 0] Epoch 029 | train_loss=0.6719 train_acc=0.7126 | val_loss=0.7553 val_acc=0.7143 | time=17.4s\n",
            "[Fold 0] Epoch 030 | train_loss=0.6612 train_acc=0.7188 | val_loss=0.7537 val_acc=0.7065 | time=17.4s\n",
            "[Fold 0] Epoch 031 | train_loss=0.6236 train_acc=0.7301 | val_loss=0.7670 val_acc=0.7127 | time=17.5s\n",
            "[Fold 0] Epoch 032 | train_loss=0.6224 train_acc=0.7301 | val_loss=0.7612 val_acc=0.7081 | time=17.4s\n",
            "[Fold 0] Epoch 033 | train_loss=0.6412 train_acc=0.7219 | val_loss=0.7281 val_acc=0.7220 | time=17.4s\n",
            "[Fold 0] Epoch 034 | train_loss=0.5887 train_acc=0.7449 | val_loss=0.7225 val_acc=0.7065 | time=17.4s\n",
            "[Fold 0] Epoch 035 | train_loss=0.5812 train_acc=0.7507 | val_loss=0.7124 val_acc=0.7345 | time=17.3s\n",
            "[Fold 0] Epoch 036 | train_loss=0.5658 train_acc=0.7573 | val_loss=0.6915 val_acc=0.7298 | time=17.2s\n",
            "[Fold 0] Epoch 037 | train_loss=0.5458 train_acc=0.7682 | val_loss=0.7724 val_acc=0.7236 | time=17.0s\n",
            "[Fold 0] Epoch 038 | train_loss=0.5373 train_acc=0.7763 | val_loss=0.6990 val_acc=0.7484 | time=17.5s\n",
            "[Fold 0] Epoch 039 | train_loss=0.5492 train_acc=0.7724 | val_loss=0.7173 val_acc=0.7484 | time=17.5s\n",
            "[Fold 0] Epoch 040 | train_loss=0.5270 train_acc=0.7802 | val_loss=0.6987 val_acc=0.7112 | time=17.5s\n",
            "[Fold 0] Epoch 041 | train_loss=0.5212 train_acc=0.7783 | val_loss=0.6818 val_acc=0.7531 | time=17.4s\n",
            "[Fold 0] Epoch 042 | train_loss=0.5098 train_acc=0.7903 | val_loss=0.7131 val_acc=0.7407 | time=17.2s\n",
            "[Fold 0] Epoch 043 | train_loss=0.5026 train_acc=0.7895 | val_loss=0.6600 val_acc=0.7422 | time=17.7s\n",
            "[Fold 0] Epoch 044 | train_loss=0.4613 train_acc=0.8105 | val_loss=0.7435 val_acc=0.7407 | time=17.3s\n",
            "[Fold 0] Epoch 045 | train_loss=0.4715 train_acc=0.8101 | val_loss=0.7076 val_acc=0.7345 | time=17.3s\n",
            "[Fold 0] Epoch 046 | train_loss=0.4282 train_acc=0.8357 | val_loss=0.6783 val_acc=0.7702 | time=17.6s\n",
            "[Fold 0] Epoch 047 | train_loss=0.4527 train_acc=0.8117 | val_loss=0.6799 val_acc=0.7671 | time=17.3s\n",
            "[Fold 0] Epoch 048 | train_loss=0.4129 train_acc=0.8307 | val_loss=0.7000 val_acc=0.7562 | time=17.5s\n",
            "[Fold 0] Epoch 049 | train_loss=0.4040 train_acc=0.8381 | val_loss=0.7642 val_acc=0.7422 | time=17.5s\n",
            "[Fold 0] Epoch 050 | train_loss=0.3760 train_acc=0.8517 | val_loss=0.7373 val_acc=0.7609 | time=17.3s\n",
            "[Fold 0] Epoch 051 | train_loss=0.3839 train_acc=0.8423 | val_loss=0.7211 val_acc=0.7624 | time=17.5s\n",
            "[Fold 0] Epoch 052 | train_loss=0.3723 train_acc=0.8497 | val_loss=0.7081 val_acc=0.7842 | time=17.7s\n",
            "[Fold 0] Epoch 053 | train_loss=0.3271 train_acc=0.8769 | val_loss=0.7001 val_acc=0.7686 | time=17.5s\n",
            "[Fold 0] Epoch 054 | train_loss=0.3245 train_acc=0.8812 | val_loss=0.6197 val_acc=0.7842 | time=17.8s\n",
            "[Fold 0] Epoch 055 | train_loss=0.3224 train_acc=0.8668 | val_loss=0.6796 val_acc=0.7640 | time=17.5s\n",
            "[Fold 0] Epoch 056 | train_loss=0.3209 train_acc=0.8757 | val_loss=0.6395 val_acc=0.7686 | time=17.2s\n",
            "[Fold 0] Epoch 057 | train_loss=0.2763 train_acc=0.8882 | val_loss=0.6929 val_acc=0.7997 | time=17.5s\n",
            "[Fold 0] Epoch 058 | train_loss=0.2771 train_acc=0.8932 | val_loss=0.6489 val_acc=0.7857 | time=17.7s\n",
            "[Fold 0] Epoch 059 | train_loss=0.2911 train_acc=0.8858 | val_loss=0.7197 val_acc=0.7717 | time=17.5s\n",
            "[Fold 0] Epoch 060 | train_loss=0.2314 train_acc=0.9107 | val_loss=0.7255 val_acc=0.7733 | time=17.4s\n",
            "[Fold 0] Epoch 061 | train_loss=0.2378 train_acc=0.9072 | val_loss=0.6758 val_acc=0.7578 | time=17.4s\n",
            "[Fold 0] Epoch 062 | train_loss=0.2268 train_acc=0.9138 | val_loss=0.7978 val_acc=0.7640 | time=17.4s\n",
            "[Fold 0] Epoch 063 | train_loss=0.2329 train_acc=0.9076 | val_loss=0.8242 val_acc=0.7578 | time=17.5s\n",
            "[Fold 0] Epoch 064 | train_loss=0.2492 train_acc=0.9099 | val_loss=0.8711 val_acc=0.7826 | time=17.4s\n",
            "[Fold 0] Epoch 065 | train_loss=0.2513 train_acc=0.9115 | val_loss=0.7827 val_acc=0.7966 | time=17.6s\n",
            "[Fold 0] Epoch 066 | train_loss=0.2028 train_acc=0.9243 | val_loss=0.7462 val_acc=0.7811 | time=17.3s\n",
            "[Fold 0] Epoch 067 | train_loss=0.1866 train_acc=0.9282 | val_loss=0.7651 val_acc=0.7904 | time=17.4s\n",
            "[Fold 0] Epoch 068 | train_loss=0.1788 train_acc=0.9336 | val_loss=0.7837 val_acc=0.7748 | time=17.6s\n",
            "[Fold 0] Epoch 069 | train_loss=0.1955 train_acc=0.9239 | val_loss=0.7740 val_acc=0.7857 | time=17.4s\n",
            "[Fold 0] Epoch 070 | train_loss=0.1823 train_acc=0.9285 | val_loss=0.7834 val_acc=0.7842 | time=17.2s\n",
            "[Fold 0] Epoch 071 | train_loss=0.1535 train_acc=0.9421 | val_loss=0.8176 val_acc=0.7826 | time=17.3s\n",
            "[Fold 0] Epoch 072 | train_loss=0.2065 train_acc=0.9223 | val_loss=0.7447 val_acc=0.7764 | time=17.3s\n",
            "[Fold 0] Epoch 073 | train_loss=0.1988 train_acc=0.9208 | val_loss=0.8040 val_acc=0.7764 | time=17.6s\n",
            "[Fold 0] Epoch 074 | train_loss=0.1661 train_acc=0.9348 | val_loss=0.8524 val_acc=0.7438 | time=17.5s\n",
            "[Fold 0] Early stopping at epoch 74, best was 54\n",
            "\n",
            "========================= Fold 1 =========================\n",
            "[Fold 1] Epoch 001 | train_loss=1.0694 train_acc=0.4311 | val_loss=1.0859 val_acc=0.4317 | time=17.3s\n",
            "[Fold 1] Epoch 002 | train_loss=1.0666 train_acc=0.4311 | val_loss=1.0841 val_acc=0.4317 | time=17.4s\n",
            "[Fold 1] Epoch 003 | train_loss=1.0663 train_acc=0.4311 | val_loss=1.0860 val_acc=0.4317 | time=17.3s\n",
            "[Fold 1] Epoch 004 | train_loss=1.0672 train_acc=0.4311 | val_loss=1.0839 val_acc=0.4317 | time=17.5s\n",
            "[Fold 1] Epoch 005 | train_loss=1.0667 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=17.5s\n",
            "[Fold 1] Epoch 006 | train_loss=1.0684 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=17.3s\n",
            "[Fold 1] Epoch 007 | train_loss=1.0671 train_acc=0.4311 | val_loss=1.0827 val_acc=0.4317 | time=17.2s\n",
            "[Fold 1] Epoch 008 | train_loss=1.0664 train_acc=0.4311 | val_loss=1.0837 val_acc=0.4317 | time=17.4s\n",
            "[Fold 1] Epoch 009 | train_loss=1.0664 train_acc=0.4311 | val_loss=1.0829 val_acc=0.4317 | time=17.4s\n",
            "[Fold 1] Epoch 010 | train_loss=1.0667 train_acc=0.4311 | val_loss=1.0828 val_acc=0.4317 | time=17.4s\n",
            "[Fold 1] Epoch 011 | train_loss=1.0658 train_acc=0.4311 | val_loss=1.0823 val_acc=0.4317 | time=17.4s\n",
            "[Fold 1] Epoch 012 | train_loss=1.0655 train_acc=0.4311 | val_loss=1.0781 val_acc=0.4317 | time=17.3s\n",
            "[Fold 1] Epoch 013 | train_loss=1.0262 train_acc=0.5076 | val_loss=1.0015 val_acc=0.5668 | time=17.2s\n",
            "[Fold 1] Epoch 014 | train_loss=0.9663 train_acc=0.5682 | val_loss=0.9918 val_acc=0.5699 | time=17.6s\n",
            "[Fold 1] Epoch 015 | train_loss=0.9436 train_acc=0.5720 | val_loss=0.9566 val_acc=0.5714 | time=17.4s\n",
            "[Fold 1] Epoch 016 | train_loss=0.9229 train_acc=0.5872 | val_loss=0.9422 val_acc=0.5792 | time=17.4s\n",
            "[Fold 1] Epoch 017 | train_loss=0.9006 train_acc=0.5930 | val_loss=0.9137 val_acc=0.6025 | time=17.2s\n",
            "[Fold 1] Epoch 018 | train_loss=0.8884 train_acc=0.5922 | val_loss=0.9216 val_acc=0.6009 | time=17.5s\n",
            "[Fold 1] Epoch 019 | train_loss=0.8748 train_acc=0.6035 | val_loss=0.9025 val_acc=0.6071 | time=17.5s\n",
            "[Fold 1] Epoch 020 | train_loss=0.8762 train_acc=0.6012 | val_loss=0.8959 val_acc=0.6056 | time=17.1s\n",
            "[Fold 1] Epoch 021 | train_loss=0.8614 train_acc=0.6031 | val_loss=0.8868 val_acc=0.6056 | time=17.2s\n",
            "[Fold 1] Epoch 022 | train_loss=0.8448 train_acc=0.6101 | val_loss=0.8614 val_acc=0.6227 | time=17.8s\n",
            "[Fold 1] Epoch 023 | train_loss=0.8353 train_acc=0.6109 | val_loss=0.8710 val_acc=0.6087 | time=17.5s\n",
            "[Fold 1] Epoch 024 | train_loss=0.8245 train_acc=0.6159 | val_loss=0.8692 val_acc=0.5963 | time=17.4s\n",
            "[Fold 1] Epoch 025 | train_loss=0.8179 train_acc=0.6252 | val_loss=0.8748 val_acc=0.6149 | time=17.6s\n",
            "[Fold 1] Epoch 026 | train_loss=0.7904 train_acc=0.6353 | val_loss=0.8334 val_acc=0.6273 | time=17.2s\n",
            "[Fold 1] Epoch 027 | train_loss=0.7858 train_acc=0.6462 | val_loss=0.8393 val_acc=0.6320 | time=17.3s\n",
            "[Fold 1] Epoch 028 | train_loss=0.7639 train_acc=0.6610 | val_loss=0.8210 val_acc=0.6398 | time=17.5s\n",
            "[Fold 1] Epoch 029 | train_loss=0.7469 train_acc=0.6586 | val_loss=0.8553 val_acc=0.6227 | time=17.5s\n",
            "[Fold 1] Epoch 030 | train_loss=0.7562 train_acc=0.6625 | val_loss=0.7722 val_acc=0.6460 | time=17.8s\n",
            "[Fold 1] Epoch 031 | train_loss=0.7341 train_acc=0.6672 | val_loss=0.7985 val_acc=0.6444 | time=17.4s\n",
            "[Fold 1] Epoch 032 | train_loss=0.7270 train_acc=0.6707 | val_loss=0.7486 val_acc=0.6568 | time=17.3s\n",
            "[Fold 1] Epoch 033 | train_loss=0.7364 train_acc=0.6575 | val_loss=0.8861 val_acc=0.5745 | time=17.5s\n",
            "[Fold 1] Epoch 034 | train_loss=0.7265 train_acc=0.6660 | val_loss=0.7775 val_acc=0.6491 | time=17.4s\n",
            "[Fold 1] Epoch 035 | train_loss=0.7036 train_acc=0.6753 | val_loss=0.7635 val_acc=0.6553 | time=17.3s\n",
            "[Fold 1] Epoch 036 | train_loss=0.6922 train_acc=0.6831 | val_loss=0.7617 val_acc=0.6522 | time=17.6s\n",
            "[Fold 1] Epoch 037 | train_loss=0.6881 train_acc=0.6800 | val_loss=0.7357 val_acc=0.6506 | time=17.4s\n",
            "[Fold 1] Epoch 038 | train_loss=0.6847 train_acc=0.6889 | val_loss=0.7325 val_acc=0.6739 | time=17.4s\n",
            "[Fold 1] Epoch 039 | train_loss=0.6659 train_acc=0.6940 | val_loss=0.7426 val_acc=0.6708 | time=17.5s\n",
            "[Fold 1] Epoch 040 | train_loss=0.6660 train_acc=0.6944 | val_loss=0.7791 val_acc=0.6273 | time=17.6s\n",
            "[Fold 1] Epoch 041 | train_loss=0.6794 train_acc=0.6936 | val_loss=0.7417 val_acc=0.6553 | time=17.5s\n",
            "[Fold 1] Epoch 042 | train_loss=0.6552 train_acc=0.6979 | val_loss=0.8591 val_acc=0.6398 | time=17.4s\n",
            "[Fold 1] Epoch 043 | train_loss=0.6743 train_acc=0.6909 | val_loss=0.7578 val_acc=0.6739 | time=17.3s\n",
            "[Fold 1] Epoch 044 | train_loss=0.6392 train_acc=0.7122 | val_loss=0.7541 val_acc=0.6568 | time=17.5s\n",
            "[Fold 1] Epoch 045 | train_loss=0.6194 train_acc=0.7146 | val_loss=0.7272 val_acc=0.6708 | time=17.2s\n",
            "[Fold 1] Epoch 046 | train_loss=0.6401 train_acc=0.7126 | val_loss=0.7812 val_acc=0.6568 | time=17.4s\n",
            "[Fold 1] Epoch 047 | train_loss=0.6087 train_acc=0.7223 | val_loss=0.7921 val_acc=0.6677 | time=17.5s\n",
            "[Fold 1] Epoch 048 | train_loss=0.6150 train_acc=0.7208 | val_loss=0.7892 val_acc=0.6693 | time=17.3s\n",
            "[Fold 1] Epoch 049 | train_loss=0.6003 train_acc=0.7278 | val_loss=0.7914 val_acc=0.6599 | time=17.3s\n",
            "[Fold 1] Epoch 050 | train_loss=0.5840 train_acc=0.7449 | val_loss=0.7323 val_acc=0.6646 | time=17.7s\n",
            "[Fold 1] Epoch 051 | train_loss=0.5754 train_acc=0.7328 | val_loss=0.8246 val_acc=0.6661 | time=17.4s\n",
            "[Fold 1] Epoch 052 | train_loss=0.5918 train_acc=0.7355 | val_loss=0.7641 val_acc=0.6661 | time=17.2s\n",
            "[Fold 1] Epoch 053 | train_loss=0.5540 train_acc=0.7487 | val_loss=0.7410 val_acc=0.6894 | time=17.6s\n",
            "[Fold 1] Epoch 054 | train_loss=0.5409 train_acc=0.7557 | val_loss=0.7204 val_acc=0.6817 | time=17.4s\n",
            "[Fold 1] Epoch 055 | train_loss=0.5414 train_acc=0.7581 | val_loss=0.7610 val_acc=0.6832 | time=17.2s\n",
            "[Fold 1] Epoch 056 | train_loss=0.5266 train_acc=0.7616 | val_loss=0.7814 val_acc=0.6615 | time=17.4s\n",
            "[Fold 1] Epoch 057 | train_loss=0.5042 train_acc=0.7705 | val_loss=0.7329 val_acc=0.6755 | time=17.4s\n",
            "[Fold 1] Epoch 058 | train_loss=0.5163 train_acc=0.7736 | val_loss=0.8366 val_acc=0.6661 | time=17.8s\n",
            "[Fold 1] Epoch 059 | train_loss=0.5092 train_acc=0.7693 | val_loss=0.7773 val_acc=0.6786 | time=17.5s\n",
            "[Fold 1] Epoch 060 | train_loss=0.4725 train_acc=0.7977 | val_loss=0.7931 val_acc=0.6429 | time=17.5s\n",
            "[Fold 1] Epoch 061 | train_loss=0.4783 train_acc=0.7856 | val_loss=0.8796 val_acc=0.6630 | time=17.6s\n",
            "[Fold 1] Epoch 062 | train_loss=0.4885 train_acc=0.7783 | val_loss=0.7812 val_acc=0.6677 | time=17.4s\n",
            "[Fold 1] Epoch 063 | train_loss=0.4589 train_acc=0.7953 | val_loss=0.7953 val_acc=0.6863 | time=17.3s\n",
            "[Fold 1] Epoch 064 | train_loss=0.4413 train_acc=0.8097 | val_loss=0.8335 val_acc=0.6506 | time=17.6s\n",
            "[Fold 1] Epoch 065 | train_loss=0.4344 train_acc=0.8105 | val_loss=0.8690 val_acc=0.6755 | time=17.2s\n",
            "[Fold 1] Epoch 066 | train_loss=0.4426 train_acc=0.8050 | val_loss=0.8167 val_acc=0.6677 | time=17.5s\n",
            "[Fold 1] Epoch 067 | train_loss=0.4190 train_acc=0.8163 | val_loss=0.9730 val_acc=0.6599 | time=17.4s\n",
            "[Fold 1] Epoch 068 | train_loss=0.4082 train_acc=0.8214 | val_loss=0.9892 val_acc=0.6599 | time=17.3s\n",
            "[Fold 1] Epoch 069 | train_loss=0.3807 train_acc=0.8373 | val_loss=0.8953 val_acc=0.6693 | time=17.3s\n",
            "[Fold 1] Epoch 070 | train_loss=0.4062 train_acc=0.8249 | val_loss=0.8837 val_acc=0.6553 | time=17.6s\n",
            "[Fold 1] Epoch 071 | train_loss=0.3858 train_acc=0.8357 | val_loss=0.9071 val_acc=0.6879 | time=17.3s\n",
            "[Fold 1] Epoch 072 | train_loss=0.3692 train_acc=0.8381 | val_loss=0.8494 val_acc=0.6661 | time=17.5s\n",
            "[Fold 1] Epoch 073 | train_loss=0.3692 train_acc=0.8427 | val_loss=0.8929 val_acc=0.6630 | time=17.3s\n",
            "[Fold 1] Epoch 074 | train_loss=0.3742 train_acc=0.8478 | val_loss=0.8650 val_acc=0.6739 | time=17.6s\n",
            "[Fold 1] Early stopping at epoch 74, best was 54\n",
            "\n",
            "========================= Fold 2 =========================\n",
            "[Fold 2] Epoch 001 | train_loss=1.0718 train_acc=0.4245 | val_loss=1.0818 val_acc=0.4301 | time=17.5s\n",
            "[Fold 2] Epoch 002 | train_loss=1.0669 train_acc=0.4315 | val_loss=1.0833 val_acc=0.4301 | time=17.3s\n",
            "[Fold 2] Epoch 003 | train_loss=1.0679 train_acc=0.4315 | val_loss=1.0834 val_acc=0.4301 | time=17.5s\n",
            "[Fold 2] Epoch 004 | train_loss=1.0629 train_acc=0.4357 | val_loss=1.0810 val_acc=0.4301 | time=17.4s\n",
            "[Fold 2] Epoch 005 | train_loss=1.0256 train_acc=0.4959 | val_loss=1.0195 val_acc=0.5342 | time=17.6s\n",
            "[Fold 2] Epoch 006 | train_loss=0.9753 train_acc=0.5565 | val_loss=1.0017 val_acc=0.5404 | time=17.3s\n",
            "[Fold 2] Epoch 007 | train_loss=0.9634 train_acc=0.5709 | val_loss=1.0489 val_acc=0.4581 | time=17.4s\n",
            "[Fold 2] Epoch 008 | train_loss=0.9422 train_acc=0.5751 | val_loss=0.9522 val_acc=0.5823 | time=17.5s\n",
            "[Fold 2] Epoch 009 | train_loss=0.9323 train_acc=0.5918 | val_loss=0.9806 val_acc=0.5776 | time=17.6s\n",
            "[Fold 2] Epoch 010 | train_loss=0.9192 train_acc=0.5903 | val_loss=0.9455 val_acc=0.5901 | time=17.4s\n",
            "[Fold 2] Epoch 011 | train_loss=0.9039 train_acc=0.6019 | val_loss=0.9354 val_acc=0.5901 | time=17.4s\n",
            "[Fold 2] Epoch 012 | train_loss=0.9192 train_acc=0.5942 | val_loss=0.9401 val_acc=0.5823 | time=17.4s\n",
            "[Fold 2] Epoch 013 | train_loss=0.8806 train_acc=0.6097 | val_loss=0.9291 val_acc=0.5978 | time=17.2s\n",
            "[Fold 2] Epoch 014 | train_loss=0.8797 train_acc=0.6190 | val_loss=0.9387 val_acc=0.6009 | time=17.6s\n",
            "[Fold 2] Epoch 015 | train_loss=0.8690 train_acc=0.6155 | val_loss=0.9107 val_acc=0.6087 | time=17.7s\n",
            "[Fold 2] Epoch 016 | train_loss=0.8605 train_acc=0.6206 | val_loss=0.9157 val_acc=0.6180 | time=17.2s\n",
            "[Fold 2] Epoch 017 | train_loss=0.8396 train_acc=0.6330 | val_loss=0.8856 val_acc=0.6351 | time=17.4s\n",
            "[Fold 2] Epoch 018 | train_loss=0.8104 train_acc=0.6489 | val_loss=0.8925 val_acc=0.6149 | time=17.7s\n",
            "[Fold 2] Epoch 019 | train_loss=0.8019 train_acc=0.6497 | val_loss=0.8736 val_acc=0.6149 | time=17.2s\n",
            "[Fold 2] Epoch 020 | train_loss=0.7840 train_acc=0.6610 | val_loss=0.8793 val_acc=0.6180 | time=17.7s\n",
            "[Fold 2] Epoch 021 | train_loss=0.7809 train_acc=0.6602 | val_loss=0.8790 val_acc=0.6351 | time=17.4s\n",
            "[Fold 2] Epoch 022 | train_loss=0.7575 train_acc=0.6664 | val_loss=0.8145 val_acc=0.6584 | time=17.5s\n",
            "[Fold 2] Epoch 023 | train_loss=0.7453 train_acc=0.6711 | val_loss=0.8128 val_acc=0.6661 | time=17.6s\n",
            "[Fold 2] Epoch 024 | train_loss=0.7180 train_acc=0.6917 | val_loss=0.8293 val_acc=0.6537 | time=17.4s\n",
            "[Fold 2] Epoch 025 | train_loss=0.7076 train_acc=0.6963 | val_loss=0.8112 val_acc=0.6630 | time=17.2s\n",
            "[Fold 2] Epoch 026 | train_loss=0.7069 train_acc=0.6959 | val_loss=0.8760 val_acc=0.6584 | time=17.4s\n",
            "[Fold 2] Epoch 027 | train_loss=0.6804 train_acc=0.7060 | val_loss=0.7831 val_acc=0.6630 | time=17.4s\n",
            "[Fold 2] Epoch 028 | train_loss=0.6614 train_acc=0.7126 | val_loss=0.8034 val_acc=0.6475 | time=17.2s\n",
            "[Fold 2] Epoch 029 | train_loss=0.6389 train_acc=0.7177 | val_loss=0.7393 val_acc=0.7096 | time=17.5s\n",
            "[Fold 2] Epoch 030 | train_loss=0.6323 train_acc=0.7285 | val_loss=0.8467 val_acc=0.6211 | time=17.3s\n",
            "[Fold 2] Epoch 031 | train_loss=0.6408 train_acc=0.7227 | val_loss=0.7184 val_acc=0.7065 | time=17.0s\n",
            "[Fold 2] Epoch 032 | train_loss=0.5890 train_acc=0.7437 | val_loss=0.7163 val_acc=0.7096 | time=17.6s\n",
            "[Fold 2] Epoch 033 | train_loss=0.5621 train_acc=0.7526 | val_loss=0.7254 val_acc=0.7143 | time=17.4s\n",
            "[Fold 2] Epoch 034 | train_loss=0.5451 train_acc=0.7623 | val_loss=0.7188 val_acc=0.7081 | time=17.4s\n",
            "[Fold 2] Epoch 035 | train_loss=0.5237 train_acc=0.7724 | val_loss=0.6859 val_acc=0.7205 | time=17.5s\n",
            "[Fold 2] Epoch 036 | train_loss=0.5089 train_acc=0.7814 | val_loss=0.8789 val_acc=0.6366 | time=17.2s\n",
            "[Fold 2] Epoch 037 | train_loss=0.5435 train_acc=0.7701 | val_loss=0.7121 val_acc=0.7220 | time=17.3s\n",
            "[Fold 2] Epoch 038 | train_loss=0.5264 train_acc=0.7806 | val_loss=0.7460 val_acc=0.7252 | time=17.5s\n",
            "[Fold 2] Epoch 039 | train_loss=0.4838 train_acc=0.7887 | val_loss=0.7572 val_acc=0.7112 | time=17.4s\n",
            "[Fold 2] Epoch 040 | train_loss=0.4655 train_acc=0.8027 | val_loss=0.7436 val_acc=0.7298 | time=17.5s\n",
            "[Fold 2] Epoch 041 | train_loss=0.4779 train_acc=0.7981 | val_loss=0.6445 val_acc=0.7500 | time=17.2s\n",
            "[Fold 2] Epoch 042 | train_loss=0.4598 train_acc=0.8066 | val_loss=0.6214 val_acc=0.7469 | time=17.2s\n",
            "[Fold 2] Epoch 043 | train_loss=0.4309 train_acc=0.8206 | val_loss=0.6054 val_acc=0.7531 | time=17.6s\n",
            "[Fold 2] Epoch 044 | train_loss=0.4077 train_acc=0.8353 | val_loss=0.6254 val_acc=0.7484 | time=17.4s\n",
            "[Fold 2] Epoch 045 | train_loss=0.4255 train_acc=0.8299 | val_loss=0.7174 val_acc=0.7329 | time=17.3s\n",
            "[Fold 2] Epoch 046 | train_loss=0.3967 train_acc=0.8412 | val_loss=0.7003 val_acc=0.7562 | time=17.4s\n",
            "[Fold 2] Epoch 047 | train_loss=0.3743 train_acc=0.8466 | val_loss=0.6193 val_acc=0.7686 | time=17.5s\n",
            "[Fold 2] Epoch 048 | train_loss=0.3716 train_acc=0.8501 | val_loss=0.6931 val_acc=0.7422 | time=17.4s\n",
            "[Fold 2] Epoch 049 | train_loss=0.3716 train_acc=0.8478 | val_loss=0.6142 val_acc=0.7686 | time=17.4s\n",
            "[Fold 2] Epoch 050 | train_loss=0.3452 train_acc=0.8586 | val_loss=0.5984 val_acc=0.7764 | time=17.2s\n",
            "[Fold 2] Epoch 051 | train_loss=0.3710 train_acc=0.8462 | val_loss=0.5823 val_acc=0.7671 | time=17.1s\n",
            "[Fold 2] Epoch 052 | train_loss=0.3394 train_acc=0.8652 | val_loss=0.6143 val_acc=0.7655 | time=17.4s\n",
            "[Fold 2] Epoch 053 | train_loss=0.3129 train_acc=0.8711 | val_loss=0.6468 val_acc=0.7780 | time=17.7s\n",
            "[Fold 2] Epoch 054 | train_loss=0.3171 train_acc=0.8757 | val_loss=0.6589 val_acc=0.7733 | time=17.7s\n",
            "[Fold 2] Epoch 055 | train_loss=0.2986 train_acc=0.8788 | val_loss=0.6778 val_acc=0.7748 | time=17.4s\n",
            "[Fold 2] Epoch 056 | train_loss=0.2854 train_acc=0.8874 | val_loss=0.6555 val_acc=0.7780 | time=17.5s\n",
            "[Fold 2] Epoch 057 | train_loss=0.3062 train_acc=0.8788 | val_loss=0.7202 val_acc=0.7050 | time=17.6s\n",
            "[Fold 2] Epoch 058 | train_loss=0.2744 train_acc=0.8885 | val_loss=0.6078 val_acc=0.7733 | time=17.0s\n",
            "[Fold 2] Epoch 059 | train_loss=0.2620 train_acc=0.8959 | val_loss=0.6456 val_acc=0.7857 | time=17.5s\n",
            "[Fold 2] Epoch 060 | train_loss=0.2646 train_acc=0.8975 | val_loss=0.6683 val_acc=0.7811 | time=17.5s\n",
            "[Fold 2] Epoch 061 | train_loss=0.2596 train_acc=0.8994 | val_loss=0.6329 val_acc=0.8028 | time=17.4s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[W 2025-04-29 00:46:21,576] Trial 1 failed with parameters: {'lr': 5.4741533345840924e-05, 'weight_decay': 6.0702711367702194e-05, 'rtm_blocks': 1, 'stm_blocks': 1, 'ttm_blocks': 2, 'rtm_heads': 3, 'stm_heads': 4, 'ttm_heads': 2, 'num_segments': 5} because of the following error: KeyboardInterrupt().\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
            "    value_or_values = func(trial)\n",
            "                      ^^^^^^^^^^^\n",
            "  File \"<ipython-input-8-d413b19da3d1>\", line 123, in objective\n",
            "    tl_sum += loss.item()\n",
            "              ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "[W 2025-04-29 00:46:21,579] Trial 1 failed with value None.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-d413b19da3d1>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    208\u001b[0m         \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     )\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjective\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_trials\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0mbest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/study.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    473\u001b[0m                 \u001b[0mIf\u001b[0m \u001b[0mnested\u001b[0m \u001b[0minvocation\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0moccurs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \"\"\"\n\u001b[0;32m--> 475\u001b[0;31m         _optimize(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0mstudy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mfunc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mn_jobs\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             _optimize_sequential(\n\u001b[0m\u001b[1;32m     64\u001b[0m                 \u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                 \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m             \u001b[0mfrozen_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_trial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstudy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m             \u001b[0;31m# The following line mitigates memory problems that can be occurred in some\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     ):\n\u001b[0;32m--> 248\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mfunc_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    249\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrozen_trial\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\u001b[0m in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_heartbeat_thread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trial_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstudy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_storage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mvalue_or_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrialPruned\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m             \u001b[0;31m# TODO(mamu): Handle multi-objective cases.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-d413b19da3d1>\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m                 \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m                 \u001b[0mtl_sum\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m                 \u001b[0mt_corr\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m                 \u001b[0mt_tot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "96rAcvmaRP31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QascdvQuVW3I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kc6IZCodVXFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optuna 범위 줄여보기\n",
        "\n",
        "- RTM Block = [1, 2]\n",
        "- STM Block = [2, 3]\n",
        "- TTM Block = [1, 2]\n",
        "\n",
        "- RTM Head = [2, 4]\n",
        "- STM Head = [2, 4]\n",
        "- TTM Head = [2, 4]\n",
        "\n",
        "- Segment Choices = [5]"
      ],
      "metadata": {
        "id": "ae6UCMlhWE-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import time\n",
        "import gc\n",
        "import multiprocessing\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "import optuna\n",
        "import wandb\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from models import EEGformer\n",
        "\n",
        "# ─── Constants ─────────────────────────\n",
        "LR_MIN, LR_MAX = 2e-5, 8e-5\n",
        "WD_MIN, WD_MAX = 3e-5, 1e-4\n",
        "FILTER_MIN = FILTER_MAX = 120\n",
        "\n",
        "# blocks -> [1,2,3] / heads -> [2,3,4]\n",
        "RTM_BLOCK_CHOICES = [1, 2]\n",
        "STM_BLOCK_CHOICES = [2, 3]\n",
        "TTM_BLOCK_CHOICES = [1, 2]\n",
        "\n",
        "RTM_HEAD_CHOICES = [2, 4]\n",
        "STM_HEAD_CHOICES = [2, 4]\n",
        "TTM_HEAD_CHOICES = [2, 4]\n",
        "\n",
        "SEGMENT_CHOICES = [5]\n",
        "\n",
        "N_FOLDS     = 5\n",
        "MAX_EPOCHS  = 100\n",
        "PATIENCE    = 20\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = max(1, min(4, os.cpu_count() - 1))\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = \"labels.json\"\n",
        "\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def objective(trial):\n",
        "    # Hyperparameter search space setup\n",
        "    lr = trial.suggest_float(\"lr\", LR_MIN, LR_MAX, log=True)\n",
        "    weight_decay = trial.suggest_float(\"weight_decay\", WD_MIN, WD_MAX, log=True)\n",
        "    num_filters = 120\n",
        "    rtm_blocks = trial.suggest_categorical(\"rtm_blocks\", RTM_BLOCK_CHOICES)\n",
        "    stm_blocks = trial.suggest_categorical(\"stm_blocks\", STM_BLOCK_CHOICES)\n",
        "    ttm_blocks = trial.suggest_categorical(\"ttm_blocks\", TTM_BLOCK_CHOICES)\n",
        "    rtm_heads = trial.suggest_categorical(\"rtm_heads\", RTM_HEAD_CHOICES)\n",
        "    stm_heads = trial.suggest_categorical(\"stm_heads\", STM_HEAD_CHOICES)\n",
        "    ttm_heads = trial.suggest_categorical(\"ttm_heads\", TTM_HEAD_CHOICES)\n",
        "    num_segments = trial.suggest_categorical(\"num_segments\", SEGMENT_CHOICES)\n",
        "\n",
        "    # Pruning condition: only proceed if num_filters is divisible by heads\n",
        "    for h in (rtm_heads, stm_heads, ttm_heads):\n",
        "        if num_filters % h != 0:\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    # Initialize wandb\n",
        "    wandb.init(project=\"eeg-cv-tuning-trial_12\", config=trial.params)\n",
        "\n",
        "    print(f\"\\n========================= Trial {trial.number} =========================\")\n",
        "    print(f\"Testing with hyperparameters: {trial.params}\")\n",
        "\n",
        "    # Data preparation\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    train_meta = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds = EEGDataset(DATA_DIR, train_meta)\n",
        "    labels = [d[\"label\"] for d in train_meta]\n",
        "    n_samples = len(full_ds)\n",
        "\n",
        "    # StratifiedKFold setup\n",
        "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
        "    fold_metrics = {\"train_loss\": [], \"train_acc\": [], \"val_loss\": [], \"val_acc\": [], \"best_epoch\": []}\n",
        "\n",
        "    for fold, (train_idx, val_idx) in enumerate(skf.split(range(n_samples), labels)):\n",
        "        # Fold separation\n",
        "        print(f\"\\n========================= Fold {fold} =========================\")\n",
        "\n",
        "        # Data loader setup\n",
        "        train_loader = DataLoader(Subset(full_ds, train_idx), batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "        val_loader = DataLoader(Subset(full_ds, val_idx), batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "        # Model and optimizer setup\n",
        "        model = EEGformer(\n",
        "            num_classes=3,\n",
        "            in_channels=19,\n",
        "            kernel_size=10,\n",
        "            num_filters=num_filters,\n",
        "            rtm_blocks=rtm_blocks,\n",
        "            stm_blocks=stm_blocks,\n",
        "            ttm_blocks=ttm_blocks,\n",
        "            rtm_heads=rtm_heads,\n",
        "            stm_heads=stm_heads,\n",
        "            ttm_heads=ttm_heads,\n",
        "            num_segments=num_segments\n",
        "        ).to(DEVICE)\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        best_val_loss = float(\"inf\")\n",
        "        epochs_no_improve = 0\n",
        "        best_epoch = 0\n",
        "        best_train_l = best_train_a = best_val_a = None\n",
        "        last_log_time = time.time()\n",
        "\n",
        "        # Epoch-wise training\n",
        "        for epoch in range(1, MAX_EPOCHS + 1):\n",
        "            model.train()\n",
        "            tl_sum = t_corr = t_tot = 0\n",
        "            for X, y in train_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                optimizer.zero_grad()\n",
        "                logits = model(X)\n",
        "                loss = criterion(logits, y)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                tl_sum += loss.item()\n",
        "                t_corr += (logits.argmax(1) == y).sum().item()\n",
        "                t_tot += y.size(0)\n",
        "\n",
        "            train_loss = tl_sum / len(train_loader)\n",
        "            train_acc = t_corr / t_tot\n",
        "\n",
        "            model.eval()\n",
        "            vl_sum = v_corr = v_tot = 0\n",
        "            with torch.no_grad():\n",
        "                for X, y in val_loader:\n",
        "                    X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                    logits = model(X)\n",
        "                    loss = criterion(logits, y)\n",
        "                    vl_sum += loss.item()\n",
        "                    v_corr += (logits.argmax(1) == y).sum().item()\n",
        "                    v_tot += y.size(0)\n",
        "\n",
        "            val_loss = vl_sum / len(val_loader)\n",
        "            val_acc = v_corr / v_tot\n",
        "\n",
        "            step = fold * MAX_EPOCHS + epoch\n",
        "            trial.report(val_loss, step=step)\n",
        "\n",
        "            # Pruning check\n",
        "            if trial.should_prune():\n",
        "                print(f\"\\u274c Trial {trial.number} pruned at fold {fold}, epoch {epoch}\")\n",
        "                # Report the metrics before returning early\n",
        "                for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                 [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                    fold_metrics[k].append(v)\n",
        "                    trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                raise optuna.TrialPruned()  # End trial completely if pruned\n",
        "\n",
        "            now = time.time()\n",
        "            print(f\"[Fold {fold}] Epoch {epoch:03d} | train_loss={train_loss:.4f} train_acc={train_acc:.4f} | \"\n",
        "                  f\"val_loss={val_loss:.4f} val_acc={val_acc:.4f} | time={now - last_log_time:.1f}s\")\n",
        "            last_log_time = now\n",
        "\n",
        "            # Early stopping: if validation loss does not improve\n",
        "            if val_loss < best_val_loss:\n",
        "                best_val_loss = val_loss\n",
        "                best_epoch = epoch\n",
        "                epochs_no_improve = 0\n",
        "                best_train_l = train_loss\n",
        "                best_train_a = train_acc\n",
        "                best_val_a = val_acc\n",
        "            else:\n",
        "                epochs_no_improve += 1\n",
        "                if epochs_no_improve >= PATIENCE:\n",
        "                    print(f\"[Fold {fold}] Early stopping at epoch {epoch}, best was {best_epoch}\")\n",
        "                    # Report the metrics before returning early\n",
        "                    for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                                     [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "                        fold_metrics[k].append(v)\n",
        "                        trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "                    break  # Break if early stopping\n",
        "\n",
        "        # Record results for the fold\n",
        "        for k, v in zip([\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"],\n",
        "                         [best_train_l, best_train_a, best_val_loss, best_val_a, best_epoch]):\n",
        "            fold_metrics[k].append(v)\n",
        "            trial.set_user_attr(f\"fold{fold}_{k}\", v)\n",
        "\n",
        "        del model, optimizer, train_loader, val_loader\n",
        "        torch.mps.empty_cache() if DEVICE.type == \"mps\" else torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "    # Calculate average metrics\n",
        "    avg = lambda k: sum(fold_metrics[k]) / N_FOLDS\n",
        "    for key in [\"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\", \"best_epoch\"]:\n",
        "        trial.set_user_attr(f\"avg_{key}\", avg(key))\n",
        "\n",
        "    wandb.finish()\n",
        "    return avg(\"val_loss\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    multiprocessing.freeze_support()\n",
        "    study = optuna.create_study(\n",
        "        direction=\"minimize\",\n",
        "        sampler=optuna.samplers.TPESampler(),\n",
        "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=1),\n",
        "        study_name=\"eegformer_optuna_cv_4\",\n",
        "        storage=\"sqlite:////content/drive/MyDrive/2025_Lab_Research/eegformer_optuna_cv_5.db\",\n",
        "        load_if_exists=True\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    best = study.best_trial\n",
        "    print(\"\\n===== Best Trial Results =====\")\n",
        "    print(f\"avg_val_loss   = {best.value:.6f}\")\n",
        "    print(f\"avg_train_loss = {best.user_attrs['avg_train_loss']:.6f}\")\n",
        "    print(f\"avg_train_acc  = {best.user_attrs['avg_train_acc']:.4f}\")\n",
        "    print(f\"avg_val_acc    = {best.user_attrs['avg_val_acc']:.4f}\")\n",
        "    print(f\"avg_best_epoch = {best.user_attrs['avg_best_epoch']:.1f}\")\n",
        "    print(\"best hyperparameters:\")\n",
        "    for k, v in best.params.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "    print(\"per-fold best metrics:\")\n",
        "    for f in range(N_FOLDS):\n",
        "        print(\n",
        "            f\"  Fold {f}: epoch={best.user_attrs[f'fold{f}_best_epoch']}, \"\n",
        "            f\"t_loss={best.user_attrs[f'fold{f}_train_loss']:.4f}, \"\n",
        "            f\"t_acc={best.user_attrs[f'fold{f}_train_acc']:.4f}, \"\n",
        "            f\"v_loss={best.user_attrs[f'fold{f}_val_loss']:.4f}, \"\n",
        "            f\"v_acc={best.user_attrs[f'fold{f}_val_acc']:.4f}\"\n",
        "        )\n",
        "\n",
        "    # ─── 전체 데이터(train)로 재학습 + loss·accuracy 출력 + 저장 ─────────────────\n",
        "    print(\"\\nRetraining on full TRAIN dataset with best params…\")\n",
        "\n",
        "    # 1) train 메타만 골라서 로드\n",
        "    with open(os.path.join(DATA_DIR, LABEL_FILE), \"r\") as f:\n",
        "        all_meta = json.load(f)\n",
        "    full_meta   = [d for d in all_meta if d[\"type\"] == \"train\"]\n",
        "    full_ds     = EEGDataset(DATA_DIR, full_meta)\n",
        "    full_loader = DataLoader(full_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "\n",
        "    # 2) 모델·옵티마이저 재설정\n",
        "    model = EEGformer(\n",
        "        num_classes=3,\n",
        "        in_channels=19,\n",
        "        kernel_size=10,\n",
        "        num_filters=120,\n",
        "        rtm_blocks=best.params[\"rtm_blocks\"],\n",
        "        stm_blocks=best.params[\"stm_blocks\"],\n",
        "        ttm_blocks=best.params[\"ttm_blocks\"],\n",
        "        rtm_heads= best.params[\"rtm_heads\"],\n",
        "        stm_heads= best.params[\"stm_heads\"],\n",
        "        ttm_heads= best.params[\"ttm_heads\"],\n",
        "        num_segments=best.params[\"num_segments\"]\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=best.params[\"lr\"],\n",
        "        weight_decay=best.params[\"weight_decay\"]\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # 3) MAX_EPOCHS 만큼 전체 학습하며 loss·accuracy 출력\n",
        "    for epoch in range(1, MAX_EPOCHS + 1):\n",
        "        model.train()\n",
        "        loss_sum = 0.0\n",
        "        correct  = 0\n",
        "        total    = 0\n",
        "        for X, y in full_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            loss_sum += loss.item()\n",
        "            correct  += (logits.argmax(1) == y).sum().item()\n",
        "            total    += y.size(0)\n",
        "\n",
        "        avg_loss = loss_sum / len(full_loader)\n",
        "        acc      = correct / total\n",
        "        print(f\"[Full Train] Epoch {epoch:03d} | loss={avg_loss:.4f} | acc={acc:.4f}\")\n",
        "\n",
        "    # 4) 체크포인트 저장\n",
        "    ckpt_dir = '/content/drive/MyDrive/2025_Lab_Research/checkpoints'\n",
        "    os.makedirs(ckpt_dir, exist_ok=True)\n",
        "    ckpt_path = os.path.join(ckpt_dir, 'eegformer_best.pth')\n",
        "    torch.save(model.state_dict(), ckpt_path)\n",
        "    print(f\"💾 Saved best model to {ckpt_path}\")\n"
      ],
      "metadata": {
        "id": "GD_KXeBrVXPH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}