{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPmGv2zIVwY9gGzckeWSQc8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Justin-Hwang/EEG-AD-FTD-Detection/blob/main/FTD_CN_Test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLikHwqphiRe",
        "outputId": "5c330e36-3108-476b-9fca-1e11b6819472"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 그중에 2025 Lab Research 폴더 안을 확인\n",
        "!ls \"/content/drive/MyDrive/2025_Lab_Research\"\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/2025_Lab_Research')\n",
        "\n",
        "import torch\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Running on\", DEVICE)  # → “cuda” 가 뜨면 GPU 정상"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ycMZERViKMs",
        "outputId": "2cc98787-3d5e-48e4-db42-b526885f0785"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'AD vs FTD vs CN Binary Classification'   eeg_holdout-5.db\n",
            "'Binary Classification LOSO Test.ipynb'   eeg_holdout-6.db\n",
            "'Colab Files'\t\t\t\t  eeg_holdout-7.db\n",
            "'Data Preparation.gdoc'\t\t\t  eeg_holdout-8.db\n",
            " eeg_dataset.py\t\t\t\t  eeg_holdout-9.db\n",
            " EEGformer_model_training.ipynb\t\t  eeg_holdout.db\n",
            " eegformer_optuna_cv_3.db\t\t  eeg_holdout_fixed_1.db\n",
            " eegformer_optuna_cv_4.db\t\t  eeg_optuna_trial_1.db\n",
            " eegformer_optuna_cv_5.db\t\t  eeg_optuna_trial_2.db\n",
            " eeg_grid_search-10.db\t\t\t  eeg_optuna_trial_3.db\n",
            " eeg_grid_search-11.db\t\t\t 'EEG Transformer Architecture.gdoc'\n",
            " eeg_grid_search-12.db\t\t\t 'Lab Info'\n",
            " eeg_grid_search-13.db\t\t\t 'Lab Research Final Report'\n",
            " eeg_grid_search-14.db\t\t\t 'Lab Research Paper Review'\n",
            " eeg_grid_search-15.db\t\t\t 'Meeting Note.gdoc'\n",
            " eeg_grid_search-16.db\t\t\t  model-data\n",
            " eeg_grid_search-17.db\t\t\t  model-data.zip\n",
            " eeg_grid_search-18.db\t\t\t  model_optimized_2.py\n",
            " eeg_grid_search-2.db\t\t\t  model_optimized_3.py\n",
            " eeg_grid_search-3.db\t\t\t  model_optimized_4.py\n",
            " eeg_grid_search-4.db\t\t\t  model_optimized_5.py\n",
            " eeg_grid_search-5.db\t\t\t  model_optimized_6.py\n",
            " eeg_grid_search-6.db\t\t\t  model_optimized_7.py\n",
            " eeg_grid_search-7.db\t\t\t  model_optimized.py\n",
            " eeg_grid_search-8.db\t\t\t  models_depracated.py\n",
            " eeg_grid_search-9.db\t\t\t  models.py\n",
            " eeg_grid_search.db\t\t\t  models_with_dropout_1.py\n",
            " eeg_holdout-1.db\t\t\t  models_with_minimal_dropout.py\n",
            " eeg_holdout-2.db\t\t\t  Practice_Note0.ipynb\n",
            " eeg_holdout-3.db\t\t\t  __pycache__\n",
            " eeg_holdout-4.db\n",
            "Running on cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "wandb.login()  # 첫 실행 시 API 키 입력"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "CsEH4cKroYKk",
        "outputId": "26abf692-b59a-4394-a5a6-11f64cc71774"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjh8032\u001b[0m (\u001b[33mjh8032-new-york-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna\n",
        "!pip install wandb\n",
        "!pip install mne"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4mAvVxaocob",
        "outputId": "c6bfe11c-5024-41f2-f43c-fec2e947ae06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.1)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m21.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.10)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.9.5)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.4)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from wandb) (6.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.32.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.27.0)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.2)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.17.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.0.0->wandb) (2025.4.26)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Collecting mne\n",
            "  Downloading mne-1.9.0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.11/dist-packages (from mne) (4.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from mne) (3.1.6)\n",
            "Requirement already satisfied: lazy-loader>=0.3 in /usr/local/lib/python3.11/dist-packages (from mne) (0.4)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from mne) (3.10.0)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.11/dist-packages (from mne) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from mne) (24.2)\n",
            "Requirement already satisfied: pooch>=1.5 in /usr/local/lib/python3.11/dist-packages (from mne) (1.8.2)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from mne) (1.15.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from mne) (4.67.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->mne) (2.9.0.post0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (4.3.7)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.5->mne) (2.32.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->mne) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.6->mne) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.5->mne) (2025.4.26)\n",
            "Downloading mne-1.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m65.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: mne\n",
            "Successfully installed mne-1.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FTD vs CN (Search the Best Hyperparameter)"
      ],
      "metadata": {
        "id": "hmgxmiZA5N_t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Baseline Results (Machine Learning) FTD vs CN\n",
        "- LightGBM\n",
        "- SVM\n",
        "- KNN\n",
        "- MLP\n",
        "- Random Forests"
      ],
      "metadata": {
        "id": "0LZF8wEEJzMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lightgbm scikit-learn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElYuB1RyKAcQ",
        "outputId": "e38f8ec8-c5e0-4c7b-9007-55553bd2abc3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline_models.py  – Classical baselines for FTD vs CN\n",
        "# -------------------------------------------------------\n",
        "import os, json, random, time, copy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.pipeline       import Pipeline\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn.metrics        import (\n",
        "    recall_score, f1_score, accuracy_score, confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.svm            import SVC\n",
        "from sklearn.neighbors      import KNeighborsClassifier\n",
        "from sklearn.ensemble       import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm               import LGBMClassifier   # pip install lightgbm\n",
        "\n",
        "from eeg_dataset import EEGDataset                 # ← your existing loader\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ─── Paths & constants (adjust as needed) ───────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = 'labels.json'\n",
        "NUM_WORKERS = 4               # DataLoader workers\n",
        "\n",
        "# ─── Dataset: returns flat NumPy feature vector + label ─────────\n",
        "class FlatEEGDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Wraps EEGDataset but outputs (1‑D float32 NumPy array, int label).\n",
        "    Works whether EEGDataset returns a tensor or an ndarray.\n",
        "    \"\"\"\n",
        "    def __init__(self, raw_ds, metas, label_map):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.labels = [label_map[d['label']] if isinstance(d['label'], str)\n",
        "                       else d['label'] for d in metas]\n",
        "\n",
        "    def __len__(self): return len(self.raw_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]                       # tensor OR ndarray\n",
        "        if isinstance(x, torch.Tensor):\n",
        "            x = x.flatten().cpu().numpy()\n",
        "        else:  # already ndarray\n",
        "            x = x.flatten()\n",
        "        return x.astype(np.float32), self.labels[idx]\n",
        "\n",
        "# ─── Utility functions ──────────────────────────────────────────\n",
        "def count_labels(meta, cls0='F', cls1='C'):\n",
        "    n0 = sum(1 for d in meta if d['label'] in (cls0, 0))\n",
        "    n1 = len(meta) - n0\n",
        "    return n0, n1\n",
        "\n",
        "def metrics(y_true, y_pred):\n",
        "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "    spec = tn / (tn + fp) if (tn + fp) else 0.0\n",
        "    return {\n",
        "        'acc':  accuracy_score(y_true, y_pred),\n",
        "        'rec':  recall_score(y_true, y_pred, zero_division=0),  # sensitivity\n",
        "        'spec': spec,\n",
        "        'f1':   f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "\n",
        "def report(name, res):\n",
        "    print(f\"{name:>11s}: \"\n",
        "          f\"Accuracy={res['acc']:.4f} \"\n",
        "          f\"Sensitivity={res['rec']:.4f} \"\n",
        "          f\"Specificity={res['spec']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\")\n",
        "\n",
        "# ─── Load label metadata (same JSON your EEGformer uses) ────────\n",
        "with open(Path(DATA_DIR) / LABEL_FILE, 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type'] == 'train']\n",
        "test_within_meta = [d for d in all_meta if d['type'] == 'test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type'] == 'test_cross']\n",
        "\n",
        "class0, class1   = 'F', 'C'            # FTD vs CN\n",
        "label_map        = {class0: 0, class1: 1}\n",
        "\n",
        "# Keep only F & C in evaluation splits\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "# ─── Balance training split (down‑sample larger class) ──────────\n",
        "data0 = [d for d in train_meta if d['label'] == class0]\n",
        "data1 = [d for d in train_meta if d['label'] == class1]\n",
        "k     = min(len(data0), len(data1))\n",
        "balanced_meta = random.sample(data0, k) + random.sample(data1, k)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]     # convert to int\n",
        "\n",
        "print(f\"\\n[Balanced] TRAIN  total={len(balanced_meta)} (FTD={k}, CN={k})\")\n",
        "\n",
        "# ─── Convert each split to NumPy (X, y) -------------------------\n",
        "def make_numpy(metas):\n",
        "    raw = EEGDataset(DATA_DIR, metas)\n",
        "    ds  = FlatEEGDataset(raw, metas, label_map)\n",
        "    loader = DataLoader(ds, batch_size=128, shuffle=False,\n",
        "                        num_workers=NUM_WORKERS)\n",
        "    X, y = [], []\n",
        "    for xb, yb in loader:          # xb is *tensor* batch\n",
        "        X.append(xb)               # keep on CPU\n",
        "        y.append(yb)\n",
        "    return torch.cat(X).numpy(), torch.cat(y).numpy()\n",
        "\n",
        "X_train, y_train = make_numpy(balanced_meta)\n",
        "X_tw,    y_tw    = make_numpy(test_within_meta)\n",
        "X_tc,    y_tc    = make_numpy(test_cross_meta)\n",
        "\n",
        "print(f\"Input feature vector length per sample = {X_train.shape[1]}\")\n",
        "\n",
        "# ─── Define classical models ------------------------------------\n",
        "models = {\n",
        "    \"LightGBM\" : LGBMClassifier(\n",
        "        n_estimators=100, learning_rate=0.001,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    ),\n",
        "    \"SVM\"      : SVC(\n",
        "        kernel='rbf', probability=False,\n",
        "        class_weight='balanced', random_state=SEED, gamma='scale'\n",
        "    ),\n",
        "    \"KNN\"      : KNeighborsClassifier(\n",
        "        n_neighbors=7, weights='distance'\n",
        "    ),\n",
        "    \"MLP\"      : MLPClassifier(\n",
        "        hidden_layer_sizes=(256, 128), max_iter=200,\n",
        "        activation='relu', solver='adam', random_state=SEED\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=None,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# ─── Train & evaluate -------------------------------------------\n",
        "print(\"\\n=== Final Evaluation on Test Sets (Classical Baselines) ===\")\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf',    clf)\n",
        "    ])\n",
        "\n",
        "    t0 = time.time()\n",
        "    pipe.fit(X_train, y_train)\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    y_pred_tw = pipe.predict(X_tw)\n",
        "    y_pred_tc = pipe.predict(X_tc)\n",
        "\n",
        "    res_tw = metrics(y_tw, y_pred_tw)\n",
        "    res_tc = metrics(y_tc, y_pred_tc)\n",
        "\n",
        "    print(f\"\\n-- {name} (train time {train_time:.1f}s) --\")\n",
        "    print(f\"   test_within  total={len(y_tw)}\")\n",
        "    report(\"test_within\", res_tw)\n",
        "    print(f\"   test_cross   total={len(y_tc)}\")\n",
        "    report(\"test_cross\",  res_tc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BKE5ZmCvKAkT",
        "outputId": "a31611d0-81ae-4a48-96dd-98ae348f6cf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Balanced] TRAIN  total=1458 (FTD=729, CN=729)\n",
            "Input feature vector length per sample = 27075\n",
            "\n",
            "=== Final Evaluation on Test Sets (Classical Baselines) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 729, number of negative: 729\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.948458 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6904125\n",
            "[LightGBM] [Info] Number of data points in the train set: 1458, number of used features: 27075\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- LightGBM (train time 70.6s) --\n",
            "   test_within  total=198\n",
            "test_within: Accuracy=0.4899 Sensitivity=0.5159 Specificity=0.4444 F1=0.5628\n",
            "   test_cross   total=554\n",
            " test_cross: Accuracy=0.4892 Sensitivity=0.4756 Specificity=0.5061 F1=0.5078\n",
            "\n",
            "-- SVM (train time 17.2s) --\n",
            "   test_within  total=198\n",
            "test_within: Accuracy=0.5202 Sensitivity=0.4841 Specificity=0.5833 F1=0.5622\n",
            "   test_cross   total=554\n",
            " test_cross: Accuracy=0.4964 Sensitivity=0.4723 Specificity=0.5263 F1=0.5097\n",
            "\n",
            "-- KNN (train time 0.5s) --\n",
            "   test_within  total=198\n",
            "test_within: Accuracy=0.5455 Sensitivity=0.5159 Specificity=0.5972 F1=0.5909\n",
            "   test_cross   total=554\n",
            " test_cross: Accuracy=0.5235 Sensitivity=0.5147 Specificity=0.5344 F1=0.5448\n",
            "\n",
            "-- MLP (train time 27.3s) --\n",
            "   test_within  total=198\n",
            "test_within: Accuracy=0.4545 Sensitivity=0.4127 Specificity=0.5278 F1=0.4906\n",
            "   test_cross   total=554\n",
            " test_cross: Accuracy=0.5090 Sensitivity=0.5212 Specificity=0.4939 F1=0.5405\n",
            "\n",
            "-- RandomForest (train time 25.2s) --\n",
            "   test_within  total=198\n",
            "test_within: Accuracy=0.5657 Sensitivity=0.5317 Specificity=0.6250 F1=0.6091\n",
            "   test_cross   total=554\n",
            " test_cross: Accuracy=0.4621 Sensitivity=0.4202 Specificity=0.5142 F1=0.4640\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# baseline_multiclass.py – Classical baselines for AD vs FTD vs CN\n",
        "# -------------------------------------------------------\n",
        "import os, json, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from sklearn.pipeline       import Pipeline\n",
        "from sklearn.preprocessing  import StandardScaler\n",
        "from sklearn.metrics        import (\n",
        "    recall_score, f1_score, accuracy_score, confusion_matrix\n",
        ")\n",
        "\n",
        "from sklearn.svm            import SVC\n",
        "from sklearn.neighbors      import KNeighborsClassifier\n",
        "from sklearn.ensemble       import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from lightgbm               import LGBMClassifier\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "# ─── Paths & constants ──────────────────────────────────────────\n",
        "DATA_DIR   = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE = 'labels.json'\n",
        "NUM_WORKERS = 4\n",
        "\n",
        "# ─── Dataset wrapper ────────────────────────────────────────────\n",
        "class FlatEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas, label_map):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.labels = [label_map[d['label']] if isinstance(d['label'], str)\n",
        "                       else d['label'] for d in metas]\n",
        "\n",
        "    def __len__(self): return len(self.raw_ds)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        x = x.flatten().cpu().numpy() if isinstance(x, torch.Tensor) else x.flatten()\n",
        "        return x.astype(np.float32), self.labels[idx]\n",
        "\n",
        "# ─── Metrics function for multiclass ────────────────────────────\n",
        "def metrics(y_true, y_pred, labels):\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "    specificity_list = []\n",
        "\n",
        "    for i in range(len(labels)):\n",
        "        tp = cm[i, i]\n",
        "        fn = cm[i, :].sum() - tp\n",
        "        fp = cm[:, i].sum() - tp\n",
        "        tn = cm.sum() - (tp + fp + fn)\n",
        "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "        specificity_list.append(specificity)\n",
        "\n",
        "    return {\n",
        "        'acc':  accuracy_score(y_true, y_pred),\n",
        "        'rec':  recall_score(y_true, y_pred, average='macro', zero_division=0),\n",
        "        'spec': np.mean(specificity_list),\n",
        "        'f1':   f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
        "    }\n",
        "\n",
        "def report(name, res):\n",
        "    print(f\"{name:>11s}: Accuracy={res['acc']:.4f} \"\n",
        "          f\"Sensitivity={res['rec']:.4f} \"\n",
        "          f\"Specificity={res['spec']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\")\n",
        "\n",
        "# ─── Load and label metadata ────────────────────────────────────\n",
        "with open(Path(DATA_DIR) / LABEL_FILE, 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "label_map = {'A': 0, 'C': 1, 'F': 2}\n",
        "train_meta       = [d for d in all_meta if d['type'] == 'train']\n",
        "test_within_meta = [d for d in all_meta if d['type'] == 'test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type'] == 'test_cross']\n",
        "\n",
        "for d in train_meta + test_within_meta + test_cross_meta:\n",
        "    if isinstance(d['label'], str):\n",
        "        d['label'] = label_map[d['label']]\n",
        "\n",
        "print(f\"[Multiclass] TRAIN total={len(train_meta)} | \"\n",
        "      f\"AD={sum(d['label']==0 for d in train_meta)}, \"\n",
        "      f\"CN={sum(d['label']==1 for d in train_meta)}, \"\n",
        "      f\"FTD={sum(d['label']==2 for d in train_meta)}\")\n",
        "\n",
        "# ─── Convert to NumPy (X, y) ─────────────────────────────────────\n",
        "def make_numpy(metas):\n",
        "    raw = EEGDataset(DATA_DIR, metas)\n",
        "    ds = FlatEEGDataset(raw, metas, label_map)\n",
        "    loader = DataLoader(ds, batch_size=128, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    X, y = [], []\n",
        "    for xb, yb in loader:\n",
        "        X.append(xb)\n",
        "        y.append(yb)\n",
        "    return torch.cat(X).numpy(), torch.cat(y).numpy()\n",
        "\n",
        "X_train, y_train = make_numpy(train_meta)\n",
        "X_tw,    y_tw    = make_numpy(test_within_meta)\n",
        "X_tc,    y_tc    = make_numpy(test_cross_meta)\n",
        "\n",
        "print(f\"Input feature vector length per sample = {X_train.shape[1]}\")\n",
        "\n",
        "# ─── Define baseline models ─────────────────────────────────────\n",
        "models = {\n",
        "    \"LightGBM\" : LGBMClassifier(\n",
        "        n_estimators=100, learning_rate=0.001,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    ),\n",
        "    \"SVM\"      : SVC(\n",
        "        kernel='rbf', probability=False,\n",
        "        class_weight='balanced', random_state=SEED, gamma='scale'\n",
        "    ),\n",
        "    \"KNN\"      : KNeighborsClassifier(\n",
        "        n_neighbors=7, weights='distance'\n",
        "    ),\n",
        "    \"MLP\"      : MLPClassifier(\n",
        "        hidden_layer_sizes=(256, 128), max_iter=200,\n",
        "        activation='relu', solver='adam', random_state=SEED\n",
        "    ),\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        n_estimators=100, max_depth=None,\n",
        "        random_state=SEED, class_weight='balanced'\n",
        "    )\n",
        "}\n",
        "\n",
        "# ─── Train and evaluate ─────────────────────────────────────────\n",
        "print(\"\\n=== Final Evaluation on Test Sets (Classical Multiclass Baselines) ===\")\n",
        "for name, clf in models.items():\n",
        "    pipe = Pipeline([\n",
        "        ('scaler', StandardScaler()),\n",
        "        ('clf',    clf)\n",
        "    ])\n",
        "\n",
        "    t0 = time.time()\n",
        "    pipe.fit(X_train, y_train)\n",
        "    train_time = time.time() - t0\n",
        "\n",
        "    y_pred_tw = pipe.predict(X_tw)\n",
        "    y_pred_tc = pipe.predict(X_tc)\n",
        "\n",
        "    res_tw = metrics(y_tw, y_pred_tw, labels=[0, 1, 2])\n",
        "    res_tc = metrics(y_tc, y_pred_tc, labels=[0, 1, 2])\n",
        "\n",
        "    print(f\"\\n-- {name} (train time {train_time:.1f}s) --\")\n",
        "    print(f\"   test_within  total={len(y_tw)}\")\n",
        "    report(\"test_within\", res_tw)\n",
        "    print(f\"   test_cross   total={len(y_tc)}\")\n",
        "    report(\"test_cross\",  res_tc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t6hxS18FKAqE",
        "outputId": "55b959cd-c521-4742-bee7-ec3f0a0cea8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Multiclass] TRAIN total=3219 | AD=1388, CN=1102, FTD=729\n",
            "Input feature vector length per sample = 27075\n",
            "\n",
            "=== Final Evaluation on Test Sets (Classical Multiclass Baselines) ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.120448 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 6904125\n",
            "[LightGBM] [Info] Number of data points in the train set: 3219, number of used features: 27075\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n",
            "[LightGBM] [Info] Start training from score -1.098612\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "-- LightGBM (train time 228.9s) --\n",
            "   test_within  total=344\n",
            "test_within: Accuracy=0.3547 Sensitivity=0.3402 Specificity=0.6715 F1=0.3390\n",
            "   test_cross   total=873\n",
            " test_cross: Accuracy=0.3276 Sensitivity=0.3214 Specificity=0.6610 F1=0.3207\n",
            "\n",
            "-- SVM (train time 104.6s) --\n",
            "   test_within  total=344\n",
            "test_within: Accuracy=0.3517 Sensitivity=0.3422 Specificity=0.6673 F1=0.3397\n",
            "   test_cross   total=873\n",
            " test_cross: Accuracy=0.3219 Sensitivity=0.3182 Specificity=0.6572 F1=0.3176\n",
            "\n",
            "-- KNN (train time 1.0s) --\n",
            "   test_within  total=344\n",
            "test_within: Accuracy=0.4215 Sensitivity=0.3737 Specificity=0.6878 F1=0.3664\n",
            "   test_cross   total=873\n",
            " test_cross: Accuracy=0.3528 Sensitivity=0.3379 Specificity=0.6678 F1=0.3272\n",
            "\n",
            "-- MLP (train time 63.3s) --\n",
            "   test_within  total=344\n",
            "test_within: Accuracy=0.4041 Sensitivity=0.3548 Specificity=0.6846 F1=0.3420\n",
            "   test_cross   total=873\n",
            " test_cross: Accuracy=0.3562 Sensitivity=0.3410 Specificity=0.6722 F1=0.3269\n",
            "\n",
            "-- RandomForest (train time 71.2s) --\n",
            "   test_within  total=344\n",
            "test_within: Accuracy=0.4041 Sensitivity=0.3239 Specificity=0.6607 F1=0.2524\n",
            "   test_cross   total=873\n",
            " test_cross: Accuracy=0.3711 Sensitivity=0.3409 Specificity=0.6711 F1=0.2519\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import time\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import wandb\n",
        "import optuna\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from torch.utils.data import DataLoader, Subset, Dataset\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── Reproducibility ────────────────────────────────────────────\n",
        "SEED = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "# ─── Configuration ───────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "MAX_EPOCHS  = 150\n",
        "BATCH_SIZE  = 32\n",
        "NUM_WORKERS = 4\n",
        "ES_PATIENCE = 15\n",
        "\n",
        "# ─── Fixed Hyper-parameters ───────────────────────────────────────\n",
        "PCT_START   = 0.2  # fixed\n",
        "\n",
        "# ─── Dataset wrapper ─────────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── Load & count splits ──────────────────────────────────────────\n",
        "with open(os.path.join(DATA_DIR, LABEL_FILE), 'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "class0, class1 = 'F','C'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "# F vs C Filtering\n",
        "train_meta       = [d for d in train_meta if d['label'] in (class0, class1)]\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in (class0, class1)]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in (class0, class1)]\n",
        "\n",
        "def count_labels(meta_list):\n",
        "    cnt0 = cnt1 = 0\n",
        "    for d in meta_list:\n",
        "        lbl = d['label']\n",
        "        if isinstance(lbl, str):\n",
        "            if lbl == class0: cnt0 += 1\n",
        "            elif lbl == class1: cnt1 += 1\n",
        "        else:\n",
        "            if lbl == 0: cnt0 += 1\n",
        "            elif lbl == 1: cnt1 += 1\n",
        "    return cnt0, cnt1\n",
        "\n",
        "n_tr0, n_tr1 = count_labels(train_meta)\n",
        "n_tw0, n_tw1 = count_labels(test_within_meta)\n",
        "n_tc0, n_tc1 = count_labels(test_cross_meta)\n",
        "\n",
        "print(f\"--> Data counts before balancing:\")\n",
        "print(f\"    TRAIN         total={len(train_meta)}  F(TD)={n_tr0}, C(N)={n_tr1}\")\n",
        "print(f\"    TEST_WITHIN   total={len(test_within_meta)}  F(TD)={n_tw0}, C(N)={n_tw1}\")\n",
        "print(f\"    TEST_CROSS    total={len(test_cross_meta)}  F(TD)={n_tc0}, C(N)={n_tc1}\\n\")\n",
        "\n",
        "# ─── Balance train set ───────────────────────────────────────────\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "min_count = min(len(data0), len(data1))\n",
        "\n",
        "balanced_meta = random.sample(data0, min_count) + random.sample(data1, min_count)\n",
        "balanced_meta = [copy.deepcopy(d) for d in balanced_meta]\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# ─── After Balancing ───────────────────────────────────────────\n",
        "bal_FTD, bal_CN = count_labels(balanced_meta)\n",
        "print(f\"[BALANCED TRAIN] total={len(balanced_meta)}  FTD={bal_FTD}, CN={bal_CN}\\n\")\n",
        "\n",
        "for d in balanced_meta:\n",
        "    d['label'] = label_map[d['label']]\n",
        "\n",
        "raw_ds_train  = EEGDataset(DATA_DIR, balanced_meta)\n",
        "dataset_train = BinaryEEGDataset(raw_ds_train, balanced_meta)\n",
        "labels_train  = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── Optuna Objective ────────────────────────────────────────────\n",
        "def objective(trial):\n",
        "    # 1) sample hyperparameters\n",
        "    lr = trial.suggest_float('lr', 1e-5, 1e-2, log=True)\n",
        "    wd = trial.suggest_float('wd', 1e-6, 1e-3, log=True)\n",
        "\n",
        "\n",
        "    print(f\"\\n--- Trial {trial.number} hyperparams --> lr={lr:.2e}, wd={wd:.2e}\")\n",
        "\n",
        "    wandb.init(\n",
        "        project='eeg-optuna-FTD-CN-test-within-cross-3',\n",
        "        name=f\"trial{trial.number}\",\n",
        "        config={'lr':lr, 'wd':wd, 'pct_start':PCT_START},\n",
        "        reinit=True\n",
        "    )\n",
        "\n",
        "    # train/validation split\n",
        "    idx = np.arange(len(dataset_train))\n",
        "    tr_idx, va_idx = train_test_split(\n",
        "        idx, test_size=0.2,\n",
        "        stratify=labels_train, random_state=SEED\n",
        "    )\n",
        "\n",
        "    tr_FTD = np.sum(labels_train[tr_idx] == 0)\n",
        "    tr_CN = np.sum(labels_train[tr_idx] == 1)\n",
        "    va_FTD = np.sum(labels_train[va_idx] == 0)\n",
        "    va_CN = np.sum(labels_train[va_idx] == 1)\n",
        "    print(f\"[Trial {trial.number}]  \"\n",
        "          f\"TRAIN n={len(tr_idx)} (FTD={tr_FTD}, CN={tr_CN}) | \"\n",
        "          f\"VAL n={len(va_idx)} (FTD={va_FTD}, CN={va_CN})\")\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        Subset(dataset_train, tr_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS\n",
        "    )\n",
        "    val_loader   = DataLoader(\n",
        "        Subset(dataset_train, va_idx),\n",
        "        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "    )\n",
        "\n",
        "    # model & optimizer & scheduler & loss\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=lr, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_val_acc  = 0.0\n",
        "    best_state= None\n",
        "    es_count      = 0\n",
        "\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # ── train ──\n",
        "        model.train()\n",
        "        train_loss_sum = train_correct = train_total = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight_decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = wd * (cur_lr / lr)\n",
        "            train_loss_sum += loss.item()\n",
        "            preds = logits.argmax(1)\n",
        "            train_correct += (preds == y).sum().item()\n",
        "            train_total   += y.size(0)\n",
        "        train_loss = train_loss_sum / len(train_loader)\n",
        "        train_acc  = train_correct / train_total\n",
        "\n",
        "        # ── validate ──\n",
        "        model.eval()\n",
        "        vloss = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "        val_loss = vloss / len(val_loader)\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        val_acc = (preds == labs).sum() / labs.size\n",
        "\n",
        "        # ── metrics ──\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "        spec = tn / (tn+fp) if (tn+fp)>0 else 0.0\n",
        "        prec = precision_score(labs, preds, zero_division=0)\n",
        "        rec  = recall_score(labs, preds, zero_division=0)\n",
        "        f1   = f1_score(labs, preds, zero_division=0)\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(\n",
        "            f\"Epoch {epoch:03d} | \"\n",
        "            f\"train_loss={train_loss:.4f} acc={train_acc:.4f} | \"\n",
        "            f\"val_loss={val_loss:.4f} acc={val_acc:.4f} | \"\n",
        "            f\"prec={prec:.4f} rec={rec:.4f} spec={spec:.4f} f1={f1:.4f} | \"\n",
        "            f\"time={elapsed:.1f}s\"\n",
        "        )\n",
        "\n",
        "        wandb.log({\n",
        "            'epoch':       epoch,\n",
        "            'train_loss':  train_loss,\n",
        "            'train_acc':   train_acc,\n",
        "            'val_loss':    val_loss,\n",
        "            'val_acc':     val_acc,\n",
        "            'specificity': spec,\n",
        "            'precision':   prec,\n",
        "            'recall':      rec,\n",
        "            'f1_score':    f1\n",
        "        })\n",
        "\n",
        "        # ── Early Stopping (val_loss 기준) ──\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_val_acc  = val_acc\n",
        "            es_count      = 0\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "\n",
        "            os.makedirs('ckpts', exist_ok=True)\n",
        "            ckpt_path = f'ckpts/trial{trial.number}_optimal_FTD_CN_3.pth'\n",
        "            torch.save(best_state, ckpt_path)\n",
        "            trial.set_user_attr('ckpt_path', ckpt_path)\n",
        "            trial.set_user_attr('best_epoch', epoch)\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    wandb.finish()\n",
        "    del model, optimizer, train_loader, val_loader, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # 다중목적 리턴\n",
        "    return best_val_loss, best_val_acc\n",
        "\n",
        "# ─── Run Optuna Study ────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    study = optuna.create_study(\n",
        "        directions=[\"minimize\", \"maximize\"],\n",
        "        study_name=\"eeg_multiobj_FTD_CN_3\"\n",
        "    )\n",
        "    study.optimize(objective, n_trials=10)\n",
        "\n",
        "    # Pareto front 중 val_acc 가 가장 높은 trial 선택\n",
        "    best       = max(study.best_trials, key=lambda t: t.values[1])\n",
        "    bv_loss, bv_acc = best.values\n",
        "    best_epoch = best.user_attrs[\"best_epoch\"]\n",
        "    ckpt_path  = best.user_attrs[\"ckpt_path\"]\n",
        "\n",
        "    print(f\"\\n=== Selected Trial #{best.number} ===\")\n",
        "    print(\n",
        "        f\" val_loss={bv_loss:.4f}, val_acc={bv_acc:.4f}, \"\n",
        "        f\"params={best.params}, best_epoch={best_epoch}, ckpt={ckpt_path}\"\n",
        "    )\n",
        "\n",
        "    # ─── Search‑model 로드 ───────────────────────────────────────\n",
        "    input_len = dataset_train[0][0].shape[-1]\n",
        "    search_model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    search_model.load_state_dict(torch.load(ckpt_path, map_location=DEVICE))\n",
        "    search_model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # ─── Evaluation 함수 ─────────────────────────────────────────\n",
        "    def evaluate(model, metas, criterion):\n",
        "        metas_copy = copy.deepcopy(metas)\n",
        "        for d in metas_copy:\n",
        "            if isinstance(d[\"label\"], str):\n",
        "                d[\"label\"] = label_map[d[\"label\"]]\n",
        "\n",
        "        ds = BinaryEEGDataset(EEGDataset(DATA_DIR, metas_copy), metas_copy)\n",
        "        loader = DataLoader(\n",
        "            ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS\n",
        "        )\n",
        "\n",
        "        model.eval()\n",
        "        vloss = vcorrect = vtotal = 0\n",
        "        all_preds, all_labels = [], []\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                vloss += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                vcorrect += (preds == y).sum().item()\n",
        "                vtotal += y.size(0)\n",
        "                all_preds.append(preds.cpu().numpy())\n",
        "                all_labels.append(y.cpu().numpy())\n",
        "\n",
        "        preds = np.concatenate(all_preds)\n",
        "        labs  = np.concatenate(all_labels)\n",
        "        tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0, 1]).ravel()\n",
        "        return {\n",
        "            \"loss\":        vloss / len(loader),\n",
        "            \"acc\":         vcorrect / vtotal,\n",
        "            \"sensitivity\": recall_score(labs, preds, zero_division=0),\n",
        "            \"specificity\": tn / (tn + fp) if (tn + fp) > 0 else 0.0,\n",
        "            \"f1\":          f1_score(labs, preds, zero_division=0),\n",
        "        }\n",
        "\n",
        "    # ─── Final Evaluation on Test Sets ────────────────────────────\n",
        "    print(\"=== Final Evaluation on Test Sets ===\")\n",
        "    for name, metas in [\n",
        "        (\"test_within\", test_within_meta),\n",
        "        (\"test_cross\",  test_cross_meta),\n",
        "    ]:\n",
        "        res = evaluate(search_model, metas, criterion)\n",
        "        n0, n1 = count_labels(metas)\n",
        "        print(f\"-- {name} -- total={len(metas)}  F(TD)={n0}, C(N)={n1}\")\n",
        "        print(\n",
        "            f\" Accuracy={res['acc']:.4f} \"\n",
        "            f\"Sensitivity={res['sensitivity']:.4f} \"\n",
        "            f\"Specificity={res['specificity']:.4f} \"\n",
        "            f\"F1={res['f1']:.4f}\\n\"\n",
        "        )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "W8cS60L05Nmz",
        "outputId": "f6dc8748-b30f-4ae1-e6c3-c8cb05445dda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 06:54:40,632] A new study created in memory with name: eeg_multiobj_FTD_CN_3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--> Data counts before balancing:\n",
            "    TRAIN         total=1831  F(TD)=729, C(N)=1102\n",
            "    TEST_WITHIN   total=198  F(TD)=72, C(N)=126\n",
            "    TEST_CROSS    total=554  F(TD)=247, C(N)=307\n",
            "\n",
            "[BALANCED TRAIN] total=1458  FTD=729, CN=729\n",
            "\n",
            "\n",
            "--- Trial 0 hyperparams --> lr=4.03e-05, wd=4.95e-05\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Using a boolean value for 'reinit' is deprecated. Use 'return_previous' or 'finish_previous' instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_065440-lgshs0ux</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/lgshs0ux' target=\"_blank\">trial0</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/lgshs0ux' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/lgshs0ux</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 0]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7520 acc=0.4837 | val_loss=0.6931 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7414 acc=0.5051 | val_loss=0.6927 acc=0.4966 | prec=0.4983 rec=0.9863 spec=0.0068 f1=0.6621 | time=10.0s\n",
            "Epoch 003 | train_loss=0.7530 acc=0.5026 | val_loss=0.6925 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.9s\n",
            "Epoch 004 | train_loss=0.7591 acc=0.4966 | val_loss=0.6938 acc=0.4623 | prec=0.4729 rec=0.6575 spec=0.2671 f1=0.5501 | time=10.0s\n",
            "Epoch 005 | train_loss=0.7264 acc=0.5154 | val_loss=0.6928 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.9s\n",
            "Epoch 006 | train_loss=0.7423 acc=0.4871 | val_loss=0.6914 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.9s\n",
            "Epoch 007 | train_loss=0.7321 acc=0.5129 | val_loss=0.6908 acc=0.5034 | prec=0.5017 rec=0.9863 spec=0.0205 f1=0.6651 | time=9.8s\n",
            "Epoch 008 | train_loss=0.7228 acc=0.5129 | val_loss=0.6934 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.8s\n",
            "Epoch 009 | train_loss=0.7312 acc=0.5257 | val_loss=0.6926 acc=0.5068 | prec=0.5088 rec=0.3973 spec=0.6164 f1=0.4462 | time=10.1s\n",
            "Epoch 010 | train_loss=0.7228 acc=0.5163 | val_loss=0.6918 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.9s\n",
            "Epoch 011 | train_loss=0.7267 acc=0.5240 | val_loss=0.6914 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=10.0s\n",
            "Epoch 012 | train_loss=0.7263 acc=0.4974 | val_loss=0.6981 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.9s\n",
            "Epoch 013 | train_loss=0.7316 acc=0.4974 | val_loss=0.6917 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=10.0s\n",
            "Epoch 014 | train_loss=0.7236 acc=0.5257 | val_loss=0.6958 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=10.0s\n",
            "Epoch 015 | train_loss=0.7270 acc=0.5086 | val_loss=0.6913 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 016 | train_loss=0.7120 acc=0.5232 | val_loss=0.6914 acc=0.5000 | prec=0.5000 rec=0.9932 spec=0.0068 f1=0.6651 | time=9.0s\n",
            "Epoch 017 | train_loss=0.7183 acc=0.5223 | val_loss=0.6953 acc=0.5034 | prec=0.6000 rec=0.0205 spec=0.9863 f1=0.0397 | time=9.1s\n",
            "Epoch 018 | train_loss=0.7240 acc=0.4974 | val_loss=0.6916 acc=0.4966 | prec=0.4983 rec=0.9863 spec=0.0068 f1=0.6621 | time=9.0s\n",
            "Epoch 019 | train_loss=0.7330 acc=0.4923 | val_loss=0.6938 acc=0.4932 | prec=0.4722 rec=0.1164 spec=0.8699 f1=0.1868 | time=9.0s\n",
            "Epoch 020 | train_loss=0.7200 acc=0.4923 | val_loss=0.6930 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 021 | train_loss=0.7177 acc=0.5129 | val_loss=0.6917 acc=0.5171 | prec=0.5110 rec=0.7945 spec=0.2397 f1=0.6220 | time=9.0s\n",
            "Epoch 022 | train_loss=0.7184 acc=0.5163 | val_loss=0.6943 acc=0.5342 | prec=0.5694 rec=0.2808 spec=0.7877 f1=0.3761 | time=9.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>███▇████▆██▁█▁██▁█▃██▅</td></tr><tr><td>precision</td><td>▇▇▇▇▇▇▇▇▇▇▇▁▇▁▇▇█▇▇▇▇█</td></tr><tr><td>recall</td><td>███▆████▄██▁█▁██▁█▂█▇▃</td></tr><tr><td>specificity</td><td>▁▁▁▃▁▁▁▁▅▁▁█▁█▁▁█▁▇▁▃▇</td></tr><tr><td>train_acc</td><td>▁▅▄▃▆▂▆▆█▆█▃▃█▅█▇▃▂▂▆▆</td></tr><tr><td>train_loss</td><td>▇▅▇█▃▆▄▃▄▃▃▃▄▃▃▁▂▃▄▂▂▂</td></tr><tr><td>val_acc</td><td>▅▄▅▁▅▅▅▅▅▅▅▅▄▅▅▅▅▄▄▅▆█</td></tr><tr><td>val_loss</td><td>▃▃▃▄▃▂▁▃▃▂▂█▂▆▂▂▅▂▄▃▂▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>22</td></tr><tr><td>f1_score</td><td>0.37615</td></tr><tr><td>precision</td><td>0.56944</td></tr><tr><td>recall</td><td>0.28082</td></tr><tr><td>specificity</td><td>0.78767</td></tr><tr><td>train_acc</td><td>0.5163</td></tr><tr><td>train_loss</td><td>0.71841</td></tr><tr><td>val_acc</td><td>0.53425</td></tr><tr><td>val_loss</td><td>0.69426</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial0</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/lgshs0ux' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/lgshs0ux</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_065440-lgshs0ux/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 06:58:12,769] Trial 0 finished with values: [0.6907883644104004, 0.5034246575342466] and parameters: {'lr': 4.0254987609007336e-05, 'wd': 4.9538793377555885e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 1 hyperparams --> lr=8.50e-03, wd=7.83e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_065812-zgtv869n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zgtv869n' target=\"_blank\">trial1</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zgtv869n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zgtv869n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 1]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7233 acc=0.4940 | val_loss=0.7359 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7152 acc=0.5043 | val_loss=0.6982 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 003 | train_loss=0.7051 acc=0.5000 | val_loss=0.7231 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7005 acc=0.5154 | val_loss=0.6747 acc=0.5034 | prec=0.5017 rec=1.0000 spec=0.0068 f1=0.6682 | time=9.2s\n",
            "Epoch 005 | train_loss=0.6593 acc=0.6115 | val_loss=0.6778 acc=0.5479 | prec=0.5259 rec=0.9726 spec=0.1233 f1=0.6827 | time=9.1s\n",
            "Epoch 006 | train_loss=0.6148 acc=0.6681 | val_loss=0.5137 acc=0.7842 | prec=0.8074 rec=0.7466 spec=0.8219 f1=0.7758 | time=9.2s\n",
            "Epoch 007 | train_loss=0.6109 acc=0.6647 | val_loss=0.5267 acc=0.8048 | prec=0.7730 rec=0.8630 spec=0.7466 f1=0.8155 | time=9.2s\n",
            "Epoch 008 | train_loss=0.5509 acc=0.7333 | val_loss=0.6238 acc=0.7329 | prec=0.8542 rec=0.5616 spec=0.9041 f1=0.6777 | time=9.2s\n",
            "Epoch 009 | train_loss=0.5533 acc=0.7221 | val_loss=0.6129 acc=0.7603 | prec=0.9130 rec=0.5753 spec=0.9452 f1=0.7059 | time=9.2s\n",
            "Epoch 010 | train_loss=0.5154 acc=0.7470 | val_loss=0.6281 acc=0.6438 | prec=0.9565 rec=0.3014 spec=0.9863 f1=0.4583 | time=9.1s\n",
            "Epoch 011 | train_loss=0.5177 acc=0.7547 | val_loss=0.5900 acc=0.6164 | prec=0.5691 rec=0.9589 spec=0.2740 f1=0.7143 | time=9.1s\n",
            "Epoch 012 | train_loss=0.4527 acc=0.7744 | val_loss=0.7697 acc=0.6575 | prec=0.9792 rec=0.3219 spec=0.9932 f1=0.4845 | time=9.0s\n",
            "Epoch 013 | train_loss=0.4258 acc=0.8079 | val_loss=0.7030 acc=0.6541 | prec=0.9592 rec=0.3219 spec=0.9863 f1=0.4821 | time=9.2s\n",
            "Epoch 014 | train_loss=0.3973 acc=0.8259 | val_loss=0.6519 acc=0.6541 | prec=0.5949 rec=0.9658 spec=0.3425 f1=0.7363 | time=9.0s\n",
            "Epoch 015 | train_loss=0.4558 acc=0.7710 | val_loss=2.5931 acc=0.5377 | prec=1.0000 rec=0.0753 spec=1.0000 f1=0.1401 | time=9.1s\n",
            "Epoch 016 | train_loss=0.4003 acc=0.8165 | val_loss=0.6415 acc=0.6096 | prec=0.5645 rec=0.9589 spec=0.2603 f1=0.7107 | time=9.4s\n",
            "Epoch 017 | train_loss=0.3803 acc=0.8259 | val_loss=0.6601 acc=0.7055 | prec=0.6471 rec=0.9041 spec=0.5068 f1=0.7543 | time=9.2s\n",
            "Epoch 018 | train_loss=0.3617 acc=0.8285 | val_loss=2.7577 acc=0.5411 | prec=1.0000 rec=0.0822 spec=1.0000 f1=0.1519 | time=9.3s\n",
            "Epoch 019 | train_loss=0.3164 acc=0.8576 | val_loss=0.7464 acc=0.6884 | prec=0.6279 rec=0.9247 spec=0.4521 f1=0.7479 | time=9.3s\n",
            "Epoch 020 | train_loss=0.3415 acc=0.8491 | val_loss=0.9253 acc=0.6507 | prec=0.9400 rec=0.3219 spec=0.9795 f1=0.4796 | time=9.4s\n",
            "Epoch 021 | train_loss=0.3173 acc=0.8533 | val_loss=0.6007 acc=0.7500 | prec=0.8288 rec=0.6301 spec=0.8699 f1=0.7160 | time=9.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▃▃▃▄▄▅▅▅▆▆▆▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▇▇██▇▇▅▇▅▅▇▂▇▇▂▇▅▇</td></tr><tr><td>precision</td><td>▁▁▁▅▅▇▆▇▇█▅██▅█▅▆█▅█▇</td></tr><tr><td>recall</td><td>▁▁▁██▆▇▅▅▃█▃▃█▂█▇▂▇▃▅</td></tr><tr><td>specificity</td><td>███▁▂▇▆▇██▃██▃█▃▅█▄█▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▃▄▄▆▅▆▆▆▇▇▆▇▇▇███</td></tr><tr><td>train_loss</td><td>████▇▆▆▅▅▄▄▃▃▂▃▂▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂██▆▇▄▄▅▅▅▂▄▆▂▅▄▇</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▁▁▁▁▁▁▂▂▁▇▁▁█▂▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>21</td></tr><tr><td>f1_score</td><td>0.71595</td></tr><tr><td>precision</td><td>0.82883</td></tr><tr><td>recall</td><td>0.63014</td></tr><tr><td>specificity</td><td>0.86986</td></tr><tr><td>train_acc</td><td>0.85334</td></tr><tr><td>train_loss</td><td>0.31725</td></tr><tr><td>val_acc</td><td>0.75</td></tr><tr><td>val_loss</td><td>0.60075</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial1</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zgtv869n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zgtv869n</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_065812-zgtv869n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:01:27,170] Trial 1 finished with values: [0.5136911898851395, 0.7842465753424658] and parameters: {'lr': 0.008499571858158883, 'wd': 7.830301668576652e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 2 hyperparams --> lr=1.81e-03, wd=1.30e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_070127-54xnxlyj</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/54xnxlyj' target=\"_blank\">trial2</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/54xnxlyj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/54xnxlyj</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 2]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7422 acc=0.4966 | val_loss=0.6933 acc=0.5034 | prec=0.5031 rec=0.5616 spec=0.4452 f1=0.5307 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7292 acc=0.4820 | val_loss=0.7013 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7128 acc=0.5077 | val_loss=0.6910 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.2s\n",
            "Epoch 004 | train_loss=0.7179 acc=0.4991 | val_loss=0.6935 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 005 | train_loss=0.7141 acc=0.5017 | val_loss=0.6746 acc=0.5411 | prec=0.5214 rec=1.0000 spec=0.0822 f1=0.6854 | time=9.1s\n",
            "Epoch 006 | train_loss=0.6885 acc=0.5497 | val_loss=0.6475 acc=0.6986 | prec=0.6986 rec=0.6986 spec=0.6986 f1=0.6986 | time=9.2s\n",
            "Epoch 007 | train_loss=0.6164 acc=0.6647 | val_loss=0.5403 acc=0.7740 | prec=0.7597 rec=0.8014 spec=0.7466 f1=0.7800 | time=9.0s\n",
            "Epoch 008 | train_loss=0.5766 acc=0.7024 | val_loss=0.6954 acc=0.5582 | prec=0.5323 rec=0.9589 spec=0.1575 f1=0.6846 | time=9.1s\n",
            "Epoch 009 | train_loss=0.5509 acc=0.7196 | val_loss=0.5017 acc=0.7534 | prec=0.6850 rec=0.9384 spec=0.5685 f1=0.7919 | time=9.1s\n",
            "Epoch 010 | train_loss=0.4977 acc=0.7599 | val_loss=0.4737 acc=0.7740 | prec=0.9167 rec=0.6027 spec=0.9452 f1=0.7273 | time=9.1s\n",
            "Epoch 011 | train_loss=0.4408 acc=0.7976 | val_loss=0.3939 acc=0.8459 | prec=0.9550 rec=0.7260 spec=0.9658 f1=0.8249 | time=9.2s\n",
            "Epoch 012 | train_loss=0.3899 acc=0.8285 | val_loss=0.6431 acc=0.7021 | prec=0.9836 rec=0.4110 spec=0.9932 f1=0.5797 | time=9.2s\n",
            "Epoch 013 | train_loss=0.3691 acc=0.8422 | val_loss=0.4415 acc=0.8185 | prec=0.9604 rec=0.6644 spec=0.9726 f1=0.7854 | time=9.2s\n",
            "Epoch 014 | train_loss=0.3069 acc=0.8748 | val_loss=0.4960 acc=0.7568 | prec=0.6904 rec=0.9315 spec=0.5822 f1=0.7930 | time=9.0s\n",
            "Epoch 015 | train_loss=0.3515 acc=0.8585 | val_loss=0.4937 acc=0.7534 | prec=0.6947 rec=0.9041 spec=0.6027 f1=0.7857 | time=9.2s\n",
            "Epoch 016 | train_loss=0.2774 acc=0.8808 | val_loss=0.4723 acc=0.7842 | prec=0.9278 rec=0.6164 spec=0.9521 f1=0.7407 | time=9.1s\n",
            "Epoch 017 | train_loss=0.2670 acc=0.8919 | val_loss=0.4649 acc=0.8048 | prec=0.7834 rec=0.8425 spec=0.7671 f1=0.8119 | time=9.1s\n",
            "Epoch 018 | train_loss=0.2621 acc=0.8945 | val_loss=0.4438 acc=0.8219 | prec=0.9123 rec=0.7123 spec=0.9315 f1=0.8000 | time=9.2s\n",
            "Epoch 019 | train_loss=0.2784 acc=0.8714 | val_loss=0.4631 acc=0.7877 | prec=0.7360 rec=0.8973 spec=0.6781 f1=0.8086 | time=9.1s\n",
            "Epoch 020 | train_loss=0.3027 acc=0.8542 | val_loss=1.6421 acc=0.5308 | prec=1.0000 rec=0.0616 spec=1.0000 f1=0.1161 | time=9.2s\n",
            "Epoch 021 | train_loss=0.2200 acc=0.9031 | val_loss=0.5940 acc=0.7808 | prec=0.7733 rec=0.7945 spec=0.7671 f1=0.7838 | time=9.1s\n",
            "Epoch 022 | train_loss=0.2104 acc=0.8971 | val_loss=0.9083 acc=0.7603 | prec=0.9318 rec=0.5616 spec=0.9589 f1=0.7009 | time=9.1s\n",
            "Epoch 023 | train_loss=0.2140 acc=0.9074 | val_loss=0.6569 acc=0.7123 | prec=0.6505 rec=0.9178 spec=0.5068 f1=0.7614 | time=9.2s\n",
            "Epoch 024 | train_loss=0.2492 acc=0.8894 | val_loss=1.2711 acc=0.6507 | prec=0.9783 rec=0.3082 spec=0.9932 f1=0.4688 | time=9.1s\n",
            "Epoch 025 | train_loss=0.1945 acc=0.9057 | val_loss=0.7182 acc=0.7466 | prec=0.9186 rec=0.5411 spec=0.9521 f1=0.6810 | time=9.1s\n",
            "Epoch 026 | train_loss=0.1816 acc=0.9117 | val_loss=0.4874 acc=0.7637 | prec=0.7421 rec=0.8082 spec=0.7192 f1=0.7738 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▆▁▇▁▇▇█▇█▇█▆███▇███▂█▇▇▅▇█</td></tr><tr><td>precision</td><td>▅▁▅▁▅▆▆▅▆▇███▆▆▇▆▇▆█▆█▆█▇▆</td></tr><tr><td>recall</td><td>▅▁█▁█▆▇██▅▆▄▆█▇▅▇▆▇▁▇▅▇▃▅▇</td></tr><tr><td>specificity</td><td>▄█▁█▂▆▆▂▅████▅▅█▆█▆█▆█▅██▆</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▄▅▅▆▆▇▇▇▇▇██▇▇██████</td></tr><tr><td>train_loss</td><td>█████▇▆▆▆▅▄▄▃▃▃▂▂▂▂▃▁▁▁▂▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▂▅▇▂▆▇█▅▇▆▆▇▇█▇▂▇▆▅▄▆▆</td></tr><tr><td>val_loss</td><td>▃▃▃▃▃▂▂▃▂▁▁▂▁▂▂▁▁▁▁█▂▄▂▆▃▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>26</td></tr><tr><td>f1_score</td><td>0.77377</td></tr><tr><td>precision</td><td>0.74214</td></tr><tr><td>recall</td><td>0.80822</td></tr><tr><td>specificity</td><td>0.71918</td></tr><tr><td>train_acc</td><td>0.91166</td></tr><tr><td>train_loss</td><td>0.18159</td></tr><tr><td>val_acc</td><td>0.7637</td></tr><tr><td>val_loss</td><td>0.48739</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial2</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/54xnxlyj' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/54xnxlyj</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_070127-54xnxlyj/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:05:25,820] Trial 2 finished with values: [0.3938667714595795, 0.8458904109589042] and parameters: {'lr': 0.0018055637962379359, 'wd': 1.304275479080804e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 3 hyperparams --> lr=1.65e-03, wd=1.23e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_070525-7qf616ki</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/7qf616ki' target=\"_blank\">trial3</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/7qf616ki' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/7qf616ki</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 3]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7136 acc=0.4966 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 002 | train_loss=0.6982 acc=0.5223 | val_loss=0.6939 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7178 acc=0.5172 | val_loss=0.6942 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.3s\n",
            "Epoch 004 | train_loss=0.7053 acc=0.4974 | val_loss=0.6890 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7017 acc=0.5232 | val_loss=0.6753 acc=0.6164 | prec=0.5726 rec=0.9178 spec=0.3151 f1=0.7053 | time=9.2s\n",
            "Epoch 006 | train_loss=0.6654 acc=0.5806 | val_loss=0.6579 acc=0.5993 | prec=0.5612 rec=0.9110 spec=0.2877 f1=0.6945 | time=9.1s\n",
            "Epoch 007 | train_loss=0.6176 acc=0.6595 | val_loss=0.5781 acc=0.7397 | prec=0.7035 rec=0.8288 spec=0.6507 f1=0.7610 | time=9.1s\n",
            "Epoch 008 | train_loss=0.6068 acc=0.6913 | val_loss=0.5347 acc=0.7705 | prec=0.8211 rec=0.6918 spec=0.8493 f1=0.7509 | time=9.2s\n",
            "Epoch 009 | train_loss=0.5697 acc=0.7247 | val_loss=0.5622 acc=0.7089 | prec=0.6419 rec=0.9452 spec=0.4726 f1=0.7645 | time=9.1s\n",
            "Epoch 010 | train_loss=0.5271 acc=0.7779 | val_loss=0.5530 acc=0.7397 | prec=0.7273 rec=0.7671 spec=0.7123 f1=0.7467 | time=9.1s\n",
            "Epoch 011 | train_loss=0.4942 acc=0.7762 | val_loss=0.6087 acc=0.6062 | prec=0.8444 rec=0.2603 spec=0.9521 f1=0.3979 | time=9.1s\n",
            "Epoch 012 | train_loss=0.4742 acc=0.7950 | val_loss=0.5071 acc=0.7432 | prec=0.6898 rec=0.8836 spec=0.6027 f1=0.7748 | time=9.2s\n",
            "Epoch 013 | train_loss=0.4345 acc=0.7847 | val_loss=0.5216 acc=0.7466 | prec=0.7857 rec=0.6781 spec=0.8151 f1=0.7279 | time=9.2s\n",
            "Epoch 014 | train_loss=0.4154 acc=0.8079 | val_loss=0.5298 acc=0.7192 | prec=0.6702 rec=0.8630 spec=0.5753 f1=0.7545 | time=9.0s\n",
            "Epoch 015 | train_loss=0.4355 acc=0.7985 | val_loss=0.7402 acc=0.5479 | prec=0.5257 rec=0.9795 spec=0.1164 f1=0.6842 | time=9.2s\n",
            "Epoch 016 | train_loss=0.3885 acc=0.8370 | val_loss=0.5580 acc=0.6918 | prec=0.6321 rec=0.9178 spec=0.4658 f1=0.7486 | time=9.1s\n",
            "Epoch 017 | train_loss=0.3436 acc=0.8585 | val_loss=0.5223 acc=0.7329 | prec=0.6889 rec=0.8493 spec=0.6164 f1=0.7607 | time=9.1s\n",
            "Epoch 018 | train_loss=0.2835 acc=0.8868 | val_loss=0.4608 acc=0.7979 | prec=0.8175 rec=0.7671 spec=0.8288 f1=0.7915 | time=9.1s\n",
            "Epoch 019 | train_loss=0.3329 acc=0.8396 | val_loss=0.6778 acc=0.6952 | prec=0.9672 rec=0.4041 spec=0.9863 f1=0.5700 | time=9.2s\n",
            "Epoch 020 | train_loss=0.2944 acc=0.8679 | val_loss=0.6270 acc=0.8048 | prec=0.8201 rec=0.7808 spec=0.8288 f1=0.8000 | time=9.2s\n",
            "Epoch 021 | train_loss=0.2668 acc=0.8799 | val_loss=0.6227 acc=0.8116 | prec=0.8640 rec=0.7397 spec=0.8836 f1=0.7970 | time=9.1s\n",
            "Epoch 022 | train_loss=0.2415 acc=0.8877 | val_loss=0.6005 acc=0.7808 | prec=0.8942 rec=0.6370 spec=0.9247 f1=0.7440 | time=9.2s\n",
            "Epoch 023 | train_loss=0.2509 acc=0.8894 | val_loss=0.5679 acc=0.8082 | prec=0.8814 rec=0.7123 spec=0.9041 f1=0.7879 | time=9.0s\n",
            "Epoch 024 | train_loss=0.2104 acc=0.9022 | val_loss=1.0279 acc=0.6370 | prec=0.5847 rec=0.9452 spec=0.3288 f1=0.7225 | time=9.0s\n",
            "Epoch 025 | train_loss=0.2194 acc=0.8962 | val_loss=0.6297 acc=0.7500 | prec=0.7212 rec=0.8151 spec=0.6849 f1=0.7653 | time=9.2s\n",
            "Epoch 026 | train_loss=0.2078 acc=0.9065 | val_loss=0.6974 acc=0.7192 | prec=0.9103 rec=0.4863 spec=0.9521 f1=0.6339 | time=9.2s\n",
            "Epoch 027 | train_loss=0.2093 acc=0.9014 | val_loss=0.8753 acc=0.6952 | prec=0.9385 rec=0.4178 spec=0.9726 f1=0.5782 | time=9.1s\n",
            "Epoch 028 | train_loss=0.2257 acc=0.8868 | val_loss=0.6205 acc=0.7363 | prec=0.6700 rec=0.9315 spec=0.5411 f1=0.7794 | time=9.1s\n",
            "Epoch 029 | train_loss=0.1667 acc=0.9151 | val_loss=0.7113 acc=0.7500 | prec=0.7160 rec=0.8288 spec=0.6712 f1=0.7683 | time=9.2s\n",
            "Epoch 030 | train_loss=0.1732 acc=0.9254 | val_loss=0.7802 acc=0.7877 | prec=0.8000 rec=0.7671 spec=0.8082 f1=0.7832 | time=9.0s\n",
            "Epoch 031 | train_loss=0.1416 acc=0.9434 | val_loss=1.0133 acc=0.7705 | prec=0.8376 rec=0.6712 spec=0.8699 f1=0.7452 | time=8.9s\n",
            "Epoch 032 | train_loss=0.1446 acc=0.9202 | val_loss=0.8585 acc=0.7705 | prec=0.8264 rec=0.6849 spec=0.8562 f1=0.7491 | time=9.2s\n",
            "Epoch 033 | train_loss=0.1335 acc=0.9383 | val_loss=1.1836 acc=0.7363 | prec=0.8876 rec=0.5411 spec=0.9315 f1=0.6723 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▇▁▇▇▇▇████▄█▇█▇███▆████▇█▇▆█████▇</td></tr><tr><td>precision</td><td>▅▁▅▅▅▅▆▇▆▆▇▆▇▆▅▆▆▇█▇▇▇▇▅▆██▆▆▇▇▇▇</td></tr><tr><td>recall</td><td>█▁██▇▇▇▆█▆▃▇▆▇█▇▇▆▄▆▆▅▆█▇▄▄█▇▆▆▆▅</td></tr><tr><td>specificity</td><td>▁█▁▁▃▃▆▇▄▆█▅▇▅▂▄▅▇█▇▇▇▇▃▆██▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▄▄▅▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_loss</td><td>█████▇▇▇▆▆▅▅▅▄▅▄▄▃▃▃▃▂▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▄▃▆▇▆▆▃▆▇▆▂▅▆█▅██▇█▄▇▆▅▆▇▇▇▇▆</td></tr><tr><td>val_loss</td><td>▃▃▃▃▃▃▂▂▂▂▂▁▂▂▄▂▂▁▃▃▃▂▂▆▃▃▅▃▃▄▆▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>33</td></tr><tr><td>f1_score</td><td>0.67234</td></tr><tr><td>precision</td><td>0.88764</td></tr><tr><td>recall</td><td>0.5411</td></tr><tr><td>specificity</td><td>0.93151</td></tr><tr><td>train_acc</td><td>0.93825</td></tr><tr><td>train_loss</td><td>0.13354</td></tr><tr><td>val_acc</td><td>0.7363</td></tr><tr><td>val_loss</td><td>1.18357</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial3</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/7qf616ki' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/7qf616ki</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_070525-7qf616ki/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:10:28,629] Trial 3 finished with values: [0.4607516020536423, 0.797945205479452] and parameters: {'lr': 0.0016475775676494141, 'wd': 0.0001227316531167146}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 4 hyperparams --> lr=4.72e-03, wd=1.61e-06\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_071028-zqbdrg6n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zqbdrg6n' target=\"_blank\">trial4</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zqbdrg6n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zqbdrg6n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 4]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7532 acc=0.5026 | val_loss=0.6952 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7192 acc=0.5017 | val_loss=0.7003 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7159 acc=0.5103 | val_loss=0.6930 acc=0.5342 | prec=0.5397 rec=0.4658 spec=0.6027 f1=0.5000 | time=9.1s\n",
            "Epoch 004 | train_loss=0.6872 acc=0.5652 | val_loss=0.6314 acc=0.6267 | prec=0.5894 rec=0.8356 spec=0.4178 f1=0.6912 | time=9.0s\n",
            "Epoch 005 | train_loss=0.6400 acc=0.6218 | val_loss=0.5456 acc=0.7363 | prec=0.7760 rec=0.6644 spec=0.8082 f1=0.7159 | time=9.2s\n",
            "Epoch 006 | train_loss=0.5931 acc=0.6835 | val_loss=0.5181 acc=0.7979 | prec=0.7843 rec=0.8219 spec=0.7740 f1=0.8027 | time=9.0s\n",
            "Epoch 007 | train_loss=0.5270 acc=0.7470 | val_loss=0.5130 acc=0.7979 | prec=0.8425 rec=0.7329 spec=0.8630 f1=0.7839 | time=9.0s\n",
            "Epoch 008 | train_loss=0.4821 acc=0.7719 | val_loss=0.5356 acc=0.7432 | prec=0.8660 rec=0.5753 spec=0.9110 f1=0.6914 | time=9.0s\n",
            "Epoch 009 | train_loss=0.4716 acc=0.7650 | val_loss=0.5044 acc=0.7671 | prec=0.7216 rec=0.8699 spec=0.6644 f1=0.7888 | time=9.0s\n",
            "Epoch 010 | train_loss=0.4075 acc=0.8199 | val_loss=0.9316 acc=0.5377 | prec=0.5196 rec=1.0000 spec=0.0753 f1=0.6838 | time=9.1s\n",
            "Epoch 011 | train_loss=0.4897 acc=0.7461 | val_loss=0.6750 acc=0.5685 | prec=1.0000 rec=0.1370 spec=1.0000 f1=0.2410 | time=9.1s\n",
            "Epoch 012 | train_loss=0.3730 acc=0.8208 | val_loss=1.7413 acc=0.5171 | prec=1.0000 rec=0.0342 spec=1.0000 f1=0.0662 | time=9.0s\n",
            "Epoch 013 | train_loss=0.4310 acc=0.7967 | val_loss=1.4267 acc=0.5445 | prec=0.5240 rec=0.9726 spec=0.1164 f1=0.6811 | time=9.1s\n",
            "Epoch 014 | train_loss=0.3607 acc=0.8268 | val_loss=0.8935 acc=0.6096 | prec=0.5645 rec=0.9589 spec=0.2603 f1=0.7107 | time=9.0s\n",
            "Epoch 015 | train_loss=0.3606 acc=0.8319 | val_loss=0.6053 acc=0.6815 | prec=0.6210 rec=0.9315 spec=0.4315 f1=0.7452 | time=9.2s\n",
            "Epoch 016 | train_loss=0.3886 acc=0.8216 | val_loss=0.5931 acc=0.7603 | prec=0.7111 rec=0.8767 spec=0.6438 f1=0.7853 | time=9.1s\n",
            "Epoch 017 | train_loss=0.3495 acc=0.8250 | val_loss=0.5811 acc=0.7534 | prec=0.7079 rec=0.8630 spec=0.6438 f1=0.7778 | time=9.1s\n",
            "Epoch 018 | train_loss=0.3766 acc=0.8405 | val_loss=1.4245 acc=0.7089 | prec=0.6517 rec=0.8973 spec=0.5205 f1=0.7550 | time=9.1s\n",
            "Epoch 019 | train_loss=0.3796 acc=0.8268 | val_loss=1.9456 acc=0.5205 | prec=0.5106 rec=0.9932 spec=0.0479 f1=0.6744 | time=9.2s\n",
            "Epoch 020 | train_loss=0.3498 acc=0.8482 | val_loss=0.5231 acc=0.7568 | prec=0.9032 rec=0.5753 spec=0.9384 f1=0.7029 | time=9.3s\n",
            "Epoch 021 | train_loss=0.2634 acc=0.8816 | val_loss=0.6088 acc=0.7295 | prec=0.6982 rec=0.8082 spec=0.6507 f1=0.7492 | time=9.2s\n",
            "Epoch 022 | train_loss=0.3171 acc=0.8559 | val_loss=0.4775 acc=0.7466 | prec=0.7143 rec=0.8219 spec=0.6712 f1=0.7643 | time=9.2s\n",
            "Epoch 023 | train_loss=0.2814 acc=0.8756 | val_loss=0.8743 acc=0.6473 | prec=0.9388 rec=0.3151 spec=0.9795 f1=0.4718 | time=9.2s\n",
            "Epoch 024 | train_loss=0.2827 acc=0.8568 | val_loss=0.7354 acc=0.6986 | prec=0.8452 rec=0.4863 spec=0.9110 f1=0.6174 | time=9.2s\n",
            "Epoch 025 | train_loss=0.2475 acc=0.8868 | val_loss=0.6727 acc=0.7226 | prec=0.8652 rec=0.5274 spec=0.9178 f1=0.6553 | time=9.2s\n",
            "Epoch 026 | train_loss=0.2462 acc=0.8894 | val_loss=0.5741 acc=0.7911 | prec=0.8972 rec=0.6575 spec=0.9247 f1=0.7589 | time=9.1s\n",
            "Epoch 027 | train_loss=0.2435 acc=0.8885 | val_loss=1.3385 acc=0.6678 | prec=0.6079 rec=0.9452 spec=0.3904 f1=0.7399 | time=9.3s\n",
            "Epoch 028 | train_loss=0.2060 acc=0.8997 | val_loss=1.1823 acc=0.6438 | prec=0.5938 rec=0.9110 spec=0.3767 f1=0.7189 | time=9.2s\n",
            "Epoch 029 | train_loss=0.2421 acc=0.8816 | val_loss=0.7851 acc=0.6575 | prec=0.9107 rec=0.3493 spec=0.9658 f1=0.5050 | time=9.2s\n",
            "Epoch 030 | train_loss=0.2295 acc=0.8945 | val_loss=0.8965 acc=0.7329 | prec=0.6809 rec=0.8767 spec=0.5890 f1=0.7665 | time=9.2s\n",
            "Epoch 031 | train_loss=0.2302 acc=0.8919 | val_loss=1.5538 acc=0.6130 | prec=0.8667 rec=0.2671 spec=0.9589 f1=0.4084 | time=9.2s\n",
            "Epoch 032 | train_loss=0.1635 acc=0.9194 | val_loss=3.0272 acc=0.5822 | prec=0.9286 rec=0.1781 spec=0.9863 f1=0.2989 | time=9.2s\n",
            "Epoch 033 | train_loss=0.2297 acc=0.8954 | val_loss=2.3515 acc=0.5753 | prec=0.5420 rec=0.9726 spec=0.1781 f1=0.6961 | time=9.2s\n",
            "Epoch 034 | train_loss=0.2070 acc=0.8945 | val_loss=0.6980 acc=0.7397 | prec=0.7244 rec=0.7740 spec=0.7055 f1=0.7483 | time=9.2s\n",
            "Epoch 035 | train_loss=0.1769 acc=0.9185 | val_loss=1.2841 acc=0.7192 | prec=0.8810 rec=0.5068 spec=0.9315 f1=0.6435 | time=9.2s\n",
            "Epoch 036 | train_loss=0.1820 acc=0.9091 | val_loss=1.1198 acc=0.7363 | prec=0.8485 rec=0.5753 spec=0.8973 f1=0.6857 | time=9.2s\n",
            "Epoch 037 | train_loss=0.1446 acc=0.9211 | val_loss=1.0766 acc=0.7466 | prec=0.7571 rec=0.7260 spec=0.7671 f1=0.7413 | time=9.2s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▇▁▅▇▇██▇█▇▃▂▇▇▇███▇▇██▅▆▇█▇▇▅█▅▄▇█▇▇▇</td></tr><tr><td>precision</td><td>▅▁▅▅▆▆▇▇▆▅██▅▅▅▆▆▆▅▇▆▆█▇▇▇▅▅▇▆▇▇▅▆▇▇▆</td></tr><tr><td>recall</td><td>█▁▄▇▆▇▆▅▇█▂▁███▇▇▇█▅▇▇▃▄▅▆█▇▃▇▃▂█▆▅▅▆</td></tr><tr><td>specificity</td><td>▁█▅▄▇▆▇▇▆▂██▂▃▄▆▆▅▁█▆▆█▇▇▇▄▄█▅██▂▆█▇▆</td></tr><tr><td>train_acc</td><td>▁▁▁▂▃▄▅▆▅▆▅▆▆▆▇▆▆▇▆▇▇▇▇▇▇▇▇█▇████████</td></tr><tr><td>train_loss</td><td>███▇▇▆▅▅▅▄▅▄▄▃▃▄▃▄▄▃▂▃▃▃▂▂▂▂▂▂▂▁▂▂▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▂▄▇██▇▇▂▃▁▂▄▅▇▇▆▁▇▆▇▄▆▆█▅▄▅▆▄▃▃▇▆▇▇</td></tr><tr><td>val_loss</td><td>▂▂▂▁▁▁▁▁▁▂▂▄▄▂▁▁▁▄▅▁▁▁▂▂▂▁▃▃▂▂▄█▆▂▃▃▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>37</td></tr><tr><td>f1_score</td><td>0.74126</td></tr><tr><td>precision</td><td>0.75714</td></tr><tr><td>recall</td><td>0.72603</td></tr><tr><td>specificity</td><td>0.76712</td></tr><tr><td>train_acc</td><td>0.9211</td></tr><tr><td>train_loss</td><td>0.1446</td></tr><tr><td>val_acc</td><td>0.74658</td></tr><tr><td>val_loss</td><td>1.07662</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial4</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zqbdrg6n' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/zqbdrg6n</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_071028-zqbdrg6n/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:16:08,481] Trial 4 finished with values: [0.47754399478435516, 0.7465753424657534] and parameters: {'lr': 0.004723503204657298, 'wd': 1.614143826961975e-06}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 5 hyperparams --> lr=1.03e-04, wd=3.58e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_071608-szg8bank</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/szg8bank' target=\"_blank\">trial5</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/szg8bank' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/szg8bank</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 5]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7535 acc=0.5197 | val_loss=0.6957 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 002 | train_loss=0.7462 acc=0.5017 | val_loss=0.6947 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 003 | train_loss=0.7322 acc=0.4974 | val_loss=0.6931 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 004 | train_loss=0.7345 acc=0.4846 | val_loss=0.6931 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7293 acc=0.4923 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7171 acc=0.4991 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 007 | train_loss=0.7291 acc=0.4880 | val_loss=0.6923 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7219 acc=0.4691 | val_loss=0.6925 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7164 acc=0.5017 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 010 | train_loss=0.7170 acc=0.4906 | val_loss=0.6927 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7169 acc=0.4889 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7085 acc=0.5000 | val_loss=0.6919 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7166 acc=0.5017 | val_loss=0.6933 acc=0.5034 | prec=1.0000 rec=0.0068 spec=1.0000 f1=0.0136 | time=9.0s\n",
            "Epoch 014 | train_loss=0.7072 acc=0.5000 | val_loss=0.6917 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 015 | train_loss=0.7117 acc=0.5094 | val_loss=0.6919 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 016 | train_loss=0.7078 acc=0.5000 | val_loss=0.6927 acc=0.4966 | prec=0.4982 rec=0.9658 spec=0.0274 f1=0.6573 | time=9.1s\n",
            "Epoch 017 | train_loss=0.7080 acc=0.5120 | val_loss=0.6917 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=8.9s\n",
            "Epoch 018 | train_loss=0.7138 acc=0.5000 | val_loss=0.6920 acc=0.5000 | prec=0.5000 rec=0.0411 spec=0.9589 f1=0.0759 | time=9.1s\n",
            "Epoch 019 | train_loss=0.7047 acc=0.5137 | val_loss=0.6851 acc=0.6473 | prec=0.8413 rec=0.3630 spec=0.9315 f1=0.5072 | time=9.1s\n",
            "Epoch 020 | train_loss=0.6836 acc=0.5780 | val_loss=0.6454 acc=0.6781 | prec=0.6398 rec=0.8151 spec=0.5411 f1=0.7169 | time=9.1s\n",
            "Epoch 021 | train_loss=0.6723 acc=0.5875 | val_loss=0.6363 acc=0.7055 | prec=0.8191 rec=0.5274 spec=0.8836 f1=0.6417 | time=8.9s\n",
            "Epoch 022 | train_loss=0.6418 acc=0.6055 | val_loss=0.5907 acc=0.7432 | prec=0.6961 rec=0.8630 spec=0.6233 f1=0.7706 | time=9.1s\n",
            "Epoch 023 | train_loss=0.6085 acc=0.6612 | val_loss=0.6225 acc=0.6815 | prec=0.9344 rec=0.3904 spec=0.9726 f1=0.5507 | time=9.1s\n",
            "Epoch 024 | train_loss=0.5894 acc=0.6835 | val_loss=0.5361 acc=0.8151 | prec=0.8485 rec=0.7671 spec=0.8630 f1=0.8058 | time=9.0s\n",
            "Epoch 025 | train_loss=0.5460 acc=0.7101 | val_loss=0.5203 acc=0.7329 | prec=0.6700 rec=0.9178 spec=0.5479 f1=0.7746 | time=9.1s\n",
            "Epoch 026 | train_loss=0.5110 acc=0.7573 | val_loss=0.4773 acc=0.8082 | prec=0.9167 rec=0.6781 spec=0.9384 f1=0.7795 | time=9.0s\n",
            "Epoch 027 | train_loss=0.4611 acc=0.7907 | val_loss=0.4327 acc=0.8219 | prec=0.7901 rec=0.8767 spec=0.7671 f1=0.8312 | time=9.0s\n",
            "Epoch 028 | train_loss=0.4184 acc=0.8053 | val_loss=0.4132 acc=0.8390 | prec=0.8960 rec=0.7671 spec=0.9110 f1=0.8266 | time=9.0s\n",
            "Epoch 029 | train_loss=0.4054 acc=0.8165 | val_loss=0.4921 acc=0.7603 | prec=0.9750 rec=0.5342 spec=0.9863 f1=0.6903 | time=9.1s\n",
            "Epoch 030 | train_loss=0.3999 acc=0.8242 | val_loss=0.4615 acc=0.7808 | prec=0.9556 rec=0.5890 spec=0.9726 f1=0.7288 | time=9.1s\n",
            "Epoch 031 | train_loss=0.3878 acc=0.8499 | val_loss=0.3886 acc=0.8493 | prec=0.8864 rec=0.8014 spec=0.8973 f1=0.8417 | time=9.0s\n",
            "Epoch 032 | train_loss=0.3523 acc=0.8568 | val_loss=0.3925 acc=0.8356 | prec=0.8828 rec=0.7740 spec=0.8973 f1=0.8248 | time=9.1s\n",
            "Epoch 033 | train_loss=0.3288 acc=0.8696 | val_loss=0.3950 acc=0.8356 | prec=0.9083 rec=0.7466 spec=0.9247 f1=0.8195 | time=9.0s\n",
            "Epoch 034 | train_loss=0.3275 acc=0.8688 | val_loss=0.4035 acc=0.8185 | prec=0.9266 rec=0.6918 spec=0.9452 f1=0.7922 | time=9.2s\n",
            "Epoch 035 | train_loss=0.3173 acc=0.8722 | val_loss=0.3897 acc=0.8459 | prec=0.9316 rec=0.7466 spec=0.9452 f1=0.8289 | time=9.0s\n",
            "Epoch 036 | train_loss=0.2944 acc=0.8919 | val_loss=0.4949 acc=0.7603 | prec=0.6939 rec=0.9315 spec=0.5890 f1=0.7953 | time=9.1s\n",
            "Epoch 037 | train_loss=0.2712 acc=0.9022 | val_loss=0.3949 acc=0.8390 | prec=0.8414 rec=0.8356 spec=0.8425 f1=0.8385 | time=9.2s\n",
            "Epoch 038 | train_loss=0.2691 acc=0.9014 | val_loss=0.4013 acc=0.8356 | prec=0.8182 rec=0.8630 spec=0.8082 f1=0.8400 | time=9.1s\n",
            "Epoch 039 | train_loss=0.2520 acc=0.9108 | val_loss=0.4249 acc=0.8151 | prec=0.9182 rec=0.6918 spec=0.9384 f1=0.7891 | time=9.2s\n",
            "Epoch 040 | train_loss=0.2465 acc=0.9014 | val_loss=0.4711 acc=0.7842 | prec=0.7173 rec=0.9384 spec=0.6301 f1=0.8131 | time=9.1s\n",
            "Epoch 041 | train_loss=0.2376 acc=0.9117 | val_loss=0.4985 acc=0.8048 | prec=0.9785 rec=0.6233 spec=0.9863 f1=0.7615 | time=9.0s\n",
            "Epoch 042 | train_loss=0.2342 acc=0.9117 | val_loss=0.4888 acc=0.7877 | prec=0.7333 rec=0.9041 spec=0.6712 f1=0.8098 | time=9.0s\n",
            "Epoch 043 | train_loss=0.2092 acc=0.9254 | val_loss=0.4083 acc=0.8596 | prec=0.8723 rec=0.8425 spec=0.8767 f1=0.8571 | time=9.0s\n",
            "Epoch 044 | train_loss=0.1968 acc=0.9288 | val_loss=0.5195 acc=0.7808 | prec=0.7158 rec=0.9315 spec=0.6301 f1=0.8095 | time=9.0s\n",
            "Epoch 045 | train_loss=0.2150 acc=0.9151 | val_loss=0.5111 acc=0.7842 | prec=0.9029 rec=0.6370 spec=0.9315 f1=0.7470 | time=9.0s\n",
            "Epoch 046 | train_loss=0.2333 acc=0.9065 | val_loss=0.5115 acc=0.7979 | prec=0.7544 rec=0.8836 spec=0.7123 f1=0.8139 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>f1_score</td><td>▆▆▆▆▆▆▆▆▆▆▆▁▆▆▆▂▅▇▆▇█▇▇██▇▇██▇█▇█▇█▇███▇</td></tr><tr><td>precision</td><td>▁▁▁▁▁▁▁▁▁▁▁█▁▁▁▁▆▃▅▄▆▃▇▅▇█▇▆▇▇▇▄▆▇▄█▄▆▄▇</td></tr><tr><td>recall</td><td>███████████▁███▁▄▇▅▇▆▇▆▇▆▅▅▆▆▆▆█▇▆█▅▇▇█▅</td></tr><tr><td>specificity</td><td>▁▁▁▁▁▁▁▁▁▁▁█▁▁▁██▅▇▅▇▅█▆▇██▇▇██▅▇█▅█▆▇▅█</td></tr><tr><td>train_acc</td><td>▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▃▃▄▅▅▆▆▆▆▇▇▇▇▇████████</td></tr><tr><td>train_loss</td><td>██████████▇█▇▇▇█▇▇▇▇▆▅▅▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▄▄▅▆▇▆▇▇█▆▆██▇█▆█▇▇▇▇█▆▇</td></tr><tr><td>val_loss</td><td>█████████████████▇▇▆▄▄▃▂▂▃▃▁▁▁▁▃▁▂▃▃▃▁▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>46</td></tr><tr><td>f1_score</td><td>0.81388</td></tr><tr><td>precision</td><td>0.75439</td></tr><tr><td>recall</td><td>0.88356</td></tr><tr><td>specificity</td><td>0.71233</td></tr><tr><td>train_acc</td><td>0.90652</td></tr><tr><td>train_loss</td><td>0.23333</td></tr><tr><td>val_acc</td><td>0.79795</td></tr><tr><td>val_loss</td><td>0.51146</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial5</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/szg8bank' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/szg8bank</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_071608-szg8bank/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:23:06,571] Trial 5 finished with values: [0.38862849175930025, 0.8493150684931506] and parameters: {'lr': 0.00010256517562285802, 'wd': 0.0003575964213900629}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 6 hyperparams --> lr=3.87e-04, wd=3.59e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_072306-xlao46ii</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/xlao46ii' target=\"_blank\">trial6</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/xlao46ii' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/xlao46ii</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 6]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7667 acc=0.4906 | val_loss=0.7004 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7390 acc=0.5111 | val_loss=0.7000 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7290 acc=0.5189 | val_loss=0.7095 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=8.9s\n",
            "Epoch 004 | train_loss=0.7380 acc=0.5077 | val_loss=0.6942 acc=0.4795 | prec=0.4571 rec=0.2192 spec=0.7397 f1=0.2963 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7194 acc=0.5180 | val_loss=0.6924 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7389 acc=0.4931 | val_loss=0.7092 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 007 | train_loss=0.7340 acc=0.5060 | val_loss=0.7065 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 008 | train_loss=0.7224 acc=0.5026 | val_loss=0.7151 acc=0.4932 | prec=0.4375 rec=0.0479 spec=0.9384 f1=0.0864 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7283 acc=0.5000 | val_loss=0.7325 acc=0.4932 | prec=0.4000 rec=0.0274 spec=0.9589 f1=0.0513 | time=9.0s\n",
            "Epoch 010 | train_loss=0.7089 acc=0.5343 | val_loss=0.6978 acc=0.5274 | prec=0.5290 rec=0.5000 spec=0.5548 f1=0.5141 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7103 acc=0.5292 | val_loss=0.6883 acc=0.5514 | prec=0.5414 rec=0.6712 spec=0.4315 f1=0.5994 | time=9.0s\n",
            "Epoch 012 | train_loss=0.7029 acc=0.5420 | val_loss=0.6920 acc=0.5719 | prec=0.5766 rec=0.5411 spec=0.6027 f1=0.5583 | time=9.1s\n",
            "Epoch 013 | train_loss=0.6687 acc=0.5815 | val_loss=0.6434 acc=0.6027 | prec=0.5620 rec=0.9315 spec=0.2740 f1=0.7010 | time=9.0s\n",
            "Epoch 014 | train_loss=0.6367 acc=0.6415 | val_loss=0.5863 acc=0.7397 | prec=0.7692 rec=0.6849 spec=0.7945 f1=0.7246 | time=9.1s\n",
            "Epoch 015 | train_loss=0.6011 acc=0.6818 | val_loss=0.5869 acc=0.7226 | prec=0.6650 rec=0.8973 spec=0.5479 f1=0.7638 | time=9.0s\n",
            "Epoch 016 | train_loss=0.5527 acc=0.7444 | val_loss=0.5762 acc=0.7192 | prec=0.6600 rec=0.9041 spec=0.5342 f1=0.7630 | time=9.0s\n",
            "Epoch 017 | train_loss=0.5240 acc=0.7470 | val_loss=0.5192 acc=0.7740 | prec=0.8030 rec=0.7260 spec=0.8219 f1=0.7626 | time=9.2s\n",
            "Epoch 018 | train_loss=0.4811 acc=0.7753 | val_loss=0.4992 acc=0.7568 | prec=0.7119 rec=0.8630 spec=0.6507 f1=0.7802 | time=9.0s\n",
            "Epoch 019 | train_loss=0.4415 acc=0.7967 | val_loss=0.5203 acc=0.7705 | prec=0.8015 rec=0.7192 spec=0.8219 f1=0.7581 | time=9.1s\n",
            "Epoch 020 | train_loss=0.4093 acc=0.8096 | val_loss=0.5211 acc=0.6815 | prec=0.6178 rec=0.9521 spec=0.4110 f1=0.7493 | time=9.2s\n",
            "Epoch 021 | train_loss=0.4085 acc=0.8148 | val_loss=0.4926 acc=0.7123 | prec=0.6490 rec=0.9247 spec=0.5000 f1=0.7627 | time=9.1s\n",
            "Epoch 022 | train_loss=0.3509 acc=0.8499 | val_loss=0.4439 acc=0.8185 | prec=0.8039 rec=0.8425 spec=0.7945 f1=0.8227 | time=9.1s\n",
            "Epoch 023 | train_loss=0.3203 acc=0.8696 | val_loss=0.4416 acc=0.8288 | prec=0.8117 rec=0.8562 spec=0.8014 f1=0.8333 | time=9.0s\n",
            "Epoch 024 | train_loss=0.3076 acc=0.8654 | val_loss=0.4287 acc=0.8322 | prec=0.8345 rec=0.8288 spec=0.8356 f1=0.8316 | time=9.1s\n",
            "Epoch 025 | train_loss=0.3199 acc=0.8568 | val_loss=0.4353 acc=0.8116 | prec=0.7974 rec=0.8356 spec=0.7877 f1=0.8161 | time=9.1s\n",
            "Epoch 026 | train_loss=0.3148 acc=0.8654 | val_loss=0.4080 acc=0.8048 | prec=0.7602 rec=0.8904 spec=0.7192 f1=0.8202 | time=9.2s\n",
            "Epoch 027 | train_loss=0.2727 acc=0.8739 | val_loss=0.4755 acc=0.8048 | prec=0.7799 rec=0.8493 spec=0.7603 f1=0.8131 | time=9.0s\n",
            "Epoch 028 | train_loss=0.2367 acc=0.8962 | val_loss=0.4897 acc=0.7842 | prec=0.7371 rec=0.8836 spec=0.6849 f1=0.8037 | time=9.2s\n",
            "Epoch 029 | train_loss=0.2320 acc=0.9065 | val_loss=0.4906 acc=0.7911 | prec=0.7515 rec=0.8699 spec=0.7123 f1=0.8063 | time=9.2s\n",
            "Epoch 030 | train_loss=0.2267 acc=0.9125 | val_loss=0.5937 acc=0.7637 | prec=0.7362 rec=0.8219 spec=0.7055 f1=0.7767 | time=9.0s\n",
            "Epoch 031 | train_loss=0.1863 acc=0.9220 | val_loss=0.5156 acc=0.8253 | prec=0.8571 rec=0.7808 spec=0.8699 f1=0.8172 | time=9.1s\n",
            "Epoch 032 | train_loss=0.1560 acc=0.9417 | val_loss=0.5936 acc=0.7295 | prec=0.6683 rec=0.9110 spec=0.5479 f1=0.7710 | time=9.0s\n",
            "Epoch 033 | train_loss=0.1725 acc=0.9228 | val_loss=0.6897 acc=0.7123 | prec=0.6566 rec=0.8904 spec=0.5342 f1=0.7558 | time=9.2s\n",
            "Epoch 034 | train_loss=0.1743 acc=0.9202 | val_loss=0.7554 acc=0.6815 | prec=0.6157 rec=0.9658 spec=0.3973 f1=0.7520 | time=9.0s\n",
            "Epoch 035 | train_loss=0.1643 acc=0.9271 | val_loss=0.5834 acc=0.8219 | prec=0.8983 rec=0.7260 spec=0.9178 f1=0.8030 | time=9.1s\n",
            "Epoch 036 | train_loss=0.1849 acc=0.9134 | val_loss=0.6055 acc=0.7911 | prec=0.7640 rec=0.8425 spec=0.7397 f1=0.8013 | time=9.0s\n",
            "Epoch 037 | train_loss=0.1463 acc=0.9365 | val_loss=0.5681 acc=0.8014 | prec=0.7973 rec=0.8082 spec=0.7945 f1=0.8027 | time=9.0s\n",
            "Epoch 038 | train_loss=0.1551 acc=0.9271 | val_loss=0.6470 acc=0.7911 | prec=0.7485 rec=0.8767 spec=0.7055 f1=0.8076 | time=9.2s\n",
            "Epoch 039 | train_loss=0.1419 acc=0.9494 | val_loss=0.6400 acc=0.8048 | prec=0.8069 rec=0.8014 spec=0.8082 f1=0.8041 | time=9.0s\n",
            "Epoch 040 | train_loss=0.1275 acc=0.9434 | val_loss=0.6818 acc=0.7911 | prec=0.7429 rec=0.8904 spec=0.6918 f1=0.8100 | time=9.0s\n",
            "Epoch 041 | train_loss=0.1294 acc=0.9228 | val_loss=0.7244 acc=0.8048 | prec=0.8248 rec=0.7740 spec=0.8356 f1=0.7986 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▃▇▁▁▂▁▅▆▆▇▇▇▇▇█▇▇▇██████████▇▇▇██████</td></tr><tr><td>precision</td><td>▁▁▁▅▅▁▁▄▄▅▅▅▅▇▆▆▇▇▇▆▆▇▇█▇▇▇▇▇▇█▆▆▆█▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▃█▁▁▁▁▅▆▅█▆▇▇▆▇▆█▇▇▇▇▇▇▇▇▇▇▆▇▇█▆▇▇▇▇▆</td></tr><tr><td>specificity</td><td>███▆▁████▅▄▅▃▇▅▅▇▆▇▄▅▇▇▇▇▆▆▆▆▆▇▅▅▄▇▆▇▆▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▁▁▁▂▂▂▂▃▄▅▅▅▆▆▆▆▇▇▇▇▇▇▇▇█████▇████</td></tr><tr><td>train_loss</td><td>████▇████▇▇▇▇▇▆▆▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁▁▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▂▂▃▃▆▆▆▇▇▇▅▆████▇▇▇▇▇█▆▆▅█▇▇▇▇▇</td></tr><tr><td>val_loss</td><td>▇▇▇▇▇▇▇▇█▇▇▇▆▅▅▄▃▃▃▃▃▂▂▁▂▁▂▃▃▅▃▅▇█▅▅▄▆▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>41</td></tr><tr><td>f1_score</td><td>0.79859</td></tr><tr><td>precision</td><td>0.82482</td></tr><tr><td>recall</td><td>0.77397</td></tr><tr><td>specificity</td><td>0.83562</td></tr><tr><td>train_acc</td><td>0.92281</td></tr><tr><td>train_loss</td><td>0.12942</td></tr><tr><td>val_acc</td><td>0.80479</td></tr><tr><td>val_loss</td><td>0.72438</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial6</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/xlao46ii' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/xlao46ii</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_072306-xlao46ii/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:29:20,826] Trial 6 finished with values: [0.407999025285244, 0.8047945205479452] and parameters: {'lr': 0.0003865812575453315, 'wd': 0.0003593899074043075}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 7 hyperparams --> lr=1.14e-03, wd=7.08e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_072920-0vkzp38j</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/0vkzp38j' target=\"_blank\">trial7</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/0vkzp38j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/0vkzp38j</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 7]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7158 acc=0.5189 | val_loss=0.6939 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 002 | train_loss=0.7161 acc=0.4991 | val_loss=0.6945 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 003 | train_loss=0.7144 acc=0.4931 | val_loss=0.7062 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7163 acc=0.5000 | val_loss=0.6925 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7112 acc=0.5069 | val_loss=0.6894 acc=0.5034 | prec=1.0000 rec=0.0068 spec=1.0000 f1=0.0136 | time=9.0s\n",
            "Epoch 006 | train_loss=0.6906 acc=0.5506 | val_loss=0.6453 acc=0.6370 | prec=0.5833 rec=0.9589 spec=0.3151 f1=0.7254 | time=9.2s\n",
            "Epoch 007 | train_loss=0.6520 acc=0.6235 | val_loss=0.6096 acc=0.7158 | prec=0.6780 rec=0.8219 spec=0.6096 f1=0.7430 | time=9.1s\n",
            "Epoch 008 | train_loss=0.6125 acc=0.6792 | val_loss=0.5679 acc=0.6815 | prec=0.6210 rec=0.9315 spec=0.4315 f1=0.7452 | time=9.2s\n",
            "Epoch 009 | train_loss=0.5863 acc=0.6938 | val_loss=0.5223 acc=0.7740 | prec=0.8279 rec=0.6918 spec=0.8562 f1=0.7537 | time=9.2s\n",
            "Epoch 010 | train_loss=0.5500 acc=0.7419 | val_loss=0.4727 acc=0.7705 | prec=0.7026 rec=0.9384 spec=0.6027 f1=0.8035 | time=9.1s\n",
            "Epoch 011 | train_loss=0.4877 acc=0.7719 | val_loss=0.5594 acc=0.7123 | prec=0.9844 rec=0.4315 spec=0.9932 f1=0.6000 | time=9.1s\n",
            "Epoch 012 | train_loss=0.4516 acc=0.7873 | val_loss=0.5043 acc=0.7637 | prec=0.9873 rec=0.5342 spec=0.9932 f1=0.6933 | time=9.2s\n",
            "Epoch 013 | train_loss=0.3954 acc=0.8328 | val_loss=0.3562 acc=0.8596 | prec=0.9487 rec=0.7603 spec=0.9589 f1=0.8441 | time=9.1s\n",
            "Epoch 014 | train_loss=0.3703 acc=0.8516 | val_loss=0.4992 acc=0.7500 | prec=0.6853 rec=0.9247 spec=0.5753 f1=0.7872 | time=9.1s\n",
            "Epoch 015 | train_loss=0.3471 acc=0.8499 | val_loss=0.4151 acc=0.8116 | prec=0.7630 rec=0.9041 spec=0.7192 f1=0.8276 | time=9.1s\n",
            "Epoch 016 | train_loss=0.2948 acc=0.8774 | val_loss=0.4200 acc=0.8219 | prec=0.8219 rec=0.8219 spec=0.8219 f1=0.8219 | time=9.1s\n",
            "Epoch 017 | train_loss=0.2890 acc=0.8774 | val_loss=0.4484 acc=0.7979 | prec=0.7605 rec=0.8699 spec=0.7260 f1=0.8115 | time=9.1s\n",
            "Epoch 018 | train_loss=0.2701 acc=0.8791 | val_loss=0.5129 acc=0.7500 | prec=0.6911 rec=0.9041 spec=0.5959 f1=0.7834 | time=9.1s\n",
            "Epoch 019 | train_loss=0.3365 acc=0.8508 | val_loss=0.4535 acc=0.8185 | prec=0.9115 rec=0.7055 spec=0.9315 f1=0.7954 | time=9.1s\n",
            "Epoch 020 | train_loss=0.2151 acc=0.9091 | val_loss=0.6754 acc=0.7534 | prec=0.6869 rec=0.9315 spec=0.5753 f1=0.7907 | time=9.1s\n",
            "Epoch 021 | train_loss=0.1863 acc=0.9065 | val_loss=0.4715 acc=0.8151 | prec=0.8333 rec=0.7877 spec=0.8425 f1=0.8099 | time=9.1s\n",
            "Epoch 022 | train_loss=0.2388 acc=0.8842 | val_loss=1.0030 acc=0.6507 | prec=1.0000 rec=0.3014 spec=1.0000 f1=0.4632 | time=9.1s\n",
            "Epoch 023 | train_loss=0.2372 acc=0.9022 | val_loss=0.7681 acc=0.7363 | prec=0.6865 rec=0.8699 spec=0.6027 f1=0.7674 | time=9.0s\n",
            "Epoch 024 | train_loss=0.2377 acc=0.9057 | val_loss=0.4392 acc=0.8185 | prec=0.8207 rec=0.8151 spec=0.8219 f1=0.8179 | time=9.0s\n",
            "Epoch 025 | train_loss=0.1772 acc=0.9117 | val_loss=0.8266 acc=0.7432 | prec=0.6784 rec=0.9247 spec=0.5616 f1=0.7826 | time=9.2s\n",
            "Epoch 026 | train_loss=0.1763 acc=0.9202 | val_loss=0.5897 acc=0.8253 | prec=0.8417 rec=0.8014 spec=0.8493 f1=0.8211 | time=9.0s\n",
            "Epoch 027 | train_loss=0.1806 acc=0.9237 | val_loss=0.5434 acc=0.8151 | prec=0.8594 rec=0.7534 spec=0.8767 f1=0.8029 | time=9.1s\n",
            "Epoch 028 | train_loss=0.1846 acc=0.9160 | val_loss=0.8989 acc=0.6541 | prec=0.9787 rec=0.3151 spec=0.9932 f1=0.4767 | time=9.0s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇██</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▇▇▇▇█▆▇█████▇███▅▇█▇██▅</td></tr><tr><td>precision</td><td>▁▁▁▁█▅▆▅▇▆███▆▆▇▆▆▇▆▇█▆▇▆▇▇█</td></tr><tr><td>recall</td><td>▁▁▁▁▁█▇█▆█▄▅▇██▇▇█▆█▇▃▇▇█▇▇▃</td></tr><tr><td>specificity</td><td>█████▁▄▂▇▄███▄▅▆▅▄▇▄▆█▄▆▄▆▇█</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▂▃▄▄▅▆▆▇▇▇▇▇▇▇██▇██████</td></tr><tr><td>train_loss</td><td>██████▇▇▆▆▅▅▄▄▃▃▂▂▃▂▁▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▄▅▅▆▆▅▆█▆▇▇▇▆▇▆▇▄▆▇▆▇▇▄</td></tr><tr><td>val_loss</td><td>▅▅▅▅▅▄▄▃▃▂▃▃▁▃▂▂▂▃▂▄▂█▅▂▆▄▃▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>28</td></tr><tr><td>f1_score</td><td>0.47668</td></tr><tr><td>precision</td><td>0.97872</td></tr><tr><td>recall</td><td>0.31507</td></tr><tr><td>specificity</td><td>0.99315</td></tr><tr><td>train_acc</td><td>0.91595</td></tr><tr><td>train_loss</td><td>0.18457</td></tr><tr><td>val_acc</td><td>0.65411</td></tr><tr><td>val_loss</td><td>0.89885</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial7</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/0vkzp38j' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/0vkzp38j</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_072920-0vkzp38j/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:33:37,430] Trial 7 finished with values: [0.3562216967344284, 0.8595890410958904] and parameters: {'lr': 0.001135452965146807, 'wd': 7.07784025088395e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 8 hyperparams --> lr=1.78e-03, wd=1.43e-04\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_073337-sgq5ql7q</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/sgq5ql7q' target=\"_blank\">trial8</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/sgq5ql7q' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/sgq5ql7q</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 8]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.7386 acc=0.4897 | val_loss=0.6939 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 002 | train_loss=0.7275 acc=0.4820 | val_loss=0.6921 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 003 | train_loss=0.7261 acc=0.4760 | val_loss=0.6911 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7168 acc=0.4983 | val_loss=0.6898 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7197 acc=0.4863 | val_loss=0.7179 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 006 | train_loss=0.7225 acc=0.4991 | val_loss=0.6943 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.3s\n",
            "Epoch 007 | train_loss=0.7165 acc=0.5129 | val_loss=0.6851 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 008 | train_loss=0.7046 acc=0.5240 | val_loss=0.6839 acc=0.5137 | prec=0.5069 rec=1.0000 spec=0.0274 f1=0.6728 | time=9.1s\n",
            "Epoch 009 | train_loss=0.7065 acc=0.5077 | val_loss=0.6603 acc=0.5171 | prec=0.5087 rec=1.0000 spec=0.0342 f1=0.6744 | time=9.1s\n",
            "Epoch 010 | train_loss=0.6550 acc=0.6278 | val_loss=0.5464 acc=0.7534 | prec=0.7056 rec=0.8699 spec=0.6370 f1=0.7791 | time=9.0s\n",
            "Epoch 011 | train_loss=0.5806 acc=0.7178 | val_loss=0.5154 acc=0.7911 | prec=0.7348 rec=0.9110 spec=0.6712 f1=0.8135 | time=9.1s\n",
            "Epoch 012 | train_loss=0.5214 acc=0.7573 | val_loss=0.5209 acc=0.7260 | prec=0.6571 rec=0.9452 spec=0.5068 f1=0.7753 | time=9.0s\n",
            "Epoch 013 | train_loss=0.4902 acc=0.7744 | val_loss=0.4625 acc=0.8014 | prec=0.9000 rec=0.6781 spec=0.9247 f1=0.7734 | time=9.1s\n",
            "Epoch 014 | train_loss=0.4169 acc=0.8130 | val_loss=0.4396 acc=0.8425 | prec=0.8333 rec=0.8562 spec=0.8288 f1=0.8446 | time=9.1s\n",
            "Epoch 015 | train_loss=0.4264 acc=0.8087 | val_loss=0.4211 acc=0.8082 | prec=0.7679 rec=0.8836 spec=0.7329 f1=0.8217 | time=9.1s\n",
            "Epoch 016 | train_loss=0.4036 acc=0.8208 | val_loss=0.7090 acc=0.6541 | prec=0.5957 rec=0.9589 spec=0.3493 f1=0.7349 | time=9.0s\n",
            "Epoch 017 | train_loss=0.3484 acc=0.8491 | val_loss=0.3835 acc=0.8493 | prec=0.8187 rec=0.8973 spec=0.8014 f1=0.8562 | time=9.0s\n",
            "Epoch 018 | train_loss=0.3189 acc=0.8714 | val_loss=0.4222 acc=0.8322 | prec=0.9619 rec=0.6918 spec=0.9726 f1=0.8048 | time=9.1s\n",
            "Epoch 019 | train_loss=0.2969 acc=0.8705 | val_loss=0.5430 acc=0.6952 | prec=0.6326 rec=0.9315 spec=0.4589 f1=0.7535 | time=9.0s\n",
            "Epoch 020 | train_loss=0.3604 acc=0.8413 | val_loss=0.7713 acc=0.6815 | prec=0.6188 rec=0.9452 spec=0.4178 f1=0.7480 | time=9.0s\n",
            "Epoch 021 | train_loss=0.2988 acc=0.8568 | val_loss=0.7626 acc=0.7329 | prec=0.6667 rec=0.9315 spec=0.5342 f1=0.7771 | time=9.0s\n",
            "Epoch 022 | train_loss=0.2597 acc=0.8859 | val_loss=0.4994 acc=0.8219 | prec=0.8013 rec=0.8562 spec=0.7877 f1=0.8278 | time=9.0s\n",
            "Epoch 023 | train_loss=0.2550 acc=0.8851 | val_loss=0.5999 acc=0.8116 | prec=0.7974 rec=0.8356 spec=0.7877 f1=0.8161 | time=9.1s\n",
            "Epoch 024 | train_loss=0.2375 acc=0.8928 | val_loss=0.7931 acc=0.8253 | prec=0.8188 rec=0.8356 spec=0.8151 f1=0.8271 | time=9.0s\n",
            "Epoch 025 | train_loss=0.3099 acc=0.8654 | val_loss=0.8927 acc=0.8151 | prec=0.8651 rec=0.7466 spec=0.8836 f1=0.8015 | time=9.1s\n",
            "Epoch 026 | train_loss=0.2559 acc=0.8902 | val_loss=0.4673 acc=0.8048 | prec=0.8201 rec=0.7808 spec=0.8288 f1=0.8000 | time=9.0s\n",
            "Epoch 027 | train_loss=0.2225 acc=0.8988 | val_loss=0.7775 acc=0.7500 | prec=0.6853 rec=0.9247 spec=0.5753 f1=0.7872 | time=9.1s\n",
            "Epoch 028 | train_loss=0.1993 acc=0.8971 | val_loss=0.4552 acc=0.8322 | prec=0.8540 rec=0.8014 spec=0.8630 f1=0.8269 | time=9.1s\n",
            "Epoch 029 | train_loss=0.1575 acc=0.9271 | val_loss=0.5647 acc=0.7911 | prec=0.7429 rec=0.8904 spec=0.6918 f1=0.8100 | time=9.2s\n",
            "Epoch 030 | train_loss=0.1673 acc=0.9228 | val_loss=2.7016 acc=0.5445 | prec=0.5233 rec=1.0000 spec=0.0890 f1=0.6871 | time=9.1s\n",
            "Epoch 031 | train_loss=0.2255 acc=0.8979 | val_loss=0.7967 acc=0.7295 | prec=0.6650 rec=0.9247 spec=0.5342 f1=0.7736 | time=9.0s\n",
            "Epoch 032 | train_loss=0.1618 acc=0.9254 | val_loss=0.9104 acc=0.7397 | prec=0.6733 rec=0.9315 spec=0.5479 f1=0.7816 | time=9.1s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▇▇▇▇███</td></tr><tr><td>f1_score</td><td>▁▆▆▆▆▁▆▇▇▇█▇▇██▇██▇▇▇█████▇██▇▇▇</td></tr><tr><td>precision</td><td>▁▅▅▅▅▁▅▅▅▆▆▆█▇▇▅▇█▆▆▆▇▇▇▇▇▆▇▆▅▆▆</td></tr><tr><td>recall</td><td>▁████▁███▇▇█▆▇▇█▇▆███▇▇▇▆▆▇▇▇█▇█</td></tr><tr><td>specificity</td><td>█▁▁▁▁█▁▁▁▅▆▅▇▇▆▃▇█▄▄▅▇▇▇▇▇▅▇▆▂▅▅</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▂▂▁▃▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>█████████▇▆▅▅▄▄▄▃▃▃▃▃▂▂▂▃▂▂▂▁▁▂▁</td></tr><tr><td>val_acc</td><td>▁▁▁▁▁▁▁▁▁▆▇▆▇█▇▄██▅▅▆▇▇█▇▇▆█▇▂▆▆</td></tr><tr><td>val_loss</td><td>▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▂▁▁▁▂▂▁▂▂▃▁▂▁▂█▂▃</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>32</td></tr><tr><td>f1_score</td><td>0.78161</td></tr><tr><td>precision</td><td>0.67327</td></tr><tr><td>recall</td><td>0.93151</td></tr><tr><td>specificity</td><td>0.54795</td></tr><tr><td>train_acc</td><td>0.92539</td></tr><tr><td>train_loss</td><td>0.16182</td></tr><tr><td>val_acc</td><td>0.73973</td></tr><tr><td>val_loss</td><td>0.91043</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial8</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/sgq5ql7q' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/sgq5ql7q</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_073337-sgq5ql7q/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:38:29,487] Trial 8 finished with values: [0.38354831039905546, 0.8493150684931506] and parameters: {'lr': 0.0017759009511164247, 'wd': 0.00014267744191477695}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Trial 9 hyperparams --> lr=1.91e-05, wd=2.22e-05\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.10"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250508_073829-kktiqqi6</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/kktiqqi6' target=\"_blank\">trial9</a></strong> to <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/kktiqqi6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/kktiqqi6</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Trial 9]  TRAIN n=1166 (FTD=583, CN=583) | VAL n=292 (FTD=146, CN=146)\n",
            "Epoch 001 | train_loss=0.8204 acc=0.4940 | val_loss=0.7336 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 002 | train_loss=0.7933 acc=0.5000 | val_loss=0.7095 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 003 | train_loss=0.7913 acc=0.5086 | val_loss=0.7075 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 004 | train_loss=0.7708 acc=0.4940 | val_loss=0.7064 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 005 | train_loss=0.7645 acc=0.4966 | val_loss=0.7054 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 006 | train_loss=0.7589 acc=0.4889 | val_loss=0.7137 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 007 | train_loss=0.7705 acc=0.4691 | val_loss=0.7178 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 008 | train_loss=0.7361 acc=0.5180 | val_loss=0.7061 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 009 | train_loss=0.7379 acc=0.4974 | val_loss=0.7085 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 010 | train_loss=0.7307 acc=0.5069 | val_loss=0.7032 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 011 | train_loss=0.7256 acc=0.5137 | val_loss=0.7114 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 012 | train_loss=0.7295 acc=0.5103 | val_loss=0.7074 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 013 | train_loss=0.7296 acc=0.5094 | val_loss=0.7017 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 014 | train_loss=0.7264 acc=0.4871 | val_loss=0.6999 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.2s\n",
            "Epoch 015 | train_loss=0.7334 acc=0.4983 | val_loss=0.7012 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.0s\n",
            "Epoch 016 | train_loss=0.7454 acc=0.4786 | val_loss=0.6915 acc=0.4966 | prec=0.4983 rec=0.9932 spec=0.0000 f1=0.6636 | time=9.1s\n",
            "Epoch 017 | train_loss=0.7259 acc=0.5111 | val_loss=0.6915 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.1s\n",
            "Epoch 018 | train_loss=0.7481 acc=0.4811 | val_loss=0.6989 acc=0.5000 | prec=0.0000 rec=0.0000 spec=1.0000 f1=0.0000 | time=9.1s\n",
            "Epoch 019 | train_loss=0.7396 acc=0.4820 | val_loss=0.6921 acc=0.5205 | prec=0.5139 rec=0.7603 spec=0.2808 f1=0.6133 | time=9.1s\n",
            "Epoch 020 | train_loss=0.7146 acc=0.5009 | val_loss=0.6929 acc=0.4658 | prec=0.4590 rec=0.3836 spec=0.5479 f1=0.4179 | time=9.0s\n",
            "Epoch 021 | train_loss=0.7209 acc=0.5223 | val_loss=0.6914 acc=0.5034 | prec=0.5017 rec=1.0000 spec=0.0068 f1=0.6682 | time=9.1s\n",
            "Epoch 022 | train_loss=0.7279 acc=0.4880 | val_loss=0.6919 acc=0.5068 | prec=0.5035 rec=0.9932 spec=0.0205 f1=0.6682 | time=9.1s\n",
            "Epoch 023 | train_loss=0.7064 acc=0.5266 | val_loss=0.6929 acc=0.5240 | prec=0.6667 rec=0.0959 spec=0.9521 f1=0.1677 | time=9.0s\n",
            "Epoch 024 | train_loss=0.7222 acc=0.4846 | val_loss=0.6922 acc=0.5000 | prec=0.5000 rec=0.2740 spec=0.7260 f1=0.3540 | time=9.1s\n",
            "Epoch 025 | train_loss=0.7135 acc=0.5146 | val_loss=0.6907 acc=0.5000 | prec=0.5000 rec=1.0000 spec=0.0000 f1=0.6667 | time=9.0s\n",
            "Epoch 026 | train_loss=0.7318 acc=0.4820 | val_loss=0.6910 acc=0.5685 | prec=0.5500 rec=0.7534 spec=0.3836 f1=0.6358 | time=9.2s\n",
            "Epoch 027 | train_loss=0.7291 acc=0.4768 | val_loss=0.6894 acc=0.5719 | prec=0.5479 rec=0.8219 spec=0.3219 f1=0.6575 | time=9.0s\n",
            "Epoch 028 | train_loss=0.7148 acc=0.5034 | val_loss=0.6898 acc=0.5377 | prec=0.5797 rec=0.2740 spec=0.8014 f1=0.3721 | time=9.1s\n",
            "Epoch 029 | train_loss=0.7275 acc=0.4949 | val_loss=0.6903 acc=0.5959 | prec=0.5642 rec=0.8425 spec=0.3493 f1=0.6758 | time=9.2s\n",
            "Epoch 030 | train_loss=0.7060 acc=0.5223 | val_loss=0.6878 acc=0.5925 | prec=0.5771 rec=0.6918 spec=0.4932 f1=0.6293 | time=9.0s\n",
            "Epoch 031 | train_loss=0.7073 acc=0.5214 | val_loss=0.6836 acc=0.6062 | prec=0.6476 rec=0.4658 spec=0.7466 f1=0.5418 | time=9.2s\n",
            "Epoch 032 | train_loss=0.7062 acc=0.5223 | val_loss=0.6758 acc=0.6336 | prec=0.5942 rec=0.8425 spec=0.4247 f1=0.6969 | time=9.1s\n",
            "Epoch 033 | train_loss=0.6851 acc=0.5575 | val_loss=0.6565 acc=0.6507 | prec=0.6279 rec=0.7397 spec=0.5616 f1=0.6792 | time=9.1s\n",
            "Epoch 034 | train_loss=0.6711 acc=0.5703 | val_loss=0.6483 acc=0.6610 | prec=0.6942 rec=0.5753 spec=0.7466 f1=0.6292 | time=9.0s\n",
            "Epoch 035 | train_loss=0.6626 acc=0.6141 | val_loss=0.6585 acc=0.5719 | prec=0.5402 rec=0.9658 spec=0.1781 f1=0.6929 | time=9.1s\n",
            "Epoch 036 | train_loss=0.6586 acc=0.6029 | val_loss=0.6209 acc=0.6849 | prec=0.6731 rec=0.7192 spec=0.6507 f1=0.6954 | time=9.1s\n",
            "Epoch 037 | train_loss=0.6422 acc=0.6175 | val_loss=0.6021 acc=0.6815 | prec=0.6587 rec=0.7534 spec=0.6096 f1=0.7029 | time=8.9s\n",
            "Epoch 038 | train_loss=0.6425 acc=0.6304 | val_loss=0.6005 acc=0.6781 | prec=0.6275 rec=0.8767 spec=0.4795 f1=0.7314 | time=9.1s\n",
            "Epoch 039 | train_loss=0.6299 acc=0.6372 | val_loss=0.5827 acc=0.7123 | prec=0.7583 rec=0.6233 spec=0.8014 f1=0.6842 | time=9.0s\n",
            "Epoch 040 | train_loss=0.6203 acc=0.6501 | val_loss=0.5797 acc=0.7158 | prec=0.8316 rec=0.5411 spec=0.8904 f1=0.6556 | time=9.1s\n",
            "Epoch 041 | train_loss=0.6066 acc=0.6750 | val_loss=0.5675 acc=0.7089 | prec=0.6704 rec=0.8219 spec=0.5959 f1=0.7385 | time=9.0s\n",
            "Epoch 042 | train_loss=0.5861 acc=0.7007 | val_loss=0.5442 acc=0.7260 | prec=0.7391 rec=0.6986 spec=0.7534 f1=0.7183 | time=9.1s\n",
            "Epoch 043 | train_loss=0.5936 acc=0.6844 | val_loss=0.5639 acc=0.6884 | prec=0.6355 rec=0.8836 spec=0.4932 f1=0.7393 | time=9.1s\n",
            "Epoch 044 | train_loss=0.5687 acc=0.7144 | val_loss=0.5339 acc=0.7432 | prec=0.7710 rec=0.6918 spec=0.7945 f1=0.7292 | time=9.1s\n",
            "Epoch 045 | train_loss=0.5664 acc=0.7144 | val_loss=0.5304 acc=0.7397 | prec=0.8302 rec=0.6027 spec=0.8767 f1=0.6984 | time=9.2s\n",
            "Epoch 046 | train_loss=0.5461 acc=0.7264 | val_loss=0.5287 acc=0.7568 | prec=0.8505 rec=0.6233 spec=0.8904 f1=0.7194 | time=9.0s\n",
            "Epoch 047 | train_loss=0.5337 acc=0.7341 | val_loss=0.5044 acc=0.7774 | prec=0.8092 rec=0.7260 spec=0.8288 f1=0.7653 | time=9.0s\n",
            "Epoch 048 | train_loss=0.5365 acc=0.7256 | val_loss=0.5002 acc=0.7671 | prec=0.8145 rec=0.6918 spec=0.8425 f1=0.7481 | time=9.0s\n",
            "Epoch 049 | train_loss=0.5170 acc=0.7521 | val_loss=0.5065 acc=0.7603 | prec=0.8455 rec=0.6370 spec=0.8836 f1=0.7266 | time=9.0s\n",
            "Epoch 050 | train_loss=0.5257 acc=0.7556 | val_loss=0.5282 acc=0.7466 | prec=0.8600 rec=0.5890 spec=0.9041 f1=0.6992 | time=9.1s\n",
            "Epoch 051 | train_loss=0.5144 acc=0.7410 | val_loss=0.4912 acc=0.7705 | prec=0.8264 rec=0.6849 spec=0.8562 f1=0.7491 | time=9.1s\n",
            "Epoch 052 | train_loss=0.5009 acc=0.7633 | val_loss=0.4772 acc=0.7911 | prec=0.7852 rec=0.8014 spec=0.7808 f1=0.7932 | time=9.1s\n",
            "Epoch 053 | train_loss=0.4975 acc=0.7676 | val_loss=0.4809 acc=0.7911 | prec=0.7891 rec=0.7945 spec=0.7877 f1=0.7918 | time=9.1s\n",
            "Epoch 054 | train_loss=0.4799 acc=0.7830 | val_loss=0.4886 acc=0.7740 | prec=0.8448 rec=0.6712 spec=0.8767 f1=0.7481 | time=8.9s\n",
            "Epoch 055 | train_loss=0.4793 acc=0.7830 | val_loss=0.4669 acc=0.7945 | prec=0.7756 rec=0.8288 spec=0.7603 f1=0.8013 | time=9.2s\n",
            "Epoch 056 | train_loss=0.4685 acc=0.7762 | val_loss=0.4834 acc=0.7774 | prec=0.8189 rec=0.7123 spec=0.8425 f1=0.7619 | time=9.1s\n",
            "Epoch 057 | train_loss=0.4609 acc=0.7916 | val_loss=0.4735 acc=0.7911 | prec=0.8295 rec=0.7329 spec=0.8493 f1=0.7782 | time=9.0s\n",
            "Epoch 058 | train_loss=0.4412 acc=0.8165 | val_loss=0.5038 acc=0.7740 | prec=0.8922 rec=0.6233 spec=0.9247 f1=0.7339 | time=9.1s\n",
            "Epoch 059 | train_loss=0.4309 acc=0.8190 | val_loss=0.4823 acc=0.7808 | prec=0.8417 rec=0.6918 spec=0.8699 f1=0.7594 | time=9.1s\n",
            "Epoch 060 | train_loss=0.4163 acc=0.8422 | val_loss=0.4529 acc=0.7979 | prec=0.7881 rec=0.8151 spec=0.7808 f1=0.8013 | time=9.2s\n",
            "Epoch 061 | train_loss=0.4304 acc=0.8130 | val_loss=0.4578 acc=0.7740 | prec=0.8030 rec=0.7260 spec=0.8219 f1=0.7626 | time=9.1s\n",
            "Epoch 062 | train_loss=0.4206 acc=0.8259 | val_loss=0.4496 acc=0.7877 | prec=0.8043 rec=0.7603 spec=0.8151 f1=0.7817 | time=9.2s\n",
            "Epoch 063 | train_loss=0.4078 acc=0.8225 | val_loss=0.4478 acc=0.7911 | prec=0.8058 rec=0.7671 spec=0.8151 f1=0.7860 | time=9.0s\n",
            "Epoch 064 | train_loss=0.4147 acc=0.8285 | val_loss=0.5234 acc=0.7432 | prec=0.9176 rec=0.5342 spec=0.9521 f1=0.6753 | time=9.0s\n",
            "Epoch 065 | train_loss=0.4293 acc=0.8070 | val_loss=0.4433 acc=0.7911 | prec=0.7931 rec=0.7877 spec=0.7945 f1=0.7904 | time=9.1s\n",
            "Epoch 066 | train_loss=0.4033 acc=0.8250 | val_loss=0.4521 acc=0.7808 | prec=0.7330 rec=0.8836 spec=0.6781 f1=0.8012 | time=9.1s\n",
            "Epoch 067 | train_loss=0.3895 acc=0.8465 | val_loss=0.4412 acc=0.7945 | prec=0.7905 rec=0.8014 spec=0.7877 f1=0.7959 | time=9.0s\n",
            "Epoch 068 | train_loss=0.3741 acc=0.8499 | val_loss=0.4440 acc=0.7911 | prec=0.7457 rec=0.8836 spec=0.6986 f1=0.8088 | time=9.0s\n",
            "Epoch 069 | train_loss=0.3959 acc=0.8456 | val_loss=0.4409 acc=0.7911 | prec=0.7972 rec=0.7808 spec=0.8014 f1=0.7889 | time=9.1s\n",
            "Epoch 070 | train_loss=0.3804 acc=0.8491 | val_loss=0.4415 acc=0.8151 | prec=0.8333 rec=0.7877 spec=0.8425 f1=0.8099 | time=9.1s\n",
            "Epoch 071 | train_loss=0.3604 acc=0.8619 | val_loss=0.4391 acc=0.7911 | prec=0.7515 rec=0.8699 spec=0.7123 f1=0.8063 | time=9.0s\n",
            "Epoch 072 | train_loss=0.3703 acc=0.8499 | val_loss=0.4834 acc=0.7842 | prec=0.8807 rec=0.6575 spec=0.9110 f1=0.7529 | time=9.1s\n",
            "Epoch 073 | train_loss=0.3776 acc=0.8559 | val_loss=0.4841 acc=0.7808 | prec=0.8661 rec=0.6644 spec=0.8973 f1=0.7519 | time=9.1s\n",
            "Epoch 074 | train_loss=0.3550 acc=0.8533 | val_loss=0.4700 acc=0.7842 | prec=0.8547 rec=0.6849 spec=0.8836 f1=0.7605 | time=9.1s\n",
            "Epoch 075 | train_loss=0.3565 acc=0.8542 | val_loss=0.4299 acc=0.7911 | prec=0.7673 rec=0.8356 spec=0.7466 f1=0.8000 | time=9.0s\n",
            "Epoch 076 | train_loss=0.3438 acc=0.8722 | val_loss=0.4357 acc=0.8048 | prec=0.8112 rec=0.7945 spec=0.8151 f1=0.8028 | time=9.2s\n",
            "Epoch 077 | train_loss=0.3402 acc=0.8731 | val_loss=0.4347 acc=0.7979 | prec=0.8271 rec=0.7534 spec=0.8425 f1=0.7885 | time=9.1s\n",
            "Epoch 078 | train_loss=0.3433 acc=0.8765 | val_loss=0.4390 acc=0.8048 | prec=0.8296 rec=0.7671 spec=0.8425 f1=0.7972 | time=9.1s\n",
            "Epoch 079 | train_loss=0.3517 acc=0.8645 | val_loss=0.4540 acc=0.8014 | prec=0.8729 rec=0.7055 spec=0.8973 f1=0.7803 | time=9.1s\n",
            "Epoch 080 | train_loss=0.3326 acc=0.8739 | val_loss=0.4293 acc=0.8082 | prec=0.8261 rec=0.7808 spec=0.8356 f1=0.8028 | time=9.0s\n",
            "Epoch 081 | train_loss=0.3281 acc=0.8714 | val_loss=0.4288 acc=0.8014 | prec=0.8014 rec=0.8014 spec=0.8014 f1=0.8014 | time=9.0s\n",
            "Epoch 082 | train_loss=0.3195 acc=0.8859 | val_loss=0.4507 acc=0.8048 | prec=0.8618 rec=0.7260 spec=0.8836 f1=0.7881 | time=9.0s\n",
            "Epoch 083 | train_loss=0.3098 acc=0.8962 | val_loss=0.4437 acc=0.8048 | prec=0.8346 rec=0.7603 spec=0.8493 f1=0.7957 | time=9.1s\n",
            "Epoch 084 | train_loss=0.3251 acc=0.8756 | val_loss=0.4487 acc=0.7979 | prec=0.8537 rec=0.7192 spec=0.8767 f1=0.7807 | time=9.1s\n",
            "Epoch 085 | train_loss=0.3370 acc=0.8533 | val_loss=0.4336 acc=0.8082 | prec=0.8409 rec=0.7603 spec=0.8562 f1=0.7986 | time=8.9s\n",
            "Epoch 086 | train_loss=0.3269 acc=0.8859 | val_loss=0.4418 acc=0.8116 | prec=0.8583 rec=0.7466 spec=0.8767 f1=0.7985 | time=9.2s\n",
            "Epoch 087 | train_loss=0.3168 acc=0.8834 | val_loss=0.4488 acc=0.7979 | prec=0.8537 rec=0.7192 spec=0.8767 f1=0.7807 | time=8.9s\n",
            "Epoch 088 | train_loss=0.3243 acc=0.8937 | val_loss=0.4276 acc=0.8082 | prec=0.8214 rec=0.7877 spec=0.8288 f1=0.8042 | time=9.0s\n",
            "Epoch 089 | train_loss=0.3140 acc=0.8808 | val_loss=0.4427 acc=0.7979 | prec=0.8425 rec=0.7329 spec=0.8630 f1=0.7839 | time=9.1s\n",
            "Epoch 090 | train_loss=0.3180 acc=0.8816 | val_loss=0.4689 acc=0.7877 | prec=0.8750 rec=0.6712 spec=0.9041 f1=0.7597 | time=9.0s\n",
            "Epoch 091 | train_loss=0.3194 acc=0.8825 | val_loss=0.4821 acc=0.7774 | prec=0.8716 rec=0.6507 spec=0.9041 f1=0.7451 | time=9.1s\n",
            "Epoch 092 | train_loss=0.3023 acc=0.8971 | val_loss=0.4377 acc=0.7979 | prec=0.8372 rec=0.7397 spec=0.8562 f1=0.7855 | time=9.0s\n",
            "Epoch 093 | train_loss=0.3148 acc=0.8825 | val_loss=0.4383 acc=0.7979 | prec=0.8372 rec=0.7397 spec=0.8562 f1=0.7855 | time=9.2s\n",
            "Epoch 094 | train_loss=0.3078 acc=0.8774 | val_loss=0.4266 acc=0.8151 | prec=0.8333 rec=0.7877 spec=0.8425 f1=0.8099 | time=9.3s\n",
            "Epoch 095 | train_loss=0.3008 acc=0.8979 | val_loss=0.4187 acc=0.7945 | prec=0.7756 rec=0.8288 spec=0.7603 f1=0.8013 | time=9.0s\n",
            "Epoch 096 | train_loss=0.3008 acc=0.8928 | val_loss=0.4397 acc=0.8048 | prec=0.8397 rec=0.7534 spec=0.8562 f1=0.7942 | time=9.1s\n",
            "Epoch 097 | train_loss=0.2939 acc=0.9099 | val_loss=0.4324 acc=0.8116 | prec=0.8370 rec=0.7740 spec=0.8493 f1=0.8043 | time=9.0s\n",
            "Epoch 098 | train_loss=0.2900 acc=0.9014 | val_loss=0.4397 acc=0.8116 | prec=0.8421 rec=0.7671 spec=0.8562 f1=0.8029 | time=9.1s\n",
            "Epoch 099 | train_loss=0.3018 acc=0.8885 | val_loss=0.4372 acc=0.8048 | prec=0.8397 rec=0.7534 spec=0.8562 f1=0.7942 | time=9.0s\n",
            "Epoch 100 | train_loss=0.2731 acc=0.9031 | val_loss=0.4367 acc=0.8082 | prec=0.8409 rec=0.7603 spec=0.8562 f1=0.7986 | time=9.2s\n",
            "Epoch 101 | train_loss=0.2816 acc=0.9074 | val_loss=0.4501 acc=0.7911 | prec=0.8455 rec=0.7123 spec=0.8699 f1=0.7732 | time=9.1s\n",
            "Epoch 102 | train_loss=0.2819 acc=0.9074 | val_loss=0.4535 acc=0.7842 | prec=0.8374 rec=0.7055 spec=0.8630 f1=0.7658 | time=9.0s\n",
            "Epoch 103 | train_loss=0.2830 acc=0.9099 | val_loss=0.4518 acc=0.7945 | prec=0.8468 rec=0.7192 spec=0.8699 f1=0.7778 | time=9.2s\n",
            "Epoch 104 | train_loss=0.2954 acc=0.8902 | val_loss=0.4565 acc=0.7877 | prec=0.8559 rec=0.6918 spec=0.8836 f1=0.7652 | time=9.0s\n",
            "Epoch 105 | train_loss=0.2985 acc=0.8937 | val_loss=0.4465 acc=0.7979 | prec=0.8480 rec=0.7260 spec=0.8699 f1=0.7823 | time=9.1s\n",
            "Epoch 106 | train_loss=0.2884 acc=0.8791 | val_loss=0.4643 acc=0.7945 | prec=0.8707 rec=0.6918 spec=0.8973 f1=0.7710 | time=9.2s\n",
            "Epoch 107 | train_loss=0.2897 acc=0.8945 | val_loss=0.4578 acc=0.7911 | prec=0.8512 rec=0.7055 spec=0.8767 f1=0.7715 | time=9.2s\n",
            "Epoch 108 | train_loss=0.2971 acc=0.8937 | val_loss=0.4431 acc=0.8048 | prec=0.8450 rec=0.7466 spec=0.8630 f1=0.7927 | time=9.1s\n",
            "Epoch 109 | train_loss=0.2860 acc=0.8937 | val_loss=0.4413 acc=0.8082 | prec=0.8462 rec=0.7534 spec=0.8630 f1=0.7971 | time=9.1s\n",
            "Epoch 110 | train_loss=0.2797 acc=0.8979 | val_loss=0.4597 acc=0.7945 | prec=0.8583 rec=0.7055 spec=0.8836 f1=0.7744 | time=9.3s\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>f1_score</td><td>▁▁▁▁▁▇▇▂▄▇▄▆▆▇▆▇▇▇▇▇▇▇██████████████████</td></tr><tr><td>precision</td><td>▁▁▁▁▁▅▅▅▅▅▆▅▆▆▇▇▇▇▇█▇▇▇█▇▇▇█▇▇████▇▇▇▇▇▇</td></tr><tr><td>recall</td><td>▁▁▁▁▁▁██▁▆██▆▇▄▆█▆▅▆▆▇▅▇▇▆▆▆▆▇▆▆▇▆▆▆▆▆▆▆</td></tr><tr><td>specificity</td><td>███████▂▆▃▄▃▄▆▁▇▇▇▇▆▇▆▆▆▆▅▇▆▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>train_acc</td><td>▁▁▁▁▁▁▂▁▂▁▁▂▁▁▂▄▄▅▅▅▆▇▆▇▇▇▇▇▇▇████████▇█</td></tr><tr><td>train_loss</td><td>██████▇▇▇▇▇▆▆▆▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▂▂▂▂▂▁▂▂▃▄▃▅▅▆▇▆▇▇▇▇█▇▇█▇███████▇██████</td></tr><tr><td>val_loss</td><td>█▇▇▇█▇▇▇▇▇▇▇▇▆▅▄▃▃▃▃▂▂▁▃▁▁▁▂▂▂▂▁▁▂▁▁▂▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>110</td></tr><tr><td>f1_score</td><td>0.77444</td></tr><tr><td>precision</td><td>0.85833</td></tr><tr><td>recall</td><td>0.70548</td></tr><tr><td>specificity</td><td>0.88356</td></tr><tr><td>train_acc</td><td>0.89794</td></tr><tr><td>train_loss</td><td>0.27966</td></tr><tr><td>val_acc</td><td>0.79452</td></tr><tr><td>val_loss</td><td>0.45966</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trial9</strong> at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/kktiqqi6' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3/runs/kktiqqi6</a><br> View project at: <a href='https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3' target=\"_blank\">https://wandb.ai/jh8032-new-york-university/eeg-optuna-FTD-CN-test-within-cross-3</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250508_073829-kktiqqi6/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-08 07:55:11,121] Trial 9 finished with values: [0.4187335431575775, 0.7945205479452054] and parameters: {'lr': 1.9133395390721948e-05, 'wd': 2.2206391286576762e-05}.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Selected Trial #7 ===\n",
            " val_loss=0.3562, val_acc=0.8596, params={'lr': 0.001135452965146807, 'wd': 7.07784025088395e-05}, best_epoch=13, ckpt=ckpts/trial7_optimal_FTD_CN_3.pth\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=198  F(TD)=72, C(N)=126\n",
            " Accuracy=0.8434 Sensitivity=0.7619 Specificity=0.9861 F1=0.8610\n",
            "\n",
            "-- test_cross -- total=554  F(TD)=247, C(N)=307\n",
            " Accuracy=0.6949 Sensitivity=0.5863 Specificity=0.8300 F1=0.6805\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Train on Full Data (include validation set)\n",
        "- We found out that our optimal hyperparameter is\n",
        "- [lr=1.23e-03, wd=3.76e-04]\n",
        "- Use this hyperparameter and include the validation set and train on entire data\n",
        "- Then test the performance within and cross\n",
        "- Fine tune it on our held-out validation fold"
      ],
      "metadata": {
        "id": "IQoHuOblOFk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train_with_val_then_full.py\n",
        "import os, json, random, time\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import recall_score, f1_score, confusion_matrix\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── 1) Config & reproducibility ───────────────────────────────\n",
        "SEED        = 0\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE   = 32\n",
        "NUM_WORKERS  = 4\n",
        "MAX_EPOCHS   = 50     # max for search\n",
        "ES_PATIENCE  = 15     # early-stop patience\n",
        "PCT_START    = 0.2\n",
        "\n",
        "# hyperparameters you’ve tuned\n",
        "LR           = 1.23e-3\n",
        "WD           = 3.76e-4\n",
        "\n",
        "# ─── 2) Dataset wrapper ────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, metas):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.metas  = metas\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y     = self.metas[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── 3) Load & prepare metadata ───────────────────────────────\n",
        "with open(Path(DATA_DIR)/LABEL_FILE,'r') as f:\n",
        "    all_meta = json.load(f)\n",
        "\n",
        "train_meta       = [d for d in all_meta if d['type']=='train']\n",
        "test_within_meta = [d for d in all_meta if d['type']=='test_within']\n",
        "test_cross_meta  = [d for d in all_meta if d['type']=='test_cross']\n",
        "\n",
        "# filter F vs C\n",
        "class0, class1 = 'F','C'\n",
        "label_map = {class0:0, class1:1}\n",
        "\n",
        "train_meta       = [d for d in train_meta       if d['label'] in label_map]\n",
        "test_within_meta = [d for d in test_within_meta if d['label'] in label_map]\n",
        "test_cross_meta  = [d for d in test_cross_meta  if d['label'] in label_map]\n",
        "\n",
        "# balance train by down-sampling\n",
        "data0 = [d for d in train_meta if d['label']==class0]\n",
        "data1 = [d for d in train_meta if d['label']==class1]\n",
        "k     = min(len(data0), len(data1))\n",
        "balanced_meta = random.sample(data0, k) + random.sample(data1, k)\n",
        "random.shuffle(balanced_meta)\n",
        "\n",
        "# convert to ints\n",
        "for d in balanced_meta:       d['label'] = label_map[d['label']]\n",
        "for d in test_within_meta:    d['label'] = label_map[d['label']]\n",
        "for d in test_cross_meta:     d['label'] = label_map[d['label']]\n",
        "\n",
        "print(f\"\\n[Balanced TRAIN] total={len(balanced_meta)} (each class={k})\")\n",
        "\n",
        "# build full balanced dataset\n",
        "full_raw    = EEGDataset(DATA_DIR, balanced_meta)\n",
        "full_ds     = BinaryEEGDataset(full_raw, balanced_meta)\n",
        "labels_full = np.array([d['label'] for d in balanced_meta])\n",
        "\n",
        "# ─── 4) Split train/val for epoch search ──────────────────────\n",
        "idx            = np.arange(len(full_ds))\n",
        "tr_idx, va_idx = train_test_split(\n",
        "    idx, test_size=0.2, stratify=labels_full, random_state=SEED\n",
        ")\n",
        "train_loader = DataLoader(\n",
        "    Subset(full_ds, tr_idx), batch_size=BATCH_SIZE,\n",
        "    shuffle=True,  num_workers=NUM_WORKERS\n",
        ")\n",
        "val_loader   = DataLoader(\n",
        "    Subset(full_ds, va_idx), batch_size=BATCH_SIZE,\n",
        "    shuffle=False, num_workers=NUM_WORKERS\n",
        ")\n",
        "\n",
        "# ─── 5) Model builder & eval fn ───────────────────────────────\n",
        "def build_model():\n",
        "    input_len = full_ds[0][0].shape[-1]\n",
        "    return EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "\n",
        "def evaluate(loader, model, criterion):\n",
        "    model.eval()\n",
        "    loss_sum = correct = total = 0\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y    = X.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model(X)\n",
        "            loss_sum += criterion(logits,y).item()*y.size(0)\n",
        "            preds = logits.argmax(1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "    return loss_sum/total, correct/total\n",
        "\n",
        "# ─── 6) Phase 1: find best_epoch with early stopping ──────────\n",
        "best_val_loss = float('inf')\n",
        "best_epoch    = 0\n",
        "es_count      = 0\n",
        "\n",
        "model     = build_model()\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer, max_lr=LR, epochs=MAX_EPOCHS,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    pct_start=PCT_START, anneal_strategy='cos',\n",
        "    cycle_momentum=False\n",
        ")\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "print(\"\\nSearching best epoch using validation set...\")\n",
        "for epoch in range(1, MAX_EPOCHS+1):\n",
        "    t0 = time.time()\n",
        "    # ── train ──\n",
        "    model.train()\n",
        "    tr_loss_sum = tr_correct = tr_total = 0\n",
        "    for X,y in train_loader:\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss   = criterion(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        tr_loss_sum  += loss.item()*y.size(0)\n",
        "        preds        = logits.argmax(1)\n",
        "        tr_correct  += (preds==y).sum().item()\n",
        "        tr_total    += y.size(0)\n",
        "\n",
        "    train_loss = tr_loss_sum / tr_total\n",
        "    train_acc  = tr_correct / tr_total\n",
        "\n",
        "    # ── validate ──\n",
        "    val_loss, val_acc = evaluate(val_loader, model, criterion)\n",
        "    elapsed = time.time() - t0\n",
        "\n",
        "    print(\n",
        "        f\"Epoch {epoch:02d} | \"\n",
        "        f\"train_loss={train_loss:.4f} | train_acc={train_acc:.4f} | \"\n",
        "        f\"val_loss={val_loss:.4f} | val_acc={val_acc:.4f} | \"\n",
        "        f\"time={elapsed:.1f}s\"\n",
        "    )\n",
        "\n",
        "    # check for improvement\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_epoch    = epoch\n",
        "        es_count      = 0\n",
        "    else:\n",
        "        es_count += 1\n",
        "        if es_count >= ES_PATIENCE:\n",
        "            print(f\"\\nEarly stopping at epoch {epoch} (no improvement for {ES_PATIENCE} epochs)\")\n",
        "            break\n",
        "\n",
        "print(f\"\\n>> Best epoch by validation: {best_epoch}\")\n",
        "\n",
        "# ─── 7) Phase 2: retrain on full balanced set ─────────────────\n",
        "print(f\"\\nRetraining from scratch on FULL set for {best_epoch} epochs...\")\n",
        "model2     = build_model()\n",
        "optimizer2 = torch.optim.AdamW(model2.parameters(), lr=LR, weight_decay=WD)\n",
        "scheduler2 = torch.optim.lr_scheduler.OneCycleLR(\n",
        "    optimizer2, max_lr=LR,\n",
        "    epochs=best_epoch,\n",
        "    steps_per_epoch=len(DataLoader(full_ds, batch_size=BATCH_SIZE)),\n",
        "    pct_start=PCT_START, anneal_strategy='cos',\n",
        "    cycle_momentum=False\n",
        ")\n",
        "criterion2 = nn.CrossEntropyLoss()\n",
        "\n",
        "for epoch in range(1, best_epoch+1):\n",
        "    t0 = time.time()\n",
        "    tr_loss_sum = tr_correct = tr_total = 0\n",
        "    model2.train()\n",
        "    for X,y in DataLoader(full_ds, batch_size=BATCH_SIZE,\n",
        "                          shuffle=True, num_workers=NUM_WORKERS):\n",
        "        X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "        optimizer2.zero_grad()\n",
        "        logits = model2(X)\n",
        "        loss   = criterion2(logits,y)\n",
        "        loss.backward()\n",
        "        optimizer2.step()\n",
        "        scheduler2.step()\n",
        "\n",
        "        tr_loss_sum  += loss.item()*y.size(0)\n",
        "        preds        = logits.argmax(1)\n",
        "        tr_correct  += (preds==y).sum().item()\n",
        "        tr_total    += y.size(0)\n",
        "\n",
        "    avg_loss = tr_loss_sum / tr_total\n",
        "    acc      = tr_correct / tr_total\n",
        "    print(f\"Epoch {epoch:02d} | train_loss={avg_loss:.4f} acc={acc:.4f} time={time.time()-t0:.1f}s\")\n",
        "\n",
        "# ─── 8) Final Evaluation on Test Sets ─────────────────────────\n",
        "def full_evaluate(metas, loader):\n",
        "    model2.eval()\n",
        "    loss_sum = correct = total = 0\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X,y in loader:\n",
        "            X,y = X.to(DEVICE), y.to(DEVICE)\n",
        "            logits = model2(X)\n",
        "            loss_sum += criterion2(logits,y).item()*y.size(0)\n",
        "            preds    = logits.argmax(1)\n",
        "            correct += (preds==y).sum().item()\n",
        "            total   += y.size(0)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(all_preds)\n",
        "    labs  = np.concatenate(all_labels)\n",
        "    tn,fp,fn,tp = confusion_matrix(labs,preds,labels=[0,1]).ravel()\n",
        "    return {\n",
        "        \"loss\": loss_sum/total,\n",
        "        \"acc\": correct/total,\n",
        "        \"sensitivity\": recall_score(labs,preds,zero_division=0),\n",
        "        \"specificity\": tn/(tn+fp) if (tn+fp)>0 else 0.0,\n",
        "        \"f1\": f1_score(labs,preds,zero_division=0)\n",
        "    }\n",
        "\n",
        "print(\"\\n=== Final Evaluation on Test Sets ===\")\n",
        "for name, metas in [(\"test_within\", test_within_meta), (\"test_cross\", test_cross_meta)]:\n",
        "    loader = DataLoader(BinaryEEGDataset(EEGDataset(DATA_DIR, metas), metas),\n",
        "                        batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "    res = full_evaluate(metas, loader)\n",
        "    n0 = sum(1 for d in metas if d['label']==0)\n",
        "    n1 = sum(1 for d in metas if d['label']==1)\n",
        "    print(f\"-- {name} -- total={len(metas)}  F(TD)={n0}, C(N)={n1}\")\n",
        "    print(f\" Loss={res['loss']:.4f} Acc={res['acc']:.4f} \"\n",
        "          f\"Sens={res['sensitivity']:.4f} Spec={res['specificity']:.4f} \"\n",
        "          f\"F1={res['f1']:.4f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifikgG5BolLa",
        "outputId": "dec8b66c-7f07-4bd4-e758-ecb5afab38e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Balanced TRAIN] total=1458 (each class=729)\n",
            "\n",
            "Searching best epoch using validation set...\n",
            "Epoch 01 | train_loss=0.7458 | train_acc=0.5026 | val_loss=0.6936 | val_acc=0.5000 | time=9.5s\n",
            "Epoch 02 | train_loss=0.7211 | train_acc=0.5154 | val_loss=0.7163 | val_acc=0.5000 | time=9.2s\n",
            "Epoch 03 | train_loss=0.7301 | train_acc=0.4871 | val_loss=0.6937 | val_acc=0.5000 | time=9.2s\n",
            "Epoch 04 | train_loss=0.7247 | train_acc=0.4871 | val_loss=0.6888 | val_acc=0.5103 | time=9.2s\n",
            "Epoch 05 | train_loss=0.6541 | train_acc=0.5935 | val_loss=0.5569 | val_acc=0.7500 | time=9.2s\n",
            "Epoch 06 | train_loss=0.5958 | train_acc=0.7033 | val_loss=0.6049 | val_acc=0.6712 | time=9.1s\n",
            "Epoch 07 | train_loss=0.5769 | train_acc=0.7075 | val_loss=0.5156 | val_acc=0.7671 | time=9.2s\n",
            "Epoch 08 | train_loss=0.5376 | train_acc=0.7504 | val_loss=0.5084 | val_acc=0.7671 | time=9.1s\n",
            "Epoch 09 | train_loss=0.4756 | train_acc=0.7762 | val_loss=0.4753 | val_acc=0.7911 | time=9.2s\n",
            "Epoch 10 | train_loss=0.4194 | train_acc=0.8199 | val_loss=0.4304 | val_acc=0.7740 | time=9.1s\n",
            "Epoch 11 | train_loss=0.3919 | train_acc=0.8079 | val_loss=0.4743 | val_acc=0.7500 | time=9.1s\n",
            "Epoch 12 | train_loss=0.2978 | train_acc=0.8542 | val_loss=0.4136 | val_acc=0.7842 | time=9.1s\n",
            "Epoch 13 | train_loss=0.3322 | train_acc=0.8413 | val_loss=0.4616 | val_acc=0.8082 | time=9.1s\n",
            "Epoch 14 | train_loss=0.3250 | train_acc=0.8431 | val_loss=0.4886 | val_acc=0.8048 | time=9.3s\n",
            "Epoch 15 | train_loss=0.2491 | train_acc=0.8937 | val_loss=0.4947 | val_acc=0.7979 | time=9.1s\n",
            "Epoch 16 | train_loss=0.2263 | train_acc=0.8988 | val_loss=0.9567 | val_acc=0.6952 | time=9.2s\n",
            "Epoch 17 | train_loss=0.2046 | train_acc=0.8962 | val_loss=0.7651 | val_acc=0.7568 | time=9.1s\n",
            "Epoch 18 | train_loss=0.2214 | train_acc=0.8919 | val_loss=0.6285 | val_acc=0.5890 | time=9.1s\n",
            "Epoch 19 | train_loss=0.2161 | train_acc=0.9099 | val_loss=0.5586 | val_acc=0.7603 | time=9.1s\n",
            "Epoch 20 | train_loss=0.1679 | train_acc=0.9220 | val_loss=0.6089 | val_acc=0.7877 | time=9.1s\n",
            "Epoch 21 | train_loss=0.1871 | train_acc=0.9151 | val_loss=0.6261 | val_acc=0.8116 | time=9.2s\n",
            "Epoch 22 | train_loss=0.1468 | train_acc=0.9288 | val_loss=0.6200 | val_acc=0.8014 | time=9.2s\n",
            "Epoch 23 | train_loss=0.1213 | train_acc=0.9425 | val_loss=0.6698 | val_acc=0.8288 | time=9.2s\n",
            "Epoch 24 | train_loss=0.1237 | train_acc=0.9297 | val_loss=0.8718 | val_acc=0.7671 | time=9.2s\n",
            "Epoch 25 | train_loss=0.1264 | train_acc=0.9314 | val_loss=1.2357 | val_acc=0.7021 | time=9.3s\n",
            "Epoch 26 | train_loss=0.1172 | train_acc=0.9374 | val_loss=0.7271 | val_acc=0.7877 | time=9.2s\n",
            "Epoch 27 | train_loss=0.1092 | train_acc=0.9391 | val_loss=0.7818 | val_acc=0.8151 | time=9.1s\n",
            "\n",
            "Early stopping at epoch 27 (no improvement for 15 epochs)\n",
            "\n",
            ">> Best epoch by validation: 12\n",
            "\n",
            "Retraining from scratch on FULL set for 12 epochs...\n",
            "Epoch 01 | train_loss=0.7210 acc=0.4904 time=8.7s\n",
            "Epoch 02 | train_loss=0.7099 acc=0.4945 time=8.7s\n",
            "Epoch 03 | train_loss=0.7028 acc=0.5082 time=8.7s\n",
            "Epoch 04 | train_loss=0.6934 acc=0.5261 time=8.7s\n",
            "Epoch 05 | train_loss=0.6864 acc=0.5412 time=8.8s\n",
            "Epoch 06 | train_loss=0.6331 acc=0.6454 time=8.8s\n",
            "Epoch 07 | train_loss=0.5815 acc=0.7051 time=8.8s\n",
            "Epoch 08 | train_loss=0.5117 acc=0.7531 time=8.7s\n",
            "Epoch 09 | train_loss=0.4713 acc=0.7771 time=8.8s\n",
            "Epoch 10 | train_loss=0.4420 acc=0.7929 time=8.8s\n",
            "Epoch 11 | train_loss=0.4187 acc=0.8080 time=8.7s\n",
            "Epoch 12 | train_loss=0.4018 acc=0.8162 time=8.9s\n",
            "\n",
            "=== Final Evaluation on Test Sets ===\n",
            "-- test_within -- total=198  F(TD)=72, C(N)=126\n",
            " Loss=0.3504 Acc=0.8838 Sens=0.8413 Spec=0.9583 F1=0.9021\n",
            "\n",
            "-- test_cross -- total=554  F(TD)=247, C(N)=307\n",
            " Loss=0.7021 Acc=0.6769 Sens=0.6352 Spec=0.7287 F1=0.6854\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Test performance Comparison (Without Full Train vs Full Train)\n",
        "\n",
        "#### Without Full Train\n",
        "=== Final Evaluation on Test Sets ===\n",
        "- test_within -- total=198  F(TD)=72, C(N)=126\n",
        " Accuracy=0.8535 Sensitivity=0.7857 Specificity=0.9722 F1=0.8722\n",
        "\n",
        "- test_cross -- total=554  F(TD)=247, C(N)=307\n",
        " Accuracy=0.7004 Sensitivity=0.6645 Specificity=0.7449 F1=0.7108\n",
        "\n",
        "#### Without Full Train (Balanced Dataset)\n",
        "=== Final Evaluation on Test Sets ===\n",
        "- test_within -- total=198  F(TD)=72, C(N)=126\n",
        " Accuracy=0.8434 Sensitivity=0.7619 Specificity=0.9861 F1=0.8610\n",
        "\n",
        "- test_cross -- total=554  F(TD)=247, C(N)=307\n",
        " Accuracy=0.6949 Sensitivity=0.5863 Specificity=0.8300 F1=0.6805\n",
        "\n",
        "#### Full Train\n",
        "=== Final Evaluation on Test Sets ===\n",
        "- test_within -- total=198  F(TD)=72, C(N)=126\n",
        " Loss=0.3504 Acc=0.8838 Sens=0.8413 Spec=0.9583 F1=0.9021\n",
        "\n",
        "- test_cross -- total=554  F(TD)=247, C(N)=307\n",
        " Loss=0.7021 Acc=0.6769 Sens=0.6352 Spec=0.7287 F1=0.6854"
      ],
      "metadata": {
        "id": "o07DCKUSf4FI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### FTD vs CN LOSO Validation Result"
      ],
      "metadata": {
        "id": "K4oEoig4oqWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import copy\n",
        "import random\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from sklearn.model_selection import GroupKFold, GroupShuffleSplit\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "from eeg_dataset import EEGDataset\n",
        "from model_optimized_6 import EEGformer\n",
        "\n",
        "# ─── 재현성 설정 ─────────────────────────────────────────────────────\n",
        "SEED = 2\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark     = False\n",
        "\n",
        "def worker_init_fn(worker_id):\n",
        "    np.random.seed(SEED + worker_id)\n",
        "    random.seed(SEED + worker_id)\n",
        "\n",
        "# ─── 설정 ─────────────────────────────────────────────────────────\n",
        "DATA_DIR    = '/content/drive/MyDrive/2025_Lab_Research/model-data'\n",
        "LABEL_FILE  = 'labels.json'\n",
        "DEVICE      = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE   = 32\n",
        "MAX_EPOCHS   = 150   # 최대 epoch\n",
        "LR           = 2.2e-05\n",
        "WD           = 2.3e-06\n",
        "PCT_START    = 0.2\n",
        "ES_PATIENCE  = 15    # validation_loss 개선 없을 때 조기종료\n",
        "VAL_SIZE     = 0.2   # train_meta 중 validation 비율\n",
        "\n",
        "# ─── Dataset 래퍼 클래스 ──────────────────────────────────────────\n",
        "class BinaryEEGDataset(Dataset):\n",
        "    def __init__(self, raw_ds, meta):\n",
        "        self.raw_ds = raw_ds\n",
        "        self.meta   = meta\n",
        "    def __len__(self):\n",
        "        return len(self.raw_ds)\n",
        "    def __getitem__(self, idx):\n",
        "        x, _ = self.raw_ds[idx]\n",
        "        y    = self.meta[idx]['label']\n",
        "        return x, y\n",
        "\n",
        "# ─── subject_id 추출 함수 ─────────────────────────────────────────\n",
        "def extract_subject_id(m):\n",
        "    fn = os.path.basename(m['file_name'])\n",
        "    return fn.split('_')[0]\n",
        "\n",
        "# ─── 메타데이터 로드 및 필터링 ─────────────────────────────────────\n",
        "all_meta = json.load(open(os.path.join(DATA_DIR, LABEL_FILE), 'r'))\n",
        "all_meta = [m for m in all_meta if m['label'] in ('F','C')]\n",
        "groups   = [extract_subject_id(m) for m in all_meta]\n",
        "subjects = sorted(set(groups))\n",
        "\n",
        "# ─── LOSO 검증 셋업 ──────────────────────────────────────────────\n",
        "gkf = GroupKFold(n_splits=len(subjects))\n",
        "total_tp = total_tn = total_fp = total_fn = 0\n",
        "\n",
        "# ─── Fold별 학습/평가 루프 ───────────────────────────────────────\n",
        "for fold, (train_idx, test_idx) in enumerate(gkf.split(all_meta, groups=groups), 1):\n",
        "    print(f\"\\n>>> Fold {fold}/{len(subjects)}  (leave out {subjects[fold-1]})\")\n",
        "\n",
        "    # 메타 분할 및 라벨 매핑\n",
        "    train_meta = [copy.deepcopy(all_meta[i]) for i in train_idx]\n",
        "    test_meta  = [copy.deepcopy(all_meta[i]) for i in test_idx]\n",
        "    for m in train_meta: m['label'] = 0 if m['label']=='F' else 1\n",
        "    for m in test_meta:  m['label'] = 0 if m['label']=='F' else 1\n",
        "\n",
        "    # train_meta 안에서 validation split by subject\n",
        "    groups_train = [extract_subject_id(m) for m in train_meta]\n",
        "    gss = GroupShuffleSplit(n_splits=1, test_size=VAL_SIZE, random_state=SEED)\n",
        "    tr_idx2, val_idx = next(gss.split(train_meta, groups=groups_train))\n",
        "    val_meta   = [train_meta[i] for i in val_idx]\n",
        "    train_meta = [train_meta[i] for i in tr_idx2]\n",
        "\n",
        "    # DataLoader 준비\n",
        "    raw_train = EEGDataset(DATA_DIR, train_meta)\n",
        "    raw_val   = EEGDataset(DATA_DIR, val_meta)\n",
        "    raw_test  = EEGDataset(DATA_DIR, test_meta)\n",
        "    train_ds  = BinaryEEGDataset(raw_train, train_meta)\n",
        "    val_ds    = BinaryEEGDataset(raw_val,   val_meta)\n",
        "    test_ds   = BinaryEEGDataset(raw_test,  test_meta)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=4, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=4, worker_init_fn=worker_init_fn\n",
        "    )\n",
        "\n",
        "    # 모델/옵티마이저/스케줄러/손실 함수\n",
        "    input_len = train_ds[0][0].shape[-1]\n",
        "    model = EEGformer(\n",
        "        in_channels=19, input_length=input_len,\n",
        "        kernel_size=10, num_filters=120,\n",
        "        num_heads=3, num_blocks=1,\n",
        "        num_segments=5, num_classes=2\n",
        "    ).to(DEVICE)\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WD)\n",
        "    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
        "        optimizer, max_lr=LR, epochs=MAX_EPOCHS,\n",
        "        steps_per_epoch=len(train_loader), pct_start=PCT_START,\n",
        "        anneal_strategy='cos', cycle_momentum=False\n",
        "    )\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Early stopping 및 체크포인트 변수\n",
        "    best_val_loss = float('inf')\n",
        "    best_state    = None\n",
        "    es_count      = 0\n",
        "\n",
        "    # ─── 학습 + 검증 루프 ──────────────────────────────────────────\n",
        "    for epoch in range(1, MAX_EPOCHS+1):\n",
        "        t0 = time.time()\n",
        "        # Train\n",
        "        model.train()\n",
        "        train_loss_sum = 0.0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(X)\n",
        "            loss   = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            # dynamic weight decay\n",
        "            cur_lr = optimizer.param_groups[0]['lr']\n",
        "            for g in optimizer.param_groups:\n",
        "                g['weight_decay'] = WD * (cur_lr / LR)\n",
        "            train_loss_sum += loss.item()\n",
        "        avg_train_loss = train_loss_sum / len(train_loader)\n",
        "\n",
        "        # Validate\n",
        "        model.eval()\n",
        "        val_loss_sum = val_correct = val_total = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "                logits = model(X)\n",
        "                val_loss_sum += criterion(logits, y).item()\n",
        "                preds = logits.argmax(1)\n",
        "                val_correct += (preds == y).sum().item()\n",
        "                val_total += y.size(0)\n",
        "        avg_val_loss = val_loss_sum / len(val_loader)\n",
        "        avg_val_acc  = val_correct / val_total if val_total>0 else 0.0\n",
        "        elapsed = time.time() - t0\n",
        "\n",
        "        print(f\"  Fold {fold} Epoch {epoch}/{MAX_EPOCHS}  \"\n",
        "              f\"Train Loss={avg_train_loss:.4f}  \"\n",
        "              f\"Val Loss={avg_val_loss:.4f}  \"\n",
        "              f\"Val Acc={avg_val_acc:.4f}  \"\n",
        "              f\"Time={elapsed:.1f}s\")\n",
        "\n",
        "        # Early stopping & checkpoint on val_loss\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            best_state    = copy.deepcopy(model.state_dict())\n",
        "            es_count      = 0\n",
        "        else:\n",
        "            es_count += 1\n",
        "            if es_count >= ES_PATIENCE:\n",
        "                print(f\"  → Early stopping at epoch {epoch} (no val_loss improvement)\")\n",
        "                break\n",
        "\n",
        "    # 최적의 validation 시점 모델 로드\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "\n",
        "    # ─── 최종 테스트 & 혼동행렬 누적 ─────────────────────────────────\n",
        "    all_preds, all_labels = [], []\n",
        "    with torch.no_grad():\n",
        "        for X, y in test_loader:\n",
        "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
        "            preds = model(X).argmax(1)\n",
        "            all_preds.append(preds.cpu().numpy())\n",
        "            all_labels.append(y.cpu().numpy())\n",
        "    preds = np.concatenate(all_preds)\n",
        "    labs  = np.concatenate(all_labels)\n",
        "    tn, fp, fn, tp = confusion_matrix(labs, preds, labels=[0,1]).ravel()\n",
        "    total_tp += tp\n",
        "    total_tn += tn\n",
        "    total_fp += fp\n",
        "    total_fn += fn\n",
        "\n",
        "    acc = (tp + tn) / (tp + tn + fp + fn)\n",
        "    print(f\"  Fold {fold} Final ACC = {acc:.4f}   (TP={tp}  TN={tn}  FP={fp}  FN={fn})\")\n",
        "\n",
        "    # 메모리 해제\n",
        "    del model, optimizer, scheduler\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# ─── 전체 LOSO 지표 계산 ─────────────────────────────────────────\n",
        "ACC  = (total_tp + total_tn) / (total_tp + total_tn + total_fp + total_fn)\n",
        "SENS = total_tp / (total_tp + total_fn)\n",
        "SPEC = total_tn / (total_tn + total_fp)\n",
        "F1   = 2 * total_tp / (2*total_tp + total_fp + total_fn)\n",
        "\n",
        "print(\"\\n=== LOSO 평가 결과 ===\")\n",
        "print(f\"Accuracy   = {ACC:.4%}\")\n",
        "print(f\"Sensitivity= {SENS:.4%}\")\n",
        "print(f\"Specificity= {SPEC:.4%}\")\n",
        "print(f\"F1-score   = {F1:.4%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DktdX3SDpzBW",
        "outputId": "9b6808e3-c1e1-4675-992c-81bd45c88e4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Now using CUDA device 0\n",
            "Enabling CUDA with 39.14 GiB available memory\n",
            "\n",
            ">>> Fold 1/52  (leave out sub-037)\n",
            "  Fold 1 Epoch 1/150  Train Loss=0.7121  Val Loss=0.6467  Val Acc=0.6742  Time=15.0s\n",
            "  Fold 1 Epoch 2/150  Train Loss=0.7085  Val Loss=0.6412  Val Acc=0.6742  Time=14.2s\n",
            "  Fold 1 Epoch 3/150  Train Loss=0.7202  Val Loss=0.6421  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 4/150  Train Loss=0.7089  Val Loss=0.6364  Val Acc=0.6742  Time=14.2s\n",
            "  Fold 1 Epoch 5/150  Train Loss=0.7044  Val Loss=0.6454  Val Acc=0.6742  Time=14.2s\n",
            "  Fold 1 Epoch 6/150  Train Loss=0.7103  Val Loss=0.6391  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 7/150  Train Loss=0.7149  Val Loss=0.6419  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 8/150  Train Loss=0.7169  Val Loss=0.6406  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 1 Epoch 9/150  Train Loss=0.7191  Val Loss=0.6755  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 1 Epoch 10/150  Train Loss=0.7088  Val Loss=0.6369  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 11/150  Train Loss=0.7193  Val Loss=0.6383  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 12/150  Train Loss=0.7067  Val Loss=0.6349  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 13/150  Train Loss=0.7013  Val Loss=0.6333  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 1 Epoch 14/150  Train Loss=0.6963  Val Loss=0.6412  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 15/150  Train Loss=0.7076  Val Loss=0.6317  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 16/150  Train Loss=0.7002  Val Loss=0.6466  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 1 Epoch 17/150  Train Loss=0.7106  Val Loss=0.6358  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 1 Epoch 18/150  Train Loss=0.7112  Val Loss=0.6439  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 1 Epoch 19/150  Train Loss=0.6930  Val Loss=0.6347  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 20/150  Train Loss=0.6980  Val Loss=0.6397  Val Acc=0.6742  Time=14.2s\n",
            "  Fold 1 Epoch 21/150  Train Loss=0.7021  Val Loss=0.6424  Val Acc=0.6742  Time=14.2s\n",
            "  Fold 1 Epoch 22/150  Train Loss=0.7012  Val Loss=0.6418  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 1 Epoch 23/150  Train Loss=0.6823  Val Loss=0.6385  Val Acc=0.6875  Time=14.4s\n",
            "  Fold 1 Epoch 24/150  Train Loss=0.6570  Val Loss=0.6872  Val Acc=0.5606  Time=14.4s\n",
            "  Fold 1 Epoch 25/150  Train Loss=0.6329  Val Loss=0.5936  Val Acc=0.7898  Time=14.5s\n",
            "  Fold 1 Epoch 26/150  Train Loss=0.5936  Val Loss=0.6467  Val Acc=0.6761  Time=14.4s\n",
            "  Fold 1 Epoch 27/150  Train Loss=0.5651  Val Loss=0.6447  Val Acc=0.7008  Time=14.3s\n",
            "  Fold 1 Epoch 28/150  Train Loss=0.5368  Val Loss=0.6322  Val Acc=0.7746  Time=14.2s\n",
            "  Fold 1 Epoch 29/150  Train Loss=0.5157  Val Loss=0.6828  Val Acc=0.7102  Time=14.3s\n",
            "  Fold 1 Epoch 30/150  Train Loss=0.4978  Val Loss=0.6898  Val Acc=0.7330  Time=14.2s\n",
            "  Fold 1 Epoch 31/150  Train Loss=0.4839  Val Loss=0.7234  Val Acc=0.7670  Time=14.2s\n",
            "  Fold 1 Epoch 32/150  Train Loss=0.4779  Val Loss=0.7558  Val Acc=0.7045  Time=14.4s\n",
            "  Fold 1 Epoch 33/150  Train Loss=0.4685  Val Loss=0.7795  Val Acc=0.7102  Time=14.5s\n",
            "  Fold 1 Epoch 34/150  Train Loss=0.4583  Val Loss=0.7825  Val Acc=0.6932  Time=14.4s\n",
            "  Fold 1 Epoch 35/150  Train Loss=0.4517  Val Loss=0.7493  Val Acc=0.7254  Time=14.3s\n",
            "  Fold 1 Epoch 36/150  Train Loss=0.4417  Val Loss=0.7279  Val Acc=0.7330  Time=14.4s\n",
            "  Fold 1 Epoch 37/150  Train Loss=0.4435  Val Loss=0.7083  Val Acc=0.7273  Time=14.2s\n",
            "  Fold 1 Epoch 38/150  Train Loss=0.4496  Val Loss=0.7295  Val Acc=0.7462  Time=14.3s\n",
            "  Fold 1 Epoch 39/150  Train Loss=0.4364  Val Loss=0.7006  Val Acc=0.7443  Time=14.4s\n",
            "  Fold 1 Epoch 40/150  Train Loss=0.4260  Val Loss=0.7788  Val Acc=0.7102  Time=14.4s\n",
            "  → Early stopping at epoch 40 (no val_loss improvement)\n",
            "  Fold 1 Final ACC = 0.0308   (TP=0  TN=2  FP=63  FN=0)\n",
            "\n",
            ">>> Fold 2/52  (leave out sub-038)\n",
            "  Fold 2 Epoch 1/150  Train Loss=0.7455  Val Loss=0.7393  Val Acc=0.3984  Time=14.6s\n",
            "  Fold 2 Epoch 2/150  Train Loss=0.7277  Val Loss=0.7356  Val Acc=0.3984  Time=14.4s\n",
            "  Fold 2 Epoch 3/150  Train Loss=0.7283  Val Loss=0.7251  Val Acc=0.3984  Time=14.5s\n",
            "  Fold 2 Epoch 4/150  Train Loss=0.7241  Val Loss=0.7143  Val Acc=0.3984  Time=14.4s\n",
            "  Fold 2 Epoch 5/150  Train Loss=0.7252  Val Loss=0.7136  Val Acc=0.3984  Time=14.4s\n",
            "  Fold 2 Epoch 6/150  Train Loss=0.7245  Val Loss=0.7091  Val Acc=0.3984  Time=14.3s\n",
            "  Fold 2 Epoch 7/150  Train Loss=0.7201  Val Loss=0.7108  Val Acc=0.3984  Time=14.4s\n",
            "  Fold 2 Epoch 8/150  Train Loss=0.7117  Val Loss=0.7100  Val Acc=0.3984  Time=14.6s\n",
            "  Fold 2 Epoch 9/150  Train Loss=0.7103  Val Loss=0.7021  Val Acc=0.3984  Time=14.5s\n",
            "  Fold 2 Epoch 10/150  Train Loss=0.7165  Val Loss=0.7060  Val Acc=0.3984  Time=14.4s\n",
            "  Fold 2 Epoch 11/150  Train Loss=0.7087  Val Loss=0.6935  Val Acc=0.4990  Time=14.4s\n",
            "  Fold 2 Epoch 12/150  Train Loss=0.7093  Val Loss=0.6945  Val Acc=0.4103  Time=14.3s\n",
            "  Fold 2 Epoch 13/150  Train Loss=0.7062  Val Loss=0.6973  Val Acc=0.3984  Time=14.2s\n",
            "  Fold 2 Epoch 14/150  Train Loss=0.7053  Val Loss=0.6920  Val Acc=0.5700  Time=14.3s\n",
            "  Fold 2 Epoch 15/150  Train Loss=0.7104  Val Loss=0.6907  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 2 Epoch 16/150  Train Loss=0.7030  Val Loss=0.6874  Val Acc=0.6016  Time=14.7s\n",
            "  Fold 2 Epoch 17/150  Train Loss=0.7048  Val Loss=0.6904  Val Acc=0.6036  Time=14.4s\n",
            "  Fold 2 Epoch 18/150  Train Loss=0.7015  Val Loss=0.6794  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 2 Epoch 19/150  Train Loss=0.6969  Val Loss=0.6845  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 2 Epoch 20/150  Train Loss=0.6940  Val Loss=0.6784  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 2 Epoch 21/150  Train Loss=0.7020  Val Loss=0.6750  Val Acc=0.6016  Time=14.3s\n",
            "  Fold 2 Epoch 22/150  Train Loss=0.6987  Val Loss=0.6812  Val Acc=0.6016  Time=14.3s\n",
            "  Fold 2 Epoch 23/150  Train Loss=0.6918  Val Loss=0.6770  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 2 Epoch 24/150  Train Loss=0.6961  Val Loss=0.6890  Val Acc=0.6233  Time=14.6s\n",
            "  Fold 2 Epoch 25/150  Train Loss=0.6907  Val Loss=0.6749  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 2 Epoch 26/150  Train Loss=0.6896  Val Loss=0.6733  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 2 Epoch 27/150  Train Loss=0.6762  Val Loss=0.6579  Val Acc=0.6312  Time=14.5s\n",
            "  Fold 2 Epoch 28/150  Train Loss=0.6454  Val Loss=0.6429  Val Acc=0.6489  Time=14.6s\n",
            "  Fold 2 Epoch 29/150  Train Loss=0.6263  Val Loss=0.6342  Val Acc=0.6667  Time=14.6s\n",
            "  Fold 2 Epoch 30/150  Train Loss=0.6082  Val Loss=0.6219  Val Acc=0.6765  Time=14.8s\n",
            "  Fold 2 Epoch 31/150  Train Loss=0.5847  Val Loss=0.6106  Val Acc=0.6805  Time=14.6s\n",
            "  Fold 2 Epoch 32/150  Train Loss=0.5601  Val Loss=0.6002  Val Acc=0.6963  Time=14.4s\n",
            "  Fold 2 Epoch 33/150  Train Loss=0.5499  Val Loss=0.5908  Val Acc=0.7041  Time=14.3s\n",
            "  Fold 2 Epoch 34/150  Train Loss=0.5361  Val Loss=0.5897  Val Acc=0.7416  Time=14.3s\n",
            "  Fold 2 Epoch 35/150  Train Loss=0.5309  Val Loss=0.5921  Val Acc=0.7061  Time=14.3s\n",
            "  Fold 2 Epoch 36/150  Train Loss=0.5143  Val Loss=0.5888  Val Acc=0.7219  Time=14.4s\n",
            "  Fold 2 Epoch 37/150  Train Loss=0.5051  Val Loss=0.5968  Val Acc=0.7002  Time=14.3s\n",
            "  Fold 2 Epoch 38/150  Train Loss=0.5010  Val Loss=0.5884  Val Acc=0.7120  Time=14.5s\n",
            "  Fold 2 Epoch 39/150  Train Loss=0.4893  Val Loss=0.5905  Val Acc=0.7278  Time=14.6s\n",
            "  Fold 2 Epoch 40/150  Train Loss=0.4835  Val Loss=0.5871  Val Acc=0.7219  Time=14.4s\n",
            "  Fold 2 Epoch 41/150  Train Loss=0.4750  Val Loss=0.5841  Val Acc=0.7377  Time=14.4s\n",
            "  Fold 2 Epoch 42/150  Train Loss=0.4518  Val Loss=0.5878  Val Acc=0.7041  Time=14.4s\n",
            "  Fold 2 Epoch 43/150  Train Loss=0.4351  Val Loss=0.5928  Val Acc=0.7101  Time=14.4s\n",
            "  Fold 2 Epoch 44/150  Train Loss=0.4451  Val Loss=0.5828  Val Acc=0.7318  Time=14.4s\n",
            "  Fold 2 Epoch 45/150  Train Loss=0.4336  Val Loss=0.5905  Val Acc=0.7061  Time=14.3s\n",
            "  Fold 2 Epoch 46/150  Train Loss=0.4203  Val Loss=0.5918  Val Acc=0.7101  Time=14.7s\n",
            "  Fold 2 Epoch 47/150  Train Loss=0.4235  Val Loss=0.5884  Val Acc=0.7061  Time=14.5s\n",
            "  Fold 2 Epoch 48/150  Train Loss=0.4181  Val Loss=0.5879  Val Acc=0.7061  Time=14.4s\n",
            "  Fold 2 Epoch 49/150  Train Loss=0.4034  Val Loss=0.5877  Val Acc=0.7081  Time=14.3s\n",
            "  Fold 2 Epoch 50/150  Train Loss=0.3882  Val Loss=0.5845  Val Acc=0.7219  Time=14.5s\n",
            "  Fold 2 Epoch 51/150  Train Loss=0.3859  Val Loss=0.5891  Val Acc=0.7160  Time=14.3s\n",
            "  Fold 2 Epoch 52/150  Train Loss=0.3795  Val Loss=0.5881  Val Acc=0.7179  Time=14.3s\n",
            "  Fold 2 Epoch 53/150  Train Loss=0.3568  Val Loss=0.5873  Val Acc=0.7160  Time=14.4s\n",
            "  Fold 2 Epoch 54/150  Train Loss=0.3656  Val Loss=0.5964  Val Acc=0.7140  Time=14.6s\n",
            "  Fold 2 Epoch 55/150  Train Loss=0.3535  Val Loss=0.5983  Val Acc=0.7061  Time=14.6s\n",
            "  Fold 2 Epoch 56/150  Train Loss=0.3547  Val Loss=0.5955  Val Acc=0.7199  Time=14.3s\n",
            "  Fold 2 Epoch 57/150  Train Loss=0.3499  Val Loss=0.6017  Val Acc=0.7179  Time=14.2s\n",
            "  Fold 2 Epoch 58/150  Train Loss=0.3348  Val Loss=0.6077  Val Acc=0.7179  Time=14.4s\n",
            "  Fold 2 Epoch 59/150  Train Loss=0.3274  Val Loss=0.6182  Val Acc=0.7140  Time=14.4s\n",
            "  → Early stopping at epoch 59 (no val_loss improvement)\n",
            "  Fold 2 Final ACC = 0.6406   (TP=41  TN=0  FP=0  FN=23)\n",
            "\n",
            ">>> Fold 3/52  (leave out sub-039)\n",
            "  Fold 3 Epoch 1/150  Train Loss=0.7516  Val Loss=0.7264  Val Acc=0.3961  Time=14.3s\n",
            "  Fold 3 Epoch 2/150  Train Loss=0.7582  Val Loss=0.7242  Val Acc=0.3961  Time=14.5s\n",
            "  Fold 3 Epoch 3/150  Train Loss=0.7498  Val Loss=0.7078  Val Acc=0.3961  Time=14.5s\n",
            "  Fold 3 Epoch 4/150  Train Loss=0.7412  Val Loss=0.7085  Val Acc=0.3961  Time=14.3s\n",
            "  Fold 3 Epoch 5/150  Train Loss=0.7392  Val Loss=0.7019  Val Acc=0.3961  Time=14.3s\n",
            "  Fold 3 Epoch 6/150  Train Loss=0.7342  Val Loss=0.6933  Val Acc=0.4863  Time=14.4s\n",
            "  Fold 3 Epoch 7/150  Train Loss=0.7443  Val Loss=0.6946  Val Acc=0.4706  Time=14.5s\n",
            "  Fold 3 Epoch 8/150  Train Loss=0.7255  Val Loss=0.6895  Val Acc=0.5980  Time=14.4s\n",
            "  Fold 3 Epoch 9/150  Train Loss=0.7318  Val Loss=0.6872  Val Acc=0.6059  Time=14.5s\n",
            "  Fold 3 Epoch 10/150  Train Loss=0.7277  Val Loss=0.6869  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 3 Epoch 11/150  Train Loss=0.7309  Val Loss=0.6806  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 3 Epoch 12/150  Train Loss=0.7280  Val Loss=0.6765  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 3 Epoch 13/150  Train Loss=0.7222  Val Loss=0.6784  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 3 Epoch 14/150  Train Loss=0.7143  Val Loss=0.6759  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 3 Epoch 15/150  Train Loss=0.7271  Val Loss=0.6745  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 3 Epoch 16/150  Train Loss=0.7035  Val Loss=0.6709  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 3 Epoch 17/150  Train Loss=0.7138  Val Loss=0.6759  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 3 Epoch 18/150  Train Loss=0.7040  Val Loss=0.6699  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 3 Epoch 19/150  Train Loss=0.6997  Val Loss=0.6794  Val Acc=0.6059  Time=14.5s\n",
            "  Fold 3 Epoch 20/150  Train Loss=0.6978  Val Loss=0.6887  Val Acc=0.5745  Time=14.3s\n",
            "  Fold 3 Epoch 21/150  Train Loss=0.6866  Val Loss=0.6683  Val Acc=0.6294  Time=14.4s\n",
            "  Fold 3 Epoch 22/150  Train Loss=0.6793  Val Loss=0.6557  Val Acc=0.6275  Time=14.3s\n",
            "  Fold 3 Epoch 23/150  Train Loss=0.6497  Val Loss=0.6614  Val Acc=0.6647  Time=14.4s\n",
            "  Fold 3 Epoch 24/150  Train Loss=0.6271  Val Loss=0.6769  Val Acc=0.7157  Time=14.4s\n",
            "  Fold 3 Epoch 25/150  Train Loss=0.6147  Val Loss=0.6624  Val Acc=0.7020  Time=14.6s\n",
            "  Fold 3 Epoch 26/150  Train Loss=0.6050  Val Loss=0.6905  Val Acc=0.6118  Time=14.6s\n",
            "  Fold 3 Epoch 27/150  Train Loss=0.5949  Val Loss=0.6658  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 3 Epoch 28/150  Train Loss=0.5807  Val Loss=0.6683  Val Acc=0.7196  Time=14.4s\n",
            "  Fold 3 Epoch 29/150  Train Loss=0.5748  Val Loss=0.6682  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 3 Epoch 30/150  Train Loss=0.5780  Val Loss=0.6757  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 3 Epoch 31/150  Train Loss=0.5448  Val Loss=0.6708  Val Acc=0.7451  Time=14.3s\n",
            "  Fold 3 Epoch 32/150  Train Loss=0.5419  Val Loss=0.6672  Val Acc=0.7078  Time=14.3s\n",
            "  Fold 3 Epoch 33/150  Train Loss=0.5350  Val Loss=0.6714  Val Acc=0.7451  Time=14.6s\n",
            "  Fold 3 Epoch 34/150  Train Loss=0.5325  Val Loss=0.6713  Val Acc=0.7314  Time=14.5s\n",
            "  Fold 3 Epoch 35/150  Train Loss=0.5155  Val Loss=0.6677  Val Acc=0.7431  Time=14.5s\n",
            "  Fold 3 Epoch 36/150  Train Loss=0.4974  Val Loss=0.6685  Val Acc=0.7098  Time=14.4s\n",
            "  Fold 3 Epoch 37/150  Train Loss=0.4890  Val Loss=0.6682  Val Acc=0.7157  Time=14.4s\n",
            "  → Early stopping at epoch 37 (no val_loss improvement)\n",
            "  Fold 3 Final ACC = 1.0000   (TP=62  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 4/52  (leave out sub-040)\n",
            "  Fold 4 Epoch 1/150  Train Loss=0.7901  Val Loss=0.7320  Val Acc=0.3961  Time=14.5s\n",
            "  Fold 4 Epoch 2/150  Train Loss=0.7510  Val Loss=0.7062  Val Acc=0.3941  Time=14.3s\n",
            "  Fold 4 Epoch 3/150  Train Loss=0.7419  Val Loss=0.6935  Val Acc=0.5118  Time=14.5s\n",
            "  Fold 4 Epoch 4/150  Train Loss=0.7367  Val Loss=0.6888  Val Acc=0.5961  Time=14.5s\n",
            "  Fold 4 Epoch 5/150  Train Loss=0.7312  Val Loss=0.6882  Val Acc=0.5863  Time=14.3s\n",
            "  Fold 4 Epoch 6/150  Train Loss=0.7298  Val Loss=0.6980  Val Acc=0.4078  Time=14.4s\n",
            "  Fold 4 Epoch 7/150  Train Loss=0.7342  Val Loss=0.6922  Val Acc=0.5216  Time=14.3s\n",
            "  Fold 4 Epoch 8/150  Train Loss=0.7266  Val Loss=0.6963  Val Acc=0.4235  Time=14.4s\n",
            "  Fold 4 Epoch 9/150  Train Loss=0.7369  Val Loss=0.6851  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 4 Epoch 10/150  Train Loss=0.7248  Val Loss=0.6886  Val Acc=0.6059  Time=14.4s\n",
            "  Fold 4 Epoch 11/150  Train Loss=0.7227  Val Loss=0.6850  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 4 Epoch 12/150  Train Loss=0.7146  Val Loss=0.6766  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 4 Epoch 13/150  Train Loss=0.7133  Val Loss=0.6759  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 4 Epoch 14/150  Train Loss=0.7173  Val Loss=0.6743  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 4 Epoch 15/150  Train Loss=0.7136  Val Loss=0.6721  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 4 Epoch 16/150  Train Loss=0.7153  Val Loss=0.6726  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 4 Epoch 17/150  Train Loss=0.7055  Val Loss=0.6834  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 4 Epoch 18/150  Train Loss=0.7048  Val Loss=0.6773  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 4 Epoch 19/150  Train Loss=0.6999  Val Loss=0.6704  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 4 Epoch 20/150  Train Loss=0.6917  Val Loss=0.6756  Val Acc=0.6549  Time=14.5s\n",
            "  Fold 4 Epoch 21/150  Train Loss=0.6536  Val Loss=0.7045  Val Acc=0.5765  Time=14.4s\n",
            "  Fold 4 Epoch 22/150  Train Loss=0.6274  Val Loss=0.6771  Val Acc=0.6961  Time=14.3s\n",
            "  Fold 4 Epoch 23/150  Train Loss=0.5928  Val Loss=0.6742  Val Acc=0.7000  Time=14.4s\n",
            "  Fold 4 Epoch 24/150  Train Loss=0.5737  Val Loss=0.6686  Val Acc=0.7196  Time=14.8s\n",
            "  Fold 4 Epoch 25/150  Train Loss=0.5726  Val Loss=0.7028  Val Acc=0.6686  Time=14.5s\n",
            "  Fold 4 Epoch 26/150  Train Loss=0.5427  Val Loss=0.6774  Val Acc=0.6824  Time=15.0s\n",
            "  Fold 4 Epoch 27/150  Train Loss=0.5303  Val Loss=0.6969  Val Acc=0.7137  Time=14.9s\n",
            "  Fold 4 Epoch 28/150  Train Loss=0.5103  Val Loss=0.6825  Val Acc=0.7471  Time=15.0s\n",
            "  Fold 4 Epoch 29/150  Train Loss=0.5142  Val Loss=0.7121  Val Acc=0.6804  Time=15.0s\n",
            "  Fold 4 Epoch 30/150  Train Loss=0.4972  Val Loss=0.7253  Val Acc=0.7000  Time=14.8s\n",
            "  Fold 4 Epoch 31/150  Train Loss=0.5007  Val Loss=0.7150  Val Acc=0.7275  Time=15.1s\n",
            "  Fold 4 Epoch 32/150  Train Loss=0.4863  Val Loss=0.7226  Val Acc=0.7039  Time=15.6s\n",
            "  Fold 4 Epoch 33/150  Train Loss=0.4861  Val Loss=0.7210  Val Acc=0.7314  Time=15.3s\n",
            "  Fold 4 Epoch 34/150  Train Loss=0.4727  Val Loss=0.6973  Val Acc=0.7098  Time=15.1s\n",
            "  Fold 4 Epoch 35/150  Train Loss=0.4629  Val Loss=0.7097  Val Acc=0.7078  Time=15.2s\n",
            "  Fold 4 Epoch 36/150  Train Loss=0.4579  Val Loss=0.7163  Val Acc=0.7294  Time=15.2s\n",
            "  Fold 4 Epoch 37/150  Train Loss=0.4549  Val Loss=0.7124  Val Acc=0.7451  Time=15.4s\n",
            "  Fold 4 Epoch 38/150  Train Loss=0.4334  Val Loss=0.7224  Val Acc=0.7039  Time=15.2s\n",
            "  Fold 4 Epoch 39/150  Train Loss=0.4355  Val Loss=0.7184  Val Acc=0.7294  Time=15.1s\n",
            "  → Early stopping at epoch 39 (no val_loss improvement)\n",
            "  Fold 4 Final ACC = 0.8548   (TP=53  TN=0  FP=0  FN=9)\n",
            "\n",
            ">>> Fold 5/52  (leave out sub-041)\n",
            "  Fold 5 Epoch 1/150  Train Loss=0.8167  Val Loss=0.8663  Val Acc=0.3167  Time=14.5s\n",
            "  Fold 5 Epoch 2/150  Train Loss=0.7981  Val Loss=0.8355  Val Acc=0.3167  Time=14.4s\n",
            "  Fold 5 Epoch 3/150  Train Loss=0.8007  Val Loss=0.8067  Val Acc=0.3167  Time=14.5s\n",
            "  Fold 5 Epoch 4/150  Train Loss=0.7918  Val Loss=0.7918  Val Acc=0.3167  Time=14.5s\n",
            "  Fold 5 Epoch 5/150  Train Loss=0.7757  Val Loss=0.7689  Val Acc=0.3167  Time=14.3s\n",
            "  Fold 5 Epoch 6/150  Train Loss=0.7532  Val Loss=0.7440  Val Acc=0.3167  Time=14.3s\n",
            "  Fold 5 Epoch 7/150  Train Loss=0.7552  Val Loss=0.7178  Val Acc=0.3167  Time=14.4s\n",
            "  Fold 5 Epoch 8/150  Train Loss=0.7297  Val Loss=0.6902  Val Acc=0.6027  Time=14.4s\n",
            "  Fold 5 Epoch 9/150  Train Loss=0.7149  Val Loss=0.6717  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 5 Epoch 10/150  Train Loss=0.7217  Val Loss=0.6616  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 5 Epoch 11/150  Train Loss=0.7179  Val Loss=0.6641  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 5 Epoch 12/150  Train Loss=0.7118  Val Loss=0.6530  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 5 Epoch 13/150  Train Loss=0.7084  Val Loss=0.6621  Val Acc=0.6833  Time=14.3s\n",
            "  Fold 5 Epoch 14/150  Train Loss=0.7047  Val Loss=0.6806  Val Acc=0.6948  Time=14.3s\n",
            "  Fold 5 Epoch 15/150  Train Loss=0.6986  Val Loss=0.6561  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 5 Epoch 16/150  Train Loss=0.6993  Val Loss=0.6794  Val Acc=0.6891  Time=14.4s\n",
            "  Fold 5 Epoch 17/150  Train Loss=0.6968  Val Loss=0.6367  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 5 Epoch 18/150  Train Loss=0.6713  Val Loss=0.6360  Val Acc=0.7025  Time=14.4s\n",
            "  Fold 5 Epoch 19/150  Train Loss=0.6540  Val Loss=0.5917  Val Acc=0.7869  Time=14.6s\n",
            "  Fold 5 Epoch 20/150  Train Loss=0.6250  Val Loss=0.5909  Val Acc=0.7505  Time=14.4s\n",
            "  Fold 5 Epoch 21/150  Train Loss=0.6048  Val Loss=0.5077  Val Acc=0.8464  Time=14.5s\n",
            "  Fold 5 Epoch 22/150  Train Loss=0.5974  Val Loss=0.5940  Val Acc=0.6679  Time=14.4s\n",
            "  Fold 5 Epoch 23/150  Train Loss=0.5686  Val Loss=0.5167  Val Acc=0.8273  Time=14.3s\n",
            "  Fold 5 Epoch 24/150  Train Loss=0.5625  Val Loss=0.5052  Val Acc=0.8253  Time=14.4s\n",
            "  Fold 5 Epoch 25/150  Train Loss=0.5453  Val Loss=0.5123  Val Acc=0.7524  Time=14.4s\n",
            "  Fold 5 Epoch 26/150  Train Loss=0.5332  Val Loss=0.5812  Val Acc=0.6756  Time=14.6s\n",
            "  Fold 5 Epoch 27/150  Train Loss=0.5138  Val Loss=0.5434  Val Acc=0.7044  Time=14.7s\n",
            "  Fold 5 Epoch 28/150  Train Loss=0.5117  Val Loss=0.5014  Val Acc=0.7678  Time=14.5s\n",
            "  Fold 5 Epoch 29/150  Train Loss=0.4942  Val Loss=0.5152  Val Acc=0.7236  Time=14.4s\n",
            "  Fold 5 Epoch 30/150  Train Loss=0.4918  Val Loss=0.5294  Val Acc=0.7351  Time=14.5s\n",
            "  Fold 5 Epoch 31/150  Train Loss=0.4783  Val Loss=0.5055  Val Acc=0.7562  Time=14.4s\n",
            "  Fold 5 Epoch 32/150  Train Loss=0.4619  Val Loss=0.5939  Val Acc=0.6679  Time=14.5s\n",
            "  Fold 5 Epoch 33/150  Train Loss=0.4617  Val Loss=0.4535  Val Acc=0.7927  Time=14.5s\n",
            "  Fold 5 Epoch 34/150  Train Loss=0.4485  Val Loss=0.4651  Val Acc=0.7889  Time=14.6s\n",
            "  Fold 5 Epoch 35/150  Train Loss=0.4416  Val Loss=0.6051  Val Acc=0.6737  Time=14.4s\n",
            "  Fold 5 Epoch 36/150  Train Loss=0.4301  Val Loss=0.4218  Val Acc=0.8330  Time=14.4s\n",
            "  Fold 5 Epoch 37/150  Train Loss=0.4241  Val Loss=0.5182  Val Acc=0.7466  Time=14.4s\n",
            "  Fold 5 Epoch 38/150  Train Loss=0.4236  Val Loss=0.5379  Val Acc=0.7159  Time=14.4s\n",
            "  Fold 5 Epoch 39/150  Train Loss=0.4055  Val Loss=0.4020  Val Acc=0.8349  Time=14.2s\n",
            "  Fold 5 Epoch 40/150  Train Loss=0.4054  Val Loss=0.3968  Val Acc=0.8388  Time=14.5s\n",
            "  Fold 5 Epoch 41/150  Train Loss=0.3922  Val Loss=0.4212  Val Acc=0.8253  Time=14.4s\n",
            "  Fold 5 Epoch 42/150  Train Loss=0.3880  Val Loss=0.4413  Val Acc=0.8061  Time=14.5s\n",
            "  Fold 5 Epoch 43/150  Train Loss=0.3922  Val Loss=0.4465  Val Acc=0.7985  Time=14.5s\n",
            "  Fold 5 Epoch 44/150  Train Loss=0.3583  Val Loss=0.4772  Val Acc=0.7908  Time=14.4s\n",
            "  Fold 5 Epoch 45/150  Train Loss=0.3603  Val Loss=0.5153  Val Acc=0.7658  Time=14.5s\n",
            "  Fold 5 Epoch 46/150  Train Loss=0.3547  Val Loss=0.5174  Val Acc=0.7735  Time=14.3s\n",
            "  Fold 5 Epoch 47/150  Train Loss=0.3466  Val Loss=0.6263  Val Acc=0.7025  Time=14.5s\n",
            "  Fold 5 Epoch 48/150  Train Loss=0.3458  Val Loss=0.5046  Val Acc=0.7793  Time=14.4s\n",
            "  Fold 5 Epoch 49/150  Train Loss=0.3293  Val Loss=0.6595  Val Acc=0.6871  Time=14.4s\n",
            "  Fold 5 Epoch 50/150  Train Loss=0.3467  Val Loss=0.6089  Val Acc=0.7025  Time=14.6s\n",
            "  Fold 5 Epoch 51/150  Train Loss=0.3309  Val Loss=0.5216  Val Acc=0.7716  Time=14.4s\n",
            "  Fold 5 Epoch 52/150  Train Loss=0.3204  Val Loss=0.4413  Val Acc=0.8215  Time=14.4s\n",
            "  Fold 5 Epoch 53/150  Train Loss=0.3226  Val Loss=0.4541  Val Acc=0.8177  Time=14.3s\n",
            "  Fold 5 Epoch 54/150  Train Loss=0.3031  Val Loss=0.5610  Val Acc=0.7543  Time=14.4s\n",
            "  Fold 5 Epoch 55/150  Train Loss=0.3168  Val Loss=0.5971  Val Acc=0.7351  Time=14.3s\n",
            "  → Early stopping at epoch 55 (no val_loss improvement)\n",
            "  Fold 5 Final ACC = 0.0000   (TP=0  TN=0  FP=59  FN=0)\n",
            "\n",
            ">>> Fold 6/52  (leave out sub-042)\n",
            "  Fold 6 Epoch 1/150  Train Loss=0.9369  Val Loss=0.8588  Val Acc=0.3167  Time=14.4s\n",
            "  Fold 6 Epoch 2/150  Train Loss=0.8696  Val Loss=0.8014  Val Acc=0.3167  Time=14.6s\n",
            "  Fold 6 Epoch 3/150  Train Loss=0.8182  Val Loss=0.7127  Val Acc=0.3167  Time=14.4s\n",
            "  Fold 6 Epoch 4/150  Train Loss=0.7808  Val Loss=0.6982  Val Acc=0.3436  Time=14.4s\n",
            "  Fold 6 Epoch 5/150  Train Loss=0.7524  Val Loss=0.6764  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 6/150  Train Loss=0.7397  Val Loss=0.6712  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 7/150  Train Loss=0.7432  Val Loss=0.6584  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 8/150  Train Loss=0.7294  Val Loss=0.6592  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 9/150  Train Loss=0.7305  Val Loss=0.6589  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 10/150  Train Loss=0.7161  Val Loss=0.6547  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 6 Epoch 11/150  Train Loss=0.7171  Val Loss=0.6517  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 12/150  Train Loss=0.7303  Val Loss=0.6532  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 13/150  Train Loss=0.7174  Val Loss=0.6500  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 14/150  Train Loss=0.7201  Val Loss=0.6513  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 15/150  Train Loss=0.7175  Val Loss=0.6452  Val Acc=0.6833  Time=14.2s\n",
            "  Fold 6 Epoch 16/150  Train Loss=0.7014  Val Loss=0.6486  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 17/150  Train Loss=0.7024  Val Loss=0.6501  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 18/150  Train Loss=0.7110  Val Loss=0.6466  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 19/150  Train Loss=0.7051  Val Loss=0.6424  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 20/150  Train Loss=0.7073  Val Loss=0.6427  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 21/150  Train Loss=0.7091  Val Loss=0.6345  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 22/150  Train Loss=0.7095  Val Loss=0.6343  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 23/150  Train Loss=0.7052  Val Loss=0.6436  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 6 Epoch 24/150  Train Loss=0.7048  Val Loss=0.6424  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 6 Epoch 25/150  Train Loss=0.7023  Val Loss=0.6145  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 6 Epoch 26/150  Train Loss=0.6949  Val Loss=0.6514  Val Acc=0.7543  Time=14.8s\n",
            "  Fold 6 Epoch 27/150  Train Loss=0.6834  Val Loss=0.6315  Val Acc=0.6814  Time=14.5s\n",
            "  Fold 6 Epoch 28/150  Train Loss=0.6674  Val Loss=0.6314  Val Acc=0.6219  Time=14.4s\n",
            "  Fold 6 Epoch 29/150  Train Loss=0.6352  Val Loss=0.5803  Val Acc=0.6948  Time=14.3s\n",
            "  Fold 6 Epoch 30/150  Train Loss=0.6340  Val Loss=0.7432  Val Acc=0.4837  Time=14.5s\n",
            "  Fold 6 Epoch 31/150  Train Loss=0.5959  Val Loss=0.4965  Val Acc=0.8004  Time=14.4s\n",
            "  Fold 6 Epoch 32/150  Train Loss=0.5832  Val Loss=0.4823  Val Acc=0.7927  Time=14.5s\n",
            "  Fold 6 Epoch 33/150  Train Loss=0.5641  Val Loss=0.4958  Val Acc=0.7697  Time=14.6s\n",
            "  Fold 6 Epoch 34/150  Train Loss=0.5510  Val Loss=0.4713  Val Acc=0.8177  Time=14.5s\n",
            "  Fold 6 Epoch 35/150  Train Loss=0.5434  Val Loss=0.5920  Val Acc=0.6718  Time=14.6s\n",
            "  Fold 6 Epoch 36/150  Train Loss=0.5315  Val Loss=0.5579  Val Acc=0.6948  Time=14.3s\n",
            "  Fold 6 Epoch 37/150  Train Loss=0.5099  Val Loss=0.5276  Val Acc=0.7562  Time=14.5s\n",
            "  Fold 6 Epoch 38/150  Train Loss=0.4981  Val Loss=0.4903  Val Acc=0.7965  Time=14.3s\n",
            "  Fold 6 Epoch 39/150  Train Loss=0.4897  Val Loss=0.6082  Val Acc=0.6775  Time=14.5s\n",
            "  Fold 6 Epoch 40/150  Train Loss=0.4874  Val Loss=0.5850  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 6 Epoch 41/150  Train Loss=0.4593  Val Loss=0.5659  Val Acc=0.7121  Time=14.5s\n",
            "  Fold 6 Epoch 42/150  Train Loss=0.4565  Val Loss=0.4878  Val Acc=0.7985  Time=14.3s\n",
            "  Fold 6 Epoch 43/150  Train Loss=0.4479  Val Loss=0.4945  Val Acc=0.7889  Time=14.4s\n",
            "  Fold 6 Epoch 44/150  Train Loss=0.4236  Val Loss=0.6012  Val Acc=0.7217  Time=14.4s\n",
            "  Fold 6 Epoch 45/150  Train Loss=0.4089  Val Loss=0.4531  Val Acc=0.8349  Time=14.4s\n",
            "  Fold 6 Epoch 46/150  Train Loss=0.3950  Val Loss=0.5380  Val Acc=0.7620  Time=14.3s\n",
            "  Fold 6 Epoch 47/150  Train Loss=0.3824  Val Loss=0.6487  Val Acc=0.7025  Time=14.5s\n",
            "  Fold 6 Epoch 48/150  Train Loss=0.3760  Val Loss=0.6720  Val Acc=0.6987  Time=14.6s\n",
            "  Fold 6 Epoch 49/150  Train Loss=0.3645  Val Loss=0.6534  Val Acc=0.7179  Time=14.5s\n",
            "  Fold 6 Epoch 50/150  Train Loss=0.3446  Val Loss=0.5858  Val Acc=0.7601  Time=14.4s\n",
            "  Fold 6 Epoch 51/150  Train Loss=0.3479  Val Loss=0.7669  Val Acc=0.6699  Time=14.4s\n",
            "  Fold 6 Epoch 52/150  Train Loss=0.3395  Val Loss=0.5691  Val Acc=0.7639  Time=14.4s\n",
            "  Fold 6 Epoch 53/150  Train Loss=0.3273  Val Loss=0.6753  Val Acc=0.7159  Time=14.5s\n",
            "  Fold 6 Epoch 54/150  Train Loss=0.3155  Val Loss=0.8158  Val Acc=0.6526  Time=14.3s\n",
            "  Fold 6 Epoch 55/150  Train Loss=0.3257  Val Loss=0.5936  Val Acc=0.7582  Time=14.6s\n",
            "  Fold 6 Epoch 56/150  Train Loss=0.3123  Val Loss=0.7525  Val Acc=0.7025  Time=14.6s\n",
            "  Fold 6 Epoch 57/150  Train Loss=0.3147  Val Loss=1.0580  Val Acc=0.5643  Time=14.4s\n",
            "  Fold 6 Epoch 58/150  Train Loss=0.3026  Val Loss=0.8315  Val Acc=0.6718  Time=14.4s\n",
            "  Fold 6 Epoch 59/150  Train Loss=0.2992  Val Loss=0.6708  Val Acc=0.7236  Time=14.3s\n",
            "  Fold 6 Epoch 60/150  Train Loss=0.2854  Val Loss=0.6807  Val Acc=0.7294  Time=14.4s\n",
            "  → Early stopping at epoch 60 (no val_loss improvement)\n",
            "  Fold 6 Final ACC = 0.6897   (TP=0  TN=40  FP=18  FN=0)\n",
            "\n",
            ">>> Fold 7/52  (leave out sub-043)\n",
            "  Fold 7 Epoch 1/150  Train Loss=0.7357  Val Loss=0.6873  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 7 Epoch 2/150  Train Loss=0.7425  Val Loss=0.6851  Val Acc=0.5936  Time=14.7s\n",
            "  Fold 7 Epoch 3/150  Train Loss=0.7347  Val Loss=0.6786  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 7 Epoch 4/150  Train Loss=0.7336  Val Loss=0.6742  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 5/150  Train Loss=0.7100  Val Loss=0.6719  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 6/150  Train Loss=0.7055  Val Loss=0.6707  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 7 Epoch 7/150  Train Loss=0.7061  Val Loss=0.6710  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 8/150  Train Loss=0.7070  Val Loss=0.6718  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 9/150  Train Loss=0.7015  Val Loss=0.6708  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 7 Epoch 10/150  Train Loss=0.7001  Val Loss=0.6723  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 7 Epoch 11/150  Train Loss=0.6933  Val Loss=0.6719  Val Acc=0.5976  Time=14.3s\n",
            "  Fold 7 Epoch 12/150  Train Loss=0.6962  Val Loss=0.6708  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 13/150  Train Loss=0.7120  Val Loss=0.6707  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 7 Epoch 14/150  Train Loss=0.6894  Val Loss=0.6710  Val Acc=0.5976  Time=14.3s\n",
            "  Fold 7 Epoch 15/150  Train Loss=0.7049  Val Loss=0.6706  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 16/150  Train Loss=0.6935  Val Loss=0.6711  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 7 Epoch 17/150  Train Loss=0.6940  Val Loss=0.6700  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 7 Epoch 18/150  Train Loss=0.6945  Val Loss=0.6692  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 7 Epoch 19/150  Train Loss=0.6901  Val Loss=0.6735  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 7 Epoch 20/150  Train Loss=0.6781  Val Loss=0.6589  Val Acc=0.6235  Time=14.5s\n",
            "  Fold 7 Epoch 21/150  Train Loss=0.6534  Val Loss=0.6527  Val Acc=0.6474  Time=14.5s\n",
            "  Fold 7 Epoch 22/150  Train Loss=0.6167  Val Loss=0.6946  Val Acc=0.6076  Time=14.4s\n",
            "  Fold 7 Epoch 23/150  Train Loss=0.5923  Val Loss=0.6864  Val Acc=0.5956  Time=14.5s\n",
            "  Fold 7 Epoch 24/150  Train Loss=0.5680  Val Loss=0.7116  Val Acc=0.6235  Time=14.6s\n",
            "  Fold 7 Epoch 25/150  Train Loss=0.5472  Val Loss=0.6785  Val Acc=0.6135  Time=14.7s\n",
            "  Fold 7 Epoch 26/150  Train Loss=0.5312  Val Loss=0.6654  Val Acc=0.6454  Time=14.4s\n",
            "  Fold 7 Epoch 27/150  Train Loss=0.5201  Val Loss=0.6625  Val Acc=0.6295  Time=14.4s\n",
            "  Fold 7 Epoch 28/150  Train Loss=0.5297  Val Loss=0.6903  Val Acc=0.6434  Time=14.4s\n",
            "  Fold 7 Epoch 29/150  Train Loss=0.4919  Val Loss=0.7098  Val Acc=0.6454  Time=14.5s\n",
            "  Fold 7 Epoch 30/150  Train Loss=0.4892  Val Loss=0.6754  Val Acc=0.6554  Time=14.5s\n",
            "  Fold 7 Epoch 31/150  Train Loss=0.4800  Val Loss=0.7400  Val Acc=0.6315  Time=14.6s\n",
            "  Fold 7 Epoch 32/150  Train Loss=0.4851  Val Loss=0.7317  Val Acc=0.6275  Time=14.7s\n",
            "  Fold 7 Epoch 33/150  Train Loss=0.4740  Val Loss=0.7078  Val Acc=0.6335  Time=14.5s\n",
            "  Fold 7 Epoch 34/150  Train Loss=0.4491  Val Loss=0.7295  Val Acc=0.6295  Time=14.5s\n",
            "  Fold 7 Epoch 35/150  Train Loss=0.4462  Val Loss=0.7240  Val Acc=0.6215  Time=14.5s\n",
            "  Fold 7 Epoch 36/150  Train Loss=0.4348  Val Loss=0.7314  Val Acc=0.6315  Time=14.4s\n",
            "  → Early stopping at epoch 36 (no val_loss improvement)\n",
            "  Fold 7 Final ACC = 1.0000   (TP=57  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 8/52  (leave out sub-044)\n",
            "  Fold 8 Epoch 1/150  Train Loss=0.7175  Val Loss=0.6807  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 8 Epoch 2/150  Train Loss=0.7152  Val Loss=0.6764  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 8 Epoch 3/150  Train Loss=0.7206  Val Loss=0.6726  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 8 Epoch 4/150  Train Loss=0.7080  Val Loss=0.6718  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 8 Epoch 5/150  Train Loss=0.7124  Val Loss=0.6707  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 8 Epoch 6/150  Train Loss=0.7008  Val Loss=0.6714  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 8 Epoch 7/150  Train Loss=0.6986  Val Loss=0.6735  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 8 Epoch 8/150  Train Loss=0.6992  Val Loss=0.6723  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 8 Epoch 9/150  Train Loss=0.7006  Val Loss=0.6735  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 8 Epoch 10/150  Train Loss=0.6963  Val Loss=0.6737  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 8 Epoch 11/150  Train Loss=0.6909  Val Loss=0.6716  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 8 Epoch 12/150  Train Loss=0.6992  Val Loss=0.6736  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 8 Epoch 13/150  Train Loss=0.6979  Val Loss=0.6755  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 8 Epoch 14/150  Train Loss=0.6937  Val Loss=0.6701  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 8 Epoch 15/150  Train Loss=0.6933  Val Loss=0.6753  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 8 Epoch 16/150  Train Loss=0.6874  Val Loss=0.6688  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 8 Epoch 17/150  Train Loss=0.6836  Val Loss=0.6763  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 8 Epoch 18/150  Train Loss=0.6584  Val Loss=0.6712  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 8 Epoch 19/150  Train Loss=0.6475  Val Loss=0.7048  Val Acc=0.6431  Time=14.8s\n",
            "  Fold 8 Epoch 20/150  Train Loss=0.6064  Val Loss=0.6449  Val Acc=0.6549  Time=14.7s\n",
            "  Fold 8 Epoch 21/150  Train Loss=0.5760  Val Loss=0.6498  Val Acc=0.6667  Time=14.6s\n",
            "  Fold 8 Epoch 22/150  Train Loss=0.5772  Val Loss=0.6510  Val Acc=0.6588  Time=14.4s\n",
            "  Fold 8 Epoch 23/150  Train Loss=0.5562  Val Loss=0.6356  Val Acc=0.7176  Time=14.4s\n",
            "  Fold 8 Epoch 24/150  Train Loss=0.5407  Val Loss=0.6320  Val Acc=0.7059  Time=14.5s\n",
            "  Fold 8 Epoch 25/150  Train Loss=0.5323  Val Loss=0.6374  Val Acc=0.7392  Time=14.6s\n",
            "  Fold 8 Epoch 26/150  Train Loss=0.5263  Val Loss=0.6221  Val Acc=0.7275  Time=14.5s\n",
            "  Fold 8 Epoch 27/150  Train Loss=0.5062  Val Loss=0.6268  Val Acc=0.6882  Time=14.3s\n",
            "  Fold 8 Epoch 28/150  Train Loss=0.4948  Val Loss=0.6192  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 8 Epoch 29/150  Train Loss=0.4968  Val Loss=0.6443  Val Acc=0.6922  Time=14.3s\n",
            "  Fold 8 Epoch 30/150  Train Loss=0.4811  Val Loss=0.6249  Val Acc=0.7255  Time=14.3s\n",
            "  Fold 8 Epoch 31/150  Train Loss=0.4850  Val Loss=0.6232  Val Acc=0.7059  Time=14.5s\n",
            "  Fold 8 Epoch 32/150  Train Loss=0.4619  Val Loss=0.6148  Val Acc=0.7471  Time=14.5s\n",
            "  Fold 8 Epoch 33/150  Train Loss=0.4568  Val Loss=0.6062  Val Acc=0.7275  Time=14.6s\n",
            "  Fold 8 Epoch 34/150  Train Loss=0.4507  Val Loss=0.5989  Val Acc=0.7157  Time=14.4s\n",
            "  Fold 8 Epoch 35/150  Train Loss=0.4517  Val Loss=0.6079  Val Acc=0.7196  Time=14.6s\n",
            "  Fold 8 Epoch 36/150  Train Loss=0.4435  Val Loss=0.6156  Val Acc=0.7078  Time=14.6s\n",
            "  Fold 8 Epoch 37/150  Train Loss=0.4300  Val Loss=0.6702  Val Acc=0.6667  Time=14.4s\n",
            "  Fold 8 Epoch 38/150  Train Loss=0.4082  Val Loss=0.6118  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 8 Epoch 39/150  Train Loss=0.3931  Val Loss=0.6079  Val Acc=0.7000  Time=14.5s\n",
            "  Fold 8 Epoch 40/150  Train Loss=0.3922  Val Loss=0.6058  Val Acc=0.7039  Time=14.7s\n",
            "  Fold 8 Epoch 41/150  Train Loss=0.3885  Val Loss=0.6356  Val Acc=0.7078  Time=14.5s\n",
            "  Fold 8 Epoch 42/150  Train Loss=0.3889  Val Loss=0.6212  Val Acc=0.7176  Time=14.4s\n",
            "  Fold 8 Epoch 43/150  Train Loss=0.3679  Val Loss=0.5969  Val Acc=0.6941  Time=14.5s\n",
            "  Fold 8 Epoch 44/150  Train Loss=0.3739  Val Loss=0.5931  Val Acc=0.7000  Time=14.3s\n",
            "  Fold 8 Epoch 45/150  Train Loss=0.3538  Val Loss=0.6380  Val Acc=0.7216  Time=14.3s\n",
            "  Fold 8 Epoch 46/150  Train Loss=0.3541  Val Loss=0.6109  Val Acc=0.7118  Time=14.5s\n",
            "  Fold 8 Epoch 47/150  Train Loss=0.3315  Val Loss=0.6392  Val Acc=0.7137  Time=14.6s\n",
            "  Fold 8 Epoch 48/150  Train Loss=0.3506  Val Loss=0.6150  Val Acc=0.6961  Time=14.6s\n",
            "  Fold 8 Epoch 49/150  Train Loss=0.3259  Val Loss=0.6357  Val Acc=0.7176  Time=14.4s\n",
            "  Fold 8 Epoch 50/150  Train Loss=0.3290  Val Loss=0.6498  Val Acc=0.7157  Time=14.6s\n",
            "  Fold 8 Epoch 51/150  Train Loss=0.3164  Val Loss=0.6201  Val Acc=0.7137  Time=14.3s\n",
            "  Fold 8 Epoch 52/150  Train Loss=0.3172  Val Loss=0.6940  Val Acc=0.6922  Time=14.4s\n",
            "  Fold 8 Epoch 53/150  Train Loss=0.3338  Val Loss=0.6378  Val Acc=0.6961  Time=14.5s\n",
            "  Fold 8 Epoch 54/150  Train Loss=0.3131  Val Loss=0.6375  Val Acc=0.7098  Time=14.6s\n",
            "  Fold 8 Epoch 55/150  Train Loss=0.2948  Val Loss=0.6296  Val Acc=0.7020  Time=14.6s\n",
            "  Fold 8 Epoch 56/150  Train Loss=0.2924  Val Loss=0.6313  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 8 Epoch 57/150  Train Loss=0.2923  Val Loss=0.6615  Val Acc=0.7020  Time=14.4s\n",
            "  Fold 8 Epoch 58/150  Train Loss=0.2887  Val Loss=0.6881  Val Acc=0.6882  Time=14.4s\n",
            "  Fold 8 Epoch 59/150  Train Loss=0.2918  Val Loss=0.6864  Val Acc=0.6941  Time=14.3s\n",
            "  → Early stopping at epoch 59 (no val_loss improvement)\n",
            "  Fold 8 Final ACC = 0.9123   (TP=52  TN=0  FP=0  FN=5)\n",
            "\n",
            ">>> Fold 9/52  (leave out sub-045)\n",
            "  Fold 9 Epoch 1/150  Train Loss=0.7138  Val Loss=0.6726  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 2/150  Train Loss=0.7105  Val Loss=0.6717  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 3/150  Train Loss=0.7075  Val Loss=0.6715  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 9 Epoch 4/150  Train Loss=0.7037  Val Loss=0.6711  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 9 Epoch 5/150  Train Loss=0.7026  Val Loss=0.6712  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 9 Epoch 6/150  Train Loss=0.7082  Val Loss=0.6710  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 9 Epoch 7/150  Train Loss=0.7059  Val Loss=0.6711  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 8/150  Train Loss=0.7061  Val Loss=0.6708  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 9/150  Train Loss=0.7025  Val Loss=0.6714  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 9 Epoch 10/150  Train Loss=0.6969  Val Loss=0.6710  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 9 Epoch 11/150  Train Loss=0.7023  Val Loss=0.6709  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 9 Epoch 12/150  Train Loss=0.7036  Val Loss=0.6733  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 13/150  Train Loss=0.7030  Val Loss=0.6710  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 9 Epoch 14/150  Train Loss=0.6946  Val Loss=0.6736  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 9 Epoch 15/150  Train Loss=0.7054  Val Loss=0.6736  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 9 Epoch 16/150  Train Loss=0.6932  Val Loss=0.6713  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 17/150  Train Loss=0.6936  Val Loss=0.6730  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 9 Epoch 18/150  Train Loss=0.6950  Val Loss=0.6719  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 9 Epoch 19/150  Train Loss=0.6914  Val Loss=0.6711  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 20/150  Train Loss=0.6961  Val Loss=0.6711  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 9 Epoch 21/150  Train Loss=0.6934  Val Loss=0.6708  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 9 Epoch 22/150  Train Loss=0.6927  Val Loss=0.6723  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 23/150  Train Loss=0.6842  Val Loss=0.6689  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 9 Epoch 24/150  Train Loss=0.6750  Val Loss=0.6670  Val Acc=0.6157  Time=14.6s\n",
            "  Fold 9 Epoch 25/150  Train Loss=0.6555  Val Loss=0.6598  Val Acc=0.6490  Time=14.7s\n",
            "  Fold 9 Epoch 26/150  Train Loss=0.6467  Val Loss=0.6446  Val Acc=0.6569  Time=14.4s\n",
            "  Fold 9 Epoch 27/150  Train Loss=0.6158  Val Loss=0.6366  Val Acc=0.6569  Time=14.4s\n",
            "  Fold 9 Epoch 28/150  Train Loss=0.5845  Val Loss=0.6476  Val Acc=0.6784  Time=14.4s\n",
            "  Fold 9 Epoch 29/150  Train Loss=0.5541  Val Loss=0.6344  Val Acc=0.6961  Time=14.4s\n",
            "  Fold 9 Epoch 30/150  Train Loss=0.5278  Val Loss=0.6296  Val Acc=0.6882  Time=14.4s\n",
            "  Fold 9 Epoch 31/150  Train Loss=0.5254  Val Loss=0.6286  Val Acc=0.6941  Time=14.5s\n",
            "  Fold 9 Epoch 32/150  Train Loss=0.5016  Val Loss=0.6236  Val Acc=0.6843  Time=14.7s\n",
            "  Fold 9 Epoch 33/150  Train Loss=0.4979  Val Loss=0.6001  Val Acc=0.7078  Time=14.7s\n",
            "  Fold 9 Epoch 34/150  Train Loss=0.4951  Val Loss=0.6071  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 9 Epoch 35/150  Train Loss=0.4758  Val Loss=0.5975  Val Acc=0.7412  Time=14.5s\n",
            "  Fold 9 Epoch 36/150  Train Loss=0.4757  Val Loss=0.6033  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 9 Epoch 37/150  Train Loss=0.4671  Val Loss=0.6104  Val Acc=0.7020  Time=14.4s\n",
            "  Fold 9 Epoch 38/150  Train Loss=0.4499  Val Loss=0.5992  Val Acc=0.7235  Time=14.7s\n",
            "  Fold 9 Epoch 39/150  Train Loss=0.4505  Val Loss=0.5918  Val Acc=0.6882  Time=14.6s\n",
            "  Fold 9 Epoch 40/150  Train Loss=0.4452  Val Loss=0.5894  Val Acc=0.7255  Time=14.6s\n",
            "  Fold 9 Epoch 41/150  Train Loss=0.4401  Val Loss=0.6432  Val Acc=0.7039  Time=14.5s\n",
            "  Fold 9 Epoch 42/150  Train Loss=0.4296  Val Loss=0.6131  Val Acc=0.7157  Time=14.5s\n",
            "  Fold 9 Epoch 43/150  Train Loss=0.4182  Val Loss=0.5843  Val Acc=0.7255  Time=14.5s\n",
            "  Fold 9 Epoch 44/150  Train Loss=0.4161  Val Loss=0.5752  Val Acc=0.7314  Time=14.5s\n",
            "  Fold 9 Epoch 45/150  Train Loss=0.4079  Val Loss=0.5767  Val Acc=0.7275  Time=14.5s\n",
            "  Fold 9 Epoch 46/150  Train Loss=0.3958  Val Loss=0.5983  Val Acc=0.7059  Time=14.6s\n",
            "  Fold 9 Epoch 47/150  Train Loss=0.3977  Val Loss=0.5754  Val Acc=0.7137  Time=14.6s\n",
            "  Fold 9 Epoch 48/150  Train Loss=0.3841  Val Loss=0.5652  Val Acc=0.7333  Time=14.6s\n",
            "  Fold 9 Epoch 49/150  Train Loss=0.3755  Val Loss=0.5843  Val Acc=0.7196  Time=14.4s\n",
            "  Fold 9 Epoch 50/150  Train Loss=0.3719  Val Loss=0.6650  Val Acc=0.6471  Time=14.4s\n",
            "  Fold 9 Epoch 51/150  Train Loss=0.3682  Val Loss=0.5891  Val Acc=0.7176  Time=14.5s\n",
            "  Fold 9 Epoch 52/150  Train Loss=0.3641  Val Loss=0.5637  Val Acc=0.7353  Time=14.6s\n",
            "  Fold 9 Epoch 53/150  Train Loss=0.3578  Val Loss=0.5836  Val Acc=0.7275  Time=14.6s\n",
            "  Fold 9 Epoch 54/150  Train Loss=0.3479  Val Loss=0.5869  Val Acc=0.7137  Time=14.6s\n",
            "  Fold 9 Epoch 55/150  Train Loss=0.3306  Val Loss=0.6189  Val Acc=0.6980  Time=14.4s\n",
            "  Fold 9 Epoch 56/150  Train Loss=0.3361  Val Loss=0.5615  Val Acc=0.7314  Time=14.4s\n",
            "  Fold 9 Epoch 57/150  Train Loss=0.3238  Val Loss=0.5915  Val Acc=0.7176  Time=14.5s\n",
            "  Fold 9 Epoch 58/150  Train Loss=0.3309  Val Loss=0.6367  Val Acc=0.6941  Time=14.6s\n",
            "  Fold 9 Epoch 59/150  Train Loss=0.3240  Val Loss=0.5764  Val Acc=0.7235  Time=14.3s\n",
            "  Fold 9 Epoch 60/150  Train Loss=0.3290  Val Loss=0.6090  Val Acc=0.7216  Time=14.6s\n",
            "  Fold 9 Epoch 61/150  Train Loss=0.3179  Val Loss=0.5890  Val Acc=0.7235  Time=14.8s\n",
            "  Fold 9 Epoch 62/150  Train Loss=0.3052  Val Loss=0.5779  Val Acc=0.7235  Time=14.6s\n",
            "  Fold 9 Epoch 63/150  Train Loss=0.3047  Val Loss=0.5925  Val Acc=0.7275  Time=14.5s\n",
            "  Fold 9 Epoch 64/150  Train Loss=0.3011  Val Loss=0.6235  Val Acc=0.7255  Time=14.4s\n",
            "  Fold 9 Epoch 65/150  Train Loss=0.2936  Val Loss=0.5973  Val Acc=0.7314  Time=14.5s\n",
            "  Fold 9 Epoch 66/150  Train Loss=0.2998  Val Loss=0.6629  Val Acc=0.7000  Time=14.4s\n",
            "  Fold 9 Epoch 67/150  Train Loss=0.2994  Val Loss=0.6513  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 9 Epoch 68/150  Train Loss=0.2810  Val Loss=0.6016  Val Acc=0.7275  Time=14.6s\n",
            "  Fold 9 Epoch 69/150  Train Loss=0.2739  Val Loss=0.6021  Val Acc=0.7294  Time=14.6s\n",
            "  Fold 9 Epoch 70/150  Train Loss=0.2753  Val Loss=0.6006  Val Acc=0.7275  Time=14.3s\n",
            "  Fold 9 Epoch 71/150  Train Loss=0.2703  Val Loss=0.6355  Val Acc=0.7196  Time=14.4s\n",
            "  → Early stopping at epoch 71 (no val_loss improvement)\n",
            "  Fold 9 Final ACC = 1.0000   (TP=57  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 10/52  (leave out sub-046)\n",
            "  Fold 10 Epoch 1/150  Train Loss=0.7183  Val Loss=0.6756  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 10 Epoch 2/150  Train Loss=0.7040  Val Loss=0.6733  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 10 Epoch 3/150  Train Loss=0.7095  Val Loss=0.6744  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 10 Epoch 4/150  Train Loss=0.7130  Val Loss=0.6761  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 10 Epoch 5/150  Train Loss=0.7129  Val Loss=0.6772  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 10 Epoch 6/150  Train Loss=0.7101  Val Loss=0.6701  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 10 Epoch 7/150  Train Loss=0.7232  Val Loss=0.6775  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 10 Epoch 8/150  Train Loss=0.7072  Val Loss=0.6772  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 10 Epoch 9/150  Train Loss=0.6958  Val Loss=0.6785  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 10 Epoch 10/150  Train Loss=0.7194  Val Loss=0.6748  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 10 Epoch 11/150  Train Loss=0.6955  Val Loss=0.6780  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 10 Epoch 12/150  Train Loss=0.7014  Val Loss=0.6764  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 10 Epoch 13/150  Train Loss=0.6919  Val Loss=0.6787  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 10 Epoch 14/150  Train Loss=0.7112  Val Loss=0.6779  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 10 Epoch 15/150  Train Loss=0.7049  Val Loss=0.6696  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 10 Epoch 16/150  Train Loss=0.6966  Val Loss=0.6766  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 10 Epoch 17/150  Train Loss=0.7131  Val Loss=0.6858  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 10 Epoch 18/150  Train Loss=0.6862  Val Loss=0.6879  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 10 Epoch 19/150  Train Loss=0.6918  Val Loss=0.6841  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 10 Epoch 20/150  Train Loss=0.6901  Val Loss=0.6924  Val Acc=0.5490  Time=14.5s\n",
            "  Fold 10 Epoch 21/150  Train Loss=0.6853  Val Loss=0.6663  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 10 Epoch 22/150  Train Loss=0.6691  Val Loss=0.7075  Val Acc=0.4902  Time=14.5s\n",
            "  Fold 10 Epoch 23/150  Train Loss=0.6338  Val Loss=0.7315  Val Acc=0.5529  Time=14.4s\n",
            "  Fold 10 Epoch 24/150  Train Loss=0.5956  Val Loss=0.7009  Val Acc=0.7275  Time=14.6s\n",
            "  Fold 10 Epoch 25/150  Train Loss=0.5888  Val Loss=0.7369  Val Acc=0.7745  Time=14.3s\n",
            "  Fold 10 Epoch 26/150  Train Loss=0.5462  Val Loss=0.7468  Val Acc=0.7039  Time=14.6s\n",
            "  Fold 10 Epoch 27/150  Train Loss=0.5420  Val Loss=0.7237  Val Acc=0.7294  Time=14.6s\n",
            "  Fold 10 Epoch 28/150  Train Loss=0.5817  Val Loss=0.7576  Val Acc=0.7784  Time=14.4s\n",
            "  Fold 10 Epoch 29/150  Train Loss=0.5616  Val Loss=0.7618  Val Acc=0.6314  Time=14.3s\n",
            "  Fold 10 Epoch 30/150  Train Loss=0.5490  Val Loss=0.7536  Val Acc=0.6471  Time=14.3s\n",
            "  Fold 10 Epoch 31/150  Train Loss=0.5338  Val Loss=0.7465  Val Acc=0.7667  Time=14.4s\n",
            "  Fold 10 Epoch 32/150  Train Loss=0.5796  Val Loss=0.6907  Val Acc=0.6902  Time=14.4s\n",
            "  Fold 10 Epoch 33/150  Train Loss=0.5600  Val Loss=0.7196  Val Acc=0.6824  Time=14.4s\n",
            "  Fold 10 Epoch 34/150  Train Loss=0.5881  Val Loss=0.6472  Val Acc=0.6784  Time=14.7s\n",
            "  Fold 10 Epoch 35/150  Train Loss=0.5526  Val Loss=0.6925  Val Acc=0.6725  Time=14.4s\n",
            "  Fold 10 Epoch 36/150  Train Loss=0.5373  Val Loss=0.6790  Val Acc=0.7333  Time=14.5s\n",
            "  Fold 10 Epoch 37/150  Train Loss=0.5404  Val Loss=0.7028  Val Acc=0.6980  Time=14.4s\n",
            "  Fold 10 Epoch 38/150  Train Loss=0.5300  Val Loss=0.7251  Val Acc=0.6961  Time=14.4s\n",
            "  Fold 10 Epoch 39/150  Train Loss=0.5051  Val Loss=0.7302  Val Acc=0.6941  Time=14.4s\n",
            "  Fold 10 Epoch 40/150  Train Loss=0.5336  Val Loss=0.7268  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 10 Epoch 41/150  Train Loss=0.5059  Val Loss=0.7915  Val Acc=0.6745  Time=14.6s\n",
            "  Fold 10 Epoch 42/150  Train Loss=0.4964  Val Loss=0.7345  Val Acc=0.7118  Time=14.6s\n",
            "  Fold 10 Epoch 43/150  Train Loss=0.4958  Val Loss=0.7116  Val Acc=0.7510  Time=14.6s\n",
            "  Fold 10 Epoch 44/150  Train Loss=0.4979  Val Loss=0.7470  Val Acc=0.7059  Time=14.5s\n",
            "  Fold 10 Epoch 45/150  Train Loss=0.5356  Val Loss=0.7251  Val Acc=0.7137  Time=14.5s\n",
            "  Fold 10 Epoch 46/150  Train Loss=0.5345  Val Loss=0.6928  Val Acc=0.7412  Time=14.4s\n",
            "  Fold 10 Epoch 47/150  Train Loss=0.5137  Val Loss=0.7207  Val Acc=0.7412  Time=14.3s\n",
            "  Fold 10 Epoch 48/150  Train Loss=0.5029  Val Loss=0.7712  Val Acc=0.7216  Time=14.5s\n",
            "  Fold 10 Epoch 49/150  Train Loss=0.5052  Val Loss=0.7813  Val Acc=0.7098  Time=14.7s\n",
            "  → Early stopping at epoch 49 (no val_loss improvement)\n",
            "  Fold 10 Final ACC = 1.0000   (TP=56  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 11/52  (leave out sub-047)\n",
            "  Fold 11 Epoch 1/150  Train Loss=0.7622  Val Loss=0.7321  Val Acc=0.4024  Time=14.5s\n",
            "  Fold 11 Epoch 2/150  Train Loss=0.7337  Val Loss=0.7109  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 11 Epoch 3/150  Train Loss=0.7337  Val Loss=0.7094  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 11 Epoch 4/150  Train Loss=0.7309  Val Loss=0.6981  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 11 Epoch 5/150  Train Loss=0.7168  Val Loss=0.6956  Val Acc=0.4343  Time=14.5s\n",
            "  Fold 11 Epoch 6/150  Train Loss=0.7202  Val Loss=0.6952  Val Acc=0.4263  Time=14.6s\n",
            "  Fold 11 Epoch 7/150  Train Loss=0.7199  Val Loss=0.6883  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 11 Epoch 8/150  Train Loss=0.7203  Val Loss=0.6900  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 9/150  Train Loss=0.7153  Val Loss=0.6833  Val Acc=0.5976  Time=14.3s\n",
            "  Fold 11 Epoch 10/150  Train Loss=0.7106  Val Loss=0.6832  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 11/150  Train Loss=0.7135  Val Loss=0.6851  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 12/150  Train Loss=0.7072  Val Loss=0.6813  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 11 Epoch 13/150  Train Loss=0.7104  Val Loss=0.6822  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 14/150  Train Loss=0.7105  Val Loss=0.6791  Val Acc=0.5976  Time=14.8s\n",
            "  Fold 11 Epoch 15/150  Train Loss=0.6995  Val Loss=0.6777  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 16/150  Train Loss=0.7017  Val Loss=0.6721  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 17/150  Train Loss=0.6965  Val Loss=0.6772  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 11 Epoch 18/150  Train Loss=0.6763  Val Loss=0.6744  Val Acc=0.6215  Time=14.4s\n",
            "  Fold 11 Epoch 19/150  Train Loss=0.6422  Val Loss=0.6875  Val Acc=0.6076  Time=14.4s\n",
            "  Fold 11 Epoch 20/150  Train Loss=0.5783  Val Loss=0.7175  Val Acc=0.6215  Time=14.4s\n",
            "  Fold 11 Epoch 21/150  Train Loss=0.5536  Val Loss=0.7031  Val Acc=0.6394  Time=14.6s\n",
            "  Fold 11 Epoch 22/150  Train Loss=0.5363  Val Loss=0.7559  Val Acc=0.6454  Time=14.6s\n",
            "  Fold 11 Epoch 23/150  Train Loss=0.5150  Val Loss=0.7823  Val Acc=0.5916  Time=14.4s\n",
            "  Fold 11 Epoch 24/150  Train Loss=0.4803  Val Loss=0.7017  Val Acc=0.6335  Time=14.5s\n",
            "  Fold 11 Epoch 25/150  Train Loss=0.4891  Val Loss=0.7067  Val Acc=0.6514  Time=14.5s\n",
            "  Fold 11 Epoch 26/150  Train Loss=0.4788  Val Loss=0.7943  Val Acc=0.6175  Time=14.5s\n",
            "  Fold 11 Epoch 27/150  Train Loss=0.4728  Val Loss=0.7777  Val Acc=0.6494  Time=14.5s\n",
            "  Fold 11 Epoch 28/150  Train Loss=0.4569  Val Loss=0.8831  Val Acc=0.5040  Time=14.5s\n",
            "  Fold 11 Epoch 29/150  Train Loss=0.4523  Val Loss=0.7617  Val Acc=0.6414  Time=14.5s\n",
            "  Fold 11 Epoch 30/150  Train Loss=0.4367  Val Loss=0.7505  Val Acc=0.6494  Time=14.4s\n",
            "  Fold 11 Epoch 31/150  Train Loss=0.4407  Val Loss=0.7906  Val Acc=0.6215  Time=14.5s\n",
            "  → Early stopping at epoch 31 (no val_loss improvement)\n",
            "  Fold 11 Final ACC = 1.0000   (TP=56  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 12/52  (leave out sub-048)\n",
            "  Fold 12 Epoch 1/150  Train Loss=0.7224  Val Loss=0.6715  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 12 Epoch 2/150  Train Loss=0.7192  Val Loss=0.6763  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 12 Epoch 3/150  Train Loss=0.7109  Val Loss=0.6780  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 12 Epoch 4/150  Train Loss=0.7035  Val Loss=0.6892  Val Acc=0.6288  Time=14.6s\n",
            "  Fold 12 Epoch 5/150  Train Loss=0.7111  Val Loss=0.6845  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 12 Epoch 6/150  Train Loss=0.7115  Val Loss=0.6856  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 12 Epoch 7/150  Train Loss=0.7077  Val Loss=0.6804  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 12 Epoch 8/150  Train Loss=0.7172  Val Loss=0.6807  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 12 Epoch 9/150  Train Loss=0.7014  Val Loss=0.6876  Val Acc=0.6648  Time=14.2s\n",
            "  Fold 12 Epoch 10/150  Train Loss=0.7050  Val Loss=0.6823  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 12 Epoch 11/150  Train Loss=0.7033  Val Loss=0.6742  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 12 Epoch 12/150  Train Loss=0.6960  Val Loss=0.6707  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 12 Epoch 13/150  Train Loss=0.7064  Val Loss=0.6671  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 12 Epoch 14/150  Train Loss=0.7006  Val Loss=0.6651  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 12 Epoch 15/150  Train Loss=0.6982  Val Loss=0.6604  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 12 Epoch 16/150  Train Loss=0.6955  Val Loss=0.6720  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 12 Epoch 17/150  Train Loss=0.6991  Val Loss=0.6758  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 12 Epoch 18/150  Train Loss=0.6919  Val Loss=0.6840  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 12 Epoch 19/150  Train Loss=0.6960  Val Loss=0.6430  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 12 Epoch 20/150  Train Loss=0.7011  Val Loss=0.6492  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 12 Epoch 21/150  Train Loss=0.6961  Val Loss=0.6654  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 12 Epoch 22/150  Train Loss=0.6916  Val Loss=0.6553  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 12 Epoch 23/150  Train Loss=0.6916  Val Loss=0.6548  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 12 Epoch 24/150  Train Loss=0.6859  Val Loss=0.6463  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 12 Epoch 25/150  Train Loss=0.6731  Val Loss=0.6752  Val Acc=0.7424  Time=14.4s\n",
            "  Fold 12 Epoch 26/150  Train Loss=0.6325  Val Loss=0.6249  Val Acc=0.7386  Time=14.3s\n",
            "  Fold 12 Epoch 27/150  Train Loss=0.5836  Val Loss=0.7010  Val Acc=0.5701  Time=14.5s\n",
            "  Fold 12 Epoch 28/150  Train Loss=0.5598  Val Loss=0.6272  Val Acc=0.7140  Time=14.4s\n",
            "  Fold 12 Epoch 29/150  Train Loss=0.5445  Val Loss=0.6784  Val Acc=0.6667  Time=14.4s\n",
            "  Fold 12 Epoch 30/150  Train Loss=0.5211  Val Loss=0.6584  Val Acc=0.6875  Time=14.5s\n",
            "  Fold 12 Epoch 31/150  Train Loss=0.5113  Val Loss=0.7629  Val Acc=0.5758  Time=14.4s\n",
            "  Fold 12 Epoch 32/150  Train Loss=0.5006  Val Loss=0.7365  Val Acc=0.6098  Time=14.5s\n",
            "  Fold 12 Epoch 33/150  Train Loss=0.4831  Val Loss=0.7334  Val Acc=0.6572  Time=14.3s\n",
            "  Fold 12 Epoch 34/150  Train Loss=0.4684  Val Loss=0.7486  Val Acc=0.6193  Time=14.7s\n",
            "  Fold 12 Epoch 35/150  Train Loss=0.4574  Val Loss=0.7143  Val Acc=0.6780  Time=14.6s\n",
            "  Fold 12 Epoch 36/150  Train Loss=0.4508  Val Loss=0.8270  Val Acc=0.5189  Time=14.5s\n",
            "  Fold 12 Epoch 37/150  Train Loss=0.4436  Val Loss=0.7172  Val Acc=0.6648  Time=14.4s\n",
            "  Fold 12 Epoch 38/150  Train Loss=0.4407  Val Loss=0.7335  Val Acc=0.6610  Time=14.3s\n",
            "  Fold 12 Epoch 39/150  Train Loss=0.4311  Val Loss=0.7080  Val Acc=0.7083  Time=14.4s\n",
            "  Fold 12 Epoch 40/150  Train Loss=0.4048  Val Loss=0.7399  Val Acc=0.6534  Time=14.3s\n",
            "  Fold 12 Epoch 41/150  Train Loss=0.4135  Val Loss=0.7199  Val Acc=0.6667  Time=14.3s\n",
            "  → Early stopping at epoch 41 (no val_loss improvement)\n",
            "  Fold 12 Final ACC = 0.9643   (TP=0  TN=54  FP=2  FN=0)\n",
            "\n",
            ">>> Fold 13/52  (leave out sub-049)\n",
            "  Fold 13 Epoch 1/150  Train Loss=0.8623  Val Loss=0.6340  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 13 Epoch 2/150  Train Loss=0.8385  Val Loss=0.6273  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 3/150  Train Loss=0.7925  Val Loss=0.6260  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 13 Epoch 4/150  Train Loss=0.7842  Val Loss=0.6258  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 5/150  Train Loss=0.7730  Val Loss=0.6260  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 6/150  Train Loss=0.7594  Val Loss=0.6260  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 7/150  Train Loss=0.7461  Val Loss=0.6347  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 8/150  Train Loss=0.7348  Val Loss=0.6358  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 13 Epoch 9/150  Train Loss=0.7232  Val Loss=0.6315  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 13 Epoch 10/150  Train Loss=0.7191  Val Loss=0.6312  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 11/150  Train Loss=0.7292  Val Loss=0.6350  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 13 Epoch 12/150  Train Loss=0.7292  Val Loss=0.6345  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 13/150  Train Loss=0.7258  Val Loss=0.6325  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 13 Epoch 14/150  Train Loss=0.7238  Val Loss=0.6452  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 15/150  Train Loss=0.7148  Val Loss=0.6634  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 16/150  Train Loss=0.6927  Val Loss=0.6590  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 13 Epoch 17/150  Train Loss=0.6968  Val Loss=0.6637  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 13 Epoch 18/150  Train Loss=0.6994  Val Loss=0.6686  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 13 Epoch 19/150  Train Loss=0.6938  Val Loss=0.6568  Val Acc=0.6742  Time=14.4s\n",
            "  → Early stopping at epoch 19 (no val_loss improvement)\n",
            "  Fold 13 Final ACC = 0.0000   (TP=0  TN=0  FP=55  FN=0)\n",
            "\n",
            ">>> Fold 14/52  (leave out sub-050)\n",
            "  Fold 14 Epoch 1/150  Train Loss=0.7622  Val Loss=0.6726  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 14 Epoch 2/150  Train Loss=0.7255  Val Loss=0.6693  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 3/150  Train Loss=0.7302  Val Loss=0.6696  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 14 Epoch 4/150  Train Loss=0.7079  Val Loss=0.6695  Val Acc=0.5976  Time=14.7s\n",
            "  Fold 14 Epoch 5/150  Train Loss=0.7146  Val Loss=0.6695  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 14 Epoch 6/150  Train Loss=0.7232  Val Loss=0.6704  Val Acc=0.5976  Time=14.3s\n",
            "  Fold 14 Epoch 7/150  Train Loss=0.7188  Val Loss=0.6705  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 14 Epoch 8/150  Train Loss=0.7238  Val Loss=0.6701  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 9/150  Train Loss=0.7177  Val Loss=0.6699  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 10/150  Train Loss=0.7116  Val Loss=0.6702  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 11/150  Train Loss=0.7040  Val Loss=0.6704  Val Acc=0.5976  Time=14.7s\n",
            "  Fold 14 Epoch 12/150  Train Loss=0.7172  Val Loss=0.6702  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 13/150  Train Loss=0.7032  Val Loss=0.6708  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 14/150  Train Loss=0.7097  Val Loss=0.6710  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 14 Epoch 15/150  Train Loss=0.7101  Val Loss=0.6697  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 14 Epoch 16/150  Train Loss=0.7133  Val Loss=0.6684  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 14 Epoch 17/150  Train Loss=0.7057  Val Loss=0.6675  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 14 Epoch 18/150  Train Loss=0.6883  Val Loss=0.6684  Val Acc=0.5996  Time=14.7s\n",
            "  Fold 14 Epoch 19/150  Train Loss=0.6636  Val Loss=0.6569  Val Acc=0.6494  Time=14.7s\n",
            "  Fold 14 Epoch 20/150  Train Loss=0.6303  Val Loss=0.6560  Val Acc=0.5797  Time=14.5s\n",
            "  Fold 14 Epoch 21/150  Train Loss=0.5740  Val Loss=0.6420  Val Acc=0.6116  Time=14.5s\n",
            "  Fold 14 Epoch 22/150  Train Loss=0.5743  Val Loss=0.6500  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 14 Epoch 23/150  Train Loss=0.5554  Val Loss=0.6436  Val Acc=0.6096  Time=14.4s\n",
            "  Fold 14 Epoch 24/150  Train Loss=0.5430  Val Loss=0.6444  Val Acc=0.6315  Time=14.4s\n",
            "  Fold 14 Epoch 25/150  Train Loss=0.5217  Val Loss=0.6421  Val Acc=0.6375  Time=14.8s\n",
            "  Fold 14 Epoch 26/150  Train Loss=0.5130  Val Loss=0.6496  Val Acc=0.6255  Time=14.5s\n",
            "  Fold 14 Epoch 27/150  Train Loss=0.4943  Val Loss=0.6948  Val Acc=0.6434  Time=14.4s\n",
            "  Fold 14 Epoch 28/150  Train Loss=0.4779  Val Loss=0.6658  Val Acc=0.6494  Time=14.4s\n",
            "  Fold 14 Epoch 29/150  Train Loss=0.4566  Val Loss=0.6796  Val Acc=0.6514  Time=14.5s\n",
            "  Fold 14 Epoch 30/150  Train Loss=0.4611  Val Loss=0.7131  Val Acc=0.6534  Time=14.6s\n",
            "  Fold 14 Epoch 31/150  Train Loss=0.4481  Val Loss=0.6737  Val Acc=0.6494  Time=14.5s\n",
            "  Fold 14 Epoch 32/150  Train Loss=0.4441  Val Loss=0.7607  Val Acc=0.6375  Time=14.6s\n",
            "  Fold 14 Epoch 33/150  Train Loss=0.4301  Val Loss=0.6878  Val Acc=0.6534  Time=14.6s\n",
            "  Fold 14 Epoch 34/150  Train Loss=0.4308  Val Loss=0.6970  Val Acc=0.6454  Time=14.4s\n",
            "  Fold 14 Epoch 35/150  Train Loss=0.4221  Val Loss=0.7093  Val Acc=0.6414  Time=14.5s\n",
            "  Fold 14 Epoch 36/150  Train Loss=0.4085  Val Loss=0.8329  Val Acc=0.6215  Time=14.4s\n",
            "  → Early stopping at epoch 36 (no val_loss improvement)\n",
            "  Fold 14 Final ACC = 1.0000   (TP=54  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 15/52  (leave out sub-051)\n",
            "  Fold 15 Epoch 1/150  Train Loss=0.7367  Val Loss=0.6752  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 15 Epoch 2/150  Train Loss=0.7331  Val Loss=0.6759  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 3/150  Train Loss=0.7280  Val Loss=0.6708  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 15 Epoch 4/150  Train Loss=0.7189  Val Loss=0.6708  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 15 Epoch 5/150  Train Loss=0.7056  Val Loss=0.6710  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 6/150  Train Loss=0.7056  Val Loss=0.6712  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 7/150  Train Loss=0.7084  Val Loss=0.6713  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 15 Epoch 8/150  Train Loss=0.6970  Val Loss=0.6715  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 15 Epoch 9/150  Train Loss=0.7048  Val Loss=0.6729  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 10/150  Train Loss=0.7036  Val Loss=0.6736  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 15 Epoch 11/150  Train Loss=0.7024  Val Loss=0.6740  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 15 Epoch 12/150  Train Loss=0.6975  Val Loss=0.6721  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 15 Epoch 13/150  Train Loss=0.6981  Val Loss=0.6712  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 15 Epoch 14/150  Train Loss=0.6958  Val Loss=0.6712  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 15 Epoch 15/150  Train Loss=0.7016  Val Loss=0.6713  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 16/150  Train Loss=0.7070  Val Loss=0.6712  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 15 Epoch 17/150  Train Loss=0.6975  Val Loss=0.6696  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 18/150  Train Loss=0.6953  Val Loss=0.6691  Val Acc=0.6039  Time=14.8s\n",
            "  Fold 15 Epoch 19/150  Train Loss=0.7008  Val Loss=0.6670  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 15 Epoch 20/150  Train Loss=0.6852  Val Loss=0.6642  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 15 Epoch 21/150  Train Loss=0.6931  Val Loss=0.6550  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 15 Epoch 22/150  Train Loss=0.6511  Val Loss=0.6583  Val Acc=0.6314  Time=14.4s\n",
            "  Fold 15 Epoch 23/150  Train Loss=0.6368  Val Loss=0.6491  Val Acc=0.6549  Time=14.5s\n",
            "  Fold 15 Epoch 24/150  Train Loss=0.6265  Val Loss=0.6607  Val Acc=0.7000  Time=14.5s\n",
            "  Fold 15 Epoch 25/150  Train Loss=0.5989  Val Loss=0.6563  Val Acc=0.7059  Time=14.8s\n",
            "  Fold 15 Epoch 26/150  Train Loss=0.5941  Val Loss=0.6531  Val Acc=0.7529  Time=14.5s\n",
            "  Fold 15 Epoch 27/150  Train Loss=0.5747  Val Loss=0.6342  Val Acc=0.7216  Time=14.5s\n",
            "  Fold 15 Epoch 28/150  Train Loss=0.5573  Val Loss=0.6317  Val Acc=0.7157  Time=14.5s\n",
            "  Fold 15 Epoch 29/150  Train Loss=0.5459  Val Loss=0.6319  Val Acc=0.7098  Time=14.5s\n",
            "  Fold 15 Epoch 30/150  Train Loss=0.5284  Val Loss=0.6376  Val Acc=0.7412  Time=14.6s\n",
            "  Fold 15 Epoch 31/150  Train Loss=0.5285  Val Loss=0.6549  Val Acc=0.7667  Time=14.3s\n",
            "  Fold 15 Epoch 32/150  Train Loss=0.5076  Val Loss=0.6438  Val Acc=0.7373  Time=14.6s\n",
            "  Fold 15 Epoch 33/150  Train Loss=0.4925  Val Loss=0.6243  Val Acc=0.7196  Time=14.6s\n",
            "  Fold 15 Epoch 34/150  Train Loss=0.4740  Val Loss=0.6161  Val Acc=0.7216  Time=14.5s\n",
            "  Fold 15 Epoch 35/150  Train Loss=0.4677  Val Loss=0.6305  Val Acc=0.7471  Time=14.4s\n",
            "  Fold 15 Epoch 36/150  Train Loss=0.4616  Val Loss=0.6218  Val Acc=0.7647  Time=14.4s\n",
            "  Fold 15 Epoch 37/150  Train Loss=0.4435  Val Loss=0.6255  Val Acc=0.7373  Time=14.6s\n",
            "  Fold 15 Epoch 38/150  Train Loss=0.4411  Val Loss=0.6328  Val Acc=0.7176  Time=14.5s\n",
            "  Fold 15 Epoch 39/150  Train Loss=0.4216  Val Loss=0.6250  Val Acc=0.7196  Time=14.6s\n",
            "  Fold 15 Epoch 40/150  Train Loss=0.4306  Val Loss=0.6101  Val Acc=0.7078  Time=14.6s\n",
            "  Fold 15 Epoch 41/150  Train Loss=0.4192  Val Loss=0.6144  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 15 Epoch 42/150  Train Loss=0.4229  Val Loss=0.6426  Val Acc=0.7059  Time=14.5s\n",
            "  Fold 15 Epoch 43/150  Train Loss=0.4030  Val Loss=0.6257  Val Acc=0.7196  Time=14.4s\n",
            "  Fold 15 Epoch 44/150  Train Loss=0.3933  Val Loss=0.6057  Val Acc=0.7275  Time=14.6s\n",
            "  Fold 15 Epoch 45/150  Train Loss=0.3745  Val Loss=0.6106  Val Acc=0.7176  Time=14.4s\n",
            "  Fold 15 Epoch 46/150  Train Loss=0.3892  Val Loss=0.6100  Val Acc=0.7118  Time=14.6s\n",
            "  Fold 15 Epoch 47/150  Train Loss=0.4066  Val Loss=0.6142  Val Acc=0.7078  Time=14.6s\n",
            "  Fold 15 Epoch 48/150  Train Loss=0.3575  Val Loss=0.6507  Val Acc=0.6863  Time=14.4s\n",
            "  Fold 15 Epoch 49/150  Train Loss=0.3488  Val Loss=0.6230  Val Acc=0.7059  Time=14.4s\n",
            "  Fold 15 Epoch 50/150  Train Loss=0.3349  Val Loss=0.6030  Val Acc=0.6941  Time=14.5s\n",
            "  Fold 15 Epoch 51/150  Train Loss=0.3377  Val Loss=0.6023  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 15 Epoch 52/150  Train Loss=0.3308  Val Loss=0.6070  Val Acc=0.7137  Time=14.4s\n",
            "  Fold 15 Epoch 53/150  Train Loss=0.3361  Val Loss=0.6085  Val Acc=0.7157  Time=14.6s\n",
            "  Fold 15 Epoch 54/150  Train Loss=0.3256  Val Loss=0.6239  Val Acc=0.7118  Time=14.7s\n",
            "  Fold 15 Epoch 55/150  Train Loss=0.3235  Val Loss=0.5936  Val Acc=0.7216  Time=14.5s\n",
            "  Fold 15 Epoch 56/150  Train Loss=0.3063  Val Loss=0.6102  Val Acc=0.7294  Time=14.5s\n",
            "  Fold 15 Epoch 57/150  Train Loss=0.3106  Val Loss=0.6109  Val Acc=0.7157  Time=14.6s\n",
            "  Fold 15 Epoch 58/150  Train Loss=0.2882  Val Loss=0.6048  Val Acc=0.7294  Time=14.5s\n",
            "  Fold 15 Epoch 59/150  Train Loss=0.2914  Val Loss=0.6197  Val Acc=0.7392  Time=14.5s\n",
            "  Fold 15 Epoch 60/150  Train Loss=0.3210  Val Loss=0.6282  Val Acc=0.7294  Time=14.5s\n",
            "  Fold 15 Epoch 61/150  Train Loss=0.3032  Val Loss=0.6315  Val Acc=0.7255  Time=14.7s\n",
            "  Fold 15 Epoch 62/150  Train Loss=0.2941  Val Loss=0.6132  Val Acc=0.7118  Time=14.5s\n",
            "  Fold 15 Epoch 63/150  Train Loss=0.2799  Val Loss=0.6188  Val Acc=0.7275  Time=14.4s\n",
            "  Fold 15 Epoch 64/150  Train Loss=0.2910  Val Loss=0.6167  Val Acc=0.7255  Time=14.4s\n",
            "  Fold 15 Epoch 65/150  Train Loss=0.2838  Val Loss=0.6286  Val Acc=0.7275  Time=14.4s\n",
            "  Fold 15 Epoch 66/150  Train Loss=0.2737  Val Loss=0.6284  Val Acc=0.7275  Time=14.5s\n",
            "  Fold 15 Epoch 67/150  Train Loss=0.3134  Val Loss=0.6848  Val Acc=0.7000  Time=14.6s\n",
            "  Fold 15 Epoch 68/150  Train Loss=0.2807  Val Loss=0.6747  Val Acc=0.7059  Time=14.9s\n",
            "  Fold 15 Epoch 69/150  Train Loss=0.2854  Val Loss=0.6748  Val Acc=0.7314  Time=14.7s\n",
            "  Fold 15 Epoch 70/150  Train Loss=0.2552  Val Loss=0.6325  Val Acc=0.7353  Time=14.5s\n",
            "  → Early stopping at epoch 70 (no val_loss improvement)\n",
            "  Fold 15 Final ACC = 0.6852   (TP=37  TN=0  FP=0  FN=17)\n",
            "\n",
            ">>> Fold 16/52  (leave out sub-052)\n",
            "  Fold 16 Epoch 1/150  Train Loss=0.7160  Val Loss=0.6758  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 2/150  Train Loss=0.7169  Val Loss=0.6726  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 3/150  Train Loss=0.7093  Val Loss=0.6713  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 16 Epoch 4/150  Train Loss=0.7114  Val Loss=0.6714  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 5/150  Train Loss=0.7034  Val Loss=0.6722  Val Acc=0.6039  Time=14.8s\n",
            "  Fold 16 Epoch 6/150  Train Loss=0.6969  Val Loss=0.6727  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 7/150  Train Loss=0.6949  Val Loss=0.6731  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 16 Epoch 8/150  Train Loss=0.6962  Val Loss=0.6740  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 16 Epoch 9/150  Train Loss=0.6908  Val Loss=0.6723  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 10/150  Train Loss=0.6960  Val Loss=0.6740  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 16 Epoch 11/150  Train Loss=0.7029  Val Loss=0.6734  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 16 Epoch 12/150  Train Loss=0.7018  Val Loss=0.6740  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 16 Epoch 13/150  Train Loss=0.6867  Val Loss=0.6735  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 16 Epoch 14/150  Train Loss=0.6912  Val Loss=0.6734  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 15/150  Train Loss=0.6907  Val Loss=0.6712  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 16/150  Train Loss=0.6946  Val Loss=0.6720  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 17/150  Train Loss=0.6921  Val Loss=0.6726  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 16 Epoch 18/150  Train Loss=0.6909  Val Loss=0.6708  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 16 Epoch 19/150  Train Loss=0.6897  Val Loss=0.6728  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 16 Epoch 20/150  Train Loss=0.6818  Val Loss=0.6713  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 16 Epoch 21/150  Train Loss=0.6782  Val Loss=0.6650  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 16 Epoch 22/150  Train Loss=0.6621  Val Loss=0.6668  Val Acc=0.6706  Time=14.5s\n",
            "  Fold 16 Epoch 23/150  Train Loss=0.6478  Val Loss=0.6594  Val Acc=0.6647  Time=14.4s\n",
            "  Fold 16 Epoch 24/150  Train Loss=0.6303  Val Loss=0.6703  Val Acc=0.7020  Time=14.5s\n",
            "  Fold 16 Epoch 25/150  Train Loss=0.6235  Val Loss=0.6653  Val Acc=0.6902  Time=14.5s\n",
            "  Fold 16 Epoch 26/150  Train Loss=0.6128  Val Loss=0.6715  Val Acc=0.7255  Time=14.5s\n",
            "  Fold 16 Epoch 27/150  Train Loss=0.6145  Val Loss=0.6689  Val Acc=0.7490  Time=14.7s\n",
            "  Fold 16 Epoch 28/150  Train Loss=0.5862  Val Loss=0.6602  Val Acc=0.7745  Time=14.5s\n",
            "  Fold 16 Epoch 29/150  Train Loss=0.5720  Val Loss=0.6616  Val Acc=0.7490  Time=14.5s\n",
            "  Fold 16 Epoch 30/150  Train Loss=0.5761  Val Loss=0.6443  Val Acc=0.7314  Time=14.3s\n",
            "  Fold 16 Epoch 31/150  Train Loss=0.5568  Val Loss=0.6216  Val Acc=0.7373  Time=14.5s\n",
            "  Fold 16 Epoch 32/150  Train Loss=0.5505  Val Loss=0.6545  Val Acc=0.7549  Time=14.5s\n",
            "  Fold 16 Epoch 33/150  Train Loss=0.5425  Val Loss=0.6101  Val Acc=0.7314  Time=14.6s\n",
            "  Fold 16 Epoch 34/150  Train Loss=0.5264  Val Loss=0.6102  Val Acc=0.7647  Time=14.7s\n",
            "  Fold 16 Epoch 35/150  Train Loss=0.5067  Val Loss=0.6208  Val Acc=0.7667  Time=14.4s\n",
            "  Fold 16 Epoch 36/150  Train Loss=0.5064  Val Loss=0.6159  Val Acc=0.7667  Time=14.4s\n",
            "  Fold 16 Epoch 37/150  Train Loss=0.4766  Val Loss=0.6354  Val Acc=0.7333  Time=14.4s\n",
            "  Fold 16 Epoch 38/150  Train Loss=0.4789  Val Loss=0.6164  Val Acc=0.7667  Time=14.5s\n",
            "  Fold 16 Epoch 39/150  Train Loss=0.4734  Val Loss=0.6340  Val Acc=0.7431  Time=14.5s\n",
            "  Fold 16 Epoch 40/150  Train Loss=0.4526  Val Loss=0.6154  Val Acc=0.7353  Time=14.6s\n",
            "  Fold 16 Epoch 41/150  Train Loss=0.4508  Val Loss=0.6373  Val Acc=0.7314  Time=14.7s\n",
            "  Fold 16 Epoch 42/150  Train Loss=0.4405  Val Loss=0.6763  Val Acc=0.6961  Time=14.6s\n",
            "  Fold 16 Epoch 43/150  Train Loss=0.4337  Val Loss=0.6100  Val Acc=0.7333  Time=14.5s\n",
            "  Fold 16 Epoch 44/150  Train Loss=0.4283  Val Loss=0.6660  Val Acc=0.6941  Time=14.6s\n",
            "  Fold 16 Epoch 45/150  Train Loss=0.4318  Val Loss=0.6242  Val Acc=0.7275  Time=14.4s\n",
            "  Fold 16 Epoch 46/150  Train Loss=0.4327  Val Loss=0.6179  Val Acc=0.7412  Time=14.5s\n",
            "  Fold 16 Epoch 47/150  Train Loss=0.4157  Val Loss=0.6188  Val Acc=0.7392  Time=14.5s\n",
            "  Fold 16 Epoch 48/150  Train Loss=0.4075  Val Loss=0.6645  Val Acc=0.6745  Time=14.7s\n",
            "  Fold 16 Epoch 49/150  Train Loss=0.3946  Val Loss=0.6248  Val Acc=0.7333  Time=14.7s\n",
            "  Fold 16 Epoch 50/150  Train Loss=0.4108  Val Loss=0.6290  Val Acc=0.7314  Time=14.5s\n",
            "  Fold 16 Epoch 51/150  Train Loss=0.3928  Val Loss=0.6726  Val Acc=0.6922  Time=14.6s\n",
            "  Fold 16 Epoch 52/150  Train Loss=0.3786  Val Loss=0.6311  Val Acc=0.7294  Time=14.6s\n",
            "  Fold 16 Epoch 53/150  Train Loss=0.3685  Val Loss=0.6454  Val Acc=0.7235  Time=14.5s\n",
            "  Fold 16 Epoch 54/150  Train Loss=0.3878  Val Loss=0.6274  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 16 Epoch 55/150  Train Loss=0.3615  Val Loss=0.6497  Val Acc=0.7294  Time=14.7s\n",
            "  Fold 16 Epoch 56/150  Train Loss=0.3460  Val Loss=0.6562  Val Acc=0.7275  Time=14.6s\n",
            "  Fold 16 Epoch 57/150  Train Loss=0.3417  Val Loss=0.6548  Val Acc=0.7137  Time=14.4s\n",
            "  Fold 16 Epoch 58/150  Train Loss=0.3269  Val Loss=0.6517  Val Acc=0.7255  Time=14.6s\n",
            "  → Early stopping at epoch 58 (no val_loss improvement)\n",
            "  Fold 16 Final ACC = 0.7037   (TP=38  TN=0  FP=0  FN=16)\n",
            "\n",
            ">>> Fold 17/52  (leave out sub-053)\n",
            "  Fold 17 Epoch 1/150  Train Loss=0.7536  Val Loss=0.6852  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 2/150  Train Loss=0.7467  Val Loss=0.6716  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 3/150  Train Loss=0.7300  Val Loss=0.6718  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 17 Epoch 4/150  Train Loss=0.7180  Val Loss=0.6718  Val Acc=0.6047  Time=14.7s\n",
            "  Fold 17 Epoch 5/150  Train Loss=0.7217  Val Loss=0.6750  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 17 Epoch 6/150  Train Loss=0.7023  Val Loss=0.6773  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 7/150  Train Loss=0.7055  Val Loss=0.6759  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 8/150  Train Loss=0.7064  Val Loss=0.6760  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 9/150  Train Loss=0.7071  Val Loss=0.6757  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 17 Epoch 10/150  Train Loss=0.7121  Val Loss=0.6933  Val Acc=0.5205  Time=14.5s\n",
            "  Fold 17 Epoch 11/150  Train Loss=0.7004  Val Loss=0.6805  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 17 Epoch 12/150  Train Loss=0.7043  Val Loss=0.6857  Val Acc=0.6008  Time=14.6s\n",
            "  Fold 17 Epoch 13/150  Train Loss=0.7005  Val Loss=0.6818  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 14/150  Train Loss=0.7019  Val Loss=0.6833  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 15/150  Train Loss=0.6919  Val Loss=0.6783  Val Acc=0.6047  Time=14.3s\n",
            "  Fold 17 Epoch 16/150  Train Loss=0.6969  Val Loss=0.6861  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 17 Epoch 17/150  Train Loss=0.6940  Val Loss=0.6782  Val Acc=0.6321  Time=14.5s\n",
            "  → Early stopping at epoch 17 (no val_loss improvement)\n",
            "  Fold 17 Final ACC = 1.0000   (TP=53  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 18/52  (leave out sub-054)\n",
            "  Fold 18 Epoch 1/150  Train Loss=0.7560  Val Loss=0.6733  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 18 Epoch 2/150  Train Loss=0.7479  Val Loss=0.6712  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 18 Epoch 3/150  Train Loss=0.7335  Val Loss=0.6708  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 4/150  Train Loss=0.7206  Val Loss=0.6708  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 5/150  Train Loss=0.7239  Val Loss=0.6712  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 18 Epoch 6/150  Train Loss=0.7162  Val Loss=0.6709  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 7/150  Train Loss=0.7112  Val Loss=0.6704  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 18 Epoch 8/150  Train Loss=0.7119  Val Loss=0.6719  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 18 Epoch 9/150  Train Loss=0.7079  Val Loss=0.6708  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 18 Epoch 10/150  Train Loss=0.7025  Val Loss=0.6734  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 11/150  Train Loss=0.7025  Val Loss=0.6761  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 12/150  Train Loss=0.7035  Val Loss=0.6768  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 18 Epoch 13/150  Train Loss=0.6972  Val Loss=0.6740  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 18 Epoch 14/150  Train Loss=0.7076  Val Loss=0.6769  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 18 Epoch 15/150  Train Loss=0.6954  Val Loss=0.6729  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 18 Epoch 16/150  Train Loss=0.6988  Val Loss=0.6745  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 18 Epoch 17/150  Train Loss=0.7004  Val Loss=0.6735  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 18/150  Train Loss=0.6918  Val Loss=0.6703  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 18 Epoch 19/150  Train Loss=0.6949  Val Loss=0.6691  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 20/150  Train Loss=0.6900  Val Loss=0.6701  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 18 Epoch 21/150  Train Loss=0.6739  Val Loss=0.6563  Val Acc=0.6118  Time=14.5s\n",
            "  Fold 18 Epoch 22/150  Train Loss=0.6593  Val Loss=0.6557  Val Acc=0.6490  Time=14.6s\n",
            "  Fold 18 Epoch 23/150  Train Loss=0.5967  Val Loss=0.6608  Val Acc=0.6804  Time=14.7s\n",
            "  Fold 18 Epoch 24/150  Train Loss=0.5724  Val Loss=0.6820  Val Acc=0.6765  Time=14.4s\n",
            "  Fold 18 Epoch 25/150  Train Loss=0.5562  Val Loss=0.6608  Val Acc=0.7431  Time=14.6s\n",
            "  Fold 18 Epoch 26/150  Train Loss=0.5433  Val Loss=0.6795  Val Acc=0.7235  Time=14.5s\n",
            "  Fold 18 Epoch 27/150  Train Loss=0.5253  Val Loss=0.6776  Val Acc=0.7373  Time=14.4s\n",
            "  Fold 18 Epoch 28/150  Train Loss=0.5123  Val Loss=0.6688  Val Acc=0.7490  Time=14.5s\n",
            "  Fold 18 Epoch 29/150  Train Loss=0.5053  Val Loss=0.6629  Val Acc=0.7608  Time=14.6s\n",
            "  Fold 18 Epoch 30/150  Train Loss=0.4983  Val Loss=0.6765  Val Acc=0.7588  Time=14.7s\n",
            "  Fold 18 Epoch 31/150  Train Loss=0.4766  Val Loss=0.6588  Val Acc=0.7529  Time=14.5s\n",
            "  Fold 18 Epoch 32/150  Train Loss=0.4745  Val Loss=0.6471  Val Acc=0.7216  Time=14.4s\n",
            "  Fold 18 Epoch 33/150  Train Loss=0.4476  Val Loss=0.6505  Val Acc=0.7490  Time=14.5s\n",
            "  Fold 18 Epoch 34/150  Train Loss=0.4568  Val Loss=0.6866  Val Acc=0.7255  Time=14.5s\n",
            "  Fold 18 Epoch 35/150  Train Loss=0.4292  Val Loss=0.6659  Val Acc=0.7451  Time=14.4s\n",
            "  Fold 18 Epoch 36/150  Train Loss=0.4230  Val Loss=0.6599  Val Acc=0.7353  Time=14.4s\n",
            "  Fold 18 Epoch 37/150  Train Loss=0.4148  Val Loss=0.6643  Val Acc=0.7196  Time=14.6s\n",
            "  Fold 18 Epoch 38/150  Train Loss=0.4022  Val Loss=0.6643  Val Acc=0.7118  Time=14.4s\n",
            "  Fold 18 Epoch 39/150  Train Loss=0.3964  Val Loss=0.6441  Val Acc=0.7275  Time=14.4s\n",
            "  Fold 18 Epoch 40/150  Train Loss=0.3991  Val Loss=0.6598  Val Acc=0.7333  Time=14.3s\n",
            "  Fold 18 Epoch 41/150  Train Loss=0.3849  Val Loss=0.6665  Val Acc=0.7020  Time=14.5s\n",
            "  Fold 18 Epoch 42/150  Train Loss=0.3754  Val Loss=0.6573  Val Acc=0.7255  Time=14.4s\n",
            "  Fold 18 Epoch 43/150  Train Loss=0.3818  Val Loss=0.6580  Val Acc=0.7314  Time=14.6s\n",
            "  Fold 18 Epoch 44/150  Train Loss=0.3562  Val Loss=0.6471  Val Acc=0.7078  Time=14.8s\n",
            "  Fold 18 Epoch 45/150  Train Loss=0.3571  Val Loss=0.6583  Val Acc=0.7118  Time=14.7s\n",
            "  Fold 18 Epoch 46/150  Train Loss=0.3513  Val Loss=0.7041  Val Acc=0.6804  Time=14.4s\n",
            "  Fold 18 Epoch 47/150  Train Loss=0.3356  Val Loss=0.6614  Val Acc=0.7235  Time=14.6s\n",
            "  Fold 18 Epoch 48/150  Train Loss=0.3325  Val Loss=0.6513  Val Acc=0.7196  Time=14.4s\n",
            "  Fold 18 Epoch 49/150  Train Loss=0.3272  Val Loss=0.6709  Val Acc=0.6941  Time=14.4s\n",
            "  Fold 18 Epoch 50/150  Train Loss=0.3169  Val Loss=0.6650  Val Acc=0.7118  Time=14.3s\n",
            "  Fold 18 Epoch 51/150  Train Loss=0.2962  Val Loss=0.6707  Val Acc=0.6941  Time=14.5s\n",
            "  Fold 18 Epoch 52/150  Train Loss=0.3091  Val Loss=0.6748  Val Acc=0.7078  Time=14.8s\n",
            "  Fold 18 Epoch 53/150  Train Loss=0.2982  Val Loss=0.6523  Val Acc=0.7137  Time=14.5s\n",
            "  Fold 18 Epoch 54/150  Train Loss=0.2943  Val Loss=0.6831  Val Acc=0.6961  Time=14.4s\n",
            "  → Early stopping at epoch 54 (no val_loss improvement)\n",
            "  Fold 18 Final ACC = 0.0755   (TP=4  TN=0  FP=0  FN=49)\n",
            "\n",
            ">>> Fold 19/52  (leave out sub-055)\n",
            "  Fold 19 Epoch 1/150  Train Loss=0.7131  Val Loss=0.6555  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 2/150  Train Loss=0.7149  Val Loss=0.6473  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 3/150  Train Loss=0.7054  Val Loss=0.6411  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 19 Epoch 4/150  Train Loss=0.7014  Val Loss=0.6402  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 19 Epoch 5/150  Train Loss=0.6978  Val Loss=0.6407  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 19 Epoch 6/150  Train Loss=0.6905  Val Loss=0.6416  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 19 Epoch 7/150  Train Loss=0.6951  Val Loss=0.6457  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 19 Epoch 8/150  Train Loss=0.6950  Val Loss=0.6332  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 19 Epoch 9/150  Train Loss=0.6930  Val Loss=0.6374  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 10/150  Train Loss=0.6971  Val Loss=0.6373  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 19 Epoch 11/150  Train Loss=0.6892  Val Loss=0.6369  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 12/150  Train Loss=0.6903  Val Loss=0.6396  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 19 Epoch 13/150  Train Loss=0.6923  Val Loss=0.6467  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 19 Epoch 14/150  Train Loss=0.6956  Val Loss=0.6403  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 15/150  Train Loss=0.6881  Val Loss=0.6416  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 19 Epoch 16/150  Train Loss=0.6850  Val Loss=0.6532  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 17/150  Train Loss=0.6718  Val Loss=0.6369  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 19 Epoch 18/150  Train Loss=0.6580  Val Loss=0.6479  Val Acc=0.6951  Time=14.3s\n",
            "  Fold 19 Epoch 19/150  Train Loss=0.6241  Val Loss=0.6641  Val Acc=0.7443  Time=14.6s\n",
            "  Fold 19 Epoch 20/150  Train Loss=0.5902  Val Loss=0.6862  Val Acc=0.6572  Time=14.6s\n",
            "  Fold 19 Epoch 21/150  Train Loss=0.5722  Val Loss=0.6693  Val Acc=0.6629  Time=14.5s\n",
            "  Fold 19 Epoch 22/150  Train Loss=0.5567  Val Loss=0.6795  Val Acc=0.5928  Time=14.4s\n",
            "  Fold 19 Epoch 23/150  Train Loss=0.5493  Val Loss=0.6460  Val Acc=0.6591  Time=14.5s\n",
            "  → Early stopping at epoch 23 (no val_loss improvement)\n",
            "  Fold 19 Final ACC = 0.0000   (TP=0  TN=0  FP=52  FN=0)\n",
            "\n",
            ">>> Fold 20/52  (leave out sub-056)\n",
            "  Fold 20 Epoch 1/150  Train Loss=0.8067  Val Loss=0.8448  Val Acc=0.3258  Time=14.9s\n",
            "  Fold 20 Epoch 2/150  Train Loss=0.7828  Val Loss=0.8019  Val Acc=0.3258  Time=15.6s\n",
            "  Fold 20 Epoch 3/150  Train Loss=0.7699  Val Loss=0.7787  Val Acc=0.3258  Time=15.6s\n",
            "  Fold 20 Epoch 4/150  Train Loss=0.7613  Val Loss=0.7571  Val Acc=0.3258  Time=15.3s\n",
            "  Fold 20 Epoch 5/150  Train Loss=0.7465  Val Loss=0.7487  Val Acc=0.3258  Time=15.3s\n",
            "  Fold 20 Epoch 6/150  Train Loss=0.7375  Val Loss=0.6801  Val Acc=0.6705  Time=15.3s\n",
            "  Fold 20 Epoch 7/150  Train Loss=0.7050  Val Loss=0.6414  Val Acc=0.6742  Time=15.5s\n",
            "  Fold 20 Epoch 8/150  Train Loss=0.6941  Val Loss=0.6386  Val Acc=0.6742  Time=15.4s\n",
            "  Fold 20 Epoch 9/150  Train Loss=0.6945  Val Loss=0.6398  Val Acc=0.6742  Time=15.2s\n",
            "  Fold 20 Epoch 10/150  Train Loss=0.6937  Val Loss=0.6382  Val Acc=0.6742  Time=15.3s\n",
            "  Fold 20 Epoch 11/150  Train Loss=0.6938  Val Loss=0.6372  Val Acc=0.6742  Time=15.4s\n",
            "  Fold 20 Epoch 12/150  Train Loss=0.6944  Val Loss=0.6358  Val Acc=0.6742  Time=15.5s\n",
            "  Fold 20 Epoch 13/150  Train Loss=0.6886  Val Loss=0.6401  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 20 Epoch 14/150  Train Loss=0.6956  Val Loss=0.6431  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 20 Epoch 15/150  Train Loss=0.6893  Val Loss=0.6416  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 20 Epoch 16/150  Train Loss=0.6892  Val Loss=0.6594  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 20 Epoch 17/150  Train Loss=0.6762  Val Loss=0.6413  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 20 Epoch 18/150  Train Loss=0.6899  Val Loss=0.6483  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 20 Epoch 19/150  Train Loss=0.6833  Val Loss=0.6412  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 20 Epoch 20/150  Train Loss=0.6862  Val Loss=0.6517  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 20 Epoch 21/150  Train Loss=0.6816  Val Loss=0.6488  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 20 Epoch 22/150  Train Loss=0.6805  Val Loss=0.6629  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 20 Epoch 23/150  Train Loss=0.6697  Val Loss=0.6613  Val Acc=0.6761  Time=14.5s\n",
            "  Fold 20 Epoch 24/150  Train Loss=0.6617  Val Loss=0.6519  Val Acc=0.7102  Time=14.4s\n",
            "  Fold 20 Epoch 25/150  Train Loss=0.6236  Val Loss=0.5940  Val Acc=0.7746  Time=14.4s\n",
            "  Fold 20 Epoch 26/150  Train Loss=0.5930  Val Loss=0.5968  Val Acc=0.7557  Time=14.6s\n",
            "  Fold 20 Epoch 27/150  Train Loss=0.5609  Val Loss=0.6808  Val Acc=0.6231  Time=14.7s\n",
            "  Fold 20 Epoch 28/150  Train Loss=0.5270  Val Loss=0.6295  Val Acc=0.7083  Time=14.6s\n",
            "  Fold 20 Epoch 29/150  Train Loss=0.5127  Val Loss=0.6825  Val Acc=0.6686  Time=14.3s\n",
            "  Fold 20 Epoch 30/150  Train Loss=0.4902  Val Loss=0.6738  Val Acc=0.6591  Time=14.4s\n",
            "  Fold 20 Epoch 31/150  Train Loss=0.4821  Val Loss=0.6839  Val Acc=0.6686  Time=14.4s\n",
            "  Fold 20 Epoch 32/150  Train Loss=0.4636  Val Loss=0.6296  Val Acc=0.7292  Time=14.4s\n",
            "  Fold 20 Epoch 33/150  Train Loss=0.4513  Val Loss=0.7039  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 20 Epoch 34/150  Train Loss=0.4372  Val Loss=0.6471  Val Acc=0.7102  Time=14.7s\n",
            "  Fold 20 Epoch 35/150  Train Loss=0.4348  Val Loss=0.8312  Val Acc=0.5341  Time=14.6s\n",
            "  Fold 20 Epoch 36/150  Train Loss=0.4325  Val Loss=0.6846  Val Acc=0.6799  Time=14.5s\n",
            "  Fold 20 Epoch 37/150  Train Loss=0.4068  Val Loss=0.7134  Val Acc=0.6629  Time=14.5s\n",
            "  Fold 20 Epoch 38/150  Train Loss=0.3975  Val Loss=0.7148  Val Acc=0.6610  Time=14.4s\n",
            "  Fold 20 Epoch 39/150  Train Loss=0.3896  Val Loss=0.6931  Val Acc=0.6837  Time=14.4s\n",
            "  Fold 20 Epoch 40/150  Train Loss=0.3837  Val Loss=0.7737  Val Acc=0.6004  Time=14.4s\n",
            "  → Early stopping at epoch 40 (no val_loss improvement)\n",
            "  Fold 20 Final ACC = 0.1154   (TP=0  TN=6  FP=46  FN=0)\n",
            "\n",
            ">>> Fold 21/52  (leave out sub-057)\n",
            "  Fold 21 Epoch 1/150  Train Loss=0.6996  Val Loss=0.6425  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 21 Epoch 2/150  Train Loss=0.6925  Val Loss=0.6504  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 21 Epoch 3/150  Train Loss=0.6996  Val Loss=0.6572  Val Acc=0.6833  Time=14.3s\n",
            "  Fold 21 Epoch 4/150  Train Loss=0.6922  Val Loss=0.6526  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 5/150  Train Loss=0.6994  Val Loss=0.6501  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 6/150  Train Loss=0.6869  Val Loss=0.6491  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 7/150  Train Loss=0.6890  Val Loss=0.6433  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 21 Epoch 8/150  Train Loss=0.6905  Val Loss=0.6473  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 21 Epoch 9/150  Train Loss=0.7067  Val Loss=0.6408  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 21 Epoch 10/150  Train Loss=0.6940  Val Loss=0.6428  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 21 Epoch 11/150  Train Loss=0.6914  Val Loss=0.6402  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 12/150  Train Loss=0.6913  Val Loss=0.6417  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 13/150  Train Loss=0.6956  Val Loss=0.6458  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 14/150  Train Loss=0.6926  Val Loss=0.6431  Val Acc=0.6833  Time=14.3s\n",
            "  Fold 21 Epoch 15/150  Train Loss=0.6866  Val Loss=0.6420  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 16/150  Train Loss=0.6897  Val Loss=0.6417  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 21 Epoch 17/150  Train Loss=0.6981  Val Loss=0.6489  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 21 Epoch 18/150  Train Loss=0.6932  Val Loss=0.6361  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 21 Epoch 19/150  Train Loss=0.6915  Val Loss=0.6375  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 20/150  Train Loss=0.6908  Val Loss=0.6399  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 21 Epoch 21/150  Train Loss=0.6886  Val Loss=0.6334  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 22/150  Train Loss=0.6825  Val Loss=0.6464  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 21 Epoch 23/150  Train Loss=0.6823  Val Loss=0.6533  Val Acc=0.6775  Time=14.7s\n",
            "  Fold 21 Epoch 24/150  Train Loss=0.6747  Val Loss=0.6609  Val Acc=0.6468  Time=14.5s\n",
            "  Fold 21 Epoch 25/150  Train Loss=0.6467  Val Loss=0.7690  Val Acc=0.3282  Time=14.4s\n",
            "  Fold 21 Epoch 26/150  Train Loss=0.6533  Val Loss=0.6102  Val Acc=0.7274  Time=14.4s\n",
            "  Fold 21 Epoch 27/150  Train Loss=0.6229  Val Loss=0.6265  Val Acc=0.6756  Time=14.5s\n",
            "  Fold 21 Epoch 28/150  Train Loss=0.6210  Val Loss=0.5398  Val Acc=0.8522  Time=14.5s\n",
            "  Fold 21 Epoch 29/150  Train Loss=0.6145  Val Loss=0.5797  Val Acc=0.7831  Time=14.4s\n",
            "  Fold 21 Epoch 30/150  Train Loss=0.5929  Val Loss=0.6357  Val Acc=0.6430  Time=14.7s\n",
            "  Fold 21 Epoch 31/150  Train Loss=0.5733  Val Loss=0.5442  Val Acc=0.8157  Time=14.6s\n",
            "  Fold 21 Epoch 32/150  Train Loss=0.5719  Val Loss=0.5276  Val Acc=0.8196  Time=14.5s\n",
            "  Fold 21 Epoch 33/150  Train Loss=0.5478  Val Loss=0.5696  Val Acc=0.7351  Time=14.5s\n",
            "  Fold 21 Epoch 34/150  Train Loss=0.5264  Val Loss=0.6048  Val Acc=0.6641  Time=14.5s\n",
            "  Fold 21 Epoch 35/150  Train Loss=0.5160  Val Loss=0.5833  Val Acc=0.6775  Time=14.6s\n",
            "  Fold 21 Epoch 36/150  Train Loss=0.5007  Val Loss=0.5364  Val Acc=0.7179  Time=14.4s\n",
            "  Fold 21 Epoch 37/150  Train Loss=0.4876  Val Loss=0.5467  Val Acc=0.6948  Time=14.7s\n",
            "  Fold 21 Epoch 38/150  Train Loss=0.4776  Val Loss=0.5322  Val Acc=0.7083  Time=14.7s\n",
            "  Fold 21 Epoch 39/150  Train Loss=0.4656  Val Loss=0.4900  Val Acc=0.7793  Time=14.6s\n",
            "  Fold 21 Epoch 40/150  Train Loss=0.4562  Val Loss=0.5202  Val Acc=0.7198  Time=14.3s\n",
            "  Fold 21 Epoch 41/150  Train Loss=0.4491  Val Loss=0.4703  Val Acc=0.7735  Time=14.4s\n",
            "  Fold 21 Epoch 42/150  Train Loss=0.4455  Val Loss=0.5086  Val Acc=0.7198  Time=14.5s\n",
            "  Fold 21 Epoch 43/150  Train Loss=0.4336  Val Loss=0.5065  Val Acc=0.7409  Time=14.5s\n",
            "  Fold 21 Epoch 44/150  Train Loss=0.4165  Val Loss=0.4704  Val Acc=0.7850  Time=14.5s\n",
            "  Fold 21 Epoch 45/150  Train Loss=0.4168  Val Loss=0.5020  Val Acc=0.7390  Time=14.6s\n",
            "  Fold 21 Epoch 46/150  Train Loss=0.4190  Val Loss=0.4932  Val Acc=0.7294  Time=14.4s\n",
            "  Fold 21 Epoch 47/150  Train Loss=0.4133  Val Loss=0.4699  Val Acc=0.7678  Time=14.5s\n",
            "  Fold 21 Epoch 48/150  Train Loss=0.3894  Val Loss=0.4530  Val Acc=0.7965  Time=14.4s\n",
            "  Fold 21 Epoch 49/150  Train Loss=0.3937  Val Loss=0.5146  Val Acc=0.7159  Time=14.4s\n",
            "  Fold 21 Epoch 50/150  Train Loss=0.3843  Val Loss=0.4863  Val Acc=0.7466  Time=14.4s\n",
            "  Fold 21 Epoch 51/150  Train Loss=0.3900  Val Loss=0.5095  Val Acc=0.7332  Time=14.5s\n",
            "  Fold 21 Epoch 52/150  Train Loss=0.3714  Val Loss=0.4570  Val Acc=0.7889  Time=14.6s\n",
            "  Fold 21 Epoch 53/150  Train Loss=0.3704  Val Loss=0.5174  Val Acc=0.7255  Time=14.5s\n",
            "  Fold 21 Epoch 54/150  Train Loss=0.3644  Val Loss=0.4223  Val Acc=0.8215  Time=14.4s\n",
            "  Fold 21 Epoch 55/150  Train Loss=0.3572  Val Loss=0.4644  Val Acc=0.7716  Time=14.6s\n",
            "  Fold 21 Epoch 56/150  Train Loss=0.3531  Val Loss=0.5473  Val Acc=0.6987  Time=14.7s\n",
            "  Fold 21 Epoch 57/150  Train Loss=0.3471  Val Loss=0.4374  Val Acc=0.8157  Time=14.6s\n",
            "  Fold 21 Epoch 58/150  Train Loss=0.3376  Val Loss=0.4922  Val Acc=0.7409  Time=14.4s\n",
            "  Fold 21 Epoch 59/150  Train Loss=0.3445  Val Loss=0.4894  Val Acc=0.7505  Time=14.7s\n",
            "  Fold 21 Epoch 60/150  Train Loss=0.3301  Val Loss=0.4494  Val Acc=0.7927  Time=14.6s\n",
            "  Fold 21 Epoch 61/150  Train Loss=0.3304  Val Loss=0.4322  Val Acc=0.8119  Time=14.4s\n",
            "  Fold 21 Epoch 62/150  Train Loss=0.3256  Val Loss=0.4593  Val Acc=0.7869  Time=14.5s\n",
            "  Fold 21 Epoch 63/150  Train Loss=0.3185  Val Loss=0.4229  Val Acc=0.8234  Time=14.6s\n",
            "  Fold 21 Epoch 64/150  Train Loss=0.3112  Val Loss=0.4841  Val Acc=0.7869  Time=14.4s\n",
            "  Fold 21 Epoch 65/150  Train Loss=0.3122  Val Loss=0.4958  Val Acc=0.7582  Time=14.4s\n",
            "  Fold 21 Epoch 66/150  Train Loss=0.2993  Val Loss=0.5000  Val Acc=0.7697  Time=14.6s\n",
            "  Fold 21 Epoch 67/150  Train Loss=0.3031  Val Loss=0.4742  Val Acc=0.7850  Time=14.7s\n",
            "  Fold 21 Epoch 68/150  Train Loss=0.2987  Val Loss=0.4427  Val Acc=0.8023  Time=14.5s\n",
            "  Fold 21 Epoch 69/150  Train Loss=0.2822  Val Loss=0.4905  Val Acc=0.7716  Time=14.4s\n",
            "  → Early stopping at epoch 69 (no val_loss improvement)\n",
            "  Fold 21 Final ACC = 0.0962   (TP=0  TN=5  FP=47  FN=0)\n",
            "\n",
            ">>> Fold 22/52  (leave out sub-058)\n",
            "  Fold 22 Epoch 1/150  Train Loss=0.7283  Val Loss=0.6945  Val Acc=0.4297  Time=14.4s\n",
            "  Fold 22 Epoch 2/150  Train Loss=0.7285  Val Loss=0.6893  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 22 Epoch 3/150  Train Loss=0.7239  Val Loss=0.6878  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 22 Epoch 4/150  Train Loss=0.7114  Val Loss=0.6840  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 22 Epoch 5/150  Train Loss=0.7179  Val Loss=0.6767  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 22 Epoch 6/150  Train Loss=0.7058  Val Loss=0.6788  Val Acc=0.6055  Time=14.3s\n",
            "  Fold 22 Epoch 7/150  Train Loss=0.7075  Val Loss=0.6743  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 22 Epoch 8/150  Train Loss=0.7084  Val Loss=0.6724  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 22 Epoch 9/150  Train Loss=0.7065  Val Loss=0.6763  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 22 Epoch 10/150  Train Loss=0.7026  Val Loss=0.6729  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 22 Epoch 11/150  Train Loss=0.7087  Val Loss=0.6734  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 22 Epoch 12/150  Train Loss=0.7063  Val Loss=0.6710  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 22 Epoch 13/150  Train Loss=0.7020  Val Loss=0.6709  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 22 Epoch 14/150  Train Loss=0.7004  Val Loss=0.6707  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 22 Epoch 15/150  Train Loss=0.7050  Val Loss=0.6722  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 22 Epoch 16/150  Train Loss=0.6878  Val Loss=0.6712  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 22 Epoch 17/150  Train Loss=0.7018  Val Loss=0.6708  Val Acc=0.6055  Time=14.3s\n",
            "  Fold 22 Epoch 18/150  Train Loss=0.6926  Val Loss=0.6824  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 22 Epoch 19/150  Train Loss=0.6985  Val Loss=0.6708  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 22 Epoch 20/150  Train Loss=0.6971  Val Loss=0.6837  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 22 Epoch 21/150  Train Loss=0.6887  Val Loss=0.6707  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 22 Epoch 22/150  Train Loss=0.6932  Val Loss=0.6692  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 22 Epoch 23/150  Train Loss=0.6710  Val Loss=0.6638  Val Acc=0.6328  Time=14.4s\n",
            "  Fold 22 Epoch 24/150  Train Loss=0.6321  Val Loss=0.6767  Val Acc=0.7246  Time=14.4s\n",
            "  Fold 22 Epoch 25/150  Train Loss=0.6026  Val Loss=0.6850  Val Acc=0.7324  Time=14.4s\n",
            "  Fold 22 Epoch 26/150  Train Loss=0.5839  Val Loss=0.6717  Val Acc=0.7285  Time=14.6s\n",
            "  Fold 22 Epoch 27/150  Train Loss=0.5723  Val Loss=0.6745  Val Acc=0.7227  Time=14.7s\n",
            "  Fold 22 Epoch 28/150  Train Loss=0.5662  Val Loss=0.6873  Val Acc=0.7246  Time=14.6s\n",
            "  Fold 22 Epoch 29/150  Train Loss=0.5462  Val Loss=0.6733  Val Acc=0.7305  Time=14.4s\n",
            "  Fold 22 Epoch 30/150  Train Loss=0.5382  Val Loss=0.6845  Val Acc=0.7129  Time=14.4s\n",
            "  Fold 22 Epoch 31/150  Train Loss=0.5211  Val Loss=0.6874  Val Acc=0.6973  Time=14.4s\n",
            "  Fold 22 Epoch 32/150  Train Loss=0.5216  Val Loss=0.6838  Val Acc=0.6934  Time=14.4s\n",
            "  Fold 22 Epoch 33/150  Train Loss=0.5161  Val Loss=0.6743  Val Acc=0.7012  Time=14.7s\n",
            "  Fold 22 Epoch 34/150  Train Loss=0.5029  Val Loss=0.6750  Val Acc=0.6973  Time=14.7s\n",
            "  Fold 22 Epoch 35/150  Train Loss=0.4923  Val Loss=0.6687  Val Acc=0.6992  Time=14.6s\n",
            "  Fold 22 Epoch 36/150  Train Loss=0.4722  Val Loss=0.6740  Val Acc=0.6895  Time=14.4s\n",
            "  Fold 22 Epoch 37/150  Train Loss=0.4584  Val Loss=0.6872  Val Acc=0.6562  Time=14.4s\n",
            "  Fold 22 Epoch 38/150  Train Loss=0.4568  Val Loss=0.7155  Val Acc=0.6367  Time=14.5s\n",
            "  → Early stopping at epoch 38 (no val_loss improvement)\n",
            "  Fold 22 Final ACC = 1.0000   (TP=52  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 23/52  (leave out sub-059)\n",
            "  Fold 23 Epoch 1/150  Train Loss=0.7969  Val Loss=0.7265  Val Acc=0.3984  Time=14.4s\n",
            "  Fold 23 Epoch 2/150  Train Loss=0.7818  Val Loss=0.7234  Val Acc=0.3984  Time=14.7s\n",
            "  Fold 23 Epoch 3/150  Train Loss=0.7912  Val Loss=0.7114  Val Acc=0.3984  Time=14.6s\n",
            "  Fold 23 Epoch 4/150  Train Loss=0.7746  Val Loss=0.7004  Val Acc=0.4004  Time=14.4s\n",
            "  Fold 23 Epoch 5/150  Train Loss=0.7658  Val Loss=0.6760  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 6/150  Train Loss=0.7334  Val Loss=0.6715  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 7/150  Train Loss=0.7265  Val Loss=0.6738  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 23 Epoch 8/150  Train Loss=0.7243  Val Loss=0.6758  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 23 Epoch 9/150  Train Loss=0.7056  Val Loss=0.6726  Val Acc=0.6016  Time=14.8s\n",
            "  Fold 23 Epoch 10/150  Train Loss=0.6979  Val Loss=0.6744  Val Acc=0.6016  Time=14.8s\n",
            "  Fold 23 Epoch 11/150  Train Loss=0.7005  Val Loss=0.6729  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 23 Epoch 12/150  Train Loss=0.6981  Val Loss=0.6733  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 23 Epoch 13/150  Train Loss=0.7015  Val Loss=0.6727  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 14/150  Train Loss=0.6988  Val Loss=0.6739  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 15/150  Train Loss=0.6989  Val Loss=0.6732  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 23 Epoch 16/150  Train Loss=0.7056  Val Loss=0.6751  Val Acc=0.6016  Time=14.7s\n",
            "  Fold 23 Epoch 17/150  Train Loss=0.6975  Val Loss=0.6736  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 23 Epoch 18/150  Train Loss=0.7005  Val Loss=0.6763  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 19/150  Train Loss=0.6996  Val Loss=0.6757  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 23 Epoch 20/150  Train Loss=0.7030  Val Loss=0.6738  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 21/150  Train Loss=0.6955  Val Loss=0.6713  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 23 Epoch 22/150  Train Loss=0.6864  Val Loss=0.6715  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 23 Epoch 23/150  Train Loss=0.6834  Val Loss=0.6653  Val Acc=0.6489  Time=14.6s\n",
            "  Fold 23 Epoch 24/150  Train Loss=0.6641  Val Loss=0.6646  Val Acc=0.6489  Time=14.7s\n",
            "  Fold 23 Epoch 25/150  Train Loss=0.6571  Val Loss=0.6525  Val Acc=0.6647  Time=14.6s\n",
            "  Fold 23 Epoch 26/150  Train Loss=0.6514  Val Loss=0.6378  Val Acc=0.6588  Time=14.5s\n",
            "  Fold 23 Epoch 27/150  Train Loss=0.6099  Val Loss=0.6298  Val Acc=0.6568  Time=14.5s\n",
            "  Fold 23 Epoch 28/150  Train Loss=0.5875  Val Loss=0.6244  Val Acc=0.6746  Time=14.4s\n",
            "  Fold 23 Epoch 29/150  Train Loss=0.5902  Val Loss=0.6167  Val Acc=0.6844  Time=14.5s\n",
            "  Fold 23 Epoch 30/150  Train Loss=0.5535  Val Loss=0.6074  Val Acc=0.6765  Time=14.7s\n",
            "  Fold 23 Epoch 31/150  Train Loss=0.5492  Val Loss=0.6089  Val Acc=0.7140  Time=14.8s\n",
            "  Fold 23 Epoch 32/150  Train Loss=0.5250  Val Loss=0.6110  Val Acc=0.7061  Time=14.6s\n",
            "  Fold 23 Epoch 33/150  Train Loss=0.5122  Val Loss=0.5980  Val Acc=0.7179  Time=14.5s\n",
            "  Fold 23 Epoch 34/150  Train Loss=0.5033  Val Loss=0.6243  Val Acc=0.7140  Time=14.6s\n",
            "  Fold 23 Epoch 35/150  Train Loss=0.5033  Val Loss=0.6022  Val Acc=0.7160  Time=14.5s\n",
            "  Fold 23 Epoch 36/150  Train Loss=0.4911  Val Loss=0.6104  Val Acc=0.7179  Time=14.5s\n",
            "  Fold 23 Epoch 37/150  Train Loss=0.4669  Val Loss=0.6227  Val Acc=0.7081  Time=14.5s\n",
            "  Fold 23 Epoch 38/150  Train Loss=0.4754  Val Loss=0.6323  Val Acc=0.7179  Time=14.7s\n",
            "  Fold 23 Epoch 39/150  Train Loss=0.4490  Val Loss=0.6681  Val Acc=0.6824  Time=14.4s\n",
            "  Fold 23 Epoch 40/150  Train Loss=0.4517  Val Loss=0.6595  Val Acc=0.6982  Time=14.4s\n",
            "  Fold 23 Epoch 41/150  Train Loss=0.4430  Val Loss=0.6862  Val Acc=0.6765  Time=14.6s\n",
            "  Fold 23 Epoch 42/150  Train Loss=0.4425  Val Loss=0.7117  Val Acc=0.6450  Time=14.6s\n",
            "  Fold 23 Epoch 43/150  Train Loss=0.4252  Val Loss=0.6746  Val Acc=0.6706  Time=14.5s\n",
            "  Fold 23 Epoch 44/150  Train Loss=0.4229  Val Loss=0.7081  Val Acc=0.6686  Time=14.6s\n",
            "  Fold 23 Epoch 45/150  Train Loss=0.4114  Val Loss=0.6694  Val Acc=0.6805  Time=14.7s\n",
            "  Fold 23 Epoch 46/150  Train Loss=0.4075  Val Loss=0.7004  Val Acc=0.6627  Time=14.5s\n",
            "  Fold 23 Epoch 47/150  Train Loss=0.3865  Val Loss=0.6910  Val Acc=0.6686  Time=14.3s\n",
            "  Fold 23 Epoch 48/150  Train Loss=0.4029  Val Loss=0.8057  Val Acc=0.6272  Time=14.6s\n",
            "  → Early stopping at epoch 48 (no val_loss improvement)\n",
            "  Fold 23 Final ACC = 0.5000   (TP=26  TN=0  FP=0  FN=26)\n",
            "\n",
            ">>> Fold 24/52  (leave out sub-060)\n",
            "  Fold 24 Epoch 1/150  Train Loss=0.7111  Val Loss=0.6791  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 24 Epoch 2/150  Train Loss=0.7032  Val Loss=0.6781  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 24 Epoch 3/150  Train Loss=0.7088  Val Loss=0.6781  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 24 Epoch 4/150  Train Loss=0.7074  Val Loss=0.6802  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 24 Epoch 5/150  Train Loss=0.7034  Val Loss=0.6804  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 24 Epoch 6/150  Train Loss=0.7015  Val Loss=0.6756  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 24 Epoch 7/150  Train Loss=0.7102  Val Loss=0.6749  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 24 Epoch 8/150  Train Loss=0.7070  Val Loss=0.6803  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 24 Epoch 9/150  Train Loss=0.7052  Val Loss=0.6775  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 24 Epoch 10/150  Train Loss=0.7010  Val Loss=0.6779  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 24 Epoch 11/150  Train Loss=0.6998  Val Loss=0.6772  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 24 Epoch 12/150  Train Loss=0.6932  Val Loss=0.6807  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 24 Epoch 13/150  Train Loss=0.7018  Val Loss=0.6811  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 24 Epoch 14/150  Train Loss=0.7051  Val Loss=0.6793  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 24 Epoch 15/150  Train Loss=0.6947  Val Loss=0.6807  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 24 Epoch 16/150  Train Loss=0.6724  Val Loss=0.6757  Val Acc=0.6562  Time=14.5s\n",
            "  Fold 24 Epoch 17/150  Train Loss=0.6634  Val Loss=0.6693  Val Acc=0.6777  Time=14.6s\n",
            "  Fold 24 Epoch 18/150  Train Loss=0.6315  Val Loss=0.6739  Val Acc=0.7070  Time=14.7s\n",
            "  Fold 24 Epoch 19/150  Train Loss=0.6094  Val Loss=0.6742  Val Acc=0.7383  Time=14.6s\n",
            "  Fold 24 Epoch 20/150  Train Loss=0.5793  Val Loss=0.6785  Val Acc=0.6660  Time=14.4s\n",
            "  Fold 24 Epoch 21/150  Train Loss=0.5570  Val Loss=0.7337  Val Acc=0.6582  Time=14.6s\n",
            "  Fold 24 Epoch 22/150  Train Loss=0.5463  Val Loss=0.7062  Val Acc=0.6719  Time=14.6s\n",
            "  Fold 24 Epoch 23/150  Train Loss=0.5208  Val Loss=0.6962  Val Acc=0.6875  Time=14.4s\n",
            "  Fold 24 Epoch 24/150  Train Loss=0.5180  Val Loss=0.7173  Val Acc=0.7070  Time=14.5s\n",
            "  Fold 24 Epoch 25/150  Train Loss=0.5145  Val Loss=0.6845  Val Acc=0.6836  Time=14.7s\n",
            "  Fold 24 Epoch 26/150  Train Loss=0.4829  Val Loss=0.6781  Val Acc=0.7031  Time=14.5s\n",
            "  Fold 24 Epoch 27/150  Train Loss=0.4887  Val Loss=0.6869  Val Acc=0.6699  Time=14.4s\n",
            "  Fold 24 Epoch 28/150  Train Loss=0.4808  Val Loss=0.6799  Val Acc=0.6602  Time=14.5s\n",
            "  Fold 24 Epoch 29/150  Train Loss=0.4635  Val Loss=0.6994  Val Acc=0.7207  Time=14.5s\n",
            "  Fold 24 Epoch 30/150  Train Loss=0.4518  Val Loss=0.7119  Val Acc=0.6992  Time=14.4s\n",
            "  Fold 24 Epoch 31/150  Train Loss=0.4533  Val Loss=0.6933  Val Acc=0.6699  Time=14.6s\n",
            "  Fold 24 Epoch 32/150  Train Loss=0.4434  Val Loss=0.7017  Val Acc=0.6895  Time=14.6s\n",
            "  → Early stopping at epoch 32 (no val_loss improvement)\n",
            "  Fold 24 Final ACC = 1.0000   (TP=51  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 25/52  (leave out sub-061)\n",
            "  Fold 25 Epoch 1/150  Train Loss=0.8260  Val Loss=0.7427  Val Acc=0.4024  Time=14.6s\n",
            "  Fold 25 Epoch 2/150  Train Loss=0.7660  Val Loss=0.7085  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 25 Epoch 3/150  Train Loss=0.7420  Val Loss=0.7019  Val Acc=0.4024  Time=14.5s\n",
            "  Fold 25 Epoch 4/150  Train Loss=0.7402  Val Loss=0.7047  Val Acc=0.4024  Time=14.6s\n",
            "  Fold 25 Epoch 5/150  Train Loss=0.7362  Val Loss=0.7014  Val Acc=0.4024  Time=14.5s\n",
            "  Fold 25 Epoch 6/150  Train Loss=0.7401  Val Loss=0.7006  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 25 Epoch 7/150  Train Loss=0.7129  Val Loss=0.7003  Val Acc=0.4024  Time=14.7s\n",
            "  Fold 25 Epoch 8/150  Train Loss=0.7291  Val Loss=0.6968  Val Acc=0.4044  Time=14.5s\n",
            "  Fold 25 Epoch 9/150  Train Loss=0.7289  Val Loss=0.6986  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 25 Epoch 10/150  Train Loss=0.7211  Val Loss=0.6993  Val Acc=0.4024  Time=14.4s\n",
            "  Fold 25 Epoch 11/150  Train Loss=0.7214  Val Loss=0.6969  Val Acc=0.4004  Time=14.5s\n",
            "  Fold 25 Epoch 12/150  Train Loss=0.7115  Val Loss=0.6907  Val Acc=0.5956  Time=14.5s\n",
            "  Fold 25 Epoch 13/150  Train Loss=0.7183  Val Loss=0.6889  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 25 Epoch 14/150  Train Loss=0.7168  Val Loss=0.6873  Val Acc=0.5976  Time=14.7s\n",
            "  Fold 25 Epoch 15/150  Train Loss=0.7133  Val Loss=0.6927  Val Acc=0.5418  Time=14.6s\n",
            "  Fold 25 Epoch 16/150  Train Loss=0.7056  Val Loss=0.6879  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 25 Epoch 17/150  Train Loss=0.7111  Val Loss=0.6843  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 25 Epoch 18/150  Train Loss=0.7080  Val Loss=0.6868  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 25 Epoch 19/150  Train Loss=0.7115  Val Loss=0.6888  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 25 Epoch 20/150  Train Loss=0.7094  Val Loss=0.6865  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 25 Epoch 21/150  Train Loss=0.7033  Val Loss=0.6842  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 25 Epoch 22/150  Train Loss=0.7044  Val Loss=0.6815  Val Acc=0.5976  Time=14.6s\n",
            "  Fold 25 Epoch 23/150  Train Loss=0.7035  Val Loss=0.6846  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 25 Epoch 24/150  Train Loss=0.6935  Val Loss=0.6793  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 25 Epoch 25/150  Train Loss=0.6947  Val Loss=0.6754  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 25 Epoch 26/150  Train Loss=0.6930  Val Loss=0.6765  Val Acc=0.5976  Time=14.5s\n",
            "  Fold 25 Epoch 27/150  Train Loss=0.6781  Val Loss=0.6857  Val Acc=0.5976  Time=14.4s\n",
            "  Fold 25 Epoch 28/150  Train Loss=0.6541  Val Loss=0.6640  Val Acc=0.6414  Time=14.6s\n",
            "  Fold 25 Epoch 29/150  Train Loss=0.6295  Val Loss=0.6566  Val Acc=0.6155  Time=14.8s\n",
            "  Fold 25 Epoch 30/150  Train Loss=0.5995  Val Loss=0.6447  Val Acc=0.6355  Time=14.4s\n",
            "  Fold 25 Epoch 31/150  Train Loss=0.5668  Val Loss=0.6463  Val Acc=0.6534  Time=14.4s\n",
            "  Fold 25 Epoch 32/150  Train Loss=0.5600  Val Loss=0.6427  Val Acc=0.6633  Time=14.5s\n",
            "  Fold 25 Epoch 33/150  Train Loss=0.5385  Val Loss=0.6361  Val Acc=0.6554  Time=14.4s\n",
            "  Fold 25 Epoch 34/150  Train Loss=0.5238  Val Loss=0.6423  Val Acc=0.6633  Time=14.6s\n",
            "  Fold 25 Epoch 35/150  Train Loss=0.5082  Val Loss=0.6387  Val Acc=0.6633  Time=14.7s\n",
            "  Fold 25 Epoch 36/150  Train Loss=0.4999  Val Loss=0.6381  Val Acc=0.6653  Time=14.7s\n",
            "  Fold 25 Epoch 37/150  Train Loss=0.4918  Val Loss=0.6650  Val Acc=0.6454  Time=14.5s\n",
            "  Fold 25 Epoch 38/150  Train Loss=0.4925  Val Loss=0.7145  Val Acc=0.5837  Time=14.4s\n",
            "  Fold 25 Epoch 39/150  Train Loss=0.4727  Val Loss=0.6932  Val Acc=0.6056  Time=14.3s\n",
            "  Fold 25 Epoch 40/150  Train Loss=0.4613  Val Loss=0.6688  Val Acc=0.6335  Time=14.4s\n",
            "  Fold 25 Epoch 41/150  Train Loss=0.4620  Val Loss=0.7100  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 25 Epoch 42/150  Train Loss=0.4511  Val Loss=0.6902  Val Acc=0.6355  Time=14.6s\n",
            "  Fold 25 Epoch 43/150  Train Loss=0.4455  Val Loss=0.6519  Val Acc=0.6474  Time=14.7s\n",
            "  Fold 25 Epoch 44/150  Train Loss=0.4223  Val Loss=0.6782  Val Acc=0.6375  Time=14.5s\n",
            "  Fold 25 Epoch 45/150  Train Loss=0.4155  Val Loss=0.6942  Val Acc=0.6394  Time=14.6s\n",
            "  Fold 25 Epoch 46/150  Train Loss=0.4109  Val Loss=0.6853  Val Acc=0.6414  Time=14.5s\n",
            "  Fold 25 Epoch 47/150  Train Loss=0.3936  Val Loss=0.7246  Val Acc=0.6275  Time=14.4s\n",
            "  Fold 25 Epoch 48/150  Train Loss=0.3935  Val Loss=0.6639  Val Acc=0.6514  Time=14.4s\n",
            "  → Early stopping at epoch 48 (no val_loss improvement)\n",
            "  Fold 25 Final ACC = 0.6275   (TP=32  TN=0  FP=0  FN=19)\n",
            "\n",
            ">>> Fold 26/52  (leave out sub-062)\n",
            "  Fold 26 Epoch 1/150  Train Loss=0.7592  Val Loss=0.7225  Val Acc=0.3976  Time=14.6s\n",
            "  Fold 26 Epoch 2/150  Train Loss=0.7727  Val Loss=0.7058  Val Acc=0.3976  Time=14.7s\n",
            "  Fold 26 Epoch 3/150  Train Loss=0.7206  Val Loss=0.6942  Val Acc=0.4646  Time=14.4s\n",
            "  Fold 26 Epoch 4/150  Train Loss=0.7205  Val Loss=0.6869  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 5/150  Train Loss=0.7256  Val Loss=0.6877  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 6/150  Train Loss=0.7176  Val Loss=0.6914  Val Acc=0.5610  Time=14.5s\n",
            "  Fold 26 Epoch 7/150  Train Loss=0.7128  Val Loss=0.6871  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 8/150  Train Loss=0.7068  Val Loss=0.6813  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 9/150  Train Loss=0.7081  Val Loss=0.6729  Val Acc=0.6024  Time=14.7s\n",
            "  Fold 26 Epoch 10/150  Train Loss=0.7057  Val Loss=0.6769  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 11/150  Train Loss=0.7052  Val Loss=0.6811  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 12/150  Train Loss=0.7028  Val Loss=0.6780  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 13/150  Train Loss=0.7046  Val Loss=0.6777  Val Acc=0.6024  Time=14.4s\n",
            "  Fold 26 Epoch 14/150  Train Loss=0.7084  Val Loss=0.6769  Val Acc=0.6024  Time=14.4s\n",
            "  Fold 26 Epoch 15/150  Train Loss=0.6999  Val Loss=0.6745  Val Acc=0.6024  Time=14.6s\n",
            "  Fold 26 Epoch 16/150  Train Loss=0.7010  Val Loss=0.6781  Val Acc=0.6024  Time=14.7s\n",
            "  Fold 26 Epoch 17/150  Train Loss=0.7055  Val Loss=0.6821  Val Acc=0.6024  Time=14.7s\n",
            "  Fold 26 Epoch 18/150  Train Loss=0.7015  Val Loss=0.6863  Val Acc=0.6024  Time=14.4s\n",
            "  Fold 26 Epoch 19/150  Train Loss=0.7000  Val Loss=0.6772  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 20/150  Train Loss=0.6980  Val Loss=0.6749  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 21/150  Train Loss=0.6891  Val Loss=0.6737  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 22/150  Train Loss=0.7043  Val Loss=0.6804  Val Acc=0.6024  Time=14.5s\n",
            "  Fold 26 Epoch 23/150  Train Loss=0.6982  Val Loss=0.6784  Val Acc=0.6024  Time=14.8s\n",
            "  Fold 26 Epoch 24/150  Train Loss=0.7034  Val Loss=0.6756  Val Acc=0.6024  Time=14.6s\n",
            "  → Early stopping at epoch 24 (no val_loss improvement)\n",
            "  Fold 26 Final ACC = 1.0000   (TP=51  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 27/52  (leave out sub-063)\n",
            "  Fold 27 Epoch 1/150  Train Loss=0.7356  Val Loss=0.7041  Val Acc=0.4024  Time=14.5s\n",
            "  Fold 27 Epoch 2/150  Train Loss=0.7251  Val Loss=0.6935  Val Acc=0.5049  Time=14.5s\n",
            "  Fold 27 Epoch 3/150  Train Loss=0.7235  Val Loss=0.6950  Val Acc=0.4655  Time=14.5s\n",
            "  Fold 27 Epoch 4/150  Train Loss=0.7213  Val Loss=0.6922  Val Acc=0.5641  Time=14.6s\n",
            "  Fold 27 Epoch 5/150  Train Loss=0.7176  Val Loss=0.6932  Val Acc=0.5207  Time=14.5s\n",
            "  Fold 27 Epoch 6/150  Train Loss=0.7257  Val Loss=0.6909  Val Acc=0.5759  Time=14.7s\n",
            "  Fold 27 Epoch 7/150  Train Loss=0.7188  Val Loss=0.6973  Val Acc=0.3905  Time=14.5s\n",
            "  Fold 27 Epoch 8/150  Train Loss=0.7073  Val Loss=0.6934  Val Acc=0.4990  Time=14.5s\n",
            "  Fold 27 Epoch 9/150  Train Loss=0.7072  Val Loss=0.6893  Val Acc=0.5996  Time=14.4s\n",
            "  Fold 27 Epoch 10/150  Train Loss=0.7185  Val Loss=0.6946  Val Acc=0.4300  Time=14.6s\n",
            "  Fold 27 Epoch 11/150  Train Loss=0.6918  Val Loss=0.6834  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 27 Epoch 12/150  Train Loss=0.7159  Val Loss=0.6800  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 27 Epoch 13/150  Train Loss=0.7121  Val Loss=0.6817  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 27 Epoch 14/150  Train Loss=0.7044  Val Loss=0.6758  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 27 Epoch 15/150  Train Loss=0.7067  Val Loss=0.6903  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 27 Epoch 16/150  Train Loss=0.7070  Val Loss=0.6863  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 27 Epoch 17/150  Train Loss=0.7020  Val Loss=0.6764  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 27 Epoch 18/150  Train Loss=0.6978  Val Loss=0.6706  Val Acc=0.6016  Time=14.3s\n",
            "  Fold 27 Epoch 19/150  Train Loss=0.7057  Val Loss=0.6835  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 27 Epoch 20/150  Train Loss=0.7035  Val Loss=0.6717  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 27 Epoch 21/150  Train Loss=0.6817  Val Loss=0.6703  Val Acc=0.6667  Time=14.6s\n",
            "  Fold 27 Epoch 22/150  Train Loss=0.6697  Val Loss=0.6720  Val Acc=0.6529  Time=14.5s\n",
            "  Fold 27 Epoch 23/150  Train Loss=0.6377  Val Loss=0.6587  Val Acc=0.7041  Time=14.5s\n",
            "  Fold 27 Epoch 24/150  Train Loss=0.5942  Val Loss=0.6567  Val Acc=0.7081  Time=14.5s\n",
            "  Fold 27 Epoch 25/150  Train Loss=0.5749  Val Loss=0.6583  Val Acc=0.7199  Time=14.4s\n",
            "  Fold 27 Epoch 26/150  Train Loss=0.5546  Val Loss=0.6646  Val Acc=0.7416  Time=14.4s\n",
            "  Fold 27 Epoch 27/150  Train Loss=0.5305  Val Loss=0.6849  Val Acc=0.6963  Time=14.6s\n",
            "  Fold 27 Epoch 28/150  Train Loss=0.5289  Val Loss=0.6704  Val Acc=0.7613  Time=14.6s\n",
            "  Fold 27 Epoch 29/150  Train Loss=0.5100  Val Loss=0.6900  Val Acc=0.7673  Time=14.5s\n",
            "  Fold 27 Epoch 30/150  Train Loss=0.5049  Val Loss=0.6767  Val Acc=0.7673  Time=14.6s\n",
            "  Fold 27 Epoch 31/150  Train Loss=0.4896  Val Loss=0.6814  Val Acc=0.7554  Time=14.6s\n",
            "  Fold 27 Epoch 32/150  Train Loss=0.4696  Val Loss=0.6873  Val Acc=0.7179  Time=14.6s\n",
            "  Fold 27 Epoch 33/150  Train Loss=0.4818  Val Loss=0.6588  Val Acc=0.7535  Time=14.5s\n",
            "  Fold 27 Epoch 34/150  Train Loss=0.4607  Val Loss=0.6971  Val Acc=0.7101  Time=14.7s\n",
            "  Fold 27 Epoch 35/150  Train Loss=0.4569  Val Loss=0.6664  Val Acc=0.7495  Time=14.6s\n",
            "  Fold 27 Epoch 36/150  Train Loss=0.4362  Val Loss=0.6709  Val Acc=0.7318  Time=14.4s\n",
            "  Fold 27 Epoch 37/150  Train Loss=0.4211  Val Loss=0.6690  Val Acc=0.7258  Time=14.4s\n",
            "  Fold 27 Epoch 38/150  Train Loss=0.4375  Val Loss=0.6897  Val Acc=0.7160  Time=14.5s\n",
            "  Fold 27 Epoch 39/150  Train Loss=0.4237  Val Loss=0.6597  Val Acc=0.7357  Time=14.5s\n",
            "  → Early stopping at epoch 39 (no val_loss improvement)\n",
            "  Fold 27 Final ACC = 0.9412   (TP=48  TN=0  FP=0  FN=3)\n",
            "\n",
            ">>> Fold 28/52  (leave out sub-064)\n",
            "  Fold 28 Epoch 1/150  Train Loss=0.7154  Val Loss=0.6859  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 2/150  Train Loss=0.7098  Val Loss=0.6811  Val Acc=0.6016  Time=14.7s\n",
            "  Fold 28 Epoch 3/150  Train Loss=0.7147  Val Loss=0.6792  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 28 Epoch 4/150  Train Loss=0.7077  Val Loss=0.6775  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 5/150  Train Loss=0.7055  Val Loss=0.6752  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 6/150  Train Loss=0.7047  Val Loss=0.6747  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 28 Epoch 7/150  Train Loss=0.6989  Val Loss=0.6736  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 8/150  Train Loss=0.7037  Val Loss=0.6712  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 28 Epoch 9/150  Train Loss=0.7053  Val Loss=0.6722  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 28 Epoch 10/150  Train Loss=0.6995  Val Loss=0.6768  Val Acc=0.6016  Time=14.7s\n",
            "  Fold 28 Epoch 11/150  Train Loss=0.6975  Val Loss=0.6777  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 28 Epoch 12/150  Train Loss=0.6956  Val Loss=0.6759  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 28 Epoch 13/150  Train Loss=0.6945  Val Loss=0.6744  Val Acc=0.6016  Time=14.6s\n",
            "  Fold 28 Epoch 14/150  Train Loss=0.6980  Val Loss=0.6764  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 15/150  Train Loss=0.6902  Val Loss=0.6743  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 16/150  Train Loss=0.6898  Val Loss=0.6759  Val Acc=0.6016  Time=14.8s\n",
            "  Fold 28 Epoch 17/150  Train Loss=0.6981  Val Loss=0.6696  Val Acc=0.6016  Time=14.7s\n",
            "  Fold 28 Epoch 18/150  Train Loss=0.6912  Val Loss=0.6694  Val Acc=0.6016  Time=14.5s\n",
            "  Fold 28 Epoch 19/150  Train Loss=0.6857  Val Loss=0.6649  Val Acc=0.6016  Time=14.4s\n",
            "  Fold 28 Epoch 20/150  Train Loss=0.6729  Val Loss=0.6798  Val Acc=0.6114  Time=14.5s\n",
            "  Fold 28 Epoch 21/150  Train Loss=0.6511  Val Loss=0.6548  Val Acc=0.6805  Time=14.4s\n",
            "  Fold 28 Epoch 22/150  Train Loss=0.6233  Val Loss=0.6543  Val Acc=0.7002  Time=14.5s\n",
            "  Fold 28 Epoch 23/150  Train Loss=0.5952  Val Loss=0.6705  Val Acc=0.6805  Time=14.6s\n",
            "  Fold 28 Epoch 24/150  Train Loss=0.5759  Val Loss=0.6351  Val Acc=0.7258  Time=14.6s\n",
            "  Fold 28 Epoch 25/150  Train Loss=0.5709  Val Loss=0.6307  Val Acc=0.6943  Time=14.5s\n",
            "  Fold 28 Epoch 26/150  Train Loss=0.5518  Val Loss=0.6489  Val Acc=0.7318  Time=14.5s\n",
            "  Fold 28 Epoch 27/150  Train Loss=0.5291  Val Loss=0.6223  Val Acc=0.7199  Time=14.6s\n",
            "  Fold 28 Epoch 28/150  Train Loss=0.5182  Val Loss=0.6383  Val Acc=0.7535  Time=14.4s\n",
            "  Fold 28 Epoch 29/150  Train Loss=0.5151  Val Loss=0.6419  Val Acc=0.7554  Time=14.4s\n",
            "  Fold 28 Epoch 30/150  Train Loss=0.4965  Val Loss=0.6448  Val Acc=0.7239  Time=14.6s\n",
            "  Fold 28 Epoch 31/150  Train Loss=0.4846  Val Loss=0.6617  Val Acc=0.7101  Time=14.6s\n",
            "  Fold 28 Epoch 32/150  Train Loss=0.4758  Val Loss=0.6542  Val Acc=0.7416  Time=14.5s\n",
            "  Fold 28 Epoch 33/150  Train Loss=0.4678  Val Loss=0.6593  Val Acc=0.7239  Time=14.4s\n",
            "  Fold 28 Epoch 34/150  Train Loss=0.4657  Val Loss=0.6621  Val Acc=0.7337  Time=14.5s\n",
            "  Fold 28 Epoch 35/150  Train Loss=0.4590  Val Loss=0.7220  Val Acc=0.6982  Time=14.4s\n",
            "  Fold 28 Epoch 36/150  Train Loss=0.4400  Val Loss=0.6593  Val Acc=0.7258  Time=14.4s\n",
            "  Fold 28 Epoch 37/150  Train Loss=0.4331  Val Loss=0.6697  Val Acc=0.7101  Time=14.6s\n",
            "  Fold 28 Epoch 38/150  Train Loss=0.4272  Val Loss=0.6936  Val Acc=0.6884  Time=14.6s\n",
            "  Fold 28 Epoch 39/150  Train Loss=0.4169  Val Loss=0.7018  Val Acc=0.6943  Time=14.6s\n",
            "  Fold 28 Epoch 40/150  Train Loss=0.4092  Val Loss=0.6758  Val Acc=0.6982  Time=14.5s\n",
            "  Fold 28 Epoch 41/150  Train Loss=0.3968  Val Loss=0.6595  Val Acc=0.7101  Time=14.4s\n",
            "  Fold 28 Epoch 42/150  Train Loss=0.4032  Val Loss=0.6535  Val Acc=0.6943  Time=14.5s\n",
            "  → Early stopping at epoch 42 (no val_loss improvement)\n",
            "  Fold 28 Final ACC = 1.0000   (TP=50  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 29/52  (leave out sub-065)\n",
            "  Fold 29 Epoch 1/150  Train Loss=0.7277  Val Loss=0.6796  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 2/150  Train Loss=0.7172  Val Loss=0.6755  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 29 Epoch 3/150  Train Loss=0.7197  Val Loss=0.6739  Val Acc=0.6047  Time=14.8s\n",
            "  Fold 29 Epoch 4/150  Train Loss=0.7150  Val Loss=0.6737  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 5/150  Train Loss=0.7076  Val Loss=0.6709  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 29 Epoch 6/150  Train Loss=0.7182  Val Loss=0.6711  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 29 Epoch 7/150  Train Loss=0.7067  Val Loss=0.6705  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 8/150  Train Loss=0.7081  Val Loss=0.6741  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 9/150  Train Loss=0.7045  Val Loss=0.6717  Val Acc=0.6047  Time=14.7s\n",
            "  Fold 29 Epoch 10/150  Train Loss=0.7102  Val Loss=0.6719  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 29 Epoch 11/150  Train Loss=0.7027  Val Loss=0.6706  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 12/150  Train Loss=0.7044  Val Loss=0.6712  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 29 Epoch 13/150  Train Loss=0.6981  Val Loss=0.6713  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 14/150  Train Loss=0.7100  Val Loss=0.6717  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 15/150  Train Loss=0.6968  Val Loss=0.6712  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 29 Epoch 16/150  Train Loss=0.7048  Val Loss=0.6703  Val Acc=0.6047  Time=14.8s\n",
            "  Fold 29 Epoch 17/150  Train Loss=0.6932  Val Loss=0.6701  Val Acc=0.6047  Time=14.7s\n",
            "  Fold 29 Epoch 18/150  Train Loss=0.7052  Val Loss=0.6723  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 29 Epoch 19/150  Train Loss=0.6898  Val Loss=0.6690  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 29 Epoch 20/150  Train Loss=0.6893  Val Loss=0.6683  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 21/150  Train Loss=0.6899  Val Loss=0.6659  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 29 Epoch 22/150  Train Loss=0.6749  Val Loss=0.6614  Val Acc=0.6047  Time=14.3s\n",
            "  Fold 29 Epoch 23/150  Train Loss=0.6634  Val Loss=0.6567  Val Acc=0.6282  Time=14.6s\n",
            "  Fold 29 Epoch 24/150  Train Loss=0.6359  Val Loss=0.6485  Val Acc=0.6614  Time=14.6s\n",
            "  Fold 29 Epoch 25/150  Train Loss=0.5898  Val Loss=0.6819  Val Acc=0.6986  Time=14.3s\n",
            "  Fold 29 Epoch 26/150  Train Loss=0.5601  Val Loss=0.6719  Val Acc=0.6517  Time=14.5s\n",
            "  Fold 29 Epoch 27/150  Train Loss=0.5455  Val Loss=0.6993  Val Acc=0.6614  Time=14.4s\n",
            "  Fold 29 Epoch 28/150  Train Loss=0.5347  Val Loss=0.6950  Val Acc=0.6614  Time=14.4s\n",
            "  Fold 29 Epoch 29/150  Train Loss=0.5215  Val Loss=0.7324  Val Acc=0.6595  Time=14.4s\n",
            "  Fold 29 Epoch 30/150  Train Loss=0.5117  Val Loss=0.6677  Val Acc=0.6536  Time=14.6s\n",
            "  Fold 29 Epoch 31/150  Train Loss=0.5015  Val Loss=0.6774  Val Acc=0.6810  Time=14.8s\n",
            "  Fold 29 Epoch 32/150  Train Loss=0.4820  Val Loss=0.7029  Val Acc=0.6556  Time=14.5s\n",
            "  Fold 29 Epoch 33/150  Train Loss=0.4866  Val Loss=0.6897  Val Acc=0.6967  Time=14.5s\n",
            "  Fold 29 Epoch 34/150  Train Loss=0.4842  Val Loss=0.6782  Val Acc=0.7202  Time=14.5s\n",
            "  Fold 29 Epoch 35/150  Train Loss=0.4675  Val Loss=0.6700  Val Acc=0.6791  Time=14.6s\n",
            "  Fold 29 Epoch 36/150  Train Loss=0.4640  Val Loss=0.6833  Val Acc=0.6888  Time=14.6s\n",
            "  Fold 29 Epoch 37/150  Train Loss=0.4533  Val Loss=0.6768  Val Acc=0.6849  Time=14.7s\n",
            "  Fold 29 Epoch 38/150  Train Loss=0.4486  Val Loss=0.6873  Val Acc=0.6791  Time=14.8s\n",
            "  Fold 29 Epoch 39/150  Train Loss=0.4481  Val Loss=0.7009  Val Acc=0.6810  Time=14.6s\n",
            "  → Early stopping at epoch 39 (no val_loss improvement)\n",
            "  Fold 29 Final ACC = 0.9200   (TP=46  TN=0  FP=0  FN=4)\n",
            "\n",
            ">>> Fold 30/52  (leave out sub-066)\n",
            "  Fold 30 Epoch 1/150  Train Loss=0.7325  Val Loss=0.6918  Val Acc=0.5273  Time=14.6s\n",
            "  Fold 30 Epoch 2/150  Train Loss=0.7210  Val Loss=0.6783  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 30 Epoch 3/150  Train Loss=0.7151  Val Loss=0.6785  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 30 Epoch 4/150  Train Loss=0.7145  Val Loss=0.6773  Val Acc=0.6055  Time=14.3s\n",
            "  Fold 30 Epoch 5/150  Train Loss=0.7159  Val Loss=0.6772  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 30 Epoch 6/150  Train Loss=0.7155  Val Loss=0.6782  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 30 Epoch 7/150  Train Loss=0.7168  Val Loss=0.6776  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 30 Epoch 8/150  Train Loss=0.7069  Val Loss=0.6769  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 30 Epoch 9/150  Train Loss=0.7058  Val Loss=0.6727  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 30 Epoch 10/150  Train Loss=0.7097  Val Loss=0.6714  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 30 Epoch 11/150  Train Loss=0.7050  Val Loss=0.6711  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 30 Epoch 12/150  Train Loss=0.7085  Val Loss=0.6733  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 30 Epoch 13/150  Train Loss=0.7084  Val Loss=0.6742  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 30 Epoch 14/150  Train Loss=0.7135  Val Loss=0.6789  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 30 Epoch 15/150  Train Loss=0.7028  Val Loss=0.6757  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 30 Epoch 16/150  Train Loss=0.7101  Val Loss=0.6761  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 30 Epoch 17/150  Train Loss=0.7061  Val Loss=0.6731  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 30 Epoch 18/150  Train Loss=0.7055  Val Loss=0.6724  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 30 Epoch 19/150  Train Loss=0.7065  Val Loss=0.6691  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 30 Epoch 20/150  Train Loss=0.7006  Val Loss=0.6672  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 30 Epoch 21/150  Train Loss=0.6881  Val Loss=0.6605  Val Acc=0.6094  Time=14.6s\n",
            "  Fold 30 Epoch 22/150  Train Loss=0.6551  Val Loss=0.6840  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 30 Epoch 23/150  Train Loss=0.6855  Val Loss=0.6533  Val Acc=0.6445  Time=14.5s\n",
            "  Fold 30 Epoch 24/150  Train Loss=0.6206  Val Loss=0.6597  Val Acc=0.6602  Time=14.5s\n",
            "  Fold 30 Epoch 25/150  Train Loss=0.5805  Val Loss=0.6623  Val Acc=0.6680  Time=14.4s\n",
            "  Fold 30 Epoch 26/150  Train Loss=0.5495  Val Loss=0.6919  Val Acc=0.6680  Time=14.7s\n",
            "  Fold 30 Epoch 27/150  Train Loss=0.5280  Val Loss=0.7011  Val Acc=0.6797  Time=14.7s\n",
            "  Fold 30 Epoch 28/150  Train Loss=0.5062  Val Loss=0.7214  Val Acc=0.6719  Time=14.5s\n",
            "  Fold 30 Epoch 29/150  Train Loss=0.5003  Val Loss=0.7022  Val Acc=0.6973  Time=14.5s\n",
            "  Fold 30 Epoch 30/150  Train Loss=0.4892  Val Loss=0.7074  Val Acc=0.7090  Time=14.5s\n",
            "  Fold 30 Epoch 31/150  Train Loss=0.4726  Val Loss=0.7091  Val Acc=0.7168  Time=14.5s\n",
            "  Fold 30 Epoch 32/150  Train Loss=0.4645  Val Loss=0.7003  Val Acc=0.7109  Time=14.5s\n",
            "  Fold 30 Epoch 33/150  Train Loss=0.4485  Val Loss=0.7138  Val Acc=0.7227  Time=14.7s\n",
            "  Fold 30 Epoch 34/150  Train Loss=0.4509  Val Loss=0.7089  Val Acc=0.7070  Time=14.7s\n",
            "  Fold 30 Epoch 35/150  Train Loss=0.4435  Val Loss=0.7300  Val Acc=0.6953  Time=14.4s\n",
            "  Fold 30 Epoch 36/150  Train Loss=0.4283  Val Loss=0.6941  Val Acc=0.6973  Time=14.5s\n",
            "  Fold 30 Epoch 37/150  Train Loss=0.4208  Val Loss=0.6989  Val Acc=0.7148  Time=14.5s\n",
            "  Fold 30 Epoch 38/150  Train Loss=0.4039  Val Loss=0.7733  Val Acc=0.6270  Time=14.4s\n",
            "  → Early stopping at epoch 38 (no val_loss improvement)\n",
            "  Fold 30 Final ACC = 1.0000   (TP=50  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 31/52  (leave out sub-067)\n",
            "  Fold 31 Epoch 1/150  Train Loss=0.8344  Val Loss=0.7781  Val Acc=0.3945  Time=14.6s\n",
            "  Fold 31 Epoch 2/150  Train Loss=0.7792  Val Loss=0.7558  Val Acc=0.3945  Time=15.0s\n",
            "  Fold 31 Epoch 3/150  Train Loss=0.7431  Val Loss=0.7063  Val Acc=0.3945  Time=14.7s\n",
            "  Fold 31 Epoch 4/150  Train Loss=0.7336  Val Loss=0.6950  Val Acc=0.4277  Time=14.6s\n",
            "  Fold 31 Epoch 5/150  Train Loss=0.7169  Val Loss=0.6879  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 6/150  Train Loss=0.7060  Val Loss=0.6823  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 7/150  Train Loss=0.7092  Val Loss=0.6838  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 31 Epoch 8/150  Train Loss=0.6984  Val Loss=0.6833  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 31 Epoch 9/150  Train Loss=0.7052  Val Loss=0.6764  Val Acc=0.6055  Time=14.9s\n",
            "  Fold 31 Epoch 10/150  Train Loss=0.7041  Val Loss=0.6748  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 31 Epoch 11/150  Train Loss=0.7030  Val Loss=0.6787  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 31 Epoch 12/150  Train Loss=0.6960  Val Loss=0.6737  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 13/150  Train Loss=0.6992  Val Loss=0.6722  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 14/150  Train Loss=0.6956  Val Loss=0.6711  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 15/150  Train Loss=0.6897  Val Loss=0.6723  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 31 Epoch 16/150  Train Loss=0.6969  Val Loss=0.6709  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 31 Epoch 17/150  Train Loss=0.6938  Val Loss=0.6713  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 31 Epoch 18/150  Train Loss=0.6899  Val Loss=0.6719  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 19/150  Train Loss=0.6924  Val Loss=0.6725  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 20/150  Train Loss=0.6902  Val Loss=0.6705  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 21/150  Train Loss=0.6843  Val Loss=0.6691  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 22/150  Train Loss=0.6831  Val Loss=0.6720  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 23/150  Train Loss=0.6871  Val Loss=0.6721  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 31 Epoch 24/150  Train Loss=0.6873  Val Loss=0.6683  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 31 Epoch 25/150  Train Loss=0.6770  Val Loss=0.6567  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 31 Epoch 26/150  Train Loss=0.6667  Val Loss=0.6676  Val Acc=0.6504  Time=14.6s\n",
            "  Fold 31 Epoch 27/150  Train Loss=0.6537  Val Loss=0.6368  Val Acc=0.6426  Time=14.6s\n",
            "  Fold 31 Epoch 28/150  Train Loss=0.6381  Val Loss=0.6431  Val Acc=0.6934  Time=14.5s\n",
            "  Fold 31 Epoch 29/150  Train Loss=0.6200  Val Loss=0.6517  Val Acc=0.7031  Time=14.8s\n",
            "  Fold 31 Epoch 30/150  Train Loss=0.5962  Val Loss=0.6215  Val Acc=0.6758  Time=14.6s\n",
            "  Fold 31 Epoch 31/150  Train Loss=0.5743  Val Loss=0.6345  Val Acc=0.6992  Time=14.6s\n",
            "  Fold 31 Epoch 32/150  Train Loss=0.5573  Val Loss=0.6628  Val Acc=0.6797  Time=14.5s\n",
            "  Fold 31 Epoch 33/150  Train Loss=0.5546  Val Loss=0.6247  Val Acc=0.6816  Time=14.7s\n",
            "  Fold 31 Epoch 34/150  Train Loss=0.5387  Val Loss=0.6607  Val Acc=0.6836  Time=14.5s\n",
            "  Fold 31 Epoch 35/150  Train Loss=0.5284  Val Loss=0.6517  Val Acc=0.6777  Time=14.5s\n",
            "  Fold 31 Epoch 36/150  Train Loss=0.5122  Val Loss=0.6584  Val Acc=0.6797  Time=14.7s\n",
            "  Fold 31 Epoch 37/150  Train Loss=0.5135  Val Loss=0.6292  Val Acc=0.7090  Time=14.8s\n",
            "  Fold 31 Epoch 38/150  Train Loss=0.5013  Val Loss=0.6427  Val Acc=0.6914  Time=14.5s\n",
            "  Fold 31 Epoch 39/150  Train Loss=0.4946  Val Loss=0.6521  Val Acc=0.6914  Time=14.6s\n",
            "  Fold 31 Epoch 40/150  Train Loss=0.4975  Val Loss=0.6902  Val Acc=0.6582  Time=14.5s\n",
            "  Fold 31 Epoch 41/150  Train Loss=0.4820  Val Loss=0.6570  Val Acc=0.6895  Time=14.5s\n",
            "  Fold 31 Epoch 42/150  Train Loss=0.4620  Val Loss=0.7284  Val Acc=0.6270  Time=14.6s\n",
            "  Fold 31 Epoch 43/150  Train Loss=0.4507  Val Loss=0.7065  Val Acc=0.6406  Time=14.7s\n",
            "  Fold 31 Epoch 44/150  Train Loss=0.4455  Val Loss=0.7429  Val Acc=0.6172  Time=14.7s\n",
            "  Fold 31 Epoch 45/150  Train Loss=0.4403  Val Loss=0.6865  Val Acc=0.6523  Time=14.5s\n",
            "  → Early stopping at epoch 45 (no val_loss improvement)\n",
            "  Fold 31 Final ACC = 0.7400   (TP=37  TN=0  FP=0  FN=13)\n",
            "\n",
            ">>> Fold 32/52  (leave out sub-068)\n",
            "  Fold 32 Epoch 1/150  Train Loss=0.7089  Val Loss=0.6486  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 2/150  Train Loss=0.7069  Val Loss=0.6469  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 32 Epoch 3/150  Train Loss=0.7061  Val Loss=0.6486  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 4/150  Train Loss=0.7181  Val Loss=0.6432  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 5/150  Train Loss=0.7014  Val Loss=0.6490  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 32 Epoch 6/150  Train Loss=0.6985  Val Loss=0.6509  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 32 Epoch 7/150  Train Loss=0.7099  Val Loss=0.6520  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 8/150  Train Loss=0.6981  Val Loss=0.6500  Val Acc=0.6781  Time=14.4s\n",
            "  Fold 32 Epoch 9/150  Train Loss=0.7041  Val Loss=0.6528  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 10/150  Train Loss=0.7042  Val Loss=0.6536  Val Acc=0.6781  Time=14.4s\n",
            "  Fold 32 Epoch 11/150  Train Loss=0.6972  Val Loss=0.6485  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 12/150  Train Loss=0.6947  Val Loss=0.6416  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 13/150  Train Loss=0.7025  Val Loss=0.6461  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 32 Epoch 14/150  Train Loss=0.6982  Val Loss=0.6530  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 15/150  Train Loss=0.6971  Val Loss=0.6530  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 16/150  Train Loss=0.7044  Val Loss=0.6429  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 17/150  Train Loss=0.6958  Val Loss=0.6553  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 18/150  Train Loss=0.6984  Val Loss=0.6553  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 19/150  Train Loss=0.7028  Val Loss=0.6562  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 20/150  Train Loss=0.6912  Val Loss=0.6428  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 32 Epoch 21/150  Train Loss=0.6864  Val Loss=0.6433  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 32 Epoch 22/150  Train Loss=0.6909  Val Loss=0.6260  Val Acc=0.6781  Time=14.3s\n",
            "  Fold 32 Epoch 23/150  Train Loss=0.6734  Val Loss=0.6008  Val Acc=0.6762  Time=14.4s\n",
            "  Fold 32 Epoch 24/150  Train Loss=0.6502  Val Loss=0.5528  Val Acc=0.7143  Time=14.3s\n",
            "  Fold 32 Epoch 25/150  Train Loss=0.6173  Val Loss=0.5041  Val Acc=0.7943  Time=14.5s\n",
            "  Fold 32 Epoch 26/150  Train Loss=0.5937  Val Loss=0.5822  Val Acc=0.6705  Time=14.5s\n",
            "  Fold 32 Epoch 27/150  Train Loss=0.5600  Val Loss=0.4977  Val Acc=0.7867  Time=14.5s\n",
            "  Fold 32 Epoch 28/150  Train Loss=0.5419  Val Loss=0.4981  Val Acc=0.7619  Time=14.5s\n",
            "  Fold 32 Epoch 29/150  Train Loss=0.5169  Val Loss=0.4742  Val Acc=0.8019  Time=14.4s\n",
            "  Fold 32 Epoch 30/150  Train Loss=0.5186  Val Loss=0.5755  Val Acc=0.6686  Time=14.4s\n",
            "  Fold 32 Epoch 31/150  Train Loss=0.4805  Val Loss=0.6127  Val Acc=0.6133  Time=14.4s\n",
            "  Fold 32 Epoch 32/150  Train Loss=0.4938  Val Loss=0.4994  Val Acc=0.7733  Time=14.5s\n",
            "  Fold 32 Epoch 33/150  Train Loss=0.4808  Val Loss=0.7128  Val Acc=0.5143  Time=14.4s\n",
            "  Fold 32 Epoch 34/150  Train Loss=0.4558  Val Loss=0.5045  Val Acc=0.7562  Time=14.5s\n",
            "  Fold 32 Epoch 35/150  Train Loss=0.4535  Val Loss=0.6138  Val Acc=0.6495  Time=14.7s\n",
            "  Fold 32 Epoch 36/150  Train Loss=0.4395  Val Loss=0.4939  Val Acc=0.7771  Time=14.4s\n",
            "  Fold 32 Epoch 37/150  Train Loss=0.4257  Val Loss=0.5936  Val Acc=0.6819  Time=14.3s\n",
            "  Fold 32 Epoch 38/150  Train Loss=0.4186  Val Loss=0.5526  Val Acc=0.7333  Time=14.3s\n",
            "  Fold 32 Epoch 39/150  Train Loss=0.4240  Val Loss=0.6369  Val Acc=0.6705  Time=14.4s\n",
            "  Fold 32 Epoch 40/150  Train Loss=0.3976  Val Loss=0.4714  Val Acc=0.7790  Time=14.4s\n",
            "  Fold 32 Epoch 41/150  Train Loss=0.3996  Val Loss=0.5283  Val Acc=0.7600  Time=14.4s\n",
            "  Fold 32 Epoch 42/150  Train Loss=0.3922  Val Loss=0.5391  Val Acc=0.7600  Time=14.6s\n",
            "  Fold 32 Epoch 43/150  Train Loss=0.3748  Val Loss=0.6031  Val Acc=0.6990  Time=14.7s\n",
            "  Fold 32 Epoch 44/150  Train Loss=0.3646  Val Loss=0.6410  Val Acc=0.6914  Time=14.4s\n",
            "  Fold 32 Epoch 45/150  Train Loss=0.3604  Val Loss=0.6341  Val Acc=0.6990  Time=14.5s\n",
            "  Fold 32 Epoch 46/150  Train Loss=0.3508  Val Loss=0.4994  Val Acc=0.7600  Time=14.4s\n",
            "  Fold 32 Epoch 47/150  Train Loss=0.3386  Val Loss=0.5881  Val Acc=0.7200  Time=14.5s\n",
            "  Fold 32 Epoch 48/150  Train Loss=0.3430  Val Loss=0.6252  Val Acc=0.6990  Time=14.4s\n",
            "  Fold 32 Epoch 49/150  Train Loss=0.3362  Val Loss=0.7343  Val Acc=0.6133  Time=14.6s\n",
            "  Fold 32 Epoch 50/150  Train Loss=0.3130  Val Loss=0.5806  Val Acc=0.7143  Time=14.6s\n",
            "  Fold 32 Epoch 51/150  Train Loss=0.3319  Val Loss=0.5899  Val Acc=0.7105  Time=14.5s\n",
            "  Fold 32 Epoch 52/150  Train Loss=0.3102  Val Loss=0.6369  Val Acc=0.7048  Time=14.4s\n",
            "  Fold 32 Epoch 53/150  Train Loss=0.3083  Val Loss=0.6543  Val Acc=0.6971  Time=14.3s\n",
            "  Fold 32 Epoch 54/150  Train Loss=0.3007  Val Loss=0.6968  Val Acc=0.6743  Time=14.2s\n",
            "  Fold 32 Epoch 55/150  Train Loss=0.2932  Val Loss=0.6579  Val Acc=0.7105  Time=14.5s\n",
            "  → Early stopping at epoch 55 (no val_loss improvement)\n",
            "  Fold 32 Final ACC = 0.0000   (TP=0  TN=0  FP=50  FN=0)\n",
            "\n",
            ">>> Fold 33/52  (leave out sub-069)\n",
            "  Fold 33 Epoch 1/150  Train Loss=0.7531  Val Loss=0.7374  Val Acc=0.3167  Time=14.6s\n",
            "  Fold 33 Epoch 2/150  Train Loss=0.7387  Val Loss=0.7210  Val Acc=0.3167  Time=14.6s\n",
            "  Fold 33 Epoch 3/150  Train Loss=0.7340  Val Loss=0.7008  Val Acc=0.3186  Time=14.5s\n",
            "  Fold 33 Epoch 4/150  Train Loss=0.7385  Val Loss=0.6908  Val Acc=0.5893  Time=14.4s\n",
            "  Fold 33 Epoch 5/150  Train Loss=0.7386  Val Loss=0.6853  Val Acc=0.6852  Time=14.5s\n",
            "  Fold 33 Epoch 6/150  Train Loss=0.7269  Val Loss=0.6812  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 33 Epoch 7/150  Train Loss=0.7272  Val Loss=0.6807  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 33 Epoch 8/150  Train Loss=0.7322  Val Loss=0.6839  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 33 Epoch 9/150  Train Loss=0.7195  Val Loss=0.6726  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 33 Epoch 10/150  Train Loss=0.7172  Val Loss=0.6737  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 11/150  Train Loss=0.7250  Val Loss=0.6753  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 12/150  Train Loss=0.7283  Val Loss=0.6651  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 13/150  Train Loss=0.7304  Val Loss=0.6656  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 33 Epoch 14/150  Train Loss=0.7324  Val Loss=0.6799  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 15/150  Train Loss=0.7324  Val Loss=0.6606  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 16/150  Train Loss=0.7103  Val Loss=0.6952  Val Acc=0.3589  Time=14.6s\n",
            "  Fold 33 Epoch 17/150  Train Loss=0.7275  Val Loss=0.6758  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 18/150  Train Loss=0.7163  Val Loss=0.6604  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 19/150  Train Loss=0.7133  Val Loss=0.6703  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 20/150  Train Loss=0.6968  Val Loss=0.6586  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 21/150  Train Loss=0.7130  Val Loss=0.6617  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 22/150  Train Loss=0.7094  Val Loss=0.6554  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 33 Epoch 23/150  Train Loss=0.7143  Val Loss=0.6539  Val Acc=0.7678  Time=14.7s\n",
            "  Fold 33 Epoch 24/150  Train Loss=0.6929  Val Loss=0.6000  Val Acc=0.8081  Time=14.6s\n",
            "  Fold 33 Epoch 25/150  Train Loss=0.6608  Val Loss=0.6655  Val Acc=0.6008  Time=14.5s\n",
            "  Fold 33 Epoch 26/150  Train Loss=0.6419  Val Loss=0.7175  Val Acc=0.4760  Time=14.4s\n",
            "  Fold 33 Epoch 27/150  Train Loss=0.6184  Val Loss=0.5719  Val Acc=0.7812  Time=14.5s\n",
            "  Fold 33 Epoch 28/150  Train Loss=0.5996  Val Loss=0.5287  Val Acc=0.7908  Time=14.4s\n",
            "  Fold 33 Epoch 29/150  Train Loss=0.5762  Val Loss=0.5174  Val Acc=0.7850  Time=14.7s\n",
            "  Fold 33 Epoch 30/150  Train Loss=0.5541  Val Loss=0.5681  Val Acc=0.7390  Time=14.7s\n",
            "  Fold 33 Epoch 31/150  Train Loss=0.5424  Val Loss=0.5140  Val Acc=0.8023  Time=14.5s\n",
            "  Fold 33 Epoch 32/150  Train Loss=0.5293  Val Loss=0.5653  Val Acc=0.7179  Time=14.4s\n",
            "  Fold 33 Epoch 33/150  Train Loss=0.5188  Val Loss=0.5419  Val Acc=0.7332  Time=14.5s\n",
            "  Fold 33 Epoch 34/150  Train Loss=0.4934  Val Loss=0.5507  Val Acc=0.7255  Time=14.4s\n",
            "  Fold 33 Epoch 35/150  Train Loss=0.4922  Val Loss=0.5519  Val Acc=0.7255  Time=14.5s\n",
            "  Fold 33 Epoch 36/150  Train Loss=0.4904  Val Loss=0.5668  Val Acc=0.7006  Time=14.4s\n",
            "  Fold 33 Epoch 37/150  Train Loss=0.4790  Val Loss=0.4469  Val Acc=0.8349  Time=14.8s\n",
            "  Fold 33 Epoch 38/150  Train Loss=0.4632  Val Loss=0.5535  Val Acc=0.7370  Time=14.7s\n",
            "  Fold 33 Epoch 39/150  Train Loss=0.4590  Val Loss=0.4674  Val Acc=0.8177  Time=14.3s\n",
            "  Fold 33 Epoch 40/150  Train Loss=0.4683  Val Loss=0.4955  Val Acc=0.7793  Time=14.5s\n",
            "  Fold 33 Epoch 41/150  Train Loss=0.4305  Val Loss=0.4515  Val Acc=0.8100  Time=14.4s\n",
            "  Fold 33 Epoch 42/150  Train Loss=0.4348  Val Loss=0.5657  Val Acc=0.7063  Time=14.4s\n",
            "  Fold 33 Epoch 43/150  Train Loss=0.4247  Val Loss=0.5005  Val Acc=0.7658  Time=14.4s\n",
            "  Fold 33 Epoch 44/150  Train Loss=0.4119  Val Loss=0.5013  Val Acc=0.7543  Time=14.6s\n",
            "  Fold 33 Epoch 45/150  Train Loss=0.4094  Val Loss=0.4515  Val Acc=0.7946  Time=14.6s\n",
            "  Fold 33 Epoch 46/150  Train Loss=0.4070  Val Loss=0.5710  Val Acc=0.6833  Time=14.4s\n",
            "  Fold 33 Epoch 47/150  Train Loss=0.3967  Val Loss=0.4937  Val Acc=0.7639  Time=14.3s\n",
            "  Fold 33 Epoch 48/150  Train Loss=0.3925  Val Loss=0.5019  Val Acc=0.7505  Time=14.4s\n",
            "  Fold 33 Epoch 49/150  Train Loss=0.3860  Val Loss=0.5519  Val Acc=0.7025  Time=14.5s\n",
            "  Fold 33 Epoch 50/150  Train Loss=0.3825  Val Loss=0.4737  Val Acc=0.7697  Time=14.4s\n",
            "  Fold 33 Epoch 51/150  Train Loss=0.3772  Val Loss=0.4417  Val Acc=0.8081  Time=14.7s\n",
            "  Fold 33 Epoch 52/150  Train Loss=0.3660  Val Loss=0.5051  Val Acc=0.7447  Time=14.5s\n",
            "  Fold 33 Epoch 53/150  Train Loss=0.3671  Val Loss=0.5637  Val Acc=0.6910  Time=14.4s\n",
            "  Fold 33 Epoch 54/150  Train Loss=0.3711  Val Loss=0.4456  Val Acc=0.8004  Time=14.6s\n",
            "  Fold 33 Epoch 55/150  Train Loss=0.3464  Val Loss=0.4902  Val Acc=0.7601  Time=14.6s\n",
            "  Fold 33 Epoch 56/150  Train Loss=0.3406  Val Loss=0.5877  Val Acc=0.6871  Time=14.3s\n",
            "  Fold 33 Epoch 57/150  Train Loss=0.3456  Val Loss=0.4498  Val Acc=0.7985  Time=14.5s\n",
            "  Fold 33 Epoch 58/150  Train Loss=0.3317  Val Loss=0.4982  Val Acc=0.7639  Time=14.6s\n",
            "  Fold 33 Epoch 59/150  Train Loss=0.3374  Val Loss=0.4286  Val Acc=0.8023  Time=14.5s\n",
            "  Fold 33 Epoch 60/150  Train Loss=0.3256  Val Loss=0.4456  Val Acc=0.8042  Time=14.3s\n",
            "  Fold 33 Epoch 61/150  Train Loss=0.3307  Val Loss=0.5504  Val Acc=0.7390  Time=14.4s\n",
            "  Fold 33 Epoch 62/150  Train Loss=0.3285  Val Loss=0.4629  Val Acc=0.7965  Time=14.4s\n",
            "  Fold 33 Epoch 63/150  Train Loss=0.3113  Val Loss=0.4666  Val Acc=0.7927  Time=14.6s\n",
            "  Fold 33 Epoch 64/150  Train Loss=0.3102  Val Loss=0.4583  Val Acc=0.8004  Time=14.5s\n",
            "  Fold 33 Epoch 65/150  Train Loss=0.3099  Val Loss=0.5584  Val Acc=0.7198  Time=14.7s\n",
            "  Fold 33 Epoch 66/150  Train Loss=0.3119  Val Loss=0.5034  Val Acc=0.7620  Time=14.4s\n",
            "  Fold 33 Epoch 67/150  Train Loss=0.3051  Val Loss=0.4789  Val Acc=0.7793  Time=14.4s\n",
            "  Fold 33 Epoch 68/150  Train Loss=0.2881  Val Loss=0.4629  Val Acc=0.7927  Time=14.3s\n",
            "  Fold 33 Epoch 69/150  Train Loss=0.3074  Val Loss=0.5134  Val Acc=0.7601  Time=14.3s\n",
            "  Fold 33 Epoch 70/150  Train Loss=0.2864  Val Loss=0.4898  Val Acc=0.7754  Time=14.5s\n",
            "  Fold 33 Epoch 71/150  Train Loss=0.2626  Val Loss=0.4766  Val Acc=0.7869  Time=14.7s\n",
            "  Fold 33 Epoch 72/150  Train Loss=0.2759  Val Loss=0.5072  Val Acc=0.7697  Time=14.5s\n",
            "  Fold 33 Epoch 73/150  Train Loss=0.2726  Val Loss=0.4696  Val Acc=0.7889  Time=14.2s\n",
            "  Fold 33 Epoch 74/150  Train Loss=0.2590  Val Loss=0.5300  Val Acc=0.7582  Time=14.4s\n",
            "  → Early stopping at epoch 74 (no val_loss improvement)\n",
            "  Fold 33 Final ACC = 0.1633   (TP=0  TN=8  FP=41  FN=0)\n",
            "\n",
            ">>> Fold 34/52  (leave out sub-070)\n",
            "  Fold 34 Epoch 1/150  Train Loss=0.7730  Val Loss=0.7182  Val Acc=0.3900  Time=14.5s\n",
            "  Fold 34 Epoch 2/150  Train Loss=0.7615  Val Loss=0.6876  Val Acc=0.6120  Time=14.6s\n",
            "  Fold 34 Epoch 3/150  Train Loss=0.7357  Val Loss=0.6679  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 4/150  Train Loss=0.7152  Val Loss=0.6618  Val Acc=0.6100  Time=14.6s\n",
            "  Fold 34 Epoch 5/150  Train Loss=0.7117  Val Loss=0.6613  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 6/150  Train Loss=0.7092  Val Loss=0.6624  Val Acc=0.6100  Time=14.4s\n",
            "  Fold 34 Epoch 7/150  Train Loss=0.7051  Val Loss=0.6621  Val Acc=0.6100  Time=14.3s\n",
            "  Fold 34 Epoch 8/150  Train Loss=0.7066  Val Loss=0.6609  Val Acc=0.6100  Time=14.3s\n",
            "  Fold 34 Epoch 9/150  Train Loss=0.7090  Val Loss=0.6612  Val Acc=0.6100  Time=14.4s\n",
            "  Fold 34 Epoch 10/150  Train Loss=0.7026  Val Loss=0.6627  Val Acc=0.6100  Time=14.6s\n",
            "  Fold 34 Epoch 11/150  Train Loss=0.7037  Val Loss=0.6610  Val Acc=0.6100  Time=14.6s\n",
            "  Fold 34 Epoch 12/150  Train Loss=0.7022  Val Loss=0.6627  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 13/150  Train Loss=0.7167  Val Loss=0.6615  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 14/150  Train Loss=0.7180  Val Loss=0.6705  Val Acc=0.6100  Time=14.4s\n",
            "  Fold 34 Epoch 15/150  Train Loss=0.7181  Val Loss=0.6617  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 16/150  Train Loss=0.7145  Val Loss=0.6615  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 17/150  Train Loss=0.7063  Val Loss=0.6606  Val Acc=0.6100  Time=14.7s\n",
            "  Fold 34 Epoch 18/150  Train Loss=0.7040  Val Loss=0.6720  Val Acc=0.6100  Time=14.4s\n",
            "  Fold 34 Epoch 19/150  Train Loss=0.6786  Val Loss=0.6624  Val Acc=0.6100  Time=14.5s\n",
            "  Fold 34 Epoch 20/150  Train Loss=0.6654  Val Loss=0.6651  Val Acc=0.6448  Time=14.5s\n",
            "  Fold 34 Epoch 21/150  Train Loss=0.6375  Val Loss=0.6719  Val Acc=0.6467  Time=14.5s\n",
            "  Fold 34 Epoch 22/150  Train Loss=0.6100  Val Loss=0.6821  Val Acc=0.6583  Time=14.4s\n",
            "  Fold 34 Epoch 23/150  Train Loss=0.5856  Val Loss=0.6625  Val Acc=0.6699  Time=14.6s\n",
            "  Fold 34 Epoch 24/150  Train Loss=0.5516  Val Loss=0.6847  Val Acc=0.7181  Time=14.5s\n",
            "  Fold 34 Epoch 25/150  Train Loss=0.5423  Val Loss=0.6487  Val Acc=0.6969  Time=14.4s\n",
            "  Fold 34 Epoch 26/150  Train Loss=0.5273  Val Loss=0.6577  Val Acc=0.7162  Time=14.5s\n",
            "  Fold 34 Epoch 27/150  Train Loss=0.5156  Val Loss=0.6517  Val Acc=0.7124  Time=14.3s\n",
            "  Fold 34 Epoch 28/150  Train Loss=0.5124  Val Loss=0.6429  Val Acc=0.7162  Time=14.4s\n",
            "  Fold 34 Epoch 29/150  Train Loss=0.5046  Val Loss=0.6432  Val Acc=0.7297  Time=14.4s\n",
            "  Fold 34 Epoch 30/150  Train Loss=0.4856  Val Loss=0.6648  Val Acc=0.7529  Time=14.9s\n",
            "  Fold 34 Epoch 31/150  Train Loss=0.4828  Val Loss=0.6507  Val Acc=0.7432  Time=14.7s\n",
            "  Fold 34 Epoch 32/150  Train Loss=0.4729  Val Loss=0.6555  Val Acc=0.7452  Time=14.4s\n",
            "  Fold 34 Epoch 33/150  Train Loss=0.4776  Val Loss=0.6864  Val Acc=0.7355  Time=14.4s\n",
            "  Fold 34 Epoch 34/150  Train Loss=0.4856  Val Loss=0.6493  Val Acc=0.7297  Time=14.5s\n",
            "  Fold 34 Epoch 35/150  Train Loss=0.4556  Val Loss=0.6851  Val Acc=0.7085  Time=14.4s\n",
            "  Fold 34 Epoch 36/150  Train Loss=0.4495  Val Loss=0.6833  Val Acc=0.7143  Time=14.7s\n",
            "  Fold 34 Epoch 37/150  Train Loss=0.4419  Val Loss=0.6944  Val Acc=0.7162  Time=14.7s\n",
            "  Fold 34 Epoch 38/150  Train Loss=0.4275  Val Loss=0.6923  Val Acc=0.7181  Time=14.4s\n",
            "  Fold 34 Epoch 39/150  Train Loss=0.4035  Val Loss=0.6768  Val Acc=0.7201  Time=14.4s\n",
            "  Fold 34 Epoch 40/150  Train Loss=0.4199  Val Loss=0.8225  Val Acc=0.5869  Time=14.5s\n",
            "  Fold 34 Epoch 41/150  Train Loss=0.4090  Val Loss=0.6954  Val Acc=0.7066  Time=14.4s\n",
            "  Fold 34 Epoch 42/150  Train Loss=0.4117  Val Loss=0.6586  Val Acc=0.7355  Time=14.5s\n",
            "  Fold 34 Epoch 43/150  Train Loss=0.3814  Val Loss=0.6756  Val Acc=0.7162  Time=14.8s\n",
            "  → Early stopping at epoch 43 (no val_loss improvement)\n",
            "  Fold 34 Final ACC = 1.0000   (TP=49  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 35/52  (leave out sub-071)\n",
            "  Fold 35 Epoch 1/150  Train Loss=0.7331  Val Loss=0.6747  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 2/150  Train Loss=0.7343  Val Loss=0.6731  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 35 Epoch 3/150  Train Loss=0.7340  Val Loss=0.6704  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 4/150  Train Loss=0.7272  Val Loss=0.6703  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 35 Epoch 5/150  Train Loss=0.7139  Val Loss=0.6706  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 35 Epoch 6/150  Train Loss=0.7193  Val Loss=0.6709  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 35 Epoch 7/150  Train Loss=0.7222  Val Loss=0.6705  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 35 Epoch 8/150  Train Loss=0.7190  Val Loss=0.6750  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 9/150  Train Loss=0.7161  Val Loss=0.6701  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 35 Epoch 10/150  Train Loss=0.7089  Val Loss=0.6704  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 11/150  Train Loss=0.7167  Val Loss=0.6907  Val Acc=0.5977  Time=14.5s\n",
            "  Fold 35 Epoch 12/150  Train Loss=0.7161  Val Loss=0.6792  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 35 Epoch 13/150  Train Loss=0.7135  Val Loss=0.6731  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 35 Epoch 14/150  Train Loss=0.7115  Val Loss=0.6802  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 35 Epoch 15/150  Train Loss=0.7195  Val Loss=0.6768  Val Acc=0.6055  Time=14.4s\n",
            "  Fold 35 Epoch 16/150  Train Loss=0.7145  Val Loss=0.6798  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 17/150  Train Loss=0.7012  Val Loss=0.6811  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 18/150  Train Loss=0.7106  Val Loss=0.6801  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 19/150  Train Loss=0.6914  Val Loss=0.6682  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 35 Epoch 20/150  Train Loss=0.7129  Val Loss=0.6707  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 35 Epoch 21/150  Train Loss=0.6965  Val Loss=0.6902  Val Acc=0.5371  Time=14.5s\n",
            "  Fold 35 Epoch 22/150  Train Loss=0.6931  Val Loss=0.6750  Val Acc=0.6543  Time=14.6s\n",
            "  Fold 35 Epoch 23/150  Train Loss=0.6673  Val Loss=0.6400  Val Acc=0.6367  Time=14.4s\n",
            "  Fold 35 Epoch 24/150  Train Loss=0.6269  Val Loss=0.6300  Val Acc=0.6758  Time=14.4s\n",
            "  Fold 35 Epoch 25/150  Train Loss=0.6024  Val Loss=0.6387  Val Acc=0.6582  Time=14.6s\n",
            "  Fold 35 Epoch 26/150  Train Loss=0.5633  Val Loss=0.6525  Val Acc=0.7129  Time=14.7s\n",
            "  Fold 35 Epoch 27/150  Train Loss=0.5312  Val Loss=0.6367  Val Acc=0.6992  Time=14.5s\n",
            "  Fold 35 Epoch 28/150  Train Loss=0.5213  Val Loss=0.6528  Val Acc=0.6738  Time=14.5s\n",
            "  Fold 35 Epoch 29/150  Train Loss=0.4936  Val Loss=0.7405  Val Acc=0.6855  Time=14.4s\n",
            "  Fold 35 Epoch 30/150  Train Loss=0.4912  Val Loss=0.6892  Val Acc=0.6680  Time=14.5s\n",
            "  Fold 35 Epoch 31/150  Train Loss=0.4766  Val Loss=0.7485  Val Acc=0.6816  Time=14.5s\n",
            "  Fold 35 Epoch 32/150  Train Loss=0.4642  Val Loss=0.6871  Val Acc=0.7227  Time=14.7s\n",
            "  Fold 35 Epoch 33/150  Train Loss=0.4553  Val Loss=0.7131  Val Acc=0.7051  Time=14.6s\n",
            "  Fold 35 Epoch 34/150  Train Loss=0.4453  Val Loss=0.7214  Val Acc=0.7090  Time=14.5s\n",
            "  Fold 35 Epoch 35/150  Train Loss=0.4352  Val Loss=0.6886  Val Acc=0.6992  Time=14.6s\n",
            "  Fold 35 Epoch 36/150  Train Loss=0.4200  Val Loss=0.6999  Val Acc=0.6934  Time=14.6s\n",
            "  Fold 35 Epoch 37/150  Train Loss=0.4247  Val Loss=0.6978  Val Acc=0.6895  Time=14.5s\n",
            "  Fold 35 Epoch 38/150  Train Loss=0.4139  Val Loss=0.7027  Val Acc=0.6895  Time=14.6s\n",
            "  Fold 35 Epoch 39/150  Train Loss=0.3884  Val Loss=0.7058  Val Acc=0.6758  Time=14.7s\n",
            "  → Early stopping at epoch 39 (no val_loss improvement)\n",
            "  Fold 35 Final ACC = 0.9375   (TP=45  TN=0  FP=0  FN=3)\n",
            "\n",
            ">>> Fold 36/52  (leave out sub-072)\n",
            "  Fold 36 Epoch 1/150  Train Loss=0.7177  Val Loss=0.6745  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 36 Epoch 2/150  Train Loss=0.7108  Val Loss=0.6726  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 36 Epoch 3/150  Train Loss=0.7116  Val Loss=0.6735  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 4/150  Train Loss=0.7186  Val Loss=0.6731  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 36 Epoch 5/150  Train Loss=0.7079  Val Loss=0.6744  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 36 Epoch 6/150  Train Loss=0.7230  Val Loss=0.6738  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 7/150  Train Loss=0.6978  Val Loss=0.6732  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 36 Epoch 8/150  Train Loss=0.7111  Val Loss=0.6732  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 9/150  Train Loss=0.7155  Val Loss=0.6719  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 10/150  Train Loss=0.7060  Val Loss=0.6742  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 36 Epoch 11/150  Train Loss=0.7046  Val Loss=0.6737  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 12/150  Train Loss=0.7144  Val Loss=0.6784  Val Acc=0.6039  Time=14.8s\n",
            "  Fold 36 Epoch 13/150  Train Loss=0.7096  Val Loss=0.6760  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 36 Epoch 14/150  Train Loss=0.7026  Val Loss=0.6781  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 15/150  Train Loss=0.7020  Val Loss=0.6753  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 16/150  Train Loss=0.7008  Val Loss=0.6834  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 36 Epoch 17/150  Train Loss=0.6991  Val Loss=0.6804  Val Acc=0.6039  Time=14.5s\n",
            "  Fold 36 Epoch 18/150  Train Loss=0.6975  Val Loss=0.6776  Val Acc=0.6039  Time=14.6s\n",
            "  Fold 36 Epoch 19/150  Train Loss=0.6982  Val Loss=0.6791  Val Acc=0.6039  Time=14.7s\n",
            "  Fold 36 Epoch 20/150  Train Loss=0.7007  Val Loss=0.6722  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 36 Epoch 21/150  Train Loss=0.6978  Val Loss=0.6749  Val Acc=0.6039  Time=14.3s\n",
            "  Fold 36 Epoch 22/150  Train Loss=0.6860  Val Loss=0.6660  Val Acc=0.6039  Time=14.4s\n",
            "  Fold 36 Epoch 23/150  Train Loss=0.6628  Val Loss=0.6554  Val Acc=0.6490  Time=14.5s\n",
            "  Fold 36 Epoch 24/150  Train Loss=0.6320  Val Loss=0.6698  Val Acc=0.6529  Time=14.5s\n",
            "  Fold 36 Epoch 25/150  Train Loss=0.6149  Val Loss=0.6398  Val Acc=0.6863  Time=14.8s\n",
            "  Fold 36 Epoch 26/150  Train Loss=0.5935  Val Loss=0.6405  Val Acc=0.6725  Time=14.7s\n",
            "  Fold 36 Epoch 27/150  Train Loss=0.5647  Val Loss=0.6314  Val Acc=0.6686  Time=14.5s\n",
            "  Fold 36 Epoch 28/150  Train Loss=0.5228  Val Loss=0.6430  Val Acc=0.7098  Time=14.5s\n",
            "  Fold 36 Epoch 29/150  Train Loss=0.5248  Val Loss=0.6297  Val Acc=0.6961  Time=14.5s\n",
            "  Fold 36 Epoch 30/150  Train Loss=0.5111  Val Loss=0.6375  Val Acc=0.7098  Time=14.6s\n",
            "  Fold 36 Epoch 31/150  Train Loss=0.5070  Val Loss=0.6324  Val Acc=0.7235  Time=14.7s\n",
            "  Fold 36 Epoch 32/150  Train Loss=0.4767  Val Loss=0.6343  Val Acc=0.7176  Time=14.7s\n",
            "  Fold 36 Epoch 33/150  Train Loss=0.4812  Val Loss=0.6914  Val Acc=0.6745  Time=14.5s\n",
            "  Fold 36 Epoch 34/150  Train Loss=0.4637  Val Loss=0.6401  Val Acc=0.7059  Time=14.4s\n",
            "  Fold 36 Epoch 35/150  Train Loss=0.4684  Val Loss=0.6435  Val Acc=0.7059  Time=14.4s\n",
            "  Fold 36 Epoch 36/150  Train Loss=0.4408  Val Loss=0.6497  Val Acc=0.7216  Time=14.5s\n",
            "  Fold 36 Epoch 37/150  Train Loss=0.4433  Val Loss=0.6365  Val Acc=0.7157  Time=14.5s\n",
            "  Fold 36 Epoch 38/150  Train Loss=0.4301  Val Loss=0.6503  Val Acc=0.7137  Time=14.7s\n",
            "  Fold 36 Epoch 39/150  Train Loss=0.4250  Val Loss=0.6478  Val Acc=0.7059  Time=14.6s\n",
            "  Fold 36 Epoch 40/150  Train Loss=0.4195  Val Loss=0.6416  Val Acc=0.7137  Time=14.6s\n",
            "  Fold 36 Epoch 41/150  Train Loss=0.4111  Val Loss=0.6376  Val Acc=0.7176  Time=14.6s\n",
            "  Fold 36 Epoch 42/150  Train Loss=0.3906  Val Loss=0.6273  Val Acc=0.7235  Time=14.4s\n",
            "  Fold 36 Epoch 43/150  Train Loss=0.3839  Val Loss=0.6452  Val Acc=0.6941  Time=14.5s\n",
            "  Fold 36 Epoch 44/150  Train Loss=0.3807  Val Loss=0.6298  Val Acc=0.7196  Time=14.8s\n",
            "  Fold 36 Epoch 45/150  Train Loss=0.3640  Val Loss=0.6280  Val Acc=0.7157  Time=14.5s\n",
            "  Fold 36 Epoch 46/150  Train Loss=0.3745  Val Loss=0.6413  Val Acc=0.7059  Time=14.6s\n",
            "  Fold 36 Epoch 47/150  Train Loss=0.3502  Val Loss=0.6425  Val Acc=0.7118  Time=14.3s\n",
            "  Fold 36 Epoch 48/150  Train Loss=0.3453  Val Loss=0.6284  Val Acc=0.7255  Time=14.6s\n",
            "  Fold 36 Epoch 49/150  Train Loss=0.3544  Val Loss=0.6352  Val Acc=0.7157  Time=14.6s\n",
            "  Fold 36 Epoch 50/150  Train Loss=0.3429  Val Loss=0.6480  Val Acc=0.7078  Time=14.8s\n",
            "  Fold 36 Epoch 51/150  Train Loss=0.3388  Val Loss=0.6486  Val Acc=0.6941  Time=14.6s\n",
            "  Fold 36 Epoch 52/150  Train Loss=0.3362  Val Loss=0.6507  Val Acc=0.7137  Time=14.4s\n",
            "  Fold 36 Epoch 53/150  Train Loss=0.3166  Val Loss=0.6480  Val Acc=0.7137  Time=14.4s\n",
            "  Fold 36 Epoch 54/150  Train Loss=0.3266  Val Loss=0.6528  Val Acc=0.7118  Time=14.5s\n",
            "  Fold 36 Epoch 55/150  Train Loss=0.3259  Val Loss=0.6604  Val Acc=0.7039  Time=14.6s\n",
            "  Fold 36 Epoch 56/150  Train Loss=0.3102  Val Loss=0.6402  Val Acc=0.7176  Time=14.6s\n",
            "  Fold 36 Epoch 57/150  Train Loss=0.2973  Val Loss=0.6610  Val Acc=0.7137  Time=14.6s\n",
            "  → Early stopping at epoch 57 (no val_loss improvement)\n",
            "  Fold 36 Final ACC = 0.9792   (TP=47  TN=0  FP=0  FN=1)\n",
            "\n",
            ">>> Fold 37/52  (leave out sub-073)\n",
            "  Fold 37 Epoch 1/150  Train Loss=0.8528  Val Loss=0.7507  Val Acc=0.3953  Time=14.6s\n",
            "  Fold 37 Epoch 2/150  Train Loss=0.8052  Val Loss=0.7079  Val Acc=0.3953  Time=14.3s\n",
            "  Fold 37 Epoch 3/150  Train Loss=0.7609  Val Loss=0.7006  Val Acc=0.3953  Time=14.4s\n",
            "  Fold 37 Epoch 4/150  Train Loss=0.7408  Val Loss=0.6875  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 37 Epoch 5/150  Train Loss=0.7327  Val Loss=0.6821  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 6/150  Train Loss=0.7230  Val Loss=0.6770  Val Acc=0.6047  Time=14.7s\n",
            "  Fold 37 Epoch 7/150  Train Loss=0.7236  Val Loss=0.6761  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 8/150  Train Loss=0.7119  Val Loss=0.6738  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 9/150  Train Loss=0.7217  Val Loss=0.6748  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 10/150  Train Loss=0.7136  Val Loss=0.6731  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 37 Epoch 11/150  Train Loss=0.7170  Val Loss=0.6813  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 37 Epoch 12/150  Train Loss=0.7126  Val Loss=0.6771  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 13/150  Train Loss=0.7061  Val Loss=0.6736  Val Acc=0.6047  Time=15.0s\n",
            "  Fold 37 Epoch 14/150  Train Loss=0.7135  Val Loss=0.6733  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 37 Epoch 15/150  Train Loss=0.6943  Val Loss=0.6734  Val Acc=0.6047  Time=14.3s\n",
            "  Fold 37 Epoch 16/150  Train Loss=0.7063  Val Loss=0.6737  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 37 Epoch 17/150  Train Loss=0.7104  Val Loss=0.6703  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 18/150  Train Loss=0.7018  Val Loss=0.6702  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 19/150  Train Loss=0.7060  Val Loss=0.6701  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 37 Epoch 20/150  Train Loss=0.6978  Val Loss=0.6680  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 37 Epoch 21/150  Train Loss=0.6959  Val Loss=0.6658  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 37 Epoch 22/150  Train Loss=0.6728  Val Loss=0.6634  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 37 Epoch 23/150  Train Loss=0.6466  Val Loss=0.6334  Val Acc=0.6517  Time=14.5s\n",
            "  Fold 37 Epoch 24/150  Train Loss=0.6157  Val Loss=0.6213  Val Acc=0.6986  Time=14.5s\n",
            "  Fold 37 Epoch 25/150  Train Loss=0.5647  Val Loss=0.6217  Val Acc=0.6673  Time=14.7s\n",
            "  Fold 37 Epoch 26/150  Train Loss=0.5573  Val Loss=0.6123  Val Acc=0.6673  Time=14.7s\n",
            "  Fold 37 Epoch 27/150  Train Loss=0.5492  Val Loss=0.6288  Val Acc=0.6654  Time=14.5s\n",
            "  Fold 37 Epoch 28/150  Train Loss=0.5317  Val Loss=0.6181  Val Acc=0.6614  Time=14.5s\n",
            "  Fold 37 Epoch 29/150  Train Loss=0.5159  Val Loss=0.6323  Val Acc=0.7221  Time=14.7s\n",
            "  Fold 37 Epoch 30/150  Train Loss=0.4985  Val Loss=0.6199  Val Acc=0.6849  Time=14.6s\n",
            "  Fold 37 Epoch 31/150  Train Loss=0.5003  Val Loss=0.6336  Val Acc=0.7025  Time=14.6s\n",
            "  Fold 37 Epoch 32/150  Train Loss=0.4881  Val Loss=0.6389  Val Acc=0.6849  Time=14.6s\n",
            "  Fold 37 Epoch 33/150  Train Loss=0.4677  Val Loss=0.6391  Val Acc=0.6810  Time=14.5s\n",
            "  Fold 37 Epoch 34/150  Train Loss=0.4602  Val Loss=0.6626  Val Acc=0.6869  Time=14.5s\n",
            "  Fold 37 Epoch 35/150  Train Loss=0.4457  Val Loss=0.6423  Val Acc=0.6751  Time=14.5s\n",
            "  Fold 37 Epoch 36/150  Train Loss=0.4468  Val Loss=0.6711  Val Acc=0.6771  Time=14.5s\n",
            "  Fold 37 Epoch 37/150  Train Loss=0.4286  Val Loss=0.6559  Val Acc=0.6556  Time=14.6s\n",
            "  Fold 37 Epoch 38/150  Train Loss=0.4195  Val Loss=0.6853  Val Acc=0.6556  Time=14.7s\n",
            "  Fold 37 Epoch 39/150  Train Loss=0.4062  Val Loss=0.6884  Val Acc=0.6419  Time=14.7s\n",
            "  Fold 37 Epoch 40/150  Train Loss=0.4010  Val Loss=0.7177  Val Acc=0.6321  Time=15.6s\n",
            "  Fold 37 Epoch 41/150  Train Loss=0.4033  Val Loss=0.7432  Val Acc=0.6321  Time=15.0s\n",
            "  → Early stopping at epoch 41 (no val_loss improvement)\n",
            "  Fold 37 Final ACC = 1.0000   (TP=48  TN=0  FP=0  FN=0)\n",
            "\n",
            ">>> Fold 38/52  (leave out sub-074)\n",
            "  Fold 38 Epoch 1/150  Train Loss=0.7619  Val Loss=0.7125  Val Acc=0.3953  Time=15.5s\n",
            "  Fold 38 Epoch 2/150  Train Loss=0.7643  Val Loss=0.7038  Val Acc=0.3953  Time=14.6s\n",
            "  Fold 38 Epoch 3/150  Train Loss=0.7522  Val Loss=0.6894  Val Acc=0.5988  Time=14.4s\n",
            "  Fold 38 Epoch 4/150  Train Loss=0.7228  Val Loss=0.6850  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 38 Epoch 5/150  Train Loss=0.7290  Val Loss=0.6854  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 38 Epoch 6/150  Train Loss=0.7374  Val Loss=0.6832  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 38 Epoch 7/150  Train Loss=0.7338  Val Loss=0.6830  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 38 Epoch 8/150  Train Loss=0.7150  Val Loss=0.6794  Val Acc=0.6047  Time=14.7s\n",
            "  Fold 38 Epoch 9/150  Train Loss=0.7237  Val Loss=0.6794  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 38 Epoch 10/150  Train Loss=0.7175  Val Loss=0.6765  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 38 Epoch 11/150  Train Loss=0.7217  Val Loss=0.6779  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 38 Epoch 12/150  Train Loss=0.7215  Val Loss=0.6789  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 38 Epoch 13/150  Train Loss=0.7083  Val Loss=0.6844  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 38 Epoch 14/150  Train Loss=0.7159  Val Loss=0.6785  Val Acc=0.6047  Time=14.9s\n",
            "  Fold 38 Epoch 15/150  Train Loss=0.7153  Val Loss=0.6788  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 38 Epoch 16/150  Train Loss=0.7117  Val Loss=0.6783  Val Acc=0.6047  Time=14.4s\n",
            "  Fold 38 Epoch 17/150  Train Loss=0.7053  Val Loss=0.6750  Val Acc=0.6047  Time=14.5s\n",
            "  Fold 38 Epoch 18/150  Train Loss=0.6952  Val Loss=0.6704  Val Acc=0.6047  Time=14.6s\n",
            "  Fold 38 Epoch 19/150  Train Loss=0.7002  Val Loss=0.6643  Val Acc=0.6595  Time=14.4s\n",
            "  Fold 38 Epoch 20/150  Train Loss=0.6600  Val Loss=0.6600  Val Acc=0.6634  Time=14.6s\n",
            "  Fold 38 Epoch 21/150  Train Loss=0.6414  Val Loss=0.6344  Val Acc=0.6888  Time=14.8s\n",
            "  Fold 38 Epoch 22/150  Train Loss=0.5896  Val Loss=0.6376  Val Acc=0.6967  Time=14.5s\n",
            "  Fold 38 Epoch 23/150  Train Loss=0.5814  Val Loss=0.6289  Val Acc=0.6595  Time=14.5s\n",
            "  Fold 38 Epoch 24/150  Train Loss=0.5643  Val Loss=0.6255  Val Acc=0.6634  Time=14.5s\n",
            "  Fold 38 Epoch 25/150  Train Loss=0.5251  Val Loss=0.6254  Val Acc=0.6751  Time=14.5s\n",
            "  Fold 38 Epoch 26/150  Train Loss=0.5175  Val Loss=0.6271  Val Acc=0.7006  Time=14.6s\n",
            "  Fold 38 Epoch 27/150  Train Loss=0.5032  Val Loss=0.6329  Val Acc=0.6849  Time=14.6s\n",
            "  Fold 38 Epoch 28/150  Train Loss=0.4953  Val Loss=0.6367  Val Acc=0.7045  Time=14.7s\n",
            "  Fold 38 Epoch 29/150  Train Loss=0.4913  Val Loss=0.6348  Val Acc=0.7065  Time=14.4s\n",
            "  Fold 38 Epoch 30/150  Train Loss=0.4833  Val Loss=0.6400  Val Acc=0.6791  Time=14.5s\n",
            "  Fold 38 Epoch 31/150  Train Loss=0.4663  Val Loss=0.6528  Val Acc=0.7104  Time=14.5s\n",
            "  Fold 38 Epoch 32/150  Train Loss=0.4642  Val Loss=0.6745  Val Acc=0.6693  Time=14.5s\n",
            "  Fold 38 Epoch 33/150  Train Loss=0.4457  Val Loss=0.6594  Val Acc=0.6830  Time=14.6s\n",
            "  Fold 38 Epoch 34/150  Train Loss=0.4419  Val Loss=0.6531  Val Acc=0.6732  Time=14.6s\n",
            "  Fold 38 Epoch 35/150  Train Loss=0.4299  Val Loss=0.6652  Val Acc=0.6673  Time=14.5s\n",
            "  Fold 38 Epoch 36/150  Train Loss=0.4272  Val Loss=0.6774  Val Acc=0.6791  Time=14.5s\n",
            "  Fold 38 Epoch 37/150  Train Loss=0.3968  Val Loss=0.6818  Val Acc=0.6556  Time=14.5s\n",
            "  Fold 38 Epoch 38/150  Train Loss=0.3898  Val Loss=0.6794  Val Acc=0.6810  Time=14.5s\n",
            "  Fold 38 Epoch 39/150  Train Loss=0.4031  Val Loss=0.7259  Val Acc=0.6654  Time=14.7s\n",
            "  Fold 38 Epoch 40/150  Train Loss=0.3859  Val Loss=0.6878  Val Acc=0.6673  Time=14.7s\n",
            "  → Early stopping at epoch 40 (no val_loss improvement)\n",
            "  Fold 38 Final ACC = 0.6667   (TP=32  TN=0  FP=0  FN=16)\n",
            "\n",
            ">>> Fold 39/52  (leave out sub-075)\n",
            "  Fold 39 Epoch 1/150  Train Loss=0.7312  Val Loss=0.6941  Val Acc=0.4473  Time=14.5s\n",
            "  Fold 39 Epoch 2/150  Train Loss=0.7124  Val Loss=0.6914  Val Acc=0.5664  Time=14.6s\n",
            "  Fold 39 Epoch 3/150  Train Loss=0.7129  Val Loss=0.6869  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 39 Epoch 4/150  Train Loss=0.7145  Val Loss=0.6871  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 39 Epoch 5/150  Train Loss=0.7196  Val Loss=0.6814  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 39 Epoch 6/150  Train Loss=0.7229  Val Loss=0.6833  Val Acc=0.6055  Time=14.8s\n",
            "  Fold 39 Epoch 7/150  Train Loss=0.7082  Val Loss=0.6835  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 39 Epoch 8/150  Train Loss=0.7070  Val Loss=0.6812  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 39 Epoch 9/150  Train Loss=0.7150  Val Loss=0.6851  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 39 Epoch 10/150  Train Loss=0.7094  Val Loss=0.6818  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 39 Epoch 11/150  Train Loss=0.7007  Val Loss=0.6791  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 39 Epoch 12/150  Train Loss=0.7006  Val Loss=0.6811  Val Acc=0.6055  Time=14.7s\n",
            "  Fold 39 Epoch 13/150  Train Loss=0.7109  Val Loss=0.6751  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 39 Epoch 14/150  Train Loss=0.6971  Val Loss=0.6792  Val Acc=0.6055  Time=14.5s\n",
            "  Fold 39 Epoch 15/150  Train Loss=0.6957  Val Loss=0.6844  Val Acc=0.6348  Time=14.5s\n",
            "  Fold 39 Epoch 16/150  Train Loss=0.7022  Val Loss=0.6646  Val Acc=0.6055  Time=14.6s\n",
            "  Fold 39 Epoch 17/150  Train Loss=0.6815  Val Loss=0.6585  Val Acc=0.6445  Time=14.6s\n",
            "  Fold 39 Epoch 18/150  Train Loss=0.6461  Val Loss=0.6768  Val Acc=0.6543  Time=14.7s\n",
            "  Fold 39 Epoch 19/150  Train Loss=0.6092  Val Loss=0.6818  Val Acc=0.6641  Time=14.5s\n",
            "  Fold 39 Epoch 20/150  Train Loss=0.5815  Val Loss=0.6562  Val Acc=0.6855  Time=14.4s\n",
            "  Fold 39 Epoch 21/150  Train Loss=0.5540  Val Loss=0.6575  Val Acc=0.7188  Time=14.6s\n",
            "  Fold 39 Epoch 22/150  Train Loss=0.5240  Val Loss=0.6288  Val Acc=0.7129  Time=14.5s\n",
            "  Fold 39 Epoch 23/150  Train Loss=0.5083  Val Loss=0.6382  Val Acc=0.6816  Time=14.5s\n",
            "  Fold 39 Epoch 24/150  Train Loss=0.5010  Val Loss=0.6565  Val Acc=0.7129  Time=14.8s\n",
            "  Fold 39 Epoch 25/150  Train Loss=0.5022  Val Loss=0.6474  Val Acc=0.7168  Time=14.7s\n",
            "  Fold 39 Epoch 26/150  Train Loss=0.4846  Val Loss=0.7334  Val Acc=0.6367  Time=14.5s\n",
            "  Fold 39 Epoch 27/150  Train Loss=0.4765  Val Loss=0.6495  Val Acc=0.6895  Time=14.6s\n",
            "  Fold 39 Epoch 28/150  Train Loss=0.4624  Val Loss=0.6971  Val Acc=0.6621  Time=14.6s\n",
            "  Fold 39 Epoch 29/150  Train Loss=0.4590  Val Loss=0.6538  Val Acc=0.6797  Time=14.5s\n",
            "  Fold 39 Epoch 30/150  Train Loss=0.4481  Val Loss=0.7229  Val Acc=0.6191  Time=14.7s\n",
            "  Fold 39 Epoch 31/150  Train Loss=0.4418  Val Loss=0.6912  Val Acc=0.6641  Time=14.7s\n",
            "  Fold 39 Epoch 32/150  Train Loss=0.4407  Val Loss=0.6979  Val Acc=0.6465  Time=14.4s\n",
            "  Fold 39 Epoch 33/150  Train Loss=0.4276  Val Loss=0.6864  Val Acc=0.6680  Time=14.6s\n",
            "  Fold 39 Epoch 34/150  Train Loss=0.4172  Val Loss=0.6944  Val Acc=0.6426  Time=14.6s\n",
            "  Fold 39 Epoch 35/150  Train Loss=0.4159  Val Loss=0.7173  Val Acc=0.6211  Time=14.5s\n",
            "  Fold 39 Epoch 36/150  Train Loss=0.3932  Val Loss=0.6842  Val Acc=0.6523  Time=14.6s\n",
            "  Fold 39 Epoch 37/150  Train Loss=0.3933  Val Loss=0.7106  Val Acc=0.6152  Time=14.7s\n",
            "  → Early stopping at epoch 37 (no val_loss improvement)\n",
            "  Fold 39 Final ACC = 0.1064   (TP=5  TN=0  FP=0  FN=42)\n",
            "\n",
            ">>> Fold 40/52  (leave out sub-076)\n",
            "  Fold 40 Epoch 1/150  Train Loss=0.7499  Val Loss=0.7167  Val Acc=0.3258  Time=14.5s\n",
            "  Fold 40 Epoch 2/150  Train Loss=0.7352  Val Loss=0.7100  Val Acc=0.3258  Time=14.4s\n",
            "  Fold 40 Epoch 3/150  Train Loss=0.7365  Val Loss=0.6889  Val Acc=0.6458  Time=14.4s\n",
            "  Fold 40 Epoch 4/150  Train Loss=0.7323  Val Loss=0.6881  Val Acc=0.6553  Time=14.5s\n",
            "  Fold 40 Epoch 5/150  Train Loss=0.7221  Val Loss=0.6776  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 40 Epoch 6/150  Train Loss=0.7263  Val Loss=0.6649  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 40 Epoch 7/150  Train Loss=0.7187  Val Loss=0.6534  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 40 Epoch 8/150  Train Loss=0.7143  Val Loss=0.6576  Val Acc=0.6742  Time=14.3s\n",
            "  Fold 40 Epoch 9/150  Train Loss=0.7209  Val Loss=0.6505  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 40 Epoch 10/150  Train Loss=0.7087  Val Loss=0.6579  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 40 Epoch 11/150  Train Loss=0.7118  Val Loss=0.6630  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 40 Epoch 12/150  Train Loss=0.7203  Val Loss=0.6585  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 40 Epoch 13/150  Train Loss=0.7046  Val Loss=0.6493  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 40 Epoch 14/150  Train Loss=0.7078  Val Loss=0.6532  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 40 Epoch 15/150  Train Loss=0.7021  Val Loss=0.6594  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 40 Epoch 16/150  Train Loss=0.7036  Val Loss=0.6769  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 40 Epoch 17/150  Train Loss=0.6890  Val Loss=0.6366  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 40 Epoch 18/150  Train Loss=0.6824  Val Loss=0.6448  Val Acc=0.6761  Time=14.4s\n",
            "  Fold 40 Epoch 19/150  Train Loss=0.6729  Val Loss=0.6713  Val Acc=0.6269  Time=14.7s\n",
            "  Fold 40 Epoch 20/150  Train Loss=0.6448  Val Loss=0.7016  Val Acc=0.5530  Time=14.5s\n",
            "  Fold 40 Epoch 21/150  Train Loss=0.6191  Val Loss=0.6422  Val Acc=0.6761  Time=14.4s\n",
            "  Fold 40 Epoch 22/150  Train Loss=0.6221  Val Loss=0.6609  Val Acc=0.6913  Time=14.4s\n",
            "  Fold 40 Epoch 23/150  Train Loss=0.6038  Val Loss=0.6928  Val Acc=0.6023  Time=14.5s\n",
            "  Fold 40 Epoch 24/150  Train Loss=0.5902  Val Loss=0.6925  Val Acc=0.6212  Time=14.5s\n",
            "  Fold 40 Epoch 25/150  Train Loss=0.5747  Val Loss=0.6950  Val Acc=0.6061  Time=14.7s\n",
            "  Fold 40 Epoch 26/150  Train Loss=0.5632  Val Loss=0.6580  Val Acc=0.6629  Time=14.7s\n",
            "  Fold 40 Epoch 27/150  Train Loss=0.5551  Val Loss=0.7879  Val Acc=0.4811  Time=14.4s\n",
            "  Fold 40 Epoch 28/150  Train Loss=0.5517  Val Loss=0.7148  Val Acc=0.5814  Time=14.5s\n",
            "  Fold 40 Epoch 29/150  Train Loss=0.5476  Val Loss=0.7294  Val Acc=0.5682  Time=14.4s\n",
            "  Fold 40 Epoch 30/150  Train Loss=0.5232  Val Loss=0.7335  Val Acc=0.5587  Time=14.4s\n",
            "  Fold 40 Epoch 31/150  Train Loss=0.5191  Val Loss=0.7855  Val Acc=0.4981  Time=14.5s\n",
            "  Fold 40 Epoch 32/150  Train Loss=0.5179  Val Loss=0.7194  Val Acc=0.5890  Time=14.7s\n",
            "  → Early stopping at epoch 32 (no val_loss improvement)\n",
            "  Fold 40 Final ACC = 0.0000   (TP=0  TN=0  FP=47  FN=0)\n",
            "\n",
            ">>> Fold 41/52  (leave out sub-077)\n",
            "  Fold 41 Epoch 1/150  Train Loss=0.8572  Val Loss=0.7626  Val Acc=0.3258  Time=14.6s\n",
            "  Fold 41 Epoch 2/150  Train Loss=0.8293  Val Loss=0.7314  Val Acc=0.3258  Time=14.4s\n",
            "  Fold 41 Epoch 3/150  Train Loss=0.8030  Val Loss=0.7271  Val Acc=0.3258  Time=14.5s\n",
            "  Fold 41 Epoch 4/150  Train Loss=0.7957  Val Loss=0.7132  Val Acc=0.3239  Time=14.5s\n",
            "  Fold 41 Epoch 5/150  Train Loss=0.7563  Val Loss=0.6901  Val Acc=0.6080  Time=14.4s\n",
            "  Fold 41 Epoch 6/150  Train Loss=0.7281  Val Loss=0.6689  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 41 Epoch 7/150  Train Loss=0.7078  Val Loss=0.6770  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 8/150  Train Loss=0.7109  Val Loss=0.6820  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 41 Epoch 9/150  Train Loss=0.7076  Val Loss=0.6788  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 10/150  Train Loss=0.7069  Val Loss=0.6700  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 41 Epoch 11/150  Train Loss=0.7024  Val Loss=0.6585  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 41 Epoch 12/150  Train Loss=0.7089  Val Loss=0.6618  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 41 Epoch 13/150  Train Loss=0.7052  Val Loss=0.6586  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 41 Epoch 14/150  Train Loss=0.6975  Val Loss=0.6512  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 15/150  Train Loss=0.7100  Val Loss=0.6582  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 16/150  Train Loss=0.6931  Val Loss=0.6611  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 17/150  Train Loss=0.6954  Val Loss=0.6475  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 41 Epoch 18/150  Train Loss=0.6993  Val Loss=0.6548  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 19/150  Train Loss=0.7006  Val Loss=0.6441  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 41 Epoch 20/150  Train Loss=0.6942  Val Loss=0.6574  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 41 Epoch 21/150  Train Loss=0.6905  Val Loss=0.6504  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 41 Epoch 22/150  Train Loss=0.6945  Val Loss=0.6557  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 41 Epoch 23/150  Train Loss=0.6811  Val Loss=0.6824  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 41 Epoch 24/150  Train Loss=0.6747  Val Loss=0.7215  Val Acc=0.3466  Time=14.5s\n",
            "  Fold 41 Epoch 25/150  Train Loss=0.6383  Val Loss=0.7014  Val Acc=0.5720  Time=14.6s\n",
            "  Fold 41 Epoch 26/150  Train Loss=0.6258  Val Loss=0.7082  Val Acc=0.5455  Time=14.7s\n",
            "  Fold 41 Epoch 27/150  Train Loss=0.5999  Val Loss=0.6558  Val Acc=0.7008  Time=14.5s\n",
            "  Fold 41 Epoch 28/150  Train Loss=0.5847  Val Loss=0.6896  Val Acc=0.6042  Time=14.4s\n",
            "  Fold 41 Epoch 29/150  Train Loss=0.5624  Val Loss=0.6625  Val Acc=0.6818  Time=14.4s\n",
            "  Fold 41 Epoch 30/150  Train Loss=0.5536  Val Loss=0.7252  Val Acc=0.5568  Time=14.4s\n",
            "  Fold 41 Epoch 31/150  Train Loss=0.5383  Val Loss=0.7067  Val Acc=0.6326  Time=14.4s\n",
            "  Fold 41 Epoch 32/150  Train Loss=0.5254  Val Loss=0.7191  Val Acc=0.5871  Time=14.7s\n",
            "  Fold 41 Epoch 33/150  Train Loss=0.5121  Val Loss=0.6870  Val Acc=0.6705  Time=14.7s\n",
            "  Fold 41 Epoch 34/150  Train Loss=0.4959  Val Loss=0.7055  Val Acc=0.6212  Time=14.5s\n",
            "  → Early stopping at epoch 34 (no val_loss improvement)\n",
            "  Fold 41 Final ACC = 0.0000   (TP=0  TN=0  FP=44  FN=0)\n",
            "\n",
            ">>> Fold 42/52  (leave out sub-078)\n",
            "  Fold 42 Epoch 1/150  Train Loss=0.6986  Val Loss=0.6396  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 2/150  Train Loss=0.7039  Val Loss=0.6386  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 3/150  Train Loss=0.6955  Val Loss=0.6364  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 4/150  Train Loss=0.7047  Val Loss=0.6393  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 42 Epoch 5/150  Train Loss=0.7106  Val Loss=0.6362  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 42 Epoch 6/150  Train Loss=0.7021  Val Loss=0.6380  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 42 Epoch 7/150  Train Loss=0.7053  Val Loss=0.6402  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 42 Epoch 8/150  Train Loss=0.7009  Val Loss=0.6399  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 9/150  Train Loss=0.6977  Val Loss=0.6475  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 42 Epoch 10/150  Train Loss=0.7042  Val Loss=0.6448  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 11/150  Train Loss=0.7034  Val Loss=0.6373  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 42 Epoch 12/150  Train Loss=0.7044  Val Loss=0.6366  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 13/150  Train Loss=0.7026  Val Loss=0.6401  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 42 Epoch 14/150  Train Loss=0.6996  Val Loss=0.6499  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 15/150  Train Loss=0.6906  Val Loss=0.6610  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 42 Epoch 16/150  Train Loss=0.7014  Val Loss=0.6461  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 17/150  Train Loss=0.6952  Val Loss=0.6509  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 42 Epoch 18/150  Train Loss=0.6880  Val Loss=0.6513  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 19/150  Train Loss=0.7004  Val Loss=0.6497  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 42 Epoch 20/150  Train Loss=0.6920  Val Loss=0.6365  Val Acc=0.6742  Time=14.5s\n",
            "  → Early stopping at epoch 20 (no val_loss improvement)\n",
            "  Fold 42 Final ACC = 0.0000   (TP=0  TN=0  FP=41  FN=0)\n",
            "\n",
            ">>> Fold 43/52  (leave out sub-079)\n",
            "  Fold 43 Epoch 1/150  Train Loss=0.7121  Val Loss=0.6385  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 2/150  Train Loss=0.7209  Val Loss=0.6350  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 3/150  Train Loss=0.7122  Val Loss=0.6308  Val Acc=0.6833  Time=14.8s\n",
            "  Fold 43 Epoch 4/150  Train Loss=0.7137  Val Loss=0.6333  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 43 Epoch 5/150  Train Loss=0.7136  Val Loss=0.6343  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 6/150  Train Loss=0.6981  Val Loss=0.6362  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 43 Epoch 7/150  Train Loss=0.6957  Val Loss=0.6400  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 43 Epoch 8/150  Train Loss=0.7003  Val Loss=0.6318  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 9/150  Train Loss=0.7059  Val Loss=0.6453  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 43 Epoch 10/150  Train Loss=0.6961  Val Loss=0.6452  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 11/150  Train Loss=0.7115  Val Loss=0.6413  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 12/150  Train Loss=0.7029  Val Loss=0.6493  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 13/150  Train Loss=0.6991  Val Loss=0.6474  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 43 Epoch 14/150  Train Loss=0.6953  Val Loss=0.6411  Val Acc=0.6833  Time=14.6s\n",
            "  Fold 43 Epoch 15/150  Train Loss=0.6964  Val Loss=0.6512  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 43 Epoch 16/150  Train Loss=0.6942  Val Loss=0.6412  Val Acc=0.6833  Time=14.7s\n",
            "  Fold 43 Epoch 17/150  Train Loss=0.6943  Val Loss=0.6402  Val Acc=0.6833  Time=14.5s\n",
            "  Fold 43 Epoch 18/150  Train Loss=0.6992  Val Loss=0.6393  Val Acc=0.6833  Time=14.4s\n",
            "  → Early stopping at epoch 18 (no val_loss improvement)\n",
            "  Fold 43 Final ACC = 0.0000   (TP=0  TN=0  FP=41  FN=0)\n",
            "\n",
            ">>> Fold 44/52  (leave out sub-080)\n",
            "  Fold 44 Epoch 1/150  Train Loss=0.8202  Val Loss=0.7575  Val Acc=0.3258  Time=14.5s\n",
            "  Fold 44 Epoch 2/150  Train Loss=0.7853  Val Loss=0.7396  Val Acc=0.3258  Time=14.4s\n",
            "  Fold 44 Epoch 3/150  Train Loss=0.7543  Val Loss=0.7213  Val Acc=0.3258  Time=14.6s\n",
            "  Fold 44 Epoch 4/150  Train Loss=0.7388  Val Loss=0.6950  Val Acc=0.4034  Time=14.8s\n",
            "  Fold 44 Epoch 5/150  Train Loss=0.7115  Val Loss=0.6814  Val Acc=0.6761  Time=14.4s\n",
            "  Fold 44 Epoch 6/150  Train Loss=0.7108  Val Loss=0.6814  Val Acc=0.6761  Time=14.4s\n",
            "  Fold 44 Epoch 7/150  Train Loss=0.7253  Val Loss=0.6785  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 44 Epoch 8/150  Train Loss=0.7193  Val Loss=0.6709  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 44 Epoch 9/150  Train Loss=0.7096  Val Loss=0.6731  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 44 Epoch 10/150  Train Loss=0.7123  Val Loss=0.6681  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 44 Epoch 11/150  Train Loss=0.7084  Val Loss=0.6700  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 44 Epoch 12/150  Train Loss=0.7046  Val Loss=0.6746  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 44 Epoch 13/150  Train Loss=0.7052  Val Loss=0.6787  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 44 Epoch 14/150  Train Loss=0.7068  Val Loss=0.6619  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 44 Epoch 15/150  Train Loss=0.7075  Val Loss=0.6488  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 44 Epoch 16/150  Train Loss=0.6990  Val Loss=0.6733  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 44 Epoch 17/150  Train Loss=0.7090  Val Loss=0.6731  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 44 Epoch 18/150  Train Loss=0.6953  Val Loss=0.6736  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 44 Epoch 19/150  Train Loss=0.6850  Val Loss=0.6900  Val Acc=0.5739  Time=14.5s\n",
            "  Fold 44 Epoch 20/150  Train Loss=0.6688  Val Loss=0.6903  Val Acc=0.6136  Time=14.5s\n",
            "  Fold 44 Epoch 21/150  Train Loss=0.6341  Val Loss=0.6866  Val Acc=0.6420  Time=14.5s\n",
            "  Fold 44 Epoch 22/150  Train Loss=0.6093  Val Loss=0.7375  Val Acc=0.5549  Time=14.7s\n",
            "  Fold 44 Epoch 23/150  Train Loss=0.5865  Val Loss=0.7440  Val Acc=0.5568  Time=14.7s\n",
            "  Fold 44 Epoch 24/150  Train Loss=0.5677  Val Loss=0.7150  Val Acc=0.6042  Time=14.7s\n",
            "  Fold 44 Epoch 25/150  Train Loss=0.5424  Val Loss=0.7343  Val Acc=0.5909  Time=14.5s\n",
            "  Fold 44 Epoch 26/150  Train Loss=0.5157  Val Loss=0.6809  Val Acc=0.6496  Time=14.4s\n",
            "  Fold 44 Epoch 27/150  Train Loss=0.5067  Val Loss=0.7627  Val Acc=0.5701  Time=14.5s\n",
            "  Fold 44 Epoch 28/150  Train Loss=0.4949  Val Loss=0.7916  Val Acc=0.5663  Time=14.6s\n",
            "  Fold 44 Epoch 29/150  Train Loss=0.4833  Val Loss=0.7299  Val Acc=0.6231  Time=14.7s\n",
            "  Fold 44 Epoch 30/150  Train Loss=0.4696  Val Loss=0.6453  Val Acc=0.6837  Time=14.6s\n",
            "  Fold 44 Epoch 31/150  Train Loss=0.4662  Val Loss=0.7220  Val Acc=0.6288  Time=14.6s\n",
            "  Fold 44 Epoch 32/150  Train Loss=0.4626  Val Loss=0.7165  Val Acc=0.6420  Time=14.6s\n",
            "  Fold 44 Epoch 33/150  Train Loss=0.4359  Val Loss=0.7609  Val Acc=0.6117  Time=14.5s\n",
            "  Fold 44 Epoch 34/150  Train Loss=0.4408  Val Loss=0.7451  Val Acc=0.6136  Time=14.6s\n",
            "  Fold 44 Epoch 35/150  Train Loss=0.4304  Val Loss=0.8230  Val Acc=0.5455  Time=14.7s\n",
            "  Fold 44 Epoch 36/150  Train Loss=0.4256  Val Loss=0.7136  Val Acc=0.6420  Time=14.6s\n",
            "  Fold 44 Epoch 37/150  Train Loss=0.4017  Val Loss=0.7474  Val Acc=0.6155  Time=14.5s\n",
            "  Fold 44 Epoch 38/150  Train Loss=0.4091  Val Loss=0.6567  Val Acc=0.6989  Time=14.5s\n",
            "  Fold 44 Epoch 39/150  Train Loss=0.3935  Val Loss=0.8777  Val Acc=0.5189  Time=14.5s\n",
            "  Fold 44 Epoch 40/150  Train Loss=0.3895  Val Loss=0.8147  Val Acc=0.5682  Time=14.6s\n",
            "  Fold 44 Epoch 41/150  Train Loss=0.3874  Val Loss=0.6532  Val Acc=0.6989  Time=14.6s\n",
            "  Fold 44 Epoch 42/150  Train Loss=0.3720  Val Loss=0.7318  Val Acc=0.6420  Time=14.7s\n",
            "  Fold 44 Epoch 43/150  Train Loss=0.3565  Val Loss=0.7670  Val Acc=0.6269  Time=14.5s\n",
            "  Fold 44 Epoch 44/150  Train Loss=0.3562  Val Loss=0.7386  Val Acc=0.6402  Time=14.4s\n",
            "  Fold 44 Epoch 45/150  Train Loss=0.3594  Val Loss=0.8005  Val Acc=0.5928  Time=14.5s\n",
            "  → Early stopping at epoch 45 (no val_loss improvement)\n",
            "  Fold 44 Final ACC = 0.1250   (TP=0  TN=5  FP=35  FN=0)\n",
            "\n",
            ">>> Fold 45/52  (leave out sub-081)\n",
            "  Fold 45 Epoch 1/150  Train Loss=0.7376  Val Loss=0.7136  Val Acc=0.3206  Time=14.5s\n",
            "  Fold 45 Epoch 2/150  Train Loss=0.7424  Val Loss=0.7243  Val Acc=0.3206  Time=14.6s\n",
            "  Fold 45 Epoch 3/150  Train Loss=0.7297  Val Loss=0.7148  Val Acc=0.3206  Time=14.7s\n",
            "  Fold 45 Epoch 4/150  Train Loss=0.7262  Val Loss=0.7069  Val Acc=0.3206  Time=14.4s\n",
            "  Fold 45 Epoch 5/150  Train Loss=0.7275  Val Loss=0.6925  Val Acc=0.5592  Time=14.5s\n",
            "  Fold 45 Epoch 6/150  Train Loss=0.7107  Val Loss=0.6828  Val Acc=0.6794  Time=14.4s\n",
            "  Fold 45 Epoch 7/150  Train Loss=0.7080  Val Loss=0.6801  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 45 Epoch 8/150  Train Loss=0.7119  Val Loss=0.6775  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 45 Epoch 9/150  Train Loss=0.7036  Val Loss=0.6670  Val Acc=0.6794  Time=14.8s\n",
            "  Fold 45 Epoch 10/150  Train Loss=0.7006  Val Loss=0.6700  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 45 Epoch 11/150  Train Loss=0.6863  Val Loss=0.6652  Val Acc=0.6794  Time=14.4s\n",
            "  Fold 45 Epoch 12/150  Train Loss=0.6967  Val Loss=0.6544  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 45 Epoch 13/150  Train Loss=0.7035  Val Loss=0.6545  Val Acc=0.6794  Time=14.4s\n",
            "  Fold 45 Epoch 14/150  Train Loss=0.6874  Val Loss=0.6576  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 45 Epoch 15/150  Train Loss=0.6938  Val Loss=0.6540  Val Acc=0.6794  Time=14.7s\n",
            "  Fold 45 Epoch 16/150  Train Loss=0.6934  Val Loss=0.6430  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 45 Epoch 17/150  Train Loss=0.6853  Val Loss=0.6516  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 45 Epoch 18/150  Train Loss=0.6917  Val Loss=0.6591  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 45 Epoch 19/150  Train Loss=0.6875  Val Loss=0.6528  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 45 Epoch 20/150  Train Loss=0.6730  Val Loss=0.6380  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 45 Epoch 21/150  Train Loss=0.6560  Val Loss=0.6405  Val Acc=0.6927  Time=14.7s\n",
            "  Fold 45 Epoch 22/150  Train Loss=0.6012  Val Loss=0.7025  Val Acc=0.5802  Time=14.7s\n",
            "  Fold 45 Epoch 23/150  Train Loss=0.5591  Val Loss=0.7279  Val Acc=0.5763  Time=14.6s\n",
            "  Fold 45 Epoch 24/150  Train Loss=0.5478  Val Loss=0.6793  Val Acc=0.6603  Time=14.6s\n",
            "  Fold 45 Epoch 25/150  Train Loss=0.5232  Val Loss=0.7000  Val Acc=0.6298  Time=14.5s\n",
            "  Fold 45 Epoch 26/150  Train Loss=0.5092  Val Loss=0.6773  Val Acc=0.6508  Time=14.6s\n",
            "  Fold 45 Epoch 27/150  Train Loss=0.4814  Val Loss=0.8071  Val Acc=0.5496  Time=14.7s\n",
            "  Fold 45 Epoch 28/150  Train Loss=0.4866  Val Loss=0.6900  Val Acc=0.6718  Time=14.8s\n",
            "  Fold 45 Epoch 29/150  Train Loss=0.4685  Val Loss=0.7033  Val Acc=0.6737  Time=14.5s\n",
            "  Fold 45 Epoch 30/150  Train Loss=0.4690  Val Loss=0.7531  Val Acc=0.6240  Time=14.6s\n",
            "  Fold 45 Epoch 31/150  Train Loss=0.4543  Val Loss=0.8418  Val Acc=0.5344  Time=14.6s\n",
            "  Fold 45 Epoch 32/150  Train Loss=0.4400  Val Loss=0.7195  Val Acc=0.6679  Time=14.8s\n",
            "  Fold 45 Epoch 33/150  Train Loss=0.4451  Val Loss=0.7820  Val Acc=0.6145  Time=14.6s\n",
            "  Fold 45 Epoch 34/150  Train Loss=0.4285  Val Loss=0.6878  Val Acc=0.6851  Time=14.7s\n",
            "  Fold 45 Epoch 35/150  Train Loss=0.4139  Val Loss=0.8019  Val Acc=0.5992  Time=14.6s\n",
            "  → Early stopping at epoch 35 (no val_loss improvement)\n",
            "  Fold 45 Final ACC = 0.0000   (TP=0  TN=0  FP=40  FN=0)\n",
            "\n",
            ">>> Fold 46/52  (leave out sub-082)\n",
            "  Fold 46 Epoch 1/150  Train Loss=0.8175  Val Loss=0.6865  Val Acc=0.6174  Time=14.6s\n",
            "  Fold 46 Epoch 2/150  Train Loss=0.7677  Val Loss=0.6795  Val Acc=0.6648  Time=14.4s\n",
            "  Fold 46 Epoch 3/150  Train Loss=0.7355  Val Loss=0.6589  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 4/150  Train Loss=0.7334  Val Loss=0.6484  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 5/150  Train Loss=0.7368  Val Loss=0.6437  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 46 Epoch 6/150  Train Loss=0.7191  Val Loss=0.6399  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 7/150  Train Loss=0.7135  Val Loss=0.6362  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 8/150  Train Loss=0.7152  Val Loss=0.6366  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 9/150  Train Loss=0.7199  Val Loss=0.6388  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 10/150  Train Loss=0.7113  Val Loss=0.6364  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 46 Epoch 11/150  Train Loss=0.7125  Val Loss=0.6319  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 46 Epoch 12/150  Train Loss=0.7154  Val Loss=0.6338  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 46 Epoch 13/150  Train Loss=0.7104  Val Loss=0.6341  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 46 Epoch 14/150  Train Loss=0.7060  Val Loss=0.6375  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 46 Epoch 15/150  Train Loss=0.7045  Val Loss=0.6394  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 46 Epoch 16/150  Train Loss=0.7012  Val Loss=0.6310  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 46 Epoch 17/150  Train Loss=0.6895  Val Loss=0.6484  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 46 Epoch 18/150  Train Loss=0.6814  Val Loss=0.6474  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 46 Epoch 19/150  Train Loss=0.6796  Val Loss=0.7147  Val Acc=0.4924  Time=14.4s\n",
            "  Fold 46 Epoch 20/150  Train Loss=0.6443  Val Loss=0.6860  Val Acc=0.6080  Time=14.5s\n",
            "  Fold 46 Epoch 21/150  Train Loss=0.5929  Val Loss=0.7354  Val Acc=0.5644  Time=14.5s\n",
            "  Fold 46 Epoch 22/150  Train Loss=0.5610  Val Loss=0.6476  Val Acc=0.7273  Time=14.5s\n",
            "  Fold 46 Epoch 23/150  Train Loss=0.5378  Val Loss=0.6579  Val Acc=0.7519  Time=14.6s\n",
            "  Fold 46 Epoch 24/150  Train Loss=0.5102  Val Loss=0.6740  Val Acc=0.6648  Time=14.8s\n",
            "  Fold 46 Epoch 25/150  Train Loss=0.5050  Val Loss=0.7293  Val Acc=0.6288  Time=14.6s\n",
            "  Fold 46 Epoch 26/150  Train Loss=0.4910  Val Loss=0.7221  Val Acc=0.6458  Time=14.6s\n",
            "  Fold 46 Epoch 27/150  Train Loss=0.4848  Val Loss=0.7443  Val Acc=0.6364  Time=14.5s\n",
            "  Fold 46 Epoch 28/150  Train Loss=0.4751  Val Loss=0.7331  Val Acc=0.6402  Time=14.5s\n",
            "  Fold 46 Epoch 29/150  Train Loss=0.4657  Val Loss=0.7726  Val Acc=0.5814  Time=14.5s\n",
            "  Fold 46 Epoch 30/150  Train Loss=0.4599  Val Loss=0.6608  Val Acc=0.6875  Time=14.7s\n",
            "  Fold 46 Epoch 31/150  Train Loss=0.4595  Val Loss=0.7684  Val Acc=0.6042  Time=14.6s\n",
            "  → Early stopping at epoch 31 (no val_loss improvement)\n",
            "  Fold 46 Final ACC = 0.0000   (TP=0  TN=0  FP=39  FN=0)\n",
            "\n",
            ">>> Fold 47/52  (leave out sub-083)\n",
            "  Fold 47 Epoch 1/150  Train Loss=0.7934  Val Loss=0.6795  Val Acc=0.6629  Time=14.5s\n",
            "  Fold 47 Epoch 2/150  Train Loss=0.7649  Val Loss=0.6288  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 47 Epoch 3/150  Train Loss=0.7420  Val Loss=0.6249  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 47 Epoch 4/150  Train Loss=0.7292  Val Loss=0.6240  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 5/150  Train Loss=0.7289  Val Loss=0.6241  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 47 Epoch 6/150  Train Loss=0.7332  Val Loss=0.6238  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 47 Epoch 7/150  Train Loss=0.7241  Val Loss=0.6274  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 8/150  Train Loss=0.7117  Val Loss=0.6271  Val Acc=0.6781  Time=14.4s\n",
            "  Fold 47 Epoch 9/150  Train Loss=0.7113  Val Loss=0.6272  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 10/150  Train Loss=0.7224  Val Loss=0.6342  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 47 Epoch 11/150  Train Loss=0.7265  Val Loss=0.6317  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 47 Epoch 12/150  Train Loss=0.7082  Val Loss=0.6306  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 47 Epoch 13/150  Train Loss=0.7017  Val Loss=0.6445  Val Acc=0.6781  Time=14.4s\n",
            "  Fold 47 Epoch 14/150  Train Loss=0.7101  Val Loss=0.6432  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 15/150  Train Loss=0.7091  Val Loss=0.6396  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 47 Epoch 16/150  Train Loss=0.7157  Val Loss=0.6429  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 17/150  Train Loss=0.7138  Val Loss=0.6409  Val Acc=0.6781  Time=14.6s\n",
            "  Fold 47 Epoch 18/150  Train Loss=0.7077  Val Loss=0.6609  Val Acc=0.6781  Time=14.7s\n",
            "  Fold 47 Epoch 19/150  Train Loss=0.7112  Val Loss=0.6535  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 20/150  Train Loss=0.7050  Val Loss=0.6578  Val Acc=0.6781  Time=14.5s\n",
            "  Fold 47 Epoch 21/150  Train Loss=0.7040  Val Loss=0.6558  Val Acc=0.6781  Time=14.4s\n",
            "  → Early stopping at epoch 21 (no val_loss improvement)\n",
            "  Fold 47 Final ACC = 0.0000   (TP=0  TN=0  FP=38  FN=0)\n",
            "\n",
            ">>> Fold 48/52  (leave out sub-084)\n",
            "  Fold 48 Epoch 1/150  Train Loss=0.7363  Val Loss=0.6553  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 2/150  Train Loss=0.7162  Val Loss=0.6542  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 3/150  Train Loss=0.7237  Val Loss=0.6584  Val Acc=0.6755  Time=14.7s\n",
            "  Fold 48 Epoch 4/150  Train Loss=0.7297  Val Loss=0.6614  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 5/150  Train Loss=0.7218  Val Loss=0.6530  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 6/150  Train Loss=0.7177  Val Loss=0.6482  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 7/150  Train Loss=0.7143  Val Loss=0.6671  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 8/150  Train Loss=0.7200  Val Loss=0.6606  Val Acc=0.6755  Time=14.4s\n",
            "  Fold 48 Epoch 9/150  Train Loss=0.7301  Val Loss=0.6680  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 10/150  Train Loss=0.7180  Val Loss=0.6613  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 11/150  Train Loss=0.7205  Val Loss=0.6460  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 12/150  Train Loss=0.7214  Val Loss=0.6427  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 13/150  Train Loss=0.7070  Val Loss=0.6497  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 14/150  Train Loss=0.7045  Val Loss=0.6574  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 15/150  Train Loss=0.7059  Val Loss=0.6572  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 16/150  Train Loss=0.7006  Val Loss=0.6570  Val Acc=0.6755  Time=14.8s\n",
            "  Fold 48 Epoch 17/150  Train Loss=0.6940  Val Loss=0.6653  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 18/150  Train Loss=0.7075  Val Loss=0.6663  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 19/150  Train Loss=0.6961  Val Loss=0.6544  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 20/150  Train Loss=0.7014  Val Loss=0.6515  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 21/150  Train Loss=0.6953  Val Loss=0.6567  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 22/150  Train Loss=0.7003  Val Loss=0.6573  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 23/150  Train Loss=0.6999  Val Loss=0.6556  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 48 Epoch 24/150  Train Loss=0.7025  Val Loss=0.6509  Val Acc=0.6755  Time=14.4s\n",
            "  Fold 48 Epoch 25/150  Train Loss=0.6998  Val Loss=0.6490  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 48 Epoch 26/150  Train Loss=0.6906  Val Loss=0.6516  Val Acc=0.6755  Time=14.7s\n",
            "  Fold 48 Epoch 27/150  Train Loss=0.7006  Val Loss=0.6609  Val Acc=0.6755  Time=14.5s\n",
            "  → Early stopping at epoch 27 (no val_loss improvement)\n",
            "  Fold 48 Final ACC = 0.0000   (TP=0  TN=0  FP=36  FN=0)\n",
            "\n",
            ">>> Fold 49/52  (leave out sub-085)\n",
            "  Fold 49 Epoch 1/150  Train Loss=0.7295  Val Loss=0.6975  Val Acc=0.3409  Time=14.7s\n",
            "  Fold 49 Epoch 2/150  Train Loss=0.7157  Val Loss=0.6948  Val Acc=0.4261  Time=14.7s\n",
            "  Fold 49 Epoch 3/150  Train Loss=0.7162  Val Loss=0.6919  Val Acc=0.5966  Time=14.5s\n",
            "  Fold 49 Epoch 4/150  Train Loss=0.7125  Val Loss=0.6919  Val Acc=0.5928  Time=14.4s\n",
            "  Fold 49 Epoch 5/150  Train Loss=0.7043  Val Loss=0.6849  Val Acc=0.6742  Time=14.4s\n",
            "  Fold 49 Epoch 6/150  Train Loss=0.6995  Val Loss=0.6845  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 7/150  Train Loss=0.6998  Val Loss=0.6803  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 49 Epoch 8/150  Train Loss=0.7086  Val Loss=0.6791  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 49 Epoch 9/150  Train Loss=0.6898  Val Loss=0.6730  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 10/150  Train Loss=0.7035  Val Loss=0.6646  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 11/150  Train Loss=0.6925  Val Loss=0.6623  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 12/150  Train Loss=0.7010  Val Loss=0.6583  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 49 Epoch 13/150  Train Loss=0.6957  Val Loss=0.6600  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 49 Epoch 14/150  Train Loss=0.7003  Val Loss=0.6596  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 49 Epoch 15/150  Train Loss=0.7014  Val Loss=0.6602  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 49 Epoch 16/150  Train Loss=0.6965  Val Loss=0.6633  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 49 Epoch 17/150  Train Loss=0.6963  Val Loss=0.6662  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 18/150  Train Loss=0.6963  Val Loss=0.6580  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 49 Epoch 19/150  Train Loss=0.6904  Val Loss=0.6595  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 20/150  Train Loss=0.6991  Val Loss=0.6640  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 49 Epoch 21/150  Train Loss=0.6900  Val Loss=0.6612  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 22/150  Train Loss=0.6881  Val Loss=0.6606  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 49 Epoch 23/150  Train Loss=0.6933  Val Loss=0.6646  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 49 Epoch 24/150  Train Loss=0.6725  Val Loss=0.6727  Val Acc=0.7235  Time=14.6s\n",
            "  Fold 49 Epoch 25/150  Train Loss=0.6515  Val Loss=0.7022  Val Acc=0.4962  Time=14.5s\n",
            "  Fold 49 Epoch 26/150  Train Loss=0.6276  Val Loss=0.6761  Val Acc=0.5701  Time=14.6s\n",
            "  Fold 49 Epoch 27/150  Train Loss=0.5832  Val Loss=0.6878  Val Acc=0.6269  Time=14.7s\n",
            "  Fold 49 Epoch 28/150  Train Loss=0.5739  Val Loss=0.7076  Val Acc=0.5701  Time=14.6s\n",
            "  Fold 49 Epoch 29/150  Train Loss=0.5645  Val Loss=0.6844  Val Acc=0.6496  Time=14.6s\n",
            "  Fold 49 Epoch 30/150  Train Loss=0.5296  Val Loss=0.7681  Val Acc=0.5455  Time=14.5s\n",
            "  Fold 49 Epoch 31/150  Train Loss=0.5053  Val Loss=0.7305  Val Acc=0.6042  Time=14.7s\n",
            "  Fold 49 Epoch 32/150  Train Loss=0.4894  Val Loss=0.7049  Val Acc=0.6610  Time=14.7s\n",
            "  Fold 49 Epoch 33/150  Train Loss=0.4776  Val Loss=0.7852  Val Acc=0.5777  Time=14.6s\n",
            "  → Early stopping at epoch 33 (no val_loss improvement)\n",
            "  Fold 49 Final ACC = 0.0000   (TP=0  TN=0  FP=36  FN=0)\n",
            "\n",
            ">>> Fold 50/52  (leave out sub-086)\n",
            "  Fold 50 Epoch 1/150  Train Loss=0.7295  Val Loss=0.7127  Val Acc=0.3245  Time=14.5s\n",
            "  Fold 50 Epoch 2/150  Train Loss=0.7310  Val Loss=0.6972  Val Acc=0.3928  Time=14.5s\n",
            "  Fold 50 Epoch 3/150  Train Loss=0.7209  Val Loss=0.6755  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 50 Epoch 4/150  Train Loss=0.7245  Val Loss=0.6630  Val Acc=0.6755  Time=14.4s\n",
            "  Fold 50 Epoch 5/150  Train Loss=0.7166  Val Loss=0.6601  Val Acc=0.6755  Time=14.7s\n",
            "  Fold 50 Epoch 6/150  Train Loss=0.7161  Val Loss=0.6605  Val Acc=0.6755  Time=14.7s\n",
            "  Fold 50 Epoch 7/150  Train Loss=0.7089  Val Loss=0.6575  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 8/150  Train Loss=0.7078  Val Loss=0.6716  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 50 Epoch 9/150  Train Loss=0.7008  Val Loss=0.6656  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 10/150  Train Loss=0.7080  Val Loss=0.6587  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 11/150  Train Loss=0.7030  Val Loss=0.6530  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 12/150  Train Loss=0.7013  Val Loss=0.6538  Val Acc=0.6755  Time=14.7s\n",
            "  Fold 50 Epoch 13/150  Train Loss=0.7026  Val Loss=0.6507  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 14/150  Train Loss=0.7096  Val Loss=0.6717  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 15/150  Train Loss=0.7050  Val Loss=0.6617  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 50 Epoch 16/150  Train Loss=0.7121  Val Loss=0.6602  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 50 Epoch 17/150  Train Loss=0.7044  Val Loss=0.6594  Val Acc=0.6755  Time=14.8s\n",
            "  Fold 50 Epoch 18/150  Train Loss=0.6973  Val Loss=0.6516  Val Acc=0.6755  Time=14.7s\n",
            "  Fold 50 Epoch 19/150  Train Loss=0.7157  Val Loss=0.6818  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 20/150  Train Loss=0.7013  Val Loss=0.6547  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 50 Epoch 21/150  Train Loss=0.6951  Val Loss=0.6426  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 22/150  Train Loss=0.6993  Val Loss=0.6448  Val Acc=0.6755  Time=14.5s\n",
            "  Fold 50 Epoch 23/150  Train Loss=0.6973  Val Loss=0.6304  Val Acc=0.6755  Time=14.6s\n",
            "  Fold 50 Epoch 24/150  Train Loss=0.6703  Val Loss=0.6213  Val Acc=0.7116  Time=14.8s\n",
            "  Fold 50 Epoch 25/150  Train Loss=0.6562  Val Loss=0.5761  Val Acc=0.7457  Time=14.6s\n",
            "  Fold 50 Epoch 26/150  Train Loss=0.6423  Val Loss=0.6916  Val Acc=0.5427  Time=14.6s\n",
            "  Fold 50 Epoch 27/150  Train Loss=0.6285  Val Loss=0.5520  Val Acc=0.7856  Time=14.4s\n",
            "  Fold 50 Epoch 28/150  Train Loss=0.6004  Val Loss=0.5720  Val Acc=0.8008  Time=14.5s\n",
            "  Fold 50 Epoch 29/150  Train Loss=0.5886  Val Loss=0.5067  Val Acc=0.8159  Time=14.6s\n",
            "  Fold 50 Epoch 30/150  Train Loss=0.5769  Val Loss=0.5363  Val Acc=0.7799  Time=14.7s\n",
            "  Fold 50 Epoch 31/150  Train Loss=0.5650  Val Loss=0.5257  Val Acc=0.8065  Time=14.7s\n",
            "  Fold 50 Epoch 32/150  Train Loss=0.5428  Val Loss=0.5431  Val Acc=0.7571  Time=14.5s\n",
            "  Fold 50 Epoch 33/150  Train Loss=0.5375  Val Loss=0.5286  Val Acc=0.7628  Time=14.6s\n",
            "  Fold 50 Epoch 34/150  Train Loss=0.5323  Val Loss=0.5250  Val Acc=0.7875  Time=14.5s\n",
            "  Fold 50 Epoch 35/150  Train Loss=0.5177  Val Loss=0.5673  Val Acc=0.7419  Time=14.6s\n",
            "  Fold 50 Epoch 36/150  Train Loss=0.5118  Val Loss=0.5078  Val Acc=0.7875  Time=14.8s\n",
            "  Fold 50 Epoch 37/150  Train Loss=0.4849  Val Loss=0.4954  Val Acc=0.7875  Time=14.7s\n",
            "  Fold 50 Epoch 38/150  Train Loss=0.4945  Val Loss=0.5777  Val Acc=0.7230  Time=14.5s\n",
            "  Fold 50 Epoch 39/150  Train Loss=0.4756  Val Loss=0.4881  Val Acc=0.7894  Time=14.5s\n",
            "  Fold 50 Epoch 40/150  Train Loss=0.4785  Val Loss=0.5779  Val Acc=0.7078  Time=14.5s\n",
            "  Fold 50 Epoch 41/150  Train Loss=0.4666  Val Loss=0.5592  Val Acc=0.7173  Time=14.6s\n",
            "  Fold 50 Epoch 42/150  Train Loss=0.4590  Val Loss=0.4984  Val Acc=0.7837  Time=14.5s\n",
            "  Fold 50 Epoch 43/150  Train Loss=0.4587  Val Loss=0.4899  Val Acc=0.7856  Time=14.7s\n",
            "  Fold 50 Epoch 44/150  Train Loss=0.4528  Val Loss=0.6379  Val Acc=0.6395  Time=14.5s\n",
            "  Fold 50 Epoch 45/150  Train Loss=0.4518  Val Loss=0.5171  Val Acc=0.7685  Time=14.5s\n",
            "  Fold 50 Epoch 46/150  Train Loss=0.4431  Val Loss=0.4626  Val Acc=0.7989  Time=14.6s\n",
            "  Fold 50 Epoch 47/150  Train Loss=0.4323  Val Loss=0.4609  Val Acc=0.8008  Time=14.5s\n",
            "  Fold 50 Epoch 48/150  Train Loss=0.4250  Val Loss=0.4797  Val Acc=0.7894  Time=14.6s\n",
            "  Fold 50 Epoch 49/150  Train Loss=0.4218  Val Loss=0.5143  Val Acc=0.7666  Time=14.8s\n",
            "  Fold 50 Epoch 50/150  Train Loss=0.4085  Val Loss=0.5367  Val Acc=0.7571  Time=14.7s\n",
            "  Fold 50 Epoch 51/150  Train Loss=0.4055  Val Loss=0.4701  Val Acc=0.7970  Time=14.6s\n",
            "  Fold 50 Epoch 52/150  Train Loss=0.4067  Val Loss=0.4512  Val Acc=0.7894  Time=14.5s\n",
            "  Fold 50 Epoch 53/150  Train Loss=0.3933  Val Loss=0.5986  Val Acc=0.6964  Time=14.6s\n",
            "  Fold 50 Epoch 54/150  Train Loss=0.4016  Val Loss=0.5603  Val Acc=0.7306  Time=14.5s\n",
            "  Fold 50 Epoch 55/150  Train Loss=0.3951  Val Loss=0.5262  Val Acc=0.7647  Time=14.7s\n",
            "  Fold 50 Epoch 56/150  Train Loss=0.3921  Val Loss=0.5317  Val Acc=0.7609  Time=14.8s\n",
            "  Fold 50 Epoch 57/150  Train Loss=0.3926  Val Loss=0.4799  Val Acc=0.7799  Time=14.5s\n",
            "  Fold 50 Epoch 58/150  Train Loss=0.3775  Val Loss=0.5328  Val Acc=0.7514  Time=14.5s\n",
            "  Fold 50 Epoch 59/150  Train Loss=0.3800  Val Loss=0.5276  Val Acc=0.7666  Time=14.6s\n",
            "  Fold 50 Epoch 60/150  Train Loss=0.3838  Val Loss=0.4373  Val Acc=0.8008  Time=14.5s\n",
            "  Fold 50 Epoch 61/150  Train Loss=0.3607  Val Loss=0.4998  Val Acc=0.7799  Time=14.7s\n",
            "  Fold 50 Epoch 62/150  Train Loss=0.3609  Val Loss=0.4952  Val Acc=0.7818  Time=14.7s\n",
            "  Fold 50 Epoch 63/150  Train Loss=0.3628  Val Loss=0.4705  Val Acc=0.7951  Time=14.6s\n",
            "  Fold 50 Epoch 64/150  Train Loss=0.3416  Val Loss=0.4587  Val Acc=0.8008  Time=14.5s\n",
            "  Fold 50 Epoch 65/150  Train Loss=0.3406  Val Loss=0.4631  Val Acc=0.7932  Time=14.6s\n",
            "  Fold 50 Epoch 66/150  Train Loss=0.3436  Val Loss=0.5161  Val Acc=0.7609  Time=14.6s\n",
            "  Fold 50 Epoch 67/150  Train Loss=0.3405  Val Loss=0.4872  Val Acc=0.7837  Time=14.7s\n",
            "  Fold 50 Epoch 68/150  Train Loss=0.3227  Val Loss=0.4793  Val Acc=0.7875  Time=14.8s\n",
            "  Fold 50 Epoch 69/150  Train Loss=0.3220  Val Loss=0.5260  Val Acc=0.7685  Time=14.5s\n",
            "  Fold 50 Epoch 70/150  Train Loss=0.3285  Val Loss=0.4958  Val Acc=0.7875  Time=14.5s\n",
            "  Fold 50 Epoch 71/150  Train Loss=0.3206  Val Loss=0.4649  Val Acc=0.7837  Time=14.6s\n",
            "  Fold 50 Epoch 72/150  Train Loss=0.3122  Val Loss=0.4792  Val Acc=0.7875  Time=14.6s\n",
            "  Fold 50 Epoch 73/150  Train Loss=0.3291  Val Loss=0.5068  Val Acc=0.7761  Time=14.7s\n",
            "  Fold 50 Epoch 74/150  Train Loss=0.3136  Val Loss=0.4962  Val Acc=0.7780  Time=14.8s\n",
            "  Fold 50 Epoch 75/150  Train Loss=0.2989  Val Loss=0.5274  Val Acc=0.7723  Time=14.5s\n",
            "  → Early stopping at epoch 75 (no val_loss improvement)\n",
            "  Fold 50 Final ACC = 0.8571   (TP=0  TN=30  FP=5  FN=0)\n",
            "\n",
            ">>> Fold 51/52  (leave out sub-087)\n",
            "  Fold 51 Epoch 1/150  Train Loss=0.7025  Val Loss=0.6583  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 2/150  Train Loss=0.7080  Val Loss=0.6571  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 3/150  Train Loss=0.7069  Val Loss=0.6598  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 4/150  Train Loss=0.7035  Val Loss=0.6579  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 5/150  Train Loss=0.6977  Val Loss=0.6546  Val Acc=0.6794  Time=14.8s\n",
            "  Fold 51 Epoch 6/150  Train Loss=0.7006  Val Loss=0.6584  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 7/150  Train Loss=0.7030  Val Loss=0.6479  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 8/150  Train Loss=0.6929  Val Loss=0.6641  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 9/150  Train Loss=0.7009  Val Loss=0.6553  Val Acc=0.6794  Time=14.5s\n",
            "  Fold 51 Epoch 10/150  Train Loss=0.6956  Val Loss=0.6529  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 11/150  Train Loss=0.6986  Val Loss=0.6489  Val Acc=0.6794  Time=14.8s\n",
            "  Fold 51 Epoch 12/150  Train Loss=0.6981  Val Loss=0.6553  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 13/150  Train Loss=0.7004  Val Loss=0.6508  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 14/150  Train Loss=0.6951  Val Loss=0.6579  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 15/150  Train Loss=0.7001  Val Loss=0.6527  Val Acc=0.6794  Time=14.6s\n",
            "  Fold 51 Epoch 16/150  Train Loss=0.7053  Val Loss=0.6555  Val Acc=0.6794  Time=14.8s\n",
            "  Fold 51 Epoch 17/150  Train Loss=0.6907  Val Loss=0.6491  Val Acc=0.6794  Time=14.8s\n",
            "  Fold 51 Epoch 18/150  Train Loss=0.6797  Val Loss=0.6345  Val Acc=0.6794  Time=14.8s\n",
            "  Fold 51 Epoch 19/150  Train Loss=0.6405  Val Loss=0.7103  Val Acc=0.5534  Time=14.8s\n",
            "  Fold 51 Epoch 20/150  Train Loss=0.5851  Val Loss=0.6909  Val Acc=0.5916  Time=14.7s\n",
            "  Fold 51 Epoch 21/150  Train Loss=0.5701  Val Loss=0.7350  Val Acc=0.5687  Time=14.5s\n",
            "  Fold 51 Epoch 22/150  Train Loss=0.5591  Val Loss=0.6963  Val Acc=0.6260  Time=14.7s\n",
            "  Fold 51 Epoch 23/150  Train Loss=0.5386  Val Loss=0.6609  Val Acc=0.7176  Time=14.9s\n",
            "  Fold 51 Epoch 24/150  Train Loss=0.5258  Val Loss=0.6788  Val Acc=0.7080  Time=14.9s\n",
            "  Fold 51 Epoch 25/150  Train Loss=0.5190  Val Loss=0.7431  Val Acc=0.6393  Time=14.6s\n",
            "  Fold 51 Epoch 26/150  Train Loss=0.5145  Val Loss=0.7208  Val Acc=0.6355  Time=14.6s\n",
            "  Fold 51 Epoch 27/150  Train Loss=0.4964  Val Loss=0.7240  Val Acc=0.6260  Time=14.7s\n",
            "  Fold 51 Epoch 28/150  Train Loss=0.4882  Val Loss=0.6846  Val Acc=0.6851  Time=14.7s\n",
            "  Fold 51 Epoch 29/150  Train Loss=0.4728  Val Loss=0.6906  Val Acc=0.6908  Time=14.9s\n",
            "  Fold 51 Epoch 30/150  Train Loss=0.4796  Val Loss=0.7409  Val Acc=0.6317  Time=14.7s\n",
            "  Fold 51 Epoch 31/150  Train Loss=0.4568  Val Loss=0.7033  Val Acc=0.6393  Time=14.6s\n",
            "  Fold 51 Epoch 32/150  Train Loss=0.4535  Val Loss=0.7304  Val Acc=0.6374  Time=14.5s\n",
            "  Fold 51 Epoch 33/150  Train Loss=0.4562  Val Loss=0.7356  Val Acc=0.6813  Time=14.8s\n",
            "  → Early stopping at epoch 33 (no val_loss improvement)\n",
            "  Fold 51 Final ACC = 0.0000   (TP=0  TN=0  FP=34  FN=0)\n",
            "\n",
            ">>> Fold 52/52  (leave out sub-088)\n",
            "  Fold 52 Epoch 1/150  Train Loss=0.7288  Val Loss=0.7258  Val Acc=0.3258  Time=14.9s\n",
            "  Fold 52 Epoch 2/150  Train Loss=0.7268  Val Loss=0.7271  Val Acc=0.3258  Time=14.8s\n",
            "  Fold 52 Epoch 3/150  Train Loss=0.7207  Val Loss=0.6979  Val Acc=0.3958  Time=14.7s\n",
            "  Fold 52 Epoch 4/150  Train Loss=0.7118  Val Loss=0.6972  Val Acc=0.3674  Time=14.8s\n",
            "  Fold 52 Epoch 5/150  Train Loss=0.7107  Val Loss=0.6871  Val Acc=0.6723  Time=14.9s\n",
            "  Fold 52 Epoch 6/150  Train Loss=0.7073  Val Loss=0.6784  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 52 Epoch 7/150  Train Loss=0.7042  Val Loss=0.6693  Val Acc=0.6742  Time=15.0s\n",
            "  Fold 52 Epoch 8/150  Train Loss=0.7032  Val Loss=0.6726  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 9/150  Train Loss=0.7071  Val Loss=0.6703  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 52 Epoch 10/150  Train Loss=0.7076  Val Loss=0.6666  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 11/150  Train Loss=0.7010  Val Loss=0.6741  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 52 Epoch 12/150  Train Loss=0.7061  Val Loss=0.6745  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 13/150  Train Loss=0.6942  Val Loss=0.6649  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 52 Epoch 14/150  Train Loss=0.6988  Val Loss=0.6664  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 52 Epoch 15/150  Train Loss=0.6914  Val Loss=0.6596  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 16/150  Train Loss=0.7073  Val Loss=0.6740  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 17/150  Train Loss=0.6975  Val Loss=0.6788  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 52 Epoch 18/150  Train Loss=0.7077  Val Loss=0.6600  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 52 Epoch 19/150  Train Loss=0.7027  Val Loss=0.6610  Val Acc=0.6742  Time=14.9s\n",
            "  Fold 52 Epoch 20/150  Train Loss=0.6989  Val Loss=0.6772  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 21/150  Train Loss=0.6967  Val Loss=0.6623  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 52 Epoch 22/150  Train Loss=0.7073  Val Loss=0.6616  Val Acc=0.6742  Time=14.8s\n",
            "  Fold 52 Epoch 23/150  Train Loss=0.7010  Val Loss=0.6660  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 24/150  Train Loss=0.6995  Val Loss=0.6669  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 52 Epoch 25/150  Train Loss=0.6972  Val Loss=0.6646  Val Acc=0.6742  Time=14.9s\n",
            "  Fold 52 Epoch 26/150  Train Loss=0.6929  Val Loss=0.6618  Val Acc=0.6742  Time=14.7s\n",
            "  Fold 52 Epoch 27/150  Train Loss=0.6970  Val Loss=0.6658  Val Acc=0.6742  Time=14.6s\n",
            "  Fold 52 Epoch 28/150  Train Loss=0.6953  Val Loss=0.6603  Val Acc=0.6742  Time=14.5s\n",
            "  Fold 52 Epoch 29/150  Train Loss=0.6825  Val Loss=0.6983  Val Acc=0.3693  Time=14.5s\n",
            "  Fold 52 Epoch 30/150  Train Loss=0.6696  Val Loss=0.6937  Val Acc=0.6742  Time=14.6s\n",
            "  → Early stopping at epoch 30 (no val_loss improvement)\n",
            "  Fold 52 Final ACC = 0.0000   (TP=0  TN=0  FP=29  FN=0)\n",
            "\n",
            "=== LOSO 평가 결과 ===\n",
            "Accuracy   = 55.7104%\n",
            "Sensitivity= 83.9739%\n",
            "Specificity= 14.3130%\n",
            "F1-score   = 69.2638%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aR2z64qtHkVK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}